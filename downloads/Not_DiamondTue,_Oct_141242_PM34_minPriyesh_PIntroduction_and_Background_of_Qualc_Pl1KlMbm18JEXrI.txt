Meeting: Not Diamond
Tue, Oct 14
12:42 PM
34 min
Priyesh P
Introduction and Background of Qualcomm Ventures
0
URL: https://otter.ai/u/Pl1KlMbm18JEXrI7nMNIIBlJcFc
Downloaded: 2025-12-21T20:03:21.445141
Method: text_extraction
============================================================

0:19Hey, hi, Tomas, can you hear me?
Hey, hi, Tomas, can you hear me?
Hey, hi, Tomas, can you hear me?
Hey, hi, Tomas, can you hear me?
0:22Priyesh, yeah, I can hear you. Can hear me? Okay,
Priyesh, yeah, I can hear you. Can hear me? Okay,
Priyesh, yeah, I can hear you. Can hear me? Okay,
Priyesh, yeah, I can hear you. Can hear me? Okay,
S Speaker 10:25yes, yes. Tomas, great to connect again. Tomas, how's How have things been
yes, yes. Tomas, great to connect again. Tomas, how's How have things been
yes, yes. Tomas, great to connect again. Tomas, how's How have things been
yes, yes. Tomas, great to connect again. Tomas, how's How have things been
0:30good? Yeah, things have been nonstop.
good? Yeah, things have been nonstop.
good? Yeah, things have been nonstop.
good? Yeah, things have been nonstop.
S Speaker 20:35So, yeah, it's been great.
So, yeah, it's been great.
So, yeah, it's been great.
So, yeah, it's been great.
1:13That would be great. Absolutely. Thank
That would be great. Absolutely. Thank
That would be great. Absolutely. Thank
That would be great. Absolutely. Thank
S Speaker 11:15you so Thomas, this is, this is actually our 25th year since inception. We have made deployed more than $2 billion in capital so far. Typically, try to do about 150 million of investment every year in US geography, but it's a global fund, so we have offices across six different regions in the world. Check Sizes range quite a lot, because we are stage agnostic. So we have done smaller checks of, say, one 1.5 million as well, in some seed investments, but checks as large as 15 million for some pre IPO rounds as well. So quite stage agnostic, but a sweet spot somewhere would be between five to seven in a Series A to B Company. And in that, what we would prefer to see is some traction, very product like teams with a strong, technically differentiated product, some traction. And just in the phase of sort of trying to crack that GTM, there's a product market fit, and you're trying to sort of work on your sales motion, so things like that. We are a strategic investor, so our the entire LP fund is coming from Qualcomm's balance sheet. But we don't need to really have say a partnership proposition to make an investment. What we really actively try to do with some of our prospects and portfolio companies is to make introductions within Qualcomm. Try to see if Qualcomm can serve as a customer, but at many points also see if Qualcomm can serve as an ecosystem partner, because we have a AI development hub where we host a lot of infra solutions and have our own developer captive audience that build on our platform. So we expose partnerships through that many times, expose them to direct OEMs and other enterprises we work with. So we do a lot of efforts around GTM, product partnerships, introductions to the right people, things like that.
you so Thomas, this is, this is actually our 25th year since inception. We have made deployed more than $2 billion in capital so far. Typically, try to do about 150 million of investment every year in US geography, but it's a global fund, so we have offices across six different regions in the world. Check Sizes range quite a lot, because we are stage agnostic. So we have done smaller checks of, say, one 1.5 million as well, in some seed investments, but checks as large as 15 million for some pre IPO rounds as well. So quite stage agnostic, but a sweet spot somewhere would be between five to seven in a Series A to B Company. And in that, what we would prefer to see is some traction, very product like teams with a strong, technically differentiated product, some traction. And just in the phase of sort of trying to crack that GTM, there's a product market fit, and you're trying to sort of work on your sales motion, so things like that. We are a strategic investor, so our the entire LP fund is coming from Qualcomm's balance sheet. But we don't need to really have say a partnership proposition to make an investment. What we really actively try to do with some of our prospects and portfolio companies is to make introductions within Qualcomm. Try to see if Qualcomm can serve as a customer, but at many points also see if Qualcomm can serve as an ecosystem partner, because we have a AI development hub where we host a lot of infra solutions and have our own developer captive audience that build on our platform. So we expose partnerships through that many times, expose them to direct OEMs and other enterprises we work with. So we do a lot of efforts around GTM, product partnerships, introductions to the right people, things like that.
you so Thomas, this is, this is actually our 25th year since inception. We have made deployed more than $2 billion in capital so far. Typically, try to do about 150 million of investment every year in US geography, but it's a global fund, so we have offices across six different regions in the world. Check Sizes range quite a lot, because we are stage agnostic. So we have done smaller checks of, say, one 1.5 million as well, in some seed investments, but checks as large as 15 million for some pre IPO rounds as well. So quite stage agnostic, but a sweet spot somewhere would be between five to seven in a Series A to B Company. And in that, what we would prefer to see is some traction, very product like teams with a strong, technically differentiated product, some traction. And just in the phase of sort of trying to crack that GTM, there's a product market fit, and you're trying to sort of work on your sales motion, so things like that. We are a strategic investor, so our the entire LP fund is coming from Qualcomm's balance sheet. But we don't need to really have say a partnership proposition to make an investment. What we really actively try to do with some of our prospects and portfolio companies is to make introductions within Qualcomm. Try to see if Qualcomm can serve as a customer, but at many points also see if Qualcomm can serve as an ecosystem partner, because we have a AI development hub where we host a lot of infra solutions and have our own developer captive audience that build on our platform. So we expose partnerships through that many times, expose them to direct OEMs and other enterprises we work with. So we do a lot of efforts around GTM, product partnerships, introductions to the right people, things like that.
you so Thomas, this is, this is actually our 25th year since inception. We have made deployed more than $2 billion in capital so far. Typically, try to do about 150 million of investment every year in US geography, but it's a global fund, so we have offices across six different regions in the world. Check Sizes range quite a lot, because we are stage agnostic. So we have done smaller checks of, say, one 1.5 million as well, in some seed investments, but checks as large as 15 million for some pre IPO rounds as well. So quite stage agnostic, but a sweet spot somewhere would be between five to seven in a Series A to B Company. And in that, what we would prefer to see is some traction, very product like teams with a strong, technically differentiated product, some traction. And just in the phase of sort of trying to crack that GTM, there's a product market fit, and you're trying to sort of work on your sales motion, so things like that. We are a strategic investor, so our the entire LP fund is coming from Qualcomm's balance sheet. But we don't need to really have say a partnership proposition to make an investment. What we really actively try to do with some of our prospects and portfolio companies is to make introductions within Qualcomm. Try to see if Qualcomm can serve as a customer, but at many points also see if Qualcomm can serve as an ecosystem partner, because we have a AI development hub where we host a lot of infra solutions and have our own developer captive audience that build on our platform. So we expose partnerships through that many times, expose them to direct OEMs and other enterprises we work with. So we do a lot of efforts around GTM, product partnerships, introductions to the right people, things like that.
S Speaker 23:12Awesome, cool. Thanks for that. That background. Yeah, I'm curious how, what's your own journey been in terms of coming to Qualcomm or, yeah, how long? How long you've been with the fund? Yeah,
Awesome, cool. Thanks for that. That background. Yeah, I'm curious how, what's your own journey been in terms of coming to Qualcomm or, yeah, how long? How long you've been with the fund? Yeah,
Awesome, cool. Thanks for that. That background. Yeah, I'm curious how, what's your own journey been in terms of coming to Qualcomm or, yeah, how long? How long you've been with the fund? Yeah,
Awesome, cool. Thanks for that. That background. Yeah, I'm curious how, what's your own journey been in terms of coming to Qualcomm or, yeah, how long? How long you've been with the fund? Yeah,
7:03i know that there's been, I
i know that there's been, I
i know that there's been, I
i know that there's been, I
S Speaker 27:07have a little bit of familiarity with, like Qualcomm looking at doing some inference providing and, yeah, but I know there's a lot of surface areas, and I'd be curious to hear more like, what your intuitions are in terms of what we're building and where it might be useful in the Qualcomm ecosystem.
have a little bit of familiarity with, like Qualcomm looking at doing some inference providing and, yeah, but I know there's a lot of surface areas, and I'd be curious to hear more like, what your intuitions are in terms of what we're building and where it might be useful in the Qualcomm ecosystem.
have a little bit of familiarity with, like Qualcomm looking at doing some inference providing and, yeah, but I know there's a lot of surface areas, and I'd be curious to hear more like, what your intuitions are in terms of what we're building and where it might be useful in the Qualcomm ecosystem.
have a little bit of familiarity with, like Qualcomm looking at doing some inference providing and, yeah, but I know there's a lot of surface areas, and I'd be curious to hear more like, what your intuitions are in terms of what we're building and where it might be useful in the Qualcomm ecosystem.
S Speaker 111:17yeah, yeah, that's part of Thomas. And then at the same time, I would say, excited to have an investment in this area. We have, say, spoken to open router sometime back, and initial thesis was that prompt engineering routing would be a layer that some applications would build themselves. And it may not be a big market, it's good to see that being proved wrong, and especially with the level of development that's happening in AI, it's great to have an external player do that. So down the line, I don't know the timing, but when the timing is right, happy to have a discussion around that as well.
yeah, yeah, that's part of Thomas. And then at the same time, I would say, excited to have an investment in this area. We have, say, spoken to open router sometime back, and initial thesis was that prompt engineering routing would be a layer that some applications would build themselves. And it may not be a big market, it's good to see that being proved wrong, and especially with the level of development that's happening in AI, it's great to have an external player do that. So down the line, I don't know the timing, but when the timing is right, happy to have a discussion around that as well.
yeah, yeah, that's part of Thomas. And then at the same time, I would say, excited to have an investment in this area. We have, say, spoken to open router sometime back, and initial thesis was that prompt engineering routing would be a layer that some applications would build themselves. And it may not be a big market, it's good to see that being proved wrong, and especially with the level of development that's happening in AI, it's great to have an external player do that. So down the line, I don't know the timing, but when the timing is right, happy to have a discussion around that as well.
yeah, yeah, that's part of Thomas. And then at the same time, I would say, excited to have an investment in this area. We have, say, spoken to open router sometime back, and initial thesis was that prompt engineering routing would be a layer that some applications would build themselves. And it may not be a big market, it's good to see that being proved wrong, and especially with the level of development that's happening in AI, it's great to have an external player do that. So down the line, I don't know the timing, but when the timing is right, happy to have a discussion around that as well.
14:22a sort of a reference again here. So this is an
a sort of a reference again here. So this is an
a sort of a reference again here. So this is an
a sort of a reference again here. So this is an
18:23that's that's kind of at a high level
that's that's kind of at a high level
that's that's kind of at a high level
that's that's kind of at a high level
S Speaker 218:27how this works. If you have any any questions,
how this works. If you have any any questions,
how this works. If you have any any questions,
how this works. If you have any any questions,
S Speaker 118:32would would love to unpack a few things over here. So you mentioned an agentic loop which auto optimizes the application for the eval set, would love to unpack what the agentic loop is doing. Are you generating simulating or are you generating, say, synthetic test sets? How are you doing that? And I also will feel that a big part of the solution would be, say, on just having the right eval set, if the evals are off, I guess the entire solution could fall off. So how have you seen or is this really a problem in with some of the clients that you are deployed with?
would would love to unpack a few things over here. So you mentioned an agentic loop which auto optimizes the application for the eval set, would love to unpack what the agentic loop is doing. Are you generating simulating or are you generating, say, synthetic test sets? How are you doing that? And I also will feel that a big part of the solution would be, say, on just having the right eval set, if the evals are off, I guess the entire solution could fall off. So how have you seen or is this really a problem in with some of the clients that you are deployed with?
would would love to unpack a few things over here. So you mentioned an agentic loop which auto optimizes the application for the eval set, would love to unpack what the agentic loop is doing. Are you generating simulating or are you generating, say, synthetic test sets? How are you doing that? And I also will feel that a big part of the solution would be, say, on just having the right eval set, if the evals are off, I guess the entire solution could fall off. So how have you seen or is this really a problem in with some of the clients that you are deployed with?
would would love to unpack a few things over here. So you mentioned an agentic loop which auto optimizes the application for the eval set, would love to unpack what the agentic loop is doing. Are you generating simulating or are you generating, say, synthetic test sets? How are you doing that? And I also will feel that a big part of the solution would be, say, on just having the right eval set, if the evals are off, I guess the entire solution could fall off. So how have you seen or is this really a problem in with some of the clients that you are deployed with?
S Speaker 219:16Yeah, so let me speak to both of those questions. So here actually you can see a diagram of roughly, roughly how this works, where we take in an original prompt, we mutate it so we create variations of that prompt, we evaluate it on a subset of the customer evaluation data. We use an LLM to analyze that the performance of these different prompts and and provide feedback on what worked and what didn't. And that goes into creating kind of new iterations, and this loops until we achieve our adapted prompt. Now this is a core piece of the algorithm where we need access to some kind of signal to guide whether a prompt is doing well or not. And so in what we've built, our customers provide the evaluation data. And yes, if you have bad evaluation data, then you're going to not going to be able to provide a good signal to the algorithm. And we're not opinionated about how a customer provides their evaluation data, because different teams may may want to optimize for very different different things, even on similar use cases. But what we do have done is the previous algorithm we were requiring like 75 data data points in terms of the eval set, and our new algorithm performs very well with this. She has even three data data points. So we did a study looking at comparing using three points versus 50 points. Recommended, obviously, the more data you have the better, but in certain prototyping scenarios, especially if you're pre production, you may not have access to hundreds of examples of evaluation data. And so the idea here is that we can even leverage a very weak but directionally correct signal and get still very strong performance. So with with three samples versus 50 samples, our algorithm was achieving about 98 99% the performance of with 50 samples. So still very strong performance, even even in a low data regime. But yeah, that's kind of high level. Does that kind of speak to your question?
Yeah, so let me speak to both of those questions. So here actually you can see a diagram of roughly, roughly how this works, where we take in an original prompt, we mutate it so we create variations of that prompt, we evaluate it on a subset of the customer evaluation data. We use an LLM to analyze that the performance of these different prompts and and provide feedback on what worked and what didn't. And that goes into creating kind of new iterations, and this loops until we achieve our adapted prompt. Now this is a core piece of the algorithm where we need access to some kind of signal to guide whether a prompt is doing well or not. And so in what we've built, our customers provide the evaluation data. And yes, if you have bad evaluation data, then you're going to not going to be able to provide a good signal to the algorithm. And we're not opinionated about how a customer provides their evaluation data, because different teams may may want to optimize for very different different things, even on similar use cases. But what we do have done is the previous algorithm we were requiring like 75 data data points in terms of the eval set, and our new algorithm performs very well with this. She has even three data data points. So we did a study looking at comparing using three points versus 50 points. Recommended, obviously, the more data you have the better, but in certain prototyping scenarios, especially if you're pre production, you may not have access to hundreds of examples of evaluation data. And so the idea here is that we can even leverage a very weak but directionally correct signal and get still very strong performance. So with with three samples versus 50 samples, our algorithm was achieving about 98 99% the performance of with 50 samples. So still very strong performance, even even in a low data regime. But yeah, that's kind of high level. Does that kind of speak to your question?
Yeah, so let me speak to both of those questions. So here actually you can see a diagram of roughly, roughly how this works, where we take in an original prompt, we mutate it so we create variations of that prompt, we evaluate it on a subset of the customer evaluation data. We use an LLM to analyze that the performance of these different prompts and and provide feedback on what worked and what didn't. And that goes into creating kind of new iterations, and this loops until we achieve our adapted prompt. Now this is a core piece of the algorithm where we need access to some kind of signal to guide whether a prompt is doing well or not. And so in what we've built, our customers provide the evaluation data. And yes, if you have bad evaluation data, then you're going to not going to be able to provide a good signal to the algorithm. And we're not opinionated about how a customer provides their evaluation data, because different teams may may want to optimize for very different different things, even on similar use cases. But what we do have done is the previous algorithm we were requiring like 75 data data points in terms of the eval set, and our new algorithm performs very well with this. She has even three data data points. So we did a study looking at comparing using three points versus 50 points. Recommended, obviously, the more data you have the better, but in certain prototyping scenarios, especially if you're pre production, you may not have access to hundreds of examples of evaluation data. And so the idea here is that we can even leverage a very weak but directionally correct signal and get still very strong performance. So with with three samples versus 50 samples, our algorithm was achieving about 98 99% the performance of with 50 samples. So still very strong performance, even even in a low data regime. But yeah, that's kind of high level. Does that kind of speak to your question?
Yeah, so let me speak to both of those questions. So here actually you can see a diagram of roughly, roughly how this works, where we take in an original prompt, we mutate it so we create variations of that prompt, we evaluate it on a subset of the customer evaluation data. We use an LLM to analyze that the performance of these different prompts and and provide feedback on what worked and what didn't. And that goes into creating kind of new iterations, and this loops until we achieve our adapted prompt. Now this is a core piece of the algorithm where we need access to some kind of signal to guide whether a prompt is doing well or not. And so in what we've built, our customers provide the evaluation data. And yes, if you have bad evaluation data, then you're going to not going to be able to provide a good signal to the algorithm. And we're not opinionated about how a customer provides their evaluation data, because different teams may may want to optimize for very different different things, even on similar use cases. But what we do have done is the previous algorithm we were requiring like 75 data data points in terms of the eval set, and our new algorithm performs very well with this. She has even three data data points. So we did a study looking at comparing using three points versus 50 points. Recommended, obviously, the more data you have the better, but in certain prototyping scenarios, especially if you're pre production, you may not have access to hundreds of examples of evaluation data. And so the idea here is that we can even leverage a very weak but directionally correct signal and get still very strong performance. So with with three samples versus 50 samples, our algorithm was achieving about 98 99% the performance of with 50 samples. So still very strong performance, even even in a low data regime. But yeah, that's kind of high level. Does that kind of speak to your question?
S Speaker 121:48It does. It does Tomas, and in case you have a white paper for me to read. I love reading through those. So would love to sort of go through something like that. And I agree, having smaller set of eval data and still being able to perform is a key unlock, from what I know at Qualcomm, for some of our solutions, it was really difficult to have a few engineers dedicated to it to create eval data sets, especially during the experimentation stage, where we did not really know where the entire solution will point to eventually. So that's a big unlock, especially for enterprises. Tomas would love to understand a little bit more around the product positioning as well. So how recurring is this problem? Is this? Say, adaptation does does it only come for some of your clients when a new model is released? Is there an active say routing that you do from the platform perspective, something like an open router based on just not accuracy, but a few more parameters, latency, costs, things like that.
It does. It does Tomas, and in case you have a white paper for me to read. I love reading through those. So would love to sort of go through something like that. And I agree, having smaller set of eval data and still being able to perform is a key unlock, from what I know at Qualcomm, for some of our solutions, it was really difficult to have a few engineers dedicated to it to create eval data sets, especially during the experimentation stage, where we did not really know where the entire solution will point to eventually. So that's a big unlock, especially for enterprises. Tomas would love to understand a little bit more around the product positioning as well. So how recurring is this problem? Is this? Say, adaptation does does it only come for some of your clients when a new model is released? Is there an active say routing that you do from the platform perspective, something like an open router based on just not accuracy, but a few more parameters, latency, costs, things like that.
It does. It does Tomas, and in case you have a white paper for me to read. I love reading through those. So would love to sort of go through something like that. And I agree, having smaller set of eval data and still being able to perform is a key unlock, from what I know at Qualcomm, for some of our solutions, it was really difficult to have a few engineers dedicated to it to create eval data sets, especially during the experimentation stage, where we did not really know where the entire solution will point to eventually. So that's a big unlock, especially for enterprises. Tomas would love to understand a little bit more around the product positioning as well. So how recurring is this problem? Is this? Say, adaptation does does it only come for some of your clients when a new model is released? Is there an active say routing that you do from the platform perspective, something like an open router based on just not accuracy, but a few more parameters, latency, costs, things like that.
It does. It does Tomas, and in case you have a white paper for me to read. I love reading through those. So would love to sort of go through something like that. And I agree, having smaller set of eval data and still being able to perform is a key unlock, from what I know at Qualcomm, for some of our solutions, it was really difficult to have a few engineers dedicated to it to create eval data sets, especially during the experimentation stage, where we did not really know where the entire solution will point to eventually. So that's a big unlock, especially for enterprises. Tomas would love to understand a little bit more around the product positioning as well. So how recurring is this problem? Is this? Say, adaptation does does it only come for some of your clients when a new model is released? Is there an active say routing that you do from the platform perspective, something like an open router based on just not accuracy, but a few more parameters, latency, costs, things like that.
26:56This is an example of a
This is an example of a
This is an example of a
This is an example of a
S Speaker 227:00router that's been trained for a rag use case where you have kind of various models here, the best average model is GPT four Oh, scoring 90% and then our router is scoring 93.5% so basically closing the gap to, you know, 100% by by about a third, and at the same time we're we're achieving a lower cost and lengthy than the kind of most expensive models here. And I'll show you, this is basically where, where we train this, where we have our our eval data split into train and test on 8020 and then we're passing in the inputs, the responses from models and the scores to those responses. And this can be any model, any score, again, similar to prompt optimization. And then we get back our custom router, which has been kind of just like our promises. Now optimize this use case and and then we can evaluate it on the test split. So this is like an input. This is the response from each model, its score. And then this is the prediction from the router of which model to use. And you can see that in this case, the prediction was correct. It scored 100% in the second case, it's predicting to use GP for from August, and for some reason, on this input, sauna is scoring 50% and to the human eye, it's very hard to understand why we struggle with this question about flooring versus schools, but our router has correctly learned to predict that you should use GPT four. Oh, here. And so at the bottom, you can see what we saw in the dashboard, where the best average model is GPT four, oh, square 90% on time, square 93.5% and then this is the breakdown of the distribution, where a quarter of these are actually going to Haiku, which is why we're both outperforming the best model, and doing so at a lower cost. And then we can drive even further cost savings using a Priyesh optimization. So maybe we want equal to GP for quality at half the cost, you know, or equal to Sonic quality at a quarter of the cost. And we can kind of flexibly adjust this as desired. So that's that's kind of for routing we charge kind of on a per routing request basis. And for prompt optimization, we charge when we successfully adapted prompted that is we drive some improvement. You can see in this scenario on sonnet there, there was no no improvement, but, but, yeah,
router that's been trained for a rag use case where you have kind of various models here, the best average model is GPT four Oh, scoring 90% and then our router is scoring 93.5% so basically closing the gap to, you know, 100% by by about a third, and at the same time we're we're achieving a lower cost and lengthy than the kind of most expensive models here. And I'll show you, this is basically where, where we train this, where we have our our eval data split into train and test on 8020 and then we're passing in the inputs, the responses from models and the scores to those responses. And this can be any model, any score, again, similar to prompt optimization. And then we get back our custom router, which has been kind of just like our promises. Now optimize this use case and and then we can evaluate it on the test split. So this is like an input. This is the response from each model, its score. And then this is the prediction from the router of which model to use. And you can see that in this case, the prediction was correct. It scored 100% in the second case, it's predicting to use GP for from August, and for some reason, on this input, sauna is scoring 50% and to the human eye, it's very hard to understand why we struggle with this question about flooring versus schools, but our router has correctly learned to predict that you should use GPT four. Oh, here. And so at the bottom, you can see what we saw in the dashboard, where the best average model is GPT four, oh, square 90% on time, square 93.5% and then this is the breakdown of the distribution, where a quarter of these are actually going to Haiku, which is why we're both outperforming the best model, and doing so at a lower cost. And then we can drive even further cost savings using a Priyesh optimization. So maybe we want equal to GP for quality at half the cost, you know, or equal to Sonic quality at a quarter of the cost. And we can kind of flexibly adjust this as desired. So that's that's kind of for routing we charge kind of on a per routing request basis. And for prompt optimization, we charge when we successfully adapted prompted that is we drive some improvement. You can see in this scenario on sonnet there, there was no no improvement, but, but, yeah,
router that's been trained for a rag use case where you have kind of various models here, the best average model is GPT four Oh, scoring 90% and then our router is scoring 93.5% so basically closing the gap to, you know, 100% by by about a third, and at the same time we're we're achieving a lower cost and lengthy than the kind of most expensive models here. And I'll show you, this is basically where, where we train this, where we have our our eval data split into train and test on 8020 and then we're passing in the inputs, the responses from models and the scores to those responses. And this can be any model, any score, again, similar to prompt optimization. And then we get back our custom router, which has been kind of just like our promises. Now optimize this use case and and then we can evaluate it on the test split. So this is like an input. This is the response from each model, its score. And then this is the prediction from the router of which model to use. And you can see that in this case, the prediction was correct. It scored 100% in the second case, it's predicting to use GP for from August, and for some reason, on this input, sauna is scoring 50% and to the human eye, it's very hard to understand why we struggle with this question about flooring versus schools, but our router has correctly learned to predict that you should use GPT four. Oh, here. And so at the bottom, you can see what we saw in the dashboard, where the best average model is GPT four, oh, square 90% on time, square 93.5% and then this is the breakdown of the distribution, where a quarter of these are actually going to Haiku, which is why we're both outperforming the best model, and doing so at a lower cost. And then we can drive even further cost savings using a Priyesh optimization. So maybe we want equal to GP for quality at half the cost, you know, or equal to Sonic quality at a quarter of the cost. And we can kind of flexibly adjust this as desired. So that's that's kind of for routing we charge kind of on a per routing request basis. And for prompt optimization, we charge when we successfully adapted prompted that is we drive some improvement. You can see in this scenario on sonnet there, there was no no improvement, but, but, yeah,
router that's been trained for a rag use case where you have kind of various models here, the best average model is GPT four Oh, scoring 90% and then our router is scoring 93.5% so basically closing the gap to, you know, 100% by by about a third, and at the same time we're we're achieving a lower cost and lengthy than the kind of most expensive models here. And I'll show you, this is basically where, where we train this, where we have our our eval data split into train and test on 8020 and then we're passing in the inputs, the responses from models and the scores to those responses. And this can be any model, any score, again, similar to prompt optimization. And then we get back our custom router, which has been kind of just like our promises. Now optimize this use case and and then we can evaluate it on the test split. So this is like an input. This is the response from each model, its score. And then this is the prediction from the router of which model to use. And you can see that in this case, the prediction was correct. It scored 100% in the second case, it's predicting to use GP for from August, and for some reason, on this input, sauna is scoring 50% and to the human eye, it's very hard to understand why we struggle with this question about flooring versus schools, but our router has correctly learned to predict that you should use GPT four. Oh, here. And so at the bottom, you can see what we saw in the dashboard, where the best average model is GPT four, oh, square 90% on time, square 93.5% and then this is the breakdown of the distribution, where a quarter of these are actually going to Haiku, which is why we're both outperforming the best model, and doing so at a lower cost. And then we can drive even further cost savings using a Priyesh optimization. So maybe we want equal to GP for quality at half the cost, you know, or equal to Sonic quality at a quarter of the cost. And we can kind of flexibly adjust this as desired. So that's that's kind of for routing we charge kind of on a per routing request basis. And for prompt optimization, we charge when we successfully adapted prompted that is we drive some improvement. You can see in this scenario on sonnet there, there was no no improvement, but, but, yeah,
S Speaker 129:47that's incredible. Tomas. I mean, I could not have imagined just from adaptation to be such a big level in this entire value chain. One one follow up question. Tomas, so some of the use cases you mentioned were just for the lack of a better word, more simpler use cases, question answering, classification, drag, things like that. How does prompt adaptation look like for a multi step agent, tech, complex workflow, things like that. And do you have live customers around those use cases?
that's incredible. Tomas. I mean, I could not have imagined just from adaptation to be such a big level in this entire value chain. One one follow up question. Tomas, so some of the use cases you mentioned were just for the lack of a better word, more simpler use cases, question answering, classification, drag, things like that. How does prompt adaptation look like for a multi step agent, tech, complex workflow, things like that. And do you have live customers around those use cases?
that's incredible. Tomas. I mean, I could not have imagined just from adaptation to be such a big level in this entire value chain. One one follow up question. Tomas, so some of the use cases you mentioned were just for the lack of a better word, more simpler use cases, question answering, classification, drag, things like that. How does prompt adaptation look like for a multi step agent, tech, complex workflow, things like that. And do you have live customers around those use cases?
that's incredible. Tomas. I mean, I could not have imagined just from adaptation to be such a big level in this entire value chain. One one follow up question. Tomas, so some of the use cases you mentioned were just for the lack of a better word, more simpler use cases, question answering, classification, drag, things like that. How does prompt adaptation look like for a multi step agent, tech, complex workflow, things like that. And do you have live customers around those use cases?
S Speaker 230:22Yeah, so I'd say almost everyone who has deployed us is using us within multi step agentic use cases. Now what we don't currently support is optimization of a an autonomous, a fully autonomous agent with tools where it iterates over kind of end steps in its own defined trajectory. We're working on that now we're actually kicking off a POC around that in the next couple weeks with like fortune 50 company. But this is still something we're actively developing, and we'll be putting out kind of later this year. Got
Yeah, so I'd say almost everyone who has deployed us is using us within multi step agentic use cases. Now what we don't currently support is optimization of a an autonomous, a fully autonomous agent with tools where it iterates over kind of end steps in its own defined trajectory. We're working on that now we're actually kicking off a POC around that in the next couple weeks with like fortune 50 company. But this is still something we're actively developing, and we'll be putting out kind of later this year. Got
Yeah, so I'd say almost everyone who has deployed us is using us within multi step agentic use cases. Now what we don't currently support is optimization of a an autonomous, a fully autonomous agent with tools where it iterates over kind of end steps in its own defined trajectory. We're working on that now we're actually kicking off a POC around that in the next couple weeks with like fortune 50 company. But this is still something we're actively developing, and we'll be putting out kind of later this year. Got
Yeah, so I'd say almost everyone who has deployed us is using us within multi step agentic use cases. Now what we don't currently support is optimization of a an autonomous, a fully autonomous agent with tools where it iterates over kind of end steps in its own defined trajectory. We're working on that now we're actually kicking off a POC around that in the next couple weeks with like fortune 50 company. But this is still something we're actively developing, and we'll be putting out kind of later this year. Got
S Speaker 131:18Super interesting Tomas. I'm sure they it's interesting, for sure. And when have you fundraised? Do you see anything coming up in near horizon?
Super interesting Tomas. I'm sure they it's interesting, for sure. And when have you fundraised? Do you see anything coming up in near horizon?
Super interesting Tomas. I'm sure they it's interesting, for sure. And when have you fundraised? Do you see anything coming up in near horizon?
Super interesting Tomas. I'm sure they it's interesting, for sure. And when have you fundraised? Do you see anything coming up in near horizon?
S Speaker 231:31So yeah, we raised a pre seed at the beginning of last year, and that was a 2.3 million pre seed with led by defy, defy VC, and participation from some great angels, Jeff Dean, Google, Julian schmuck from hugging face, young Stoker from Databricks and any scale and many others. And then we actually, we haven't announced it, but we're pretty much wrapping up a $9 million seed round with participation from defy, from SAP, IBM, a number of other kind of enterprise focused investors, along with another great group of angels So Akshay Kothari from notion, Guillermo rush from Purcell, a rush for dosi from from Dropbox, Olivier pommel from Datadog and many others. So I think we, you know, we, technically, we have a little bit of space left in that, but it's like, you know, if, if you want to explore that, it'd be like on the order of, like, one or $200,000 check is, you know, which I think is probably too small. And what I would would imagine might make more sense would be like, I'd love to kind of both see if there's an opportunity to work with folks inside the Qualcomm ecosystem, and then also just keep getting to know each other ahead of our next round, which may be a good fit for the kind of like kind of scope of where you guys are typically focusing.
So yeah, we raised a pre seed at the beginning of last year, and that was a 2.3 million pre seed with led by defy, defy VC, and participation from some great angels, Jeff Dean, Google, Julian schmuck from hugging face, young Stoker from Databricks and any scale and many others. And then we actually, we haven't announced it, but we're pretty much wrapping up a $9 million seed round with participation from defy, from SAP, IBM, a number of other kind of enterprise focused investors, along with another great group of angels So Akshay Kothari from notion, Guillermo rush from Purcell, a rush for dosi from from Dropbox, Olivier pommel from Datadog and many others. So I think we, you know, we, technically, we have a little bit of space left in that, but it's like, you know, if, if you want to explore that, it'd be like on the order of, like, one or $200,000 check is, you know, which I think is probably too small. And what I would would imagine might make more sense would be like, I'd love to kind of both see if there's an opportunity to work with folks inside the Qualcomm ecosystem, and then also just keep getting to know each other ahead of our next round, which may be a good fit for the kind of like kind of scope of where you guys are typically focusing.
So yeah, we raised a pre seed at the beginning of last year, and that was a 2.3 million pre seed with led by defy, defy VC, and participation from some great angels, Jeff Dean, Google, Julian schmuck from hugging face, young Stoker from Databricks and any scale and many others. And then we actually, we haven't announced it, but we're pretty much wrapping up a $9 million seed round with participation from defy, from SAP, IBM, a number of other kind of enterprise focused investors, along with another great group of angels So Akshay Kothari from notion, Guillermo rush from Purcell, a rush for dosi from from Dropbox, Olivier pommel from Datadog and many others. So I think we, you know, we, technically, we have a little bit of space left in that, but it's like, you know, if, if you want to explore that, it'd be like on the order of, like, one or $200,000 check is, you know, which I think is probably too small. And what I would would imagine might make more sense would be like, I'd love to kind of both see if there's an opportunity to work with folks inside the Qualcomm ecosystem, and then also just keep getting to know each other ahead of our next round, which may be a good fit for the kind of like kind of scope of where you guys are typically focusing.
So yeah, we raised a pre seed at the beginning of last year, and that was a 2.3 million pre seed with led by defy, defy VC, and participation from some great angels, Jeff Dean, Google, Julian schmuck from hugging face, young Stoker from Databricks and any scale and many others. And then we actually, we haven't announced it, but we're pretty much wrapping up a $9 million seed round with participation from defy, from SAP, IBM, a number of other kind of enterprise focused investors, along with another great group of angels So Akshay Kothari from notion, Guillermo rush from Purcell, a rush for dosi from from Dropbox, Olivier pommel from Datadog and many others. So I think we, you know, we, technically, we have a little bit of space left in that, but it's like, you know, if, if you want to explore that, it'd be like on the order of, like, one or $200,000 check is, you know, which I think is probably too small. And what I would would imagine might make more sense would be like, I'd love to kind of both see if there's an opportunity to work with folks inside the Qualcomm ecosystem, and then also just keep getting to know each other ahead of our next round, which may be a good fit for the kind of like kind of scope of where you guys are typically focusing.
S Speaker 234:11That sounds great. Okay, fantastic. Well, thanks so much, Priyesh, yeah. Follow up with all of that. I'll introduce that over to you today, if not otherwise, tomorrow. But yeah, glad we could follow
That sounds great. Okay, fantastic. Well, thanks so much, Priyesh, yeah. Follow up with all of that. I'll introduce that over to you today, if not otherwise, tomorrow. But yeah, glad we could follow
That sounds great. Okay, fantastic. Well, thanks so much, Priyesh, yeah. Follow up with all of that. I'll introduce that over to you today, if not otherwise, tomorrow. But yeah, glad we could follow
That sounds great. Okay, fantastic. Well, thanks so much, Priyesh, yeah. Follow up with all of that. I'll introduce that over to you today, if not otherwise, tomorrow. But yeah, glad we could follow
S Speaker 134:24up and connect Absolutely. Thomas and great product. I really enjoyed the conversation. It did sort of challenge a few notions that I had about this area coming in and love what you guys are building. Would love to sort of follow this very closely, see all the updates as they come along.
up and connect Absolutely. Thomas and great product. I really enjoyed the conversation. It did sort of challenge a few notions that I had about this area coming in and love what you guys are building. Would love to sort of follow this very closely, see all the updates as they come along.
up and connect Absolutely. Thomas and great product. I really enjoyed the conversation. It did sort of challenge a few notions that I had about this area coming in and love what you guys are building. Would love to sort of follow this very closely, see all the updates as they come along.
up and connect Absolutely. Thomas and great product. I really enjoyed the conversation. It did sort of challenge a few notions that I had about this area coming in and love what you guys are building. Would love to sort of follow this very closely, see all the updates as they come along.
S Speaker 234:42Yeah, appreciate it. Thanks so much. Thanks so much. See you around. Okay, see you. Bye. You.
Yeah, appreciate it. Thanks so much. Thanks so much. See you around. Okay, see you. Bye. You.
Yeah, appreciate it. Thanks so much. Thanks so much. See you around. Okay, see you. Bye. You.
Yeah, appreciate it. Thanks so much. Thanks so much. See you around. Okay, see you. Bye. You.