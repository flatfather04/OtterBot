Meeting: kindo product Demo
Mon, Aug 25
10:31 AM
38 min
Priyesh P
Call Recording and Initial Setup
0:00
Intro
URL: https://otter.ai/u/ozdXQswiJngbx520csQtpE2mu0M
Downloaded: 2025-12-21T20:55:13.836527
Method: text_extraction
============================================================

S Speaker 10:00You I think I think I think Hey, Priyesh, yes, yeah, they've accepted the call, right?
You I think I think I think Hey, Priyesh, yes, yeah, they've accepted the call, right?
You I think I think I think Hey, Priyesh, yes, yeah, they've accepted the call, right?
You I think I think I think Hey, Priyesh, yes, yeah, they've accepted the call, right?
1:47They have not no but by the way, we attached
They have not no but by the way, we attached
They have not no but by the way, we attached
They have not no but by the way, we attached
S Speaker 21:54the last call. Yes, yes, I went through that Deepak. It had only 30 minutes, I guess you, you had a 90 minute call or something. So it was only 30 minutes, but,
the last call. Yes, yes, I went through that Deepak. It had only 30 minutes, I guess you, you had a 90 minute call or something. So it was only 30 minutes, but,
the last call. Yes, yes, I went through that Deepak. It had only 30 minutes, I guess you, you had a 90 minute call or something. So it was only 30 minutes, but,
the last call. Yes, yes, I went through that Deepak. It had only 30 minutes, I guess you, you had a 90 minute call or something. So it was only 30 minutes, but,
2:05yeah, probably Gemini auto cut off
yeah, probably Gemini auto cut off
yeah, probably Gemini auto cut off
yeah, probably Gemini auto cut off
S Speaker 12:09because it's only this the original time was only 30 minutes. Yeah. All right,
because it's only this the original time was only 30 minutes. Yeah. All right,
because it's only this the original time was only 30 minutes. Yeah. All right,
because it's only this the original time was only 30 minutes. Yeah. All right,
2:18no worries. All right,
no worries. All right,
no worries. All right,
no worries. All right,
S Speaker 22:20I'll wait a couple of minutes and then drop them a follow on, follow up. Me email.
I'll wait a couple of minutes and then drop them a follow on, follow up. Me email.
I'll wait a couple of minutes and then drop them a follow on, follow up. Me email.
I'll wait a couple of minutes and then drop them a follow on, follow up. Me email.
S Speaker 12:26Yeah. I mean, let's, let's wait till 35 Yeah, since you're here on the call, you know the agenda is live today, right? So they don't have, we don't have a lot of prospects, advanced prospects, right? And we don't have a lot of portfolio decisions too. Yeah, so let me see if we can squeeze in with the caveat that there's some work to do on witness. Just wanted to give a preview of the opportunity before we keep that. Yeah, okay, let me, let me see. I'll bring to sharks. Hey Ron,
Yeah. I mean, let's, let's wait till 35 Yeah, since you're here on the call, you know the agenda is live today, right? So they don't have, we don't have a lot of prospects, advanced prospects, right? And we don't have a lot of portfolio decisions too. Yeah, so let me see if we can squeeze in with the caveat that there's some work to do on witness. Just wanted to give a preview of the opportunity before we keep that. Yeah, okay, let me, let me see. I'll bring to sharks. Hey Ron,
Yeah. I mean, let's, let's wait till 35 Yeah, since you're here on the call, you know the agenda is live today, right? So they don't have, we don't have a lot of prospects, advanced prospects, right? And we don't have a lot of portfolio decisions too. Yeah, so let me see if we can squeeze in with the caveat that there's some work to do on witness. Just wanted to give a preview of the opportunity before we keep that. Yeah, okay, let me, let me see. I'll bring to sharks. Hey Ron,
Yeah. I mean, let's, let's wait till 35 Yeah, since you're here on the call, you know the agenda is live today, right? So they don't have, we don't have a lot of prospects, advanced prospects, right? And we don't have a lot of portfolio decisions too. Yeah, so let me see if we can squeeze in with the caveat that there's some work to do on witness. Just wanted to give a preview of the opportunity before we keep that. Yeah, okay, let me, let me see. I'll bring to sharks. Hey Ron,
3:01Hey guys, how y'all doing?
Hey guys, how y'all doing?
Hey guys, how y'all doing?
Hey guys, how y'all doing?
S Speaker 23:03Getting great. Sorry I could not join the last call,
Getting great. Sorry I could not join the last call,
Getting great. Sorry I could not join the last call,
Getting great. Sorry I could not join the last call,
3:11right? So you should be able to watch the recording.
right? So you should be able to watch the recording.
right? So you should be able to watch the recording.
right? So you should be able to watch the recording.
S Speaker 23:15Yeah, I think the recording just auto cut off at 30 minutes. You I heard from Deepak. You guys had a very good conversation. Went on for a long time. Yeah, that
Yeah, I think the recording just auto cut off at 30 minutes. You I heard from Deepak. You guys had a very good conversation. Went on for a long time. Yeah, that
Yeah, I think the recording just auto cut off at 30 minutes. You I heard from Deepak. You guys had a very good conversation. Went on for a long time. Yeah, that
Yeah, I think the recording just auto cut off at 30 minutes. You I heard from Deepak. You guys had a very good conversation. Went on for a long time. Yeah, that
S Speaker 13:24was funny. Ron, I thought the call recording would go on. I think it cuts off at the designated tightness. Oh, wow,
was funny. Ron, I thought the call recording would go on. I think it cuts off at the designated tightness. Oh, wow,
was funny. Ron, I thought the call recording would go on. I think it cuts off at the designated tightness. Oh, wow,
was funny. Ron, I thought the call recording would go on. I think it cuts off at the designated tightness. Oh, wow,
3:32yeah, that's, that's a that's weird, you know, Google product,
yeah, that's, that's a that's weird, you know, Google product,
yeah, that's, that's a that's weird, you know, Google product,
yeah, that's, that's a that's weird, you know, Google product,
S Speaker 33:38it's so weird. Like, the other crazy thing to do is, I was trying to video a demo for the team, so I just jumped on a call and started recording it. And if there's not another person on the call, Google just ends it, like in five minutes or 10, you know. And they have this excellent tool that I could just use to basically compete with loom, but they wouldn't like to use it that way, you know, that's right there. It's just, it's just crazy. But anyways,
it's so weird. Like, the other crazy thing to do is, I was trying to video a demo for the team, so I just jumped on a call and started recording it. And if there's not another person on the call, Google just ends it, like in five minutes or 10, you know. And they have this excellent tool that I could just use to basically compete with loom, but they wouldn't like to use it that way, you know, that's right there. It's just, it's just crazy. But anyways,
it's so weird. Like, the other crazy thing to do is, I was trying to video a demo for the team, so I just jumped on a call and started recording it. And if there's not another person on the call, Google just ends it, like in five minutes or 10, you know. And they have this excellent tool that I could just use to basically compete with loom, but they wouldn't like to use it that way, you know, that's right there. It's just, it's just crazy. But anyways,
it's so weird. Like, the other crazy thing to do is, I was trying to video a demo for the team, so I just jumped on a call and started recording it. And if there's not another person on the call, Google just ends it, like in five minutes or 10, you know. And they have this excellent tool that I could just use to basically compete with loom, but they wouldn't like to use it that way, you know, that's right there. It's just, it's just crazy. But anyways,
S Speaker 34:06try to cover if we too, we pull Brian or Charlie or founding, injured whoever on the
try to cover if we too, we pull Brian or Charlie or founding, injured whoever on the
try to cover if we too, we pull Brian or Charlie or founding, injured whoever on the
try to cover if we too, we pull Brian or Charlie or founding, injured whoever on the
S Speaker 35:13Okay, sounds good. Give me one second. I'll do
Okay, sounds good. Give me one second. I'll do
Okay, sounds good. Give me one second. I'll do
Okay, sounds good. Give me one second. I'll do
S Speaker 15:23we shall forward the architecture slides to you as we speak. It just came in just like five minutes ago.
we shall forward the architecture slides to you as we speak. It just came in just like five minutes ago.
we shall forward the architecture slides to you as we speak. It just came in just like five minutes ago.
we shall forward the architecture slides to you as we speak. It just came in just like five minutes ago.
5:29Yeah, thanks, people.
Yeah, thanks, people.
Yeah, thanks, people.
Yeah, thanks, people.
5:55and all the M A discussion,
and all the M A discussion,
and all the M A discussion,
and all the M A discussion,
5:57Since as you loop somebody in,
Since as you loop somebody in,
Since as you loop somebody in,
Since as you loop somebody in,
S Speaker 36:01yeah, that is it's progressing quite a bit. We've got a few people around Black Hat
yeah, that is it's progressing quite a bit. We've got a few people around Black Hat
yeah, that is it's progressing quite a bit. We've got a few people around Black Hat
yeah, that is it's progressing quite a bit. We've got a few people around Black Hat
6:09switch my screen. Can you guys see my screen? Okay, yes.
switch my screen. Can you guys see my screen? Okay, yes.
switch my screen. Can you guys see my screen? Okay, yes.
switch my screen. Can you guys see my screen? Okay, yes.
S Speaker 36:20you know, really got us on the radar. You know, we had a pretty good sized booth there, and so we've had a few other of the bigger kind of players reach out, but the main interest is pretty far ahead of everybody, because we were in there. We initially met them as a potential customer back in June, early June, and then we had a whole bunch of conversations with their head amendment. A was on every call the whole time, sitting in the background, you know, kind of wondering what that guy's thinking. And then, and then, at the end of July, we ended call with wanting to move into a partnership, and we were starting a POC with them this month, right after Black Hat, which we're in the middle of right now. And but then at Black Hat, a bunch of their people came by the booth and had a conversation with several and a lot of them got demos, a lot of the kind of senior leadership in different parts of their org. And then then the Friday, that Friday that week, is when they reached out and said, Hey, we actually would like to move into m a conversation with you and so, so they have a good head start on on everybody else's since reached out on Tuesday after Black Hat, one of the big kind of hardware security incumbents reached out. And then, then another one reached out the next day, and then, and then one of the big, older, giant companies reached out last week, and so, and then the board is like, you know, we obviously want to build a big company, and we're still pushing at that board meeting Thursday, essentially today. And in the end of the board meeting, the board's like, you know, hey, this is it's all great interest, run it down. Mostly, I think one of the things they're interesting is it's just kind of standing how that market values the company, you know, both potentially in a, you know, will we get some kind of term or something, some set of terms that that kind of price the company, but, but also just kind of how they're thinking about use cases, how they're thinking about where it fits in against their strategy, like it's a really good learning opportunity for us, though, it is a big distraction to kind of run down and talk through it, and we'll probably be in that process at least another two to four weeks. I think it kind of just depends if some of the the new guys just drop out and, you know, after quick check, or, you know, we'll see. And then, then, then I get, I can get back in 100% focus on driving this thing through the B round. But the same time, we are very busy getting to be round. Revenue continues to accelerate. I just got off of our weekly status call. Leadership Team. We now have nine POCs. We're juggling that we're moving into. We've never had more than one in a month, and which was pretty busy for us. But nine is getting kind of crazy, and so it's and there's just more pipeline kind of building. You know, Matthew coming in and hiring two excellent, very experienced security sales people. It's just not just pouring fuel on all that. And I was just telling the team, we're gonna have a crazy end of q3 and a very busy q4 and they need to be ready for it.
you know, really got us on the radar. You know, we had a pretty good sized booth there, and so we've had a few other of the bigger kind of players reach out, but the main interest is pretty far ahead of everybody, because we were in there. We initially met them as a potential customer back in June, early June, and then we had a whole bunch of conversations with their head amendment. A was on every call the whole time, sitting in the background, you know, kind of wondering what that guy's thinking. And then, and then, at the end of July, we ended call with wanting to move into a partnership, and we were starting a POC with them this month, right after Black Hat, which we're in the middle of right now. And but then at Black Hat, a bunch of their people came by the booth and had a conversation with several and a lot of them got demos, a lot of the kind of senior leadership in different parts of their org. And then then the Friday, that Friday that week, is when they reached out and said, Hey, we actually would like to move into m a conversation with you and so, so they have a good head start on on everybody else's since reached out on Tuesday after Black Hat, one of the big kind of hardware security incumbents reached out. And then, then another one reached out the next day, and then, and then one of the big, older, giant companies reached out last week, and so, and then the board is like, you know, we obviously want to build a big company, and we're still pushing at that board meeting Thursday, essentially today. And in the end of the board meeting, the board's like, you know, hey, this is it's all great interest, run it down. Mostly, I think one of the things they're interesting is it's just kind of standing how that market values the company, you know, both potentially in a, you know, will we get some kind of term or something, some set of terms that that kind of price the company, but, but also just kind of how they're thinking about use cases, how they're thinking about where it fits in against their strategy, like it's a really good learning opportunity for us, though, it is a big distraction to kind of run down and talk through it, and we'll probably be in that process at least another two to four weeks. I think it kind of just depends if some of the the new guys just drop out and, you know, after quick check, or, you know, we'll see. And then, then, then I get, I can get back in 100% focus on driving this thing through the B round. But the same time, we are very busy getting to be round. Revenue continues to accelerate. I just got off of our weekly status call. Leadership Team. We now have nine POCs. We're juggling that we're moving into. We've never had more than one in a month, and which was pretty busy for us. But nine is getting kind of crazy, and so it's and there's just more pipeline kind of building. You know, Matthew coming in and hiring two excellent, very experienced security sales people. It's just not just pouring fuel on all that. And I was just telling the team, we're gonna have a crazy end of q3 and a very busy q4 and they need to be ready for it.
you know, really got us on the radar. You know, we had a pretty good sized booth there, and so we've had a few other of the bigger kind of players reach out, but the main interest is pretty far ahead of everybody, because we were in there. We initially met them as a potential customer back in June, early June, and then we had a whole bunch of conversations with their head amendment. A was on every call the whole time, sitting in the background, you know, kind of wondering what that guy's thinking. And then, and then, at the end of July, we ended call with wanting to move into a partnership, and we were starting a POC with them this month, right after Black Hat, which we're in the middle of right now. And but then at Black Hat, a bunch of their people came by the booth and had a conversation with several and a lot of them got demos, a lot of the kind of senior leadership in different parts of their org. And then then the Friday, that Friday that week, is when they reached out and said, Hey, we actually would like to move into m a conversation with you and so, so they have a good head start on on everybody else's since reached out on Tuesday after Black Hat, one of the big kind of hardware security incumbents reached out. And then, then another one reached out the next day, and then, and then one of the big, older, giant companies reached out last week, and so, and then the board is like, you know, we obviously want to build a big company, and we're still pushing at that board meeting Thursday, essentially today. And in the end of the board meeting, the board's like, you know, hey, this is it's all great interest, run it down. Mostly, I think one of the things they're interesting is it's just kind of standing how that market values the company, you know, both potentially in a, you know, will we get some kind of term or something, some set of terms that that kind of price the company, but, but also just kind of how they're thinking about use cases, how they're thinking about where it fits in against their strategy, like it's a really good learning opportunity for us, though, it is a big distraction to kind of run down and talk through it, and we'll probably be in that process at least another two to four weeks. I think it kind of just depends if some of the the new guys just drop out and, you know, after quick check, or, you know, we'll see. And then, then, then I get, I can get back in 100% focus on driving this thing through the B round. But the same time, we are very busy getting to be round. Revenue continues to accelerate. I just got off of our weekly status call. Leadership Team. We now have nine POCs. We're juggling that we're moving into. We've never had more than one in a month, and which was pretty busy for us. But nine is getting kind of crazy, and so it's and there's just more pipeline kind of building. You know, Matthew coming in and hiring two excellent, very experienced security sales people. It's just not just pouring fuel on all that. And I was just telling the team, we're gonna have a crazy end of q3 and a very busy q4 and they need to be ready for it.
you know, really got us on the radar. You know, we had a pretty good sized booth there, and so we've had a few other of the bigger kind of players reach out, but the main interest is pretty far ahead of everybody, because we were in there. We initially met them as a potential customer back in June, early June, and then we had a whole bunch of conversations with their head amendment. A was on every call the whole time, sitting in the background, you know, kind of wondering what that guy's thinking. And then, and then, at the end of July, we ended call with wanting to move into a partnership, and we were starting a POC with them this month, right after Black Hat, which we're in the middle of right now. And but then at Black Hat, a bunch of their people came by the booth and had a conversation with several and a lot of them got demos, a lot of the kind of senior leadership in different parts of their org. And then then the Friday, that Friday that week, is when they reached out and said, Hey, we actually would like to move into m a conversation with you and so, so they have a good head start on on everybody else's since reached out on Tuesday after Black Hat, one of the big kind of hardware security incumbents reached out. And then, then another one reached out the next day, and then, and then one of the big, older, giant companies reached out last week, and so, and then the board is like, you know, we obviously want to build a big company, and we're still pushing at that board meeting Thursday, essentially today. And in the end of the board meeting, the board's like, you know, hey, this is it's all great interest, run it down. Mostly, I think one of the things they're interesting is it's just kind of standing how that market values the company, you know, both potentially in a, you know, will we get some kind of term or something, some set of terms that that kind of price the company, but, but also just kind of how they're thinking about use cases, how they're thinking about where it fits in against their strategy, like it's a really good learning opportunity for us, though, it is a big distraction to kind of run down and talk through it, and we'll probably be in that process at least another two to four weeks. I think it kind of just depends if some of the the new guys just drop out and, you know, after quick check, or, you know, we'll see. And then, then, then I get, I can get back in 100% focus on driving this thing through the B round. But the same time, we are very busy getting to be round. Revenue continues to accelerate. I just got off of our weekly status call. Leadership Team. We now have nine POCs. We're juggling that we're moving into. We've never had more than one in a month, and which was pretty busy for us. But nine is getting kind of crazy, and so it's and there's just more pipeline kind of building. You know, Matthew coming in and hiring two excellent, very experienced security sales people. It's just not just pouring fuel on all that. And I was just telling the team, we're gonna have a crazy end of q3 and a very busy q4 and they need to be ready for it.
S Speaker 311:02Yeah, that is the guidance I got from the end of the board. Is, especially the company that's for the salon, is, let's try to loop them into the round. I don't, I don't think we'd loop them with the first right of refusal, though you would want them just to come in clean, like, you know, like, I don't think I don't think I put us in that position with them, but because our insiders are supporting us, right, like we're, you know, drive has huge fund, billion a half dollar company. They're not great startups or something. And and Riot Ventures has a ton of money, and they can do SPVs even though they're a seed fund. They can, they usually participate via SPVs effort, and so, you know, for them, it's more about like, you know, I think part of this process, there's a chance we could just catalyze to be right, like someone really, if one of these bigger companies really want to do something, we might be able to just, let's just do the B now, but, but I think overall, we want to just get a few more points on the board so we get A great valuation of the B round for everybody that comes in right now. And so the board's like, you know, just start circling people who are interested. Let's figure out how to get them in. That is a little tricky right now, because initially I was, I was leaning towards, if someone wants to price around, let's price the round. But if we priced around right now, the M and A guys suddenly have a price, and then that gets so we want to do it on a safe with a discount. I don't know if that interests you guys. And we could do a really good discount on the safe so that you get so to get a place in the beat, and you got a great price going into the B and then. And so I'm circling everybody around that this week, like, Hey, here's where we are. Let's, let's just, let's get a safe out with a with a really good discount, and let's just get this done so that I can just keep focusing on building business. And it's something crazy. I think the board would only do it if it was an irrational makes
Yeah, that is the guidance I got from the end of the board. Is, especially the company that's for the salon, is, let's try to loop them into the round. I don't, I don't think we'd loop them with the first right of refusal, though you would want them just to come in clean, like, you know, like, I don't think I don't think I put us in that position with them, but because our insiders are supporting us, right, like we're, you know, drive has huge fund, billion a half dollar company. They're not great startups or something. And and Riot Ventures has a ton of money, and they can do SPVs even though they're a seed fund. They can, they usually participate via SPVs effort, and so, you know, for them, it's more about like, you know, I think part of this process, there's a chance we could just catalyze to be right, like someone really, if one of these bigger companies really want to do something, we might be able to just, let's just do the B now, but, but I think overall, we want to just get a few more points on the board so we get A great valuation of the B round for everybody that comes in right now. And so the board's like, you know, just start circling people who are interested. Let's figure out how to get them in. That is a little tricky right now, because initially I was, I was leaning towards, if someone wants to price around, let's price the round. But if we priced around right now, the M and A guys suddenly have a price, and then that gets so we want to do it on a safe with a discount. I don't know if that interests you guys. And we could do a really good discount on the safe so that you get so to get a place in the beat, and you got a great price going into the B and then. And so I'm circling everybody around that this week, like, Hey, here's where we are. Let's, let's just, let's get a safe out with a with a really good discount, and let's just get this done so that I can just keep focusing on building business. And it's something crazy. I think the board would only do it if it was an irrational makes
Yeah, that is the guidance I got from the end of the board. Is, especially the company that's for the salon, is, let's try to loop them into the round. I don't, I don't think we'd loop them with the first right of refusal, though you would want them just to come in clean, like, you know, like, I don't think I don't think I put us in that position with them, but because our insiders are supporting us, right, like we're, you know, drive has huge fund, billion a half dollar company. They're not great startups or something. And and Riot Ventures has a ton of money, and they can do SPVs even though they're a seed fund. They can, they usually participate via SPVs effort, and so, you know, for them, it's more about like, you know, I think part of this process, there's a chance we could just catalyze to be right, like someone really, if one of these bigger companies really want to do something, we might be able to just, let's just do the B now, but, but I think overall, we want to just get a few more points on the board so we get A great valuation of the B round for everybody that comes in right now. And so the board's like, you know, just start circling people who are interested. Let's figure out how to get them in. That is a little tricky right now, because initially I was, I was leaning towards, if someone wants to price around, let's price the round. But if we priced around right now, the M and A guys suddenly have a price, and then that gets so we want to do it on a safe with a discount. I don't know if that interests you guys. And we could do a really good discount on the safe so that you get so to get a place in the beat, and you got a great price going into the B and then. And so I'm circling everybody around that this week, like, Hey, here's where we are. Let's, let's just, let's get a safe out with a with a really good discount, and let's just get this done so that I can just keep focusing on building business. And it's something crazy. I think the board would only do it if it was an irrational makes
Yeah, that is the guidance I got from the end of the board. Is, especially the company that's for the salon, is, let's try to loop them into the round. I don't, I don't think we'd loop them with the first right of refusal, though you would want them just to come in clean, like, you know, like, I don't think I don't think I put us in that position with them, but because our insiders are supporting us, right, like we're, you know, drive has huge fund, billion a half dollar company. They're not great startups or something. And and Riot Ventures has a ton of money, and they can do SPVs even though they're a seed fund. They can, they usually participate via SPVs effort, and so, you know, for them, it's more about like, you know, I think part of this process, there's a chance we could just catalyze to be right, like someone really, if one of these bigger companies really want to do something, we might be able to just, let's just do the B now, but, but I think overall, we want to just get a few more points on the board so we get A great valuation of the B round for everybody that comes in right now. And so the board's like, you know, just start circling people who are interested. Let's figure out how to get them in. That is a little tricky right now, because initially I was, I was leaning towards, if someone wants to price around, let's price the round. But if we priced around right now, the M and A guys suddenly have a price, and then that gets so we want to do it on a safe with a discount. I don't know if that interests you guys. And we could do a really good discount on the safe so that you get so to get a place in the beat, and you got a great price going into the B and then. And so I'm circling everybody around that this week, like, Hey, here's where we are. Let's, let's just, let's get a safe out with a with a really good discount, and let's just get this done so that I can just keep focusing on building business. And it's something crazy. I think the board would only do it if it was an irrational makes
12:58complete sense. And
S Speaker 313:01then we could put terms in the safe where it won't be a waste of your time also, right? So, yeah, figure that out, so that you know, if you guys, you know, so we get you to some kind of great return, if we somehow. So before we do it, be around, but I but they're leaning towards beer. They're pretty excited about the be around. They see the acceleration too. And so that's where we're heading, but except that crazy can happen. That's crazy times,
then we could put terms in the safe where it won't be a waste of your time also, right? So, yeah, figure that out, so that you know, if you guys, you know, so we get you to some kind of great return, if we somehow. So before we do it, be around, but I but they're leaning towards beer. They're pretty excited about the be around. They see the acceleration too. And so that's where we're heading, but except that crazy can happen. That's crazy times,
then we could put terms in the safe where it won't be a waste of your time also, right? So, yeah, figure that out, so that you know, if you guys, you know, so we get you to some kind of great return, if we somehow. So before we do it, be around, but I but they're leaning towards beer. They're pretty excited about the be around. They see the acceleration too. And so that's where we're heading, but except that crazy can happen. That's crazy times,
then we could put terms in the safe where it won't be a waste of your time also, right? So, yeah, figure that out, so that you know, if you guys, you know, so we get you to some kind of great return, if we somehow. So before we do it, be around, but I but they're leaning towards beer. They're pretty excited about the be around. They see the acceleration too. And so that's where we're heading, but except that crazy can happen. That's crazy times,
S Speaker 113:27crazy times. You know, it's funny. You mentioned that two companies, I mean different spaces, both have M and A list, including yours, right? So I'm talking to and it's never this. It's never been this active, you know, you, of course, the incumbents are like, hey, we need an edge in the space. Great teams, great products out there. So I completely understand. I know we have 15 minutes. I can stay a little bit, but can we appreciate Can you stay over the time? Yeah, let's just do the
crazy times. You know, it's funny. You mentioned that two companies, I mean different spaces, both have M and A list, including yours, right? So I'm talking to and it's never this. It's never been this active, you know, you, of course, the incumbents are like, hey, we need an edge in the space. Great teams, great products out there. So I completely understand. I know we have 15 minutes. I can stay a little bit, but can we appreciate Can you stay over the time? Yeah, let's just do the
crazy times. You know, it's funny. You mentioned that two companies, I mean different spaces, both have M and A list, including yours, right? So I'm talking to and it's never this. It's never been this active, you know, you, of course, the incumbents are like, hey, we need an edge in the space. Great teams, great products out there. So I completely understand. I know we have 15 minutes. I can stay a little bit, but can we appreciate Can you stay over the time? Yeah, let's just do the
crazy times. You know, it's funny. You mentioned that two companies, I mean different spaces, both have M and A list, including yours, right? So I'm talking to and it's never this. It's never been this active, you know, you, of course, the incumbents are like, hey, we need an edge in the space. Great teams, great products out there. So I completely understand. I know we have 15 minutes. I can stay a little bit, but can we appreciate Can you stay over the time? Yeah, let's just do the
S Speaker 313:59demo real quick while we have Priyesh, and we can definitely do a lot 15 minutes. Charlie joyous, Charlie's our founding engineer. We worked together last at a startup that that we're both employees at. They ended up shutting down building using, you know, building a global network for real time communications, and doing building our own routers. Charlie's really smart guy. And then before that, he ran about a third of bird scooters engineering team and his own startup. Before that, getting out of college, so legit, 10x engineer. And we are super lucky to have him in but Charlie, you want to maybe, let's show transactions first, and then, then we talked to kind of rest of things, the use cases you guys missed was maybe DevOps style, and was there another one that you remember? So DevOps is
demo real quick while we have Priyesh, and we can definitely do a lot 15 minutes. Charlie joyous, Charlie's our founding engineer. We worked together last at a startup that that we're both employees at. They ended up shutting down building using, you know, building a global network for real time communications, and doing building our own routers. Charlie's really smart guy. And then before that, he ran about a third of bird scooters engineering team and his own startup. Before that, getting out of college, so legit, 10x engineer. And we are super lucky to have him in but Charlie, you want to maybe, let's show transactions first, and then, then we talked to kind of rest of things, the use cases you guys missed was maybe DevOps style, and was there another one that you remember? So DevOps is
demo real quick while we have Priyesh, and we can definitely do a lot 15 minutes. Charlie joyous, Charlie's our founding engineer. We worked together last at a startup that that we're both employees at. They ended up shutting down building using, you know, building a global network for real time communications, and doing building our own routers. Charlie's really smart guy. And then before that, he ran about a third of bird scooters engineering team and his own startup. Before that, getting out of college, so legit, 10x engineer. And we are super lucky to have him in but Charlie, you want to maybe, let's show transactions first, and then, then we talked to kind of rest of things, the use cases you guys missed was maybe DevOps style, and was there another one that you remember? So DevOps is
demo real quick while we have Priyesh, and we can definitely do a lot 15 minutes. Charlie joyous, Charlie's our founding engineer. We worked together last at a startup that that we're both employees at. They ended up shutting down building using, you know, building a global network for real time communications, and doing building our own routers. Charlie's really smart guy. And then before that, he ran about a third of bird scooters engineering team and his own startup. Before that, getting out of college, so legit, 10x engineer. And we are super lucky to have him in but Charlie, you want to maybe, let's show transactions first, and then, then we talked to kind of rest of things, the use cases you guys missed was maybe DevOps style, and was there another one that you remember? So DevOps is
S Speaker 114:51the easiest, lowest hanging proof, because that's, I think, the one that you attack the most frequently, right? So let's do that one, if it's easier to them all, and then one of the non, non DevOps use case, right? Which is, maybe can be IT ops, if you have that,
the easiest, lowest hanging proof, because that's, I think, the one that you attack the most frequently, right? So let's do that one, if it's easier to them all, and then one of the non, non DevOps use case, right? Which is, maybe can be IT ops, if you have that,
the easiest, lowest hanging proof, because that's, I think, the one that you attack the most frequently, right? So let's do that one, if it's easier to them all, and then one of the non, non DevOps use case, right? Which is, maybe can be IT ops, if you have that,
the easiest, lowest hanging proof, because that's, I think, the one that you attack the most frequently, right? So let's do that one, if it's easier to them all, and then one of the non, non DevOps use case, right? Which is, maybe can be IT ops, if you have that,
S Speaker 315:10Okay, guess just Charlie. What do you have queued up you can run now
Okay, guess just Charlie. What do you have queued up you can run now
Okay, guess just Charlie. What do you have queued up you can run now
Okay, guess just Charlie. What do you have queued up you can run now
S Speaker 415:13I've mostly got like scans, like vulnerability scans, kind of SecOps oriented use cases, but that's
I've mostly got like scans, like vulnerability scans, kind of SecOps oriented use cases, but that's
I've mostly got like scans, like vulnerability scans, kind of SecOps oriented use cases, but that's
I've mostly got like scans, like vulnerability scans, kind of SecOps oriented use cases, but that's
S Speaker 315:22fine, All right, sounds good, and then we can talk through some of those meetings.
fine, All right, sounds good, and then we can talk through some of those meetings.
fine, All right, sounds good, and then we can talk through some of those meetings.
fine, All right, sounds good, and then we can talk through some of those meetings.
15:26Yeah, great to meet you guys.
Yeah, great to meet you guys.
Yeah, great to meet you guys.
Yeah, great to meet you guys.
17:53But what's really cool? Oh, go ahead.
But what's really cool? Oh, go ahead.
But what's really cool? Oh, go ahead.
But what's really cool? Oh, go ahead.
S Speaker 117:55Yeah, you know. So for pen testing, is it delaying the plan and then figuring out, because it could be multiple ways to pen test. But if there's no How does it? How does it come up and in a little background, if you can talk about, you have to train, you have to fine tune in the way that you would like to pen test it, or you would like the system to pen test, versus go out and do course pen testing right? You can take either approach and either approach is right. I'm not saying it's wrong, but I'm trying to understand how how nuanced the product is. Yeah,
Yeah, you know. So for pen testing, is it delaying the plan and then figuring out, because it could be multiple ways to pen test. But if there's no How does it? How does it come up and in a little background, if you can talk about, you have to train, you have to fine tune in the way that you would like to pen test it, or you would like the system to pen test, versus go out and do course pen testing right? You can take either approach and either approach is right. I'm not saying it's wrong, but I'm trying to understand how how nuanced the product is. Yeah,
Yeah, you know. So for pen testing, is it delaying the plan and then figuring out, because it could be multiple ways to pen test. But if there's no How does it? How does it come up and in a little background, if you can talk about, you have to train, you have to fine tune in the way that you would like to pen test it, or you would like the system to pen test, versus go out and do course pen testing right? You can take either approach and either approach is right. I'm not saying it's wrong, but I'm trying to understand how how nuanced the product is. Yeah,
Yeah, you know. So for pen testing, is it delaying the plan and then figuring out, because it could be multiple ways to pen test. But if there's no How does it? How does it come up and in a little background, if you can talk about, you have to train, you have to fine tune in the way that you would like to pen test it, or you would like the system to pen test, versus go out and do course pen testing right? You can take either approach and either approach is right. I'm not saying it's wrong, but I'm trying to understand how how nuanced the product is. Yeah,
S Speaker 120:02and you need to be reliable, right? Most of the agents have,
and you need to be reliable, right? Most of the agents have,
and you need to be reliable, right? Most of the agents have,
and you need to be reliable, right? Most of the agents have,
S Speaker 420:07because even if we, like kind of jailbroke this, even if that was reliable, then we you could potentially risk the monitoring tools blocking access to your model if they detect you doing things that are against their terms of service. So if we wanted to break this into multi stage, like write a plan and then run it based on the plan, add more guidance, and then run this on a schedule, we'd move us from this ad hoc chatting mode into a repeatable kind of workflow, or agent in the background. By clicking this, it has agent button, yeah, and that's what will kind of digest the conversation and progress we've had so far into repeatable steps with input variables for the end user. And now we can run this based on triggers like from an integration like a linear ticket or a JIRA ticket, and run that as kind of a background agent. That's how we kind of get our ambient agent swarm going.
because even if we, like kind of jailbroke this, even if that was reliable, then we you could potentially risk the monitoring tools blocking access to your model if they detect you doing things that are against their terms of service. So if we wanted to break this into multi stage, like write a plan and then run it based on the plan, add more guidance, and then run this on a schedule, we'd move us from this ad hoc chatting mode into a repeatable kind of workflow, or agent in the background. By clicking this, it has agent button, yeah, and that's what will kind of digest the conversation and progress we've had so far into repeatable steps with input variables for the end user. And now we can run this based on triggers like from an integration like a linear ticket or a JIRA ticket, and run that as kind of a background agent. That's how we kind of get our ambient agent swarm going.
because even if we, like kind of jailbroke this, even if that was reliable, then we you could potentially risk the monitoring tools blocking access to your model if they detect you doing things that are against their terms of service. So if we wanted to break this into multi stage, like write a plan and then run it based on the plan, add more guidance, and then run this on a schedule, we'd move us from this ad hoc chatting mode into a repeatable kind of workflow, or agent in the background. By clicking this, it has agent button, yeah, and that's what will kind of digest the conversation and progress we've had so far into repeatable steps with input variables for the end user. And now we can run this based on triggers like from an integration like a linear ticket or a JIRA ticket, and run that as kind of a background agent. That's how we kind of get our ambient agent swarm going.
because even if we, like kind of jailbroke this, even if that was reliable, then we you could potentially risk the monitoring tools blocking access to your model if they detect you doing things that are against their terms of service. So if we wanted to break this into multi stage, like write a plan and then run it based on the plan, add more guidance, and then run this on a schedule, we'd move us from this ad hoc chatting mode into a repeatable kind of workflow, or agent in the background. By clicking this, it has agent button, yeah, and that's what will kind of digest the conversation and progress we've had so far into repeatable steps with input variables for the end user. And now we can run this based on triggers like from an integration like a linear ticket or a JIRA ticket, and run that as kind of a background agent. That's how we kind of get our ambient agent swarm going.
S Speaker 321:04So a real world use case here would be, we're doing a POC with LinkedIn financial right now, his team, you know, security blog post comes out. Hey, there's a new CDE that affects something that they have at Link Financial, and they want to find out they're vulnerable. Well, it's, it could be a couple of weeks work to get that test structure, do the inventory, all that stuff. So now, with an agent, it can be done, like, in 10 or 15 minutes, and all they have to do is is add, like, Here, here's a new blog post on the CVE right. Go find out if we're vulnerable, right? And then, with the tool use, they have start calling the infrastructure. It'll figure out, you know, is there infrastructure that's that is vulnerable to this? Let's write a test to run at let's go see right? So very quickly now, as these new things come in, and you're aware of your team can just knock that stuff out, right? It's a big shift, and how fast you can address these things and learn what's going on in the organization.
So a real world use case here would be, we're doing a POC with LinkedIn financial right now, his team, you know, security blog post comes out. Hey, there's a new CDE that affects something that they have at Link Financial, and they want to find out they're vulnerable. Well, it's, it could be a couple of weeks work to get that test structure, do the inventory, all that stuff. So now, with an agent, it can be done, like, in 10 or 15 minutes, and all they have to do is is add, like, Here, here's a new blog post on the CVE right. Go find out if we're vulnerable, right? And then, with the tool use, they have start calling the infrastructure. It'll figure out, you know, is there infrastructure that's that is vulnerable to this? Let's write a test to run at let's go see right? So very quickly now, as these new things come in, and you're aware of your team can just knock that stuff out, right? It's a big shift, and how fast you can address these things and learn what's going on in the organization.
So a real world use case here would be, we're doing a POC with LinkedIn financial right now, his team, you know, security blog post comes out. Hey, there's a new CDE that affects something that they have at Link Financial, and they want to find out they're vulnerable. Well, it's, it could be a couple of weeks work to get that test structure, do the inventory, all that stuff. So now, with an agent, it can be done, like, in 10 or 15 minutes, and all they have to do is is add, like, Here, here's a new blog post on the CVE right. Go find out if we're vulnerable, right? And then, with the tool use, they have start calling the infrastructure. It'll figure out, you know, is there infrastructure that's that is vulnerable to this? Let's write a test to run at let's go see right? So very quickly now, as these new things come in, and you're aware of your team can just knock that stuff out, right? It's a big shift, and how fast you can address these things and learn what's going on in the organization.
So a real world use case here would be, we're doing a POC with LinkedIn financial right now, his team, you know, security blog post comes out. Hey, there's a new CDE that affects something that they have at Link Financial, and they want to find out they're vulnerable. Well, it's, it could be a couple of weeks work to get that test structure, do the inventory, all that stuff. So now, with an agent, it can be done, like, in 10 or 15 minutes, and all they have to do is is add, like, Here, here's a new blog post on the CVE right. Go find out if we're vulnerable, right? And then, with the tool use, they have start calling the infrastructure. It'll figure out, you know, is there infrastructure that's that is vulnerable to this? Let's write a test to run at let's go see right? So very quickly now, as these new things come in, and you're aware of your team can just knock that stuff out, right? It's a big shift, and how fast you can address these things and learn what's going on in the organization.
S Speaker 122:06Moving from chat to chat actions, I think, is this step to make it a repeatable agent, which becomes like a one click action. Is that the conversation that you're talking about
Moving from chat to chat actions, I think, is this step to make it a repeatable agent, which becomes like a one click action. Is that the conversation that you're talking about
Moving from chat to chat actions, I think, is this step to make it a repeatable agent, which becomes like a one click action. Is that the conversation that you're talking about
Moving from chat to chat actions, I think, is this step to make it a repeatable agent, which becomes like a one click action. Is that the conversation that you're talking about
22:21from a chat to chat actions?
from a chat to chat actions?
from a chat to chat actions?
from a chat to chat actions?
S Speaker 322:23Yeah, it could be. It's actually two things. So chats just, you know, chat like, you know, chats, and then we're talking with chat actions, and it's early days. By summer, it's gonna look quite a bit different. We're basically creating your day to day security driver, like cursor is for developers. There's never been like an ID for security team Gen, right? There's always, like, 30 tools I gotta bounce around. And so now, with the LLM there, you can just go call those tools for you, and you just stay in one window inside a kendo, and you're just doing your work, right? And then when you create the agent to become an autonomous or one, or run it whenever you want to agent, you make it autonomous by, like, putting a trigger on it, so it's just always watching a log file or a firewall entry or one of the tools that you have, a ticket or whatever. And when it sees, hey, there's a new thing for me to pick up in this ticket, it just grabs it reads the ticket, and then starts the agent automatically running, and then returns the results back to you. And then, or you can just have one off him in the loop running of them too. You can just go to agents see what it does. You have to say, Oh, I'd like to run this one pen test agent on this blog post, you know, or whatever. And they just kick it off to that. So that's how it goes. And then the the other class that we have, our Q and A agents, you can build. So we call them just chat agents, where you can give it a whole bunch of things, like, here's all my security policies, and tell employees ask this chat bot before you talk to us, which is kind of creating TPTs, right? Yeah, and that's very useful. A lot of enterprise like Arion loves those, you know. So it saved the security team a lot of headaches. They have very complicated change management policies and Security Risk Management Policies, and it's always bugging the teams, can I do this? And now they have these chat bots that people just ask and so that those are the four. But we think by summer, you're probably even have quite a bit different UI, because you're gonna have all these autonomous agents running, watching your tickets, watching your log files, watching CrowdStrike whatever, and then bringing back the things you need to deal with as a human or that you're interested in. And so the UI was will probably shift to more of a dashboard kind of style that you can then mostly call chat actions up when you want to to just dive in and work with stuff. And then you'd always have an agent builder you can always use to, you know, manage things.
Yeah, it could be. It's actually two things. So chats just, you know, chat like, you know, chats, and then we're talking with chat actions, and it's early days. By summer, it's gonna look quite a bit different. We're basically creating your day to day security driver, like cursor is for developers. There's never been like an ID for security team Gen, right? There's always, like, 30 tools I gotta bounce around. And so now, with the LLM there, you can just go call those tools for you, and you just stay in one window inside a kendo, and you're just doing your work, right? And then when you create the agent to become an autonomous or one, or run it whenever you want to agent, you make it autonomous by, like, putting a trigger on it, so it's just always watching a log file or a firewall entry or one of the tools that you have, a ticket or whatever. And when it sees, hey, there's a new thing for me to pick up in this ticket, it just grabs it reads the ticket, and then starts the agent automatically running, and then returns the results back to you. And then, or you can just have one off him in the loop running of them too. You can just go to agents see what it does. You have to say, Oh, I'd like to run this one pen test agent on this blog post, you know, or whatever. And they just kick it off to that. So that's how it goes. And then the the other class that we have, our Q and A agents, you can build. So we call them just chat agents, where you can give it a whole bunch of things, like, here's all my security policies, and tell employees ask this chat bot before you talk to us, which is kind of creating TPTs, right? Yeah, and that's very useful. A lot of enterprise like Arion loves those, you know. So it saved the security team a lot of headaches. They have very complicated change management policies and Security Risk Management Policies, and it's always bugging the teams, can I do this? And now they have these chat bots that people just ask and so that those are the four. But we think by summer, you're probably even have quite a bit different UI, because you're gonna have all these autonomous agents running, watching your tickets, watching your log files, watching CrowdStrike whatever, and then bringing back the things you need to deal with as a human or that you're interested in. And so the UI was will probably shift to more of a dashboard kind of style that you can then mostly call chat actions up when you want to to just dive in and work with stuff. And then you'd always have an agent builder you can always use to, you know, manage things.
Yeah, it could be. It's actually two things. So chats just, you know, chat like, you know, chats, and then we're talking with chat actions, and it's early days. By summer, it's gonna look quite a bit different. We're basically creating your day to day security driver, like cursor is for developers. There's never been like an ID for security team Gen, right? There's always, like, 30 tools I gotta bounce around. And so now, with the LLM there, you can just go call those tools for you, and you just stay in one window inside a kendo, and you're just doing your work, right? And then when you create the agent to become an autonomous or one, or run it whenever you want to agent, you make it autonomous by, like, putting a trigger on it, so it's just always watching a log file or a firewall entry or one of the tools that you have, a ticket or whatever. And when it sees, hey, there's a new thing for me to pick up in this ticket, it just grabs it reads the ticket, and then starts the agent automatically running, and then returns the results back to you. And then, or you can just have one off him in the loop running of them too. You can just go to agents see what it does. You have to say, Oh, I'd like to run this one pen test agent on this blog post, you know, or whatever. And they just kick it off to that. So that's how it goes. And then the the other class that we have, our Q and A agents, you can build. So we call them just chat agents, where you can give it a whole bunch of things, like, here's all my security policies, and tell employees ask this chat bot before you talk to us, which is kind of creating TPTs, right? Yeah, and that's very useful. A lot of enterprise like Arion loves those, you know. So it saved the security team a lot of headaches. They have very complicated change management policies and Security Risk Management Policies, and it's always bugging the teams, can I do this? And now they have these chat bots that people just ask and so that those are the four. But we think by summer, you're probably even have quite a bit different UI, because you're gonna have all these autonomous agents running, watching your tickets, watching your log files, watching CrowdStrike whatever, and then bringing back the things you need to deal with as a human or that you're interested in. And so the UI was will probably shift to more of a dashboard kind of style that you can then mostly call chat actions up when you want to to just dive in and work with stuff. And then you'd always have an agent builder you can always use to, you know, manage things.
Yeah, it could be. It's actually two things. So chats just, you know, chat like, you know, chats, and then we're talking with chat actions, and it's early days. By summer, it's gonna look quite a bit different. We're basically creating your day to day security driver, like cursor is for developers. There's never been like an ID for security team Gen, right? There's always, like, 30 tools I gotta bounce around. And so now, with the LLM there, you can just go call those tools for you, and you just stay in one window inside a kendo, and you're just doing your work, right? And then when you create the agent to become an autonomous or one, or run it whenever you want to agent, you make it autonomous by, like, putting a trigger on it, so it's just always watching a log file or a firewall entry or one of the tools that you have, a ticket or whatever. And when it sees, hey, there's a new thing for me to pick up in this ticket, it just grabs it reads the ticket, and then starts the agent automatically running, and then returns the results back to you. And then, or you can just have one off him in the loop running of them too. You can just go to agents see what it does. You have to say, Oh, I'd like to run this one pen test agent on this blog post, you know, or whatever. And they just kick it off to that. So that's how it goes. And then the the other class that we have, our Q and A agents, you can build. So we call them just chat agents, where you can give it a whole bunch of things, like, here's all my security policies, and tell employees ask this chat bot before you talk to us, which is kind of creating TPTs, right? Yeah, and that's very useful. A lot of enterprise like Arion loves those, you know. So it saved the security team a lot of headaches. They have very complicated change management policies and Security Risk Management Policies, and it's always bugging the teams, can I do this? And now they have these chat bots that people just ask and so that those are the four. But we think by summer, you're probably even have quite a bit different UI, because you're gonna have all these autonomous agents running, watching your tickets, watching your log files, watching CrowdStrike whatever, and then bringing back the things you need to deal with as a human or that you're interested in. And so the UI was will probably shift to more of a dashboard kind of style that you can then mostly call chat actions up when you want to to just dive in and work with stuff. And then you'd always have an agent builder you can always use to, you know, manage things.
S Speaker 325:24Yeah, so today you can do that. So that's what our triggered agents do? You hook it into an API or an endpoint that has that information you want it to track, and then it's a listener that just sits there waiting for an event to happen that matches right? So this case like Charlie showing you a trigger on linear, which is like Jira, and then, right? And then you can be like, hey, anytime you see a password change request ticket, right? This is another good use case a lot of people have. This is how the MGM got hacked, right? The bad process happened with a password change request and social engineering after, right? So what you could do is you could say, oh, this password change request before, before you send it to a human pick up this ticket, let's go research the user the Senate. Are they currently in the building? Did they have they logged in recently? Are they currently logged into the system? When? Where did they log in last? Are they actually having problems with the login to get all that in the ticket and then analyze it and make a recommendation to the human that, hey, you should pay more attention to this. This could be fraud, right? Are you being social engineered or, like, here's everything. Here's where the user is having problems, here's the suggested thing you should do, right? That's kind of a low hanging technical kind of problem solve. But you could go much further. Like, it could be, hey, my service is down somewhere. You're picking out one of your IT tools, and then that gets created, and then agents just go out to all that research a human would do, check everything about the service pool, the TerraForm config, examine it, make sure there's not some weird authorization problem in that or whatever, and then enrich the ticket with all that information with a suggested problem and fix for the human. And then the human can just take it. It could actually, if you trusted it, it could run it, but people don't trust them yet with production changes, right? And so, so the human can now look at it, and then it could be a suggested code fix it could do, and then say, Hey, would you like me to go ahead and create a pull request for this? And you know that all can be built into the agent, like you have 100 step agent, if you want to or more, talking to different models. And so that that's the power of triggers, like the triggers that you know, basically that listener is what gives you that 24/7 automation waiting for it to run. And what's nice about it is it triggers very reliable, right? It's just like
Yeah, so today you can do that. So that's what our triggered agents do? You hook it into an API or an endpoint that has that information you want it to track, and then it's a listener that just sits there waiting for an event to happen that matches right? So this case like Charlie showing you a trigger on linear, which is like Jira, and then, right? And then you can be like, hey, anytime you see a password change request ticket, right? This is another good use case a lot of people have. This is how the MGM got hacked, right? The bad process happened with a password change request and social engineering after, right? So what you could do is you could say, oh, this password change request before, before you send it to a human pick up this ticket, let's go research the user the Senate. Are they currently in the building? Did they have they logged in recently? Are they currently logged into the system? When? Where did they log in last? Are they actually having problems with the login to get all that in the ticket and then analyze it and make a recommendation to the human that, hey, you should pay more attention to this. This could be fraud, right? Are you being social engineered or, like, here's everything. Here's where the user is having problems, here's the suggested thing you should do, right? That's kind of a low hanging technical kind of problem solve. But you could go much further. Like, it could be, hey, my service is down somewhere. You're picking out one of your IT tools, and then that gets created, and then agents just go out to all that research a human would do, check everything about the service pool, the TerraForm config, examine it, make sure there's not some weird authorization problem in that or whatever, and then enrich the ticket with all that information with a suggested problem and fix for the human. And then the human can just take it. It could actually, if you trusted it, it could run it, but people don't trust them yet with production changes, right? And so, so the human can now look at it, and then it could be a suggested code fix it could do, and then say, Hey, would you like me to go ahead and create a pull request for this? And you know that all can be built into the agent, like you have 100 step agent, if you want to or more, talking to different models. And so that that's the power of triggers, like the triggers that you know, basically that listener is what gives you that 24/7 automation waiting for it to run. And what's nice about it is it triggers very reliable, right? It's just like
Yeah, so today you can do that. So that's what our triggered agents do? You hook it into an API or an endpoint that has that information you want it to track, and then it's a listener that just sits there waiting for an event to happen that matches right? So this case like Charlie showing you a trigger on linear, which is like Jira, and then, right? And then you can be like, hey, anytime you see a password change request ticket, right? This is another good use case a lot of people have. This is how the MGM got hacked, right? The bad process happened with a password change request and social engineering after, right? So what you could do is you could say, oh, this password change request before, before you send it to a human pick up this ticket, let's go research the user the Senate. Are they currently in the building? Did they have they logged in recently? Are they currently logged into the system? When? Where did they log in last? Are they actually having problems with the login to get all that in the ticket and then analyze it and make a recommendation to the human that, hey, you should pay more attention to this. This could be fraud, right? Are you being social engineered or, like, here's everything. Here's where the user is having problems, here's the suggested thing you should do, right? That's kind of a low hanging technical kind of problem solve. But you could go much further. Like, it could be, hey, my service is down somewhere. You're picking out one of your IT tools, and then that gets created, and then agents just go out to all that research a human would do, check everything about the service pool, the TerraForm config, examine it, make sure there's not some weird authorization problem in that or whatever, and then enrich the ticket with all that information with a suggested problem and fix for the human. And then the human can just take it. It could actually, if you trusted it, it could run it, but people don't trust them yet with production changes, right? And so, so the human can now look at it, and then it could be a suggested code fix it could do, and then say, Hey, would you like me to go ahead and create a pull request for this? And you know that all can be built into the agent, like you have 100 step agent, if you want to or more, talking to different models. And so that that's the power of triggers, like the triggers that you know, basically that listener is what gives you that 24/7 automation waiting for it to run. And what's nice about it is it triggers very reliable, right? It's just like
Yeah, so today you can do that. So that's what our triggered agents do? You hook it into an API or an endpoint that has that information you want it to track, and then it's a listener that just sits there waiting for an event to happen that matches right? So this case like Charlie showing you a trigger on linear, which is like Jira, and then, right? And then you can be like, hey, anytime you see a password change request ticket, right? This is another good use case a lot of people have. This is how the MGM got hacked, right? The bad process happened with a password change request and social engineering after, right? So what you could do is you could say, oh, this password change request before, before you send it to a human pick up this ticket, let's go research the user the Senate. Are they currently in the building? Did they have they logged in recently? Are they currently logged into the system? When? Where did they log in last? Are they actually having problems with the login to get all that in the ticket and then analyze it and make a recommendation to the human that, hey, you should pay more attention to this. This could be fraud, right? Are you being social engineered or, like, here's everything. Here's where the user is having problems, here's the suggested thing you should do, right? That's kind of a low hanging technical kind of problem solve. But you could go much further. Like, it could be, hey, my service is down somewhere. You're picking out one of your IT tools, and then that gets created, and then agents just go out to all that research a human would do, check everything about the service pool, the TerraForm config, examine it, make sure there's not some weird authorization problem in that or whatever, and then enrich the ticket with all that information with a suggested problem and fix for the human. And then the human can just take it. It could actually, if you trusted it, it could run it, but people don't trust them yet with production changes, right? And so, so the human can now look at it, and then it could be a suggested code fix it could do, and then say, Hey, would you like me to go ahead and create a pull request for this? And you know that all can be built into the agent, like you have 100 step agent, if you want to or more, talking to different models. And so that that's the power of triggers, like the triggers that you know, basically that listener is what gives you that 24/7 automation waiting for it to run. And what's nice about it is it triggers very reliable, right? It's just like
27:40a web looks, yeah, it
a web looks, yeah, it
a web looks, yeah, it
a web looks, yeah, it
S Speaker 327:42felt, yeah. It's not like the agent itself is running. You know, you're not because, because llms Don't work reliably for two, four hours on something today yet, right? They're getting there. But so you wouldn't want this to be a native LLM finger, yeah. And so the web looks in there, listening, and then it'll call the LLM to do the work it needs and then update whatever the human wants. So it's pretty powerful thing. And maybe one
felt, yeah. It's not like the agent itself is running. You know, you're not because, because llms Don't work reliably for two, four hours on something today yet, right? They're getting there. But so you wouldn't want this to be a native LLM finger, yeah. And so the web looks in there, listening, and then it'll call the LLM to do the work it needs and then update whatever the human wants. So it's pretty powerful thing. And maybe one
felt, yeah. It's not like the agent itself is running. You know, you're not because, because llms Don't work reliably for two, four hours on something today yet, right? They're getting there. But so you wouldn't want this to be a native LLM finger, yeah. And so the web looks in there, listening, and then it'll call the LLM to do the work it needs and then update whatever the human wants. So it's pretty powerful thing. And maybe one
felt, yeah. It's not like the agent itself is running. You know, you're not because, because llms Don't work reliably for two, four hours on something today yet, right? They're getting there. But so you wouldn't want this to be a native LLM finger, yeah. And so the web looks in there, listening, and then it'll call the LLM to do the work it needs and then update whatever the human wants. So it's pretty powerful thing. And maybe one
S Speaker 128:05follow up on that question is, how, like, what did you have to do in the core model, which is, let's say, the D pad model, to be able to further fine tune this and refine this approach, right? Because this is a lot of it. This could be out of the bat, and it could be working out of the box in it could be working right. Like, I can take a plug yes on it model and what, what do you like? How do you develop on top of it, right? Like, do you do specialized training on certain use cases? You get the data, and then, before launching, you validate your internal tests. And what I'm trying to get to is lot of this is, like, a great deal, right? There could be a lot of use cases, a lot of different scenarios. How do you make sure that the customer can actually reliably trust this? Yeah,
follow up on that question is, how, like, what did you have to do in the core model, which is, let's say, the D pad model, to be able to further fine tune this and refine this approach, right? Because this is a lot of it. This could be out of the bat, and it could be working out of the box in it could be working right. Like, I can take a plug yes on it model and what, what do you like? How do you develop on top of it, right? Like, do you do specialized training on certain use cases? You get the data, and then, before launching, you validate your internal tests. And what I'm trying to get to is lot of this is, like, a great deal, right? There could be a lot of use cases, a lot of different scenarios. How do you make sure that the customer can actually reliably trust this? Yeah,
follow up on that question is, how, like, what did you have to do in the core model, which is, let's say, the D pad model, to be able to further fine tune this and refine this approach, right? Because this is a lot of it. This could be out of the bat, and it could be working out of the box in it could be working right. Like, I can take a plug yes on it model and what, what do you like? How do you develop on top of it, right? Like, do you do specialized training on certain use cases? You get the data, and then, before launching, you validate your internal tests. And what I'm trying to get to is lot of this is, like, a great deal, right? There could be a lot of use cases, a lot of different scenarios. How do you make sure that the customer can actually reliably trust this? Yeah,
follow up on that question is, how, like, what did you have to do in the core model, which is, let's say, the D pad model, to be able to further fine tune this and refine this approach, right? Because this is a lot of it. This could be out of the bat, and it could be working out of the box in it could be working right. Like, I can take a plug yes on it model and what, what do you like? How do you develop on top of it, right? Like, do you do specialized training on certain use cases? You get the data, and then, before launching, you validate your internal tests. And what I'm trying to get to is lot of this is, like, a great deal, right? There could be a lot of use cases, a lot of different scenarios. How do you make sure that the customer can actually reliably trust this? Yeah,
S Speaker 130:39about. That's a great point on and just if you, if you have to double click on that, sorry, yep. How do you have to use data? Where do you get to use data from? Is that at all, or it's just observing, observing in the past?
about. That's a great point on and just if you, if you have to double click on that, sorry, yep. How do you have to use data? Where do you get to use data from? Is that at all, or it's just observing, observing in the past?
about. That's a great point on and just if you, if you have to double click on that, sorry, yep. How do you have to use data? Where do you get to use data from? Is that at all, or it's just observing, observing in the past?
about. That's a great point on and just if you, if you have to double click on that, sorry, yep. How do you have to use data? Where do you get to use data from? Is that at all, or it's just observing, observing in the past?
S Speaker 330:53Yes. So today, it's it. We, we have observations of users trying things, both in free product and in our tool. And then we're building, though, a bunch too, so using models to build more, more kind of tool use attempt and then, and then judging them, and then improving right. And then, then we have a whole eval harness that we built our our head of AI is a meta researcher, worked on llama four and three and stuff and so, so we have a pretty for a small team, you know, pretty sophisticated training pipeline build, and keeps getting better all the time, but the and so they're constantly, they're constantly both looking at the better models. So like the new open AI, open source model is actually a very good tool calling model. And so now we're like, okay, will that also accept our refusal data set? Will it accept our dark web data set? And then we got to eval that. And then, if so, then now we have an even better tool calling model. And then, and then, where do we need to improve from there? And then, of course, like deep seek 3.1 that just got released, they've improved their tool calling a lot, and so that's also in the same boat with us now, is like, hey, we trained deep sea three before. So let's go back, and now let's eval it to see if the tool calling is really better and where there's gaps. So that's what we're doing there, and it's just an ongoing kind of process.
Yes. So today, it's it. We, we have observations of users trying things, both in free product and in our tool. And then we're building, though, a bunch too, so using models to build more, more kind of tool use attempt and then, and then judging them, and then improving right. And then, then we have a whole eval harness that we built our our head of AI is a meta researcher, worked on llama four and three and stuff and so, so we have a pretty for a small team, you know, pretty sophisticated training pipeline build, and keeps getting better all the time, but the and so they're constantly, they're constantly both looking at the better models. So like the new open AI, open source model is actually a very good tool calling model. And so now we're like, okay, will that also accept our refusal data set? Will it accept our dark web data set? And then we got to eval that. And then, if so, then now we have an even better tool calling model. And then, and then, where do we need to improve from there? And then, of course, like deep seek 3.1 that just got released, they've improved their tool calling a lot, and so that's also in the same boat with us now, is like, hey, we trained deep sea three before. So let's go back, and now let's eval it to see if the tool calling is really better and where there's gaps. So that's what we're doing there, and it's just an ongoing kind of process.
Yes. So today, it's it. We, we have observations of users trying things, both in free product and in our tool. And then we're building, though, a bunch too, so using models to build more, more kind of tool use attempt and then, and then judging them, and then improving right. And then, then we have a whole eval harness that we built our our head of AI is a meta researcher, worked on llama four and three and stuff and so, so we have a pretty for a small team, you know, pretty sophisticated training pipeline build, and keeps getting better all the time, but the and so they're constantly, they're constantly both looking at the better models. So like the new open AI, open source model is actually a very good tool calling model. And so now we're like, okay, will that also accept our refusal data set? Will it accept our dark web data set? And then we got to eval that. And then, if so, then now we have an even better tool calling model. And then, and then, where do we need to improve from there? And then, of course, like deep seek 3.1 that just got released, they've improved their tool calling a lot, and so that's also in the same boat with us now, is like, hey, we trained deep sea three before. So let's go back, and now let's eval it to see if the tool calling is really better and where there's gaps. So that's what we're doing there, and it's just an ongoing kind of process.
Yes. So today, it's it. We, we have observations of users trying things, both in free product and in our tool. And then we're building, though, a bunch too, so using models to build more, more kind of tool use attempt and then, and then judging them, and then improving right. And then, then we have a whole eval harness that we built our our head of AI is a meta researcher, worked on llama four and three and stuff and so, so we have a pretty for a small team, you know, pretty sophisticated training pipeline build, and keeps getting better all the time, but the and so they're constantly, they're constantly both looking at the better models. So like the new open AI, open source model is actually a very good tool calling model. And so now we're like, okay, will that also accept our refusal data set? Will it accept our dark web data set? And then we got to eval that. And then, if so, then now we have an even better tool calling model. And then, and then, where do we need to improve from there? And then, of course, like deep seek 3.1 that just got released, they've improved their tool calling a lot, and so that's also in the same boat with us now, is like, hey, we trained deep sea three before. So let's go back, and now let's eval it to see if the tool calling is really better and where there's gaps. So that's what we're doing there, and it's just an ongoing kind of process.
S Speaker 232:22Yeah, couple of questions. Yeah, a couple of questions there. Ron, so the datasets that are proprietary to you today, do you see as kind of scales? Do you see adding on to that? How do you plan to sort of keep ahead of the curve?
Yeah, couple of questions. Yeah, a couple of questions there. Ron, so the datasets that are proprietary to you today, do you see as kind of scales? Do you see adding on to that? How do you plan to sort of keep ahead of the curve?
Yeah, couple of questions. Yeah, a couple of questions there. Ron, so the datasets that are proprietary to you today, do you see as kind of scales? Do you see adding on to that? How do you plan to sort of keep ahead of the curve?
Yeah, couple of questions. Yeah, a couple of questions there. Ron, so the datasets that are proprietary to you today, do you see as kind of scales? Do you see adding on to that? How do you plan to sort of keep ahead of the curve?
S Speaker 332:34Yeah, so we are two ways that the base model improves. You know, the new Quinn comes out, or the new whatever, the new llama whatever. That brings a lot and then, and then we're constantly reaching into and then also, you know, the models always have a three or six month lag on training. So, so there's also, kind of, like a big rag data set of current things, you know, Hacker News or whatever, or just in chat action you can tell it, you know, go find this latest blog, and let's work with that data. And so you have a few options that kind of keep the model current on security stuff, and then we're always trying to ratchet it faster as well. So
Yeah, so we are two ways that the base model improves. You know, the new Quinn comes out, or the new whatever, the new llama whatever. That brings a lot and then, and then we're constantly reaching into and then also, you know, the models always have a three or six month lag on training. So, so there's also, kind of, like a big rag data set of current things, you know, Hacker News or whatever, or just in chat action you can tell it, you know, go find this latest blog, and let's work with that data. And so you have a few options that kind of keep the model current on security stuff, and then we're always trying to ratchet it faster as well. So
Yeah, so we are two ways that the base model improves. You know, the new Quinn comes out, or the new whatever, the new llama whatever. That brings a lot and then, and then we're constantly reaching into and then also, you know, the models always have a three or six month lag on training. So, so there's also, kind of, like a big rag data set of current things, you know, Hacker News or whatever, or just in chat action you can tell it, you know, go find this latest blog, and let's work with that data. And so you have a few options that kind of keep the model current on security stuff, and then we're always trying to ratchet it faster as well. So
Yeah, so we are two ways that the base model improves. You know, the new Quinn comes out, or the new whatever, the new llama whatever. That brings a lot and then, and then we're constantly reaching into and then also, you know, the models always have a three or six month lag on training. So, so there's also, kind of, like a big rag data set of current things, you know, Hacker News or whatever, or just in chat action you can tell it, you know, go find this latest blog, and let's work with that data. And so you have a few options that kind of keep the model current on security stuff, and then we're always trying to ratchet it faster as well. So
S Speaker 333:35So today, it's things are static. We lot. You can actually Charlie. You want to show them the how an agent handles failure.
So today, it's things are static. We lot. You can actually Charlie. You want to show them the how an agent handles failure.
So today, it's things are static. We lot. You can actually Charlie. You want to show them the how an agent handles failure.
So today, it's things are static. We lot. You can actually Charlie. You want to show them the how an agent handles failure.
S Speaker 433:45Yeah, so the one I'm sharing is showing an error, if that's what you mean. I did want to talk about that.
Yeah, so the one I'm sharing is showing an error, if that's what you mean. I did want to talk about that.
Yeah, so the one I'm sharing is showing an error, if that's what you mean. I did want to talk about that.
Yeah, so the one I'm sharing is showing an error, if that's what you mean. I did want to talk about that.
S Speaker 434:45Yeah, we're storing all the data. We've just got to close that kind of last mile to expose cases of where the agent encountered errors and overcame them in historical examples in a way that improves all of our evals before we expose that to customers. But yeah, for showing the example of self healing, I did just want to talk through this example for a minute. This is looking at a historical trace of A agent run so it was kicked off by a linear ticket. So this is on linear it could be Jira, any ticketing system, and there's a vulnerability scan that's run on a schedule and publishes into that ticket. So that kicked off this agent along with the first instruction. And so we first enrich the vulnerability scan with information from the National Vulnerability Database. But we can see here the MVD tool encountered an error when making that first query. But because we're not like a tree style brittle logic flow tree style system, the LLM was able to see the content of the error and actually recover with using the tool in a different way, and a tool that was working at the time to get that enrichment information that it needed and then continue with enriching further information from the GitHub repository that's related to the scan, and finally, actually propose back a code change for a human to review a pull request that would address vulnerabilities found in that vulnerability scan. So I think just really the thing to emphasize is when we talk to our customers, and they're on swim lane, and they have these long tree style structures with normalization and all this data munching, and it's really like a Rube Goldberg machine here. The LLM is seeing all of the data in natural language and able to, like we're using it as a reasoning brain to get to the final objective all the way. We found that's a lot more resilient than kind of the no code style tools
Yeah, we're storing all the data. We've just got to close that kind of last mile to expose cases of where the agent encountered errors and overcame them in historical examples in a way that improves all of our evals before we expose that to customers. But yeah, for showing the example of self healing, I did just want to talk through this example for a minute. This is looking at a historical trace of A agent run so it was kicked off by a linear ticket. So this is on linear it could be Jira, any ticketing system, and there's a vulnerability scan that's run on a schedule and publishes into that ticket. So that kicked off this agent along with the first instruction. And so we first enrich the vulnerability scan with information from the National Vulnerability Database. But we can see here the MVD tool encountered an error when making that first query. But because we're not like a tree style brittle logic flow tree style system, the LLM was able to see the content of the error and actually recover with using the tool in a different way, and a tool that was working at the time to get that enrichment information that it needed and then continue with enriching further information from the GitHub repository that's related to the scan, and finally, actually propose back a code change for a human to review a pull request that would address vulnerabilities found in that vulnerability scan. So I think just really the thing to emphasize is when we talk to our customers, and they're on swim lane, and they have these long tree style structures with normalization and all this data munching, and it's really like a Rube Goldberg machine here. The LLM is seeing all of the data in natural language and able to, like we're using it as a reasoning brain to get to the final objective all the way. We found that's a lot more resilient than kind of the no code style tools
Yeah, we're storing all the data. We've just got to close that kind of last mile to expose cases of where the agent encountered errors and overcame them in historical examples in a way that improves all of our evals before we expose that to customers. But yeah, for showing the example of self healing, I did just want to talk through this example for a minute. This is looking at a historical trace of A agent run so it was kicked off by a linear ticket. So this is on linear it could be Jira, any ticketing system, and there's a vulnerability scan that's run on a schedule and publishes into that ticket. So that kicked off this agent along with the first instruction. And so we first enrich the vulnerability scan with information from the National Vulnerability Database. But we can see here the MVD tool encountered an error when making that first query. But because we're not like a tree style brittle logic flow tree style system, the LLM was able to see the content of the error and actually recover with using the tool in a different way, and a tool that was working at the time to get that enrichment information that it needed and then continue with enriching further information from the GitHub repository that's related to the scan, and finally, actually propose back a code change for a human to review a pull request that would address vulnerabilities found in that vulnerability scan. So I think just really the thing to emphasize is when we talk to our customers, and they're on swim lane, and they have these long tree style structures with normalization and all this data munching, and it's really like a Rube Goldberg machine here. The LLM is seeing all of the data in natural language and able to, like we're using it as a reasoning brain to get to the final objective all the way. We found that's a lot more resilient than kind of the no code style tools
Yeah, we're storing all the data. We've just got to close that kind of last mile to expose cases of where the agent encountered errors and overcame them in historical examples in a way that improves all of our evals before we expose that to customers. But yeah, for showing the example of self healing, I did just want to talk through this example for a minute. This is looking at a historical trace of A agent run so it was kicked off by a linear ticket. So this is on linear it could be Jira, any ticketing system, and there's a vulnerability scan that's run on a schedule and publishes into that ticket. So that kicked off this agent along with the first instruction. And so we first enrich the vulnerability scan with information from the National Vulnerability Database. But we can see here the MVD tool encountered an error when making that first query. But because we're not like a tree style brittle logic flow tree style system, the LLM was able to see the content of the error and actually recover with using the tool in a different way, and a tool that was working at the time to get that enrichment information that it needed and then continue with enriching further information from the GitHub repository that's related to the scan, and finally, actually propose back a code change for a human to review a pull request that would address vulnerabilities found in that vulnerability scan. So I think just really the thing to emphasize is when we talk to our customers, and they're on swim lane, and they have these long tree style structures with normalization and all this data munching, and it's really like a Rube Goldberg machine here. The LLM is seeing all of the data in natural language and able to, like we're using it as a reasoning brain to get to the final objective all the way. We found that's a lot more resilient than kind of the no code style tools
S Speaker 236:44and Charlie does the model today only fix tool call errors, or do you also sort of go back and dynamically change the workflows that the agent can execute
and Charlie does the model today only fix tool call errors, or do you also sort of go back and dynamically change the workflows that the agent can execute
and Charlie does the model today only fix tool call errors, or do you also sort of go back and dynamically change the workflows that the agent can execute
and Charlie does the model today only fix tool call errors, or do you also sort of go back and dynamically change the workflows that the agent can execute
S Speaker 436:57so the agent is able to so it will handle tool errors, but it's not rigidly attached to the workflow, so that the workflow is defined as natural language in terms of guidance of what tools to use and what sequence. But it's not constrained from pursuing a different path to get to the outcome that's been demanded. The only constraints it has are the tool policies that we define for each step, and it cannot go outside of those bounds.
so the agent is able to so it will handle tool errors, but it's not rigidly attached to the workflow, so that the workflow is defined as natural language in terms of guidance of what tools to use and what sequence. But it's not constrained from pursuing a different path to get to the outcome that's been demanded. The only constraints it has are the tool policies that we define for each step, and it cannot go outside of those bounds.
so the agent is able to so it will handle tool errors, but it's not rigidly attached to the workflow, so that the workflow is defined as natural language in terms of guidance of what tools to use and what sequence. But it's not constrained from pursuing a different path to get to the outcome that's been demanded. The only constraints it has are the tool policies that we define for each step, and it cannot go outside of those bounds.
so the agent is able to so it will handle tool errors, but it's not rigidly attached to the workflow, so that the workflow is defined as natural language in terms of guidance of what tools to use and what sequence. But it's not constrained from pursuing a different path to get to the outcome that's been demanded. The only constraints it has are the tool policies that we define for each step, and it cannot go outside of those bounds.
38:00yeah. Thanks so much. Really appreciate guys. No, this was super
yeah. Thanks so much. Really appreciate guys. No, this was super
yeah. Thanks so much. Really appreciate guys. No, this was super
yeah. Thanks so much. Really appreciate guys. No, this was super