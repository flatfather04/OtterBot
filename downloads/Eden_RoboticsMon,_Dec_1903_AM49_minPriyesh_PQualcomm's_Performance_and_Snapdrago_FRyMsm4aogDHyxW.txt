Meeting: Eden Robotics
Mon, Dec 1
9:03 AM
49 min
Priyesh P
Qualcomm's Performance and Snapdragon Platforms
0:
URL: https://otter.ai/u/FRyMsm4aogDHyxWo1TFmweg6Kdg
Downloaded: 2025-12-21T19:26:14.702517
Method: text_extraction
============================================================

S Speaker 10:11These run models, take models that were running. I was invited Priyesh from my team
These run models, take models that were running. I was invited Priyesh from my team
These run models, take models that were running. I was invited Priyesh from my team
These run models, take models that were running. I was invited Priyesh from my team
S Speaker 20:19during the call and so we
during the call and so we
during the call and so we
during the call and so we
S Speaker 10:26so in the autonomous driving world, also, it was easier to take models that were trained on GPUs to run them on the GPU to begin with. But now we're at a point where, essentially, where Qualcomm's products really shine is performance per watt, which is essentially delivering the maximum throughput that is required under a power umbrella, which on devices such as robots, they matter a lot.
so in the autonomous driving world, also, it was easier to take models that were trained on GPUs to run them on the GPU to begin with. But now we're at a point where, essentially, where Qualcomm's products really shine is performance per watt, which is essentially delivering the maximum throughput that is required under a power umbrella, which on devices such as robots, they matter a lot.
so in the autonomous driving world, also, it was easier to take models that were trained on GPUs to run them on the GPU to begin with. But now we're at a point where, essentially, where Qualcomm's products really shine is performance per watt, which is essentially delivering the maximum throughput that is required under a power umbrella, which on devices such as robots, they matter a lot.
so in the autonomous driving world, also, it was easier to take models that were trained on GPUs to run them on the GPU to begin with. But now we're at a point where, essentially, where Qualcomm's products really shine is performance per watt, which is essentially delivering the maximum throughput that is required under a power umbrella, which on devices such as robots, they matter a lot.
S Speaker 10:58fast forward to today for most of the cars that are going to ship on the planet going forward, are going to be using Snapdragon platforms, Qualcomm, Snapdragon platforms. So similarly, we think we're building a roadmap for robots. We're engaged with many, many robot companies. Some are humanoid, some are non humanoid form factors. And then, similarly, we want to build a solid platform and ecosystem on robots as well. And this is where Stan,
fast forward to today for most of the cars that are going to ship on the planet going forward, are going to be using Snapdragon platforms, Qualcomm, Snapdragon platforms. So similarly, we think we're building a roadmap for robots. We're engaged with many, many robot companies. Some are humanoid, some are non humanoid form factors. And then, similarly, we want to build a solid platform and ecosystem on robots as well. And this is where Stan,
fast forward to today for most of the cars that are going to ship on the planet going forward, are going to be using Snapdragon platforms, Qualcomm, Snapdragon platforms. So similarly, we think we're building a roadmap for robots. We're engaged with many, many robot companies. Some are humanoid, some are non humanoid form factors. And then, similarly, we want to build a solid platform and ecosystem on robots as well. And this is where Stan,
fast forward to today for most of the cars that are going to ship on the planet going forward, are going to be using Snapdragon platforms, Qualcomm, Snapdragon platforms. So similarly, we think we're building a roadmap for robots. We're engaged with many, many robot companies. Some are humanoid, some are non humanoid form factors. And then, similarly, we want to build a solid platform and ecosystem on robots as well. And this is where Stan,
1:32this was interesting. And
this was interesting. And
this was interesting. And
this was interesting. And
S Speaker 11:35what you're doing, there's a lot of talk in general about, okay, what's going to be the scale for scale, AI for robots. And then when rich told me that you're building something on the same lines, then I was like, Okay, let me I'm excited to hear what you're doing.
what you're doing, there's a lot of talk in general about, okay, what's going to be the scale for scale, AI for robots. And then when rich told me that you're building something on the same lines, then I was like, Okay, let me I'm excited to hear what you're doing.
what you're doing, there's a lot of talk in general about, okay, what's going to be the scale for scale, AI for robots. And then when rich told me that you're building something on the same lines, then I was like, Okay, let me I'm excited to hear what you're doing.
what you're doing, there's a lot of talk in general about, okay, what's going to be the scale for scale, AI for robots. And then when rich told me that you're building something on the same lines, then I was like, Okay, let me I'm excited to hear what you're doing.
S Speaker 31:52Great, great stuff. Yeah, so I'll tell you more. I'll tell you a little bit about myself first, and also make a quick introduction about me, and then we'll go exactly to what we're building,
Great, great stuff. Yeah, so I'll tell you more. I'll tell you a little bit about myself first, and also make a quick introduction about me, and then we'll go exactly to what we're building,
Great, great stuff. Yeah, so I'll tell you more. I'll tell you a little bit about myself first, and also make a quick introduction about me, and then we'll go exactly to what we're building,
Great, great stuff. Yeah, so I'll tell you more. I'll tell you a little bit about myself first, and also make a quick introduction about me, and then we'll go exactly to what we're building,
2:04why and how. I started
why and how. I started
why and how. I started
why and how. I started
S Speaker 32:08my first company when I was 13. I was in ad tech platform. This went really well throughout high school, and as I became an adult as well, where I, at that point, decided this is not really what I want to do for the rest of my life. And so, you know, I sold the major contracts. So you
my first company when I was 13. I was in ad tech platform. This went really well throughout high school, and as I became an adult as well, where I, at that point, decided this is not really what I want to do for the rest of my life. And so, you know, I sold the major contracts. So you
my first company when I was 13. I was in ad tech platform. This went really well throughout high school, and as I became an adult as well, where I, at that point, decided this is not really what I want to do for the rest of my life. And so, you know, I sold the major contracts. So you
my first company when I was 13. I was in ad tech platform. This went really well throughout high school, and as I became an adult as well, where I, at that point, decided this is not really what I want to do for the rest of my life. And so, you know, I sold the major contracts. So you
S Speaker 12:29started the first company in third at the age of 13. What was that company? So we
started the first company in third at the age of 13. What was that company? So we
started the first company in third at the age of 13. What was that company? So we
started the first company in third at the age of 13. What was that company? So we
S Speaker 32:34were essentially, it would build a or actually, I had built and have a co founder. I had built a platform where it was connecting advertisers all kinds of sizes. For example, we worked with Netflix, we worked with Amazon Prime Video, Apple TV, mostly media businesses, connecting them with advertisers that were seriously underserved, in the sense that they were basically, basically had no reach in the market. And at the time, those were kind of micro creators on Instagram, and then also on Tik Tok and so on. And we're talking about these media pages that were reaching billions of views every month, and nobody was paying for these views. So essentially, we built a place to connect them. And we were, because this was a very particular kind of advertising. We were also running these campaigns for a lot of these people. So for example, we ran a campaign for Netflix like they came in. They said, We want to promote this show, bird box. And, you know, we want to make it viral. And then we would go in, they would give us a budget, then we just go and prepare the whole thing for them and connect them with the right advertisers, like in a fully managed kind of ask for the service. It was doing quite well, but it didn't feel like this. What I wanted to do for this my life. I mean, we, did 650,000 ARR before I was 18, and so on. But it just didn't feel like this is what I want to do for the rest of my life, or for the next, like 1020, years. So at the time, I decided to sell off the biggest part of this business to the biggest competitor at the time and venture into something where I I think it's something I want to spend the next 1020, years and and that's when I got into robotics for the first time. And so in the process of, kind of becoming familiar with the field, learning everything to know, and, most importantly, learning what are the problems in the field? Because, of course, you cannot solve something if you don't know it's a problem. I spoke to 200 people, 190 something, people in the field who are industry experts, who are in academia or an industry, to kind of understand what is really holding back the industry, especially when it comes to humanoid robotics. Is it the hardware? Is it the control software? Is it compute? What is the fundamental problem? And they all basically said the same thing, which was, well, these are quite good already. Actually, the biggest problem is that the robots are stupid. They can't do anything. And then being deeper into this, I was like, Well, why are they stupid? Then what's the problem? And obviously the problem is that there's simply no data to train these models on. There's no data. There is no internet equivalent for the physical world, for manipulating the physical world, and for capturing the incredible nuance there is in simple motions like picking up a notebook might seem like such a, such a such a trivial thing to understand, and yet it packs so much information it's actually incredibly difficult to even describe to someone in full. And so in the process of speaking with a lot of people, I met my two co founders, dimitras canrules, who's the professor of AI and robotics at UCL in London, and Joey Humphreys, who's a PhD researcher. Was he finished his PhD in legged robotics, again, here in in London.
were essentially, it would build a or actually, I had built and have a co founder. I had built a platform where it was connecting advertisers all kinds of sizes. For example, we worked with Netflix, we worked with Amazon Prime Video, Apple TV, mostly media businesses, connecting them with advertisers that were seriously underserved, in the sense that they were basically, basically had no reach in the market. And at the time, those were kind of micro creators on Instagram, and then also on Tik Tok and so on. And we're talking about these media pages that were reaching billions of views every month, and nobody was paying for these views. So essentially, we built a place to connect them. And we were, because this was a very particular kind of advertising. We were also running these campaigns for a lot of these people. So for example, we ran a campaign for Netflix like they came in. They said, We want to promote this show, bird box. And, you know, we want to make it viral. And then we would go in, they would give us a budget, then we just go and prepare the whole thing for them and connect them with the right advertisers, like in a fully managed kind of ask for the service. It was doing quite well, but it didn't feel like this. What I wanted to do for this my life. I mean, we, did 650,000 ARR before I was 18, and so on. But it just didn't feel like this is what I want to do for the rest of my life, or for the next, like 1020, years. So at the time, I decided to sell off the biggest part of this business to the biggest competitor at the time and venture into something where I I think it's something I want to spend the next 1020, years and and that's when I got into robotics for the first time. And so in the process of, kind of becoming familiar with the field, learning everything to know, and, most importantly, learning what are the problems in the field? Because, of course, you cannot solve something if you don't know it's a problem. I spoke to 200 people, 190 something, people in the field who are industry experts, who are in academia or an industry, to kind of understand what is really holding back the industry, especially when it comes to humanoid robotics. Is it the hardware? Is it the control software? Is it compute? What is the fundamental problem? And they all basically said the same thing, which was, well, these are quite good already. Actually, the biggest problem is that the robots are stupid. They can't do anything. And then being deeper into this, I was like, Well, why are they stupid? Then what's the problem? And obviously the problem is that there's simply no data to train these models on. There's no data. There is no internet equivalent for the physical world, for manipulating the physical world, and for capturing the incredible nuance there is in simple motions like picking up a notebook might seem like such a, such a such a trivial thing to understand, and yet it packs so much information it's actually incredibly difficult to even describe to someone in full. And so in the process of speaking with a lot of people, I met my two co founders, dimitras canrules, who's the professor of AI and robotics at UCL in London, and Joey Humphreys, who's a PhD researcher. Was he finished his PhD in legged robotics, again, here in in London.
were essentially, it would build a or actually, I had built and have a co founder. I had built a platform where it was connecting advertisers all kinds of sizes. For example, we worked with Netflix, we worked with Amazon Prime Video, Apple TV, mostly media businesses, connecting them with advertisers that were seriously underserved, in the sense that they were basically, basically had no reach in the market. And at the time, those were kind of micro creators on Instagram, and then also on Tik Tok and so on. And we're talking about these media pages that were reaching billions of views every month, and nobody was paying for these views. So essentially, we built a place to connect them. And we were, because this was a very particular kind of advertising. We were also running these campaigns for a lot of these people. So for example, we ran a campaign for Netflix like they came in. They said, We want to promote this show, bird box. And, you know, we want to make it viral. And then we would go in, they would give us a budget, then we just go and prepare the whole thing for them and connect them with the right advertisers, like in a fully managed kind of ask for the service. It was doing quite well, but it didn't feel like this. What I wanted to do for this my life. I mean, we, did 650,000 ARR before I was 18, and so on. But it just didn't feel like this is what I want to do for the rest of my life, or for the next, like 1020, years. So at the time, I decided to sell off the biggest part of this business to the biggest competitor at the time and venture into something where I I think it's something I want to spend the next 1020, years and and that's when I got into robotics for the first time. And so in the process of, kind of becoming familiar with the field, learning everything to know, and, most importantly, learning what are the problems in the field? Because, of course, you cannot solve something if you don't know it's a problem. I spoke to 200 people, 190 something, people in the field who are industry experts, who are in academia or an industry, to kind of understand what is really holding back the industry, especially when it comes to humanoid robotics. Is it the hardware? Is it the control software? Is it compute? What is the fundamental problem? And they all basically said the same thing, which was, well, these are quite good already. Actually, the biggest problem is that the robots are stupid. They can't do anything. And then being deeper into this, I was like, Well, why are they stupid? Then what's the problem? And obviously the problem is that there's simply no data to train these models on. There's no data. There is no internet equivalent for the physical world, for manipulating the physical world, and for capturing the incredible nuance there is in simple motions like picking up a notebook might seem like such a, such a such a trivial thing to understand, and yet it packs so much information it's actually incredibly difficult to even describe to someone in full. And so in the process of speaking with a lot of people, I met my two co founders, dimitras canrules, who's the professor of AI and robotics at UCL in London, and Joey Humphreys, who's a PhD researcher. Was he finished his PhD in legged robotics, again, here in in London.
were essentially, it would build a or actually, I had built and have a co founder. I had built a platform where it was connecting advertisers all kinds of sizes. For example, we worked with Netflix, we worked with Amazon Prime Video, Apple TV, mostly media businesses, connecting them with advertisers that were seriously underserved, in the sense that they were basically, basically had no reach in the market. And at the time, those were kind of micro creators on Instagram, and then also on Tik Tok and so on. And we're talking about these media pages that were reaching billions of views every month, and nobody was paying for these views. So essentially, we built a place to connect them. And we were, because this was a very particular kind of advertising. We were also running these campaigns for a lot of these people. So for example, we ran a campaign for Netflix like they came in. They said, We want to promote this show, bird box. And, you know, we want to make it viral. And then we would go in, they would give us a budget, then we just go and prepare the whole thing for them and connect them with the right advertisers, like in a fully managed kind of ask for the service. It was doing quite well, but it didn't feel like this. What I wanted to do for this my life. I mean, we, did 650,000 ARR before I was 18, and so on. But it just didn't feel like this is what I want to do for the rest of my life, or for the next, like 1020, years. So at the time, I decided to sell off the biggest part of this business to the biggest competitor at the time and venture into something where I I think it's something I want to spend the next 1020, years and and that's when I got into robotics for the first time. And so in the process of, kind of becoming familiar with the field, learning everything to know, and, most importantly, learning what are the problems in the field? Because, of course, you cannot solve something if you don't know it's a problem. I spoke to 200 people, 190 something, people in the field who are industry experts, who are in academia or an industry, to kind of understand what is really holding back the industry, especially when it comes to humanoid robotics. Is it the hardware? Is it the control software? Is it compute? What is the fundamental problem? And they all basically said the same thing, which was, well, these are quite good already. Actually, the biggest problem is that the robots are stupid. They can't do anything. And then being deeper into this, I was like, Well, why are they stupid? Then what's the problem? And obviously the problem is that there's simply no data to train these models on. There's no data. There is no internet equivalent for the physical world, for manipulating the physical world, and for capturing the incredible nuance there is in simple motions like picking up a notebook might seem like such a, such a such a trivial thing to understand, and yet it packs so much information it's actually incredibly difficult to even describe to someone in full. And so in the process of speaking with a lot of people, I met my two co founders, dimitras canrules, who's the professor of AI and robotics at UCL in London, and Joey Humphreys, who's a PhD researcher. Was he finished his PhD in legged robotics, again, here in in London.
S Speaker 16:28Stan, are you based in London? I remember Rich mentioned you're moving here right to the bay.
Stan, are you based in London? I remember Rich mentioned you're moving here right to the bay.
Stan, are you based in London? I remember Rich mentioned you're moving here right to the bay.
Stan, are you based in London? I remember Rich mentioned you're moving here right to the bay.
S Speaker 36:34Yes, yeah. So currently we're, we're based in London, because we're speaking with a lot of these London based companies. But I mean, we're clearly moving to to the US, as it makes a more strategic sense. Yes. So where was I? We are kind of describing this, yeah. And so we kind of came together, and, you know, we spoke about this, like, what's the fundamental issue here, and what is that people don't understand? Because, see, the humanoid space is super trendy. There's a lot of hype around it. So a lot of people who maybe don't really understand what is the problem coming in and try building this. So for example, there's a great misunderstanding in what kind of data quality is needed for these robots. A lot of people, for example, think, okay, if I just go out and capture a million hours of video, or 2 million hours of video, just video, this is going to be enough for a robot to then take this information, generalize it, and be able to apply it in the real world. However, we think this is completely wrong. And not only do we think this, but I think, but the potential customers also think this. This video simply does not pack enough information. Also. Video does not pack actually packs quite a bit of information. It just doesn't pack it in a structured way, in a way that is easier to digest by these models, right? So if I give you a video as a human, you might be able to estimate the depth, for example, as a robot, you won't be able to do this. You need depth data. You might be able to estimate as a human kind, even if you have, like an object where you can't really see the hands, because you know this object, you might be able to infer what the hands kind of what the pose of the hands is. In that video, a robot can't do this. It cannot know what it cannot see. And so we are very, very focused on, instead of just scaling, getting your 10,000 hours of data in a factory where the it's basically the same task again and again, and it's just video data which doesn't back as much information. On getting the most information dense data that is possible, and getting it in the most information dense environments as well. And that's the domestic right inside homes and so on.
Yes, yeah. So currently we're, we're based in London, because we're speaking with a lot of these London based companies. But I mean, we're clearly moving to to the US, as it makes a more strategic sense. Yes. So where was I? We are kind of describing this, yeah. And so we kind of came together, and, you know, we spoke about this, like, what's the fundamental issue here, and what is that people don't understand? Because, see, the humanoid space is super trendy. There's a lot of hype around it. So a lot of people who maybe don't really understand what is the problem coming in and try building this. So for example, there's a great misunderstanding in what kind of data quality is needed for these robots. A lot of people, for example, think, okay, if I just go out and capture a million hours of video, or 2 million hours of video, just video, this is going to be enough for a robot to then take this information, generalize it, and be able to apply it in the real world. However, we think this is completely wrong. And not only do we think this, but I think, but the potential customers also think this. This video simply does not pack enough information. Also. Video does not pack actually packs quite a bit of information. It just doesn't pack it in a structured way, in a way that is easier to digest by these models, right? So if I give you a video as a human, you might be able to estimate the depth, for example, as a robot, you won't be able to do this. You need depth data. You might be able to estimate as a human kind, even if you have, like an object where you can't really see the hands, because you know this object, you might be able to infer what the hands kind of what the pose of the hands is. In that video, a robot can't do this. It cannot know what it cannot see. And so we are very, very focused on, instead of just scaling, getting your 10,000 hours of data in a factory where the it's basically the same task again and again, and it's just video data which doesn't back as much information. On getting the most information dense data that is possible, and getting it in the most information dense environments as well. And that's the domestic right inside homes and so on.
Yes, yeah. So currently we're, we're based in London, because we're speaking with a lot of these London based companies. But I mean, we're clearly moving to to the US, as it makes a more strategic sense. Yes. So where was I? We are kind of describing this, yeah. And so we kind of came together, and, you know, we spoke about this, like, what's the fundamental issue here, and what is that people don't understand? Because, see, the humanoid space is super trendy. There's a lot of hype around it. So a lot of people who maybe don't really understand what is the problem coming in and try building this. So for example, there's a great misunderstanding in what kind of data quality is needed for these robots. A lot of people, for example, think, okay, if I just go out and capture a million hours of video, or 2 million hours of video, just video, this is going to be enough for a robot to then take this information, generalize it, and be able to apply it in the real world. However, we think this is completely wrong. And not only do we think this, but I think, but the potential customers also think this. This video simply does not pack enough information. Also. Video does not pack actually packs quite a bit of information. It just doesn't pack it in a structured way, in a way that is easier to digest by these models, right? So if I give you a video as a human, you might be able to estimate the depth, for example, as a robot, you won't be able to do this. You need depth data. You might be able to estimate as a human kind, even if you have, like an object where you can't really see the hands, because you know this object, you might be able to infer what the hands kind of what the pose of the hands is. In that video, a robot can't do this. It cannot know what it cannot see. And so we are very, very focused on, instead of just scaling, getting your 10,000 hours of data in a factory where the it's basically the same task again and again, and it's just video data which doesn't back as much information. On getting the most information dense data that is possible, and getting it in the most information dense environments as well. And that's the domestic right inside homes and so on.
Yes, yeah. So currently we're, we're based in London, because we're speaking with a lot of these London based companies. But I mean, we're clearly moving to to the US, as it makes a more strategic sense. Yes. So where was I? We are kind of describing this, yeah. And so we kind of came together, and, you know, we spoke about this, like, what's the fundamental issue here, and what is that people don't understand? Because, see, the humanoid space is super trendy. There's a lot of hype around it. So a lot of people who maybe don't really understand what is the problem coming in and try building this. So for example, there's a great misunderstanding in what kind of data quality is needed for these robots. A lot of people, for example, think, okay, if I just go out and capture a million hours of video, or 2 million hours of video, just video, this is going to be enough for a robot to then take this information, generalize it, and be able to apply it in the real world. However, we think this is completely wrong. And not only do we think this, but I think, but the potential customers also think this. This video simply does not pack enough information. Also. Video does not pack actually packs quite a bit of information. It just doesn't pack it in a structured way, in a way that is easier to digest by these models, right? So if I give you a video as a human, you might be able to estimate the depth, for example, as a robot, you won't be able to do this. You need depth data. You might be able to estimate as a human kind, even if you have, like an object where you can't really see the hands, because you know this object, you might be able to infer what the hands kind of what the pose of the hands is. In that video, a robot can't do this. It cannot know what it cannot see. And so we are very, very focused on, instead of just scaling, getting your 10,000 hours of data in a factory where the it's basically the same task again and again, and it's just video data which doesn't back as much information. On getting the most information dense data that is possible, and getting it in the most information dense environments as well. And that's the domestic right inside homes and so on.
S Speaker 19:10And why do you call it? I'm so sure you're focused on inside home data for robots. Is that right today?
And why do you call it? I'm so sure you're focused on inside home data for robots. Is that right today?
And why do you call it? I'm so sure you're focused on inside home data for robots. Is that right today?
And why do you call it? I'm so sure you're focused on inside home data for robots. Is that right today?
S Speaker 39:18Yes, yes, we're actually also able to do industry and so on. It's just that our priority in collecting this data is in the harder to access, hardest action to access environment, which is
Yes, yes, we're actually also able to do industry and so on. It's just that our priority in collecting this data is in the harder to access, hardest action to access environment, which is
Yes, yes, we're actually also able to do industry and so on. It's just that our priority in collecting this data is in the harder to access, hardest action to access environment, which is
Yes, yes, we're actually also able to do industry and so on. It's just that our priority in collecting this data is in the harder to access, hardest action to access environment, which is
S Speaker 19:31the home Why do you think I would argue even the harder
the home Why do you think I would argue even the harder
the home Why do you think I would argue even the harder
the home Why do you think I would argue even the harder
9:35it's even harder to access
it's even harder to access
it's even harder to access
it's even harder to access
S Speaker 19:37the industrial data. Because, you know, essentially, you need to have a partnership with somebody who's working in an industrial environment, and then being able to deploy it and in that industrial environment.
the industrial data. Because, you know, essentially, you need to have a partnership with somebody who's working in an industrial environment, and then being able to deploy it and in that industrial environment.
the industrial data. Because, you know, essentially, you need to have a partnership with somebody who's working in an industrial environment, and then being able to deploy it and in that industrial environment.
the industrial data. Because, you know, essentially, you need to have a partnership with somebody who's working in an industrial environment, and then being able to deploy it and in that industrial environment.
S Speaker 39:47But you only need one partnership to get the same number of data in the home. You're getting 1000 partnerships with 1000 homeowners or with 1000 essentially locations, right? But for the factory, I make one I make one partnership. I get 1000 data points. No.
But you only need one partnership to get the same number of data in the home. You're getting 1000 partnerships with 1000 homeowners or with 1000 essentially locations, right? But for the factory, I make one I make one partnership. I get 1000 data points. No.
But you only need one partnership to get the same number of data in the home. You're getting 1000 partnerships with 1000 homeowners or with 1000 essentially locations, right? But for the factory, I make one I make one partnership. I get 1000 data points. No.
But you only need one partnership to get the same number of data in the home. You're getting 1000 partnerships with 1000 homeowners or with 1000 essentially locations, right? But for the factory, I make one I make one partnership. I get 1000 data points. No.
S Speaker 110:05So what about you renting apartments yourself? And if you have the capital, then you don't essentially need a partnership, right? You can. So like, like, you've seen physical intelligence bigger, AI, all these people, it's public. They're ranking Airbnbs
So what about you renting apartments yourself? And if you have the capital, then you don't essentially need a partnership, right? You can. So like, like, you've seen physical intelligence bigger, AI, all these people, it's public. They're ranking Airbnbs
So what about you renting apartments yourself? And if you have the capital, then you don't essentially need a partnership, right? You can. So like, like, you've seen physical intelligence bigger, AI, all these people, it's public. They're ranking Airbnbs
So what about you renting apartments yourself? And if you have the capital, then you don't essentially need a partnership, right? You can. So like, like, you've seen physical intelligence bigger, AI, all these people, it's public. They're ranking Airbnbs
10:25and collecting data from there in home.
and collecting data from there in home.
and collecting data from there in home.
and collecting data from there in home.
S Speaker 310:27Yes. So, yes. So I also think this is not going to be you can support this at the scale. You need this data because, like, the true scale of data required is absurd, like we're talking about internet scale, physical world data, I don't think you're going to be able to rent your way there. So what we're doing for that instead, is we are creating partnerships with real estate portfolio companies who you know, they might own 2000 residential units in one city, or 5000 residential units in one city and so on, and we essentially take over their maintenance procedures. So how they would normally send, for example, housekeeping stuff themselves, and we either take it over entirely, or we subsidize it so they still use their own stuff stuff, but it's that these the stuff is now sensorized With our kids to collect the data that way
Yes. So, yes. So I also think this is not going to be you can support this at the scale. You need this data because, like, the true scale of data required is absurd, like we're talking about internet scale, physical world data, I don't think you're going to be able to rent your way there. So what we're doing for that instead, is we are creating partnerships with real estate portfolio companies who you know, they might own 2000 residential units in one city, or 5000 residential units in one city and so on, and we essentially take over their maintenance procedures. So how they would normally send, for example, housekeeping stuff themselves, and we either take it over entirely, or we subsidize it so they still use their own stuff stuff, but it's that these the stuff is now sensorized With our kids to collect the data that way
Yes. So, yes. So I also think this is not going to be you can support this at the scale. You need this data because, like, the true scale of data required is absurd, like we're talking about internet scale, physical world data, I don't think you're going to be able to rent your way there. So what we're doing for that instead, is we are creating partnerships with real estate portfolio companies who you know, they might own 2000 residential units in one city, or 5000 residential units in one city and so on, and we essentially take over their maintenance procedures. So how they would normally send, for example, housekeeping stuff themselves, and we either take it over entirely, or we subsidize it so they still use their own stuff stuff, but it's that these the stuff is now sensorized With our kids to collect the data that way
Yes. So, yes. So I also think this is not going to be you can support this at the scale. You need this data because, like, the true scale of data required is absurd, like we're talking about internet scale, physical world data, I don't think you're going to be able to rent your way there. So what we're doing for that instead, is we are creating partnerships with real estate portfolio companies who you know, they might own 2000 residential units in one city, or 5000 residential units in one city and so on, and we essentially take over their maintenance procedures. So how they would normally send, for example, housekeeping stuff themselves, and we either take it over entirely, or we subsidize it so they still use their own stuff stuff, but it's that these the stuff is now sensorized With our kids to collect the data that way
S Speaker 111:30you elaborate a little more? I get it, but when you say you need internet, scale, physical, yes. Can you elaborate a little more. Why? Because I don't need the knowledge of the world. I don't need to solve math problems. I don't need to solve physics problems. I don't need to know the history of mankind, and I don't need to know the latest news. So where, why do you still say it's internet for physical tasks?
you elaborate a little more? I get it, but when you say you need internet, scale, physical, yes. Can you elaborate a little more. Why? Because I don't need the knowledge of the world. I don't need to solve math problems. I don't need to solve physics problems. I don't need to know the history of mankind, and I don't need to know the latest news. So where, why do you still say it's internet for physical tasks?
you elaborate a little more? I get it, but when you say you need internet, scale, physical, yes. Can you elaborate a little more. Why? Because I don't need the knowledge of the world. I don't need to solve math problems. I don't need to solve physics problems. I don't need to know the history of mankind, and I don't need to know the latest news. So where, why do you still say it's internet for physical tasks?
you elaborate a little more? I get it, but when you say you need internet, scale, physical, yes. Can you elaborate a little more. Why? Because I don't need the knowledge of the world. I don't need to solve math problems. I don't need to solve physics problems. I don't need to know the history of mankind, and I don't need to know the latest news. So where, why do you still say it's internet for physical tasks?
S Speaker 312:06Because you have to solve something much more so essentially, when you when you have large language models, for example, and they need to solve a math problem, in reality, they're not solving the math problem. They're simply next token predictors, and they're essentially guessing what the correct answer would look like, but at least figuring out, okay, which tool to call. But these days, exactly they figure Exactly. They figure out which, which cool to call, which tool to call, and so on. The thing is, these tools that they call have a very, very well defined description of what they do, so the model can tell Okay, so the user asked me x thing I need to call y tool, because I know that y tool gives me set output. The difference with the physical world is, first of all, there's basically two differences. One is what we call affordances. If I see a glass on the floor, I don't need to know something. I can I understand that this is not the place of the glass without anyone telling me something and that I have to go pick it up. Now, for a robot, this is incredibly difficult to understand just by seeing a glass on the floor, very difficult, because this, this simple image, packs so much information. Like, first of all, you need to know what a glass is. Secondly, you need to know where a glass normally is. Then you need to know that a glass being on the floor is in an appropriate position, and it's dangerous, and it's all this kind of things, so that you then know that, oh, I have to pick it up, and then you have to go and actually pick it up and place it where it's supposed to be.
Because you have to solve something much more so essentially, when you when you have large language models, for example, and they need to solve a math problem, in reality, they're not solving the math problem. They're simply next token predictors, and they're essentially guessing what the correct answer would look like, but at least figuring out, okay, which tool to call. But these days, exactly they figure Exactly. They figure out which, which cool to call, which tool to call, and so on. The thing is, these tools that they call have a very, very well defined description of what they do, so the model can tell Okay, so the user asked me x thing I need to call y tool, because I know that y tool gives me set output. The difference with the physical world is, first of all, there's basically two differences. One is what we call affordances. If I see a glass on the floor, I don't need to know something. I can I understand that this is not the place of the glass without anyone telling me something and that I have to go pick it up. Now, for a robot, this is incredibly difficult to understand just by seeing a glass on the floor, very difficult, because this, this simple image, packs so much information. Like, first of all, you need to know what a glass is. Secondly, you need to know where a glass normally is. Then you need to know that a glass being on the floor is in an appropriate position, and it's dangerous, and it's all this kind of things, so that you then know that, oh, I have to pick it up, and then you have to go and actually pick it up and place it where it's supposed to be.
Because you have to solve something much more so essentially, when you when you have large language models, for example, and they need to solve a math problem, in reality, they're not solving the math problem. They're simply next token predictors, and they're essentially guessing what the correct answer would look like, but at least figuring out, okay, which tool to call. But these days, exactly they figure Exactly. They figure out which, which cool to call, which tool to call, and so on. The thing is, these tools that they call have a very, very well defined description of what they do, so the model can tell Okay, so the user asked me x thing I need to call y tool, because I know that y tool gives me set output. The difference with the physical world is, first of all, there's basically two differences. One is what we call affordances. If I see a glass on the floor, I don't need to know something. I can I understand that this is not the place of the glass without anyone telling me something and that I have to go pick it up. Now, for a robot, this is incredibly difficult to understand just by seeing a glass on the floor, very difficult, because this, this simple image, packs so much information. Like, first of all, you need to know what a glass is. Secondly, you need to know where a glass normally is. Then you need to know that a glass being on the floor is in an appropriate position, and it's dangerous, and it's all this kind of things, so that you then know that, oh, I have to pick it up, and then you have to go and actually pick it up and place it where it's supposed to be.
Because you have to solve something much more so essentially, when you when you have large language models, for example, and they need to solve a math problem, in reality, they're not solving the math problem. They're simply next token predictors, and they're essentially guessing what the correct answer would look like, but at least figuring out, okay, which tool to call. But these days, exactly they figure Exactly. They figure out which, which cool to call, which tool to call, and so on. The thing is, these tools that they call have a very, very well defined description of what they do, so the model can tell Okay, so the user asked me x thing I need to call y tool, because I know that y tool gives me set output. The difference with the physical world is, first of all, there's basically two differences. One is what we call affordances. If I see a glass on the floor, I don't need to know something. I can I understand that this is not the place of the glass without anyone telling me something and that I have to go pick it up. Now, for a robot, this is incredibly difficult to understand just by seeing a glass on the floor, very difficult, because this, this simple image, packs so much information. Like, first of all, you need to know what a glass is. Secondly, you need to know where a glass normally is. Then you need to know that a glass being on the floor is in an appropriate position, and it's dangerous, and it's all this kind of things, so that you then know that, oh, I have to pick it up, and then you have to go and actually pick it up and place it where it's supposed to be.
13:48And that's very difficult. That is very, very difficult,
And that's very difficult. That is very, very difficult,
And that's very difficult. That is very, very difficult,
And that's very difficult. That is very, very difficult,
S Speaker 313:54to kind of transfer this information to a robot that's one two is, instead of pretending you understand math and physics, you actually need to understand math and physics, because you need to know what happens in the physical world when you manipulate it. And this to humans, you know, it's super kind of intuitive, like, I know, for example, have this notebook here. I know that if I hold this notebook here and I leave it, it's going to fall, and I can put my hand here to catch it. But this has come with millions and millions of years of evolution that humans are so able to intuitively understand that okay, if I drop this notebook, I can catch it, and I have a true understanding of the physics, and I don't even have to talk to you about what forces are being applied on this book and so on, and you know, what's the gravity vector and stuff like that, because I don't need to. But what I'm doing has so much information in it that it is simply not as it's subconscious, subconscious understanding of physics, which is actually incredible, humans and actually all living things, having an incredible subconscious understanding of physics that is very, very difficult to pass on to machines. So I agree with you that you don't need to know the news and don't need to know extra how to solve, you know, Frontier math equations. But you definitely have to have intimate skill data, because things that for us as humans come super intuitive and are so simple. And we're like, we don't even need data for this, like we I don't go to school to know if I drop a notebook, I can catch it with this hand, like this. And yet, there's so much information this simple in this one move. I just did that machines to simply do not have currently the capacity to
to kind of transfer this information to a robot that's one two is, instead of pretending you understand math and physics, you actually need to understand math and physics, because you need to know what happens in the physical world when you manipulate it. And this to humans, you know, it's super kind of intuitive, like, I know, for example, have this notebook here. I know that if I hold this notebook here and I leave it, it's going to fall, and I can put my hand here to catch it. But this has come with millions and millions of years of evolution that humans are so able to intuitively understand that okay, if I drop this notebook, I can catch it, and I have a true understanding of the physics, and I don't even have to talk to you about what forces are being applied on this book and so on, and you know, what's the gravity vector and stuff like that, because I don't need to. But what I'm doing has so much information in it that it is simply not as it's subconscious, subconscious understanding of physics, which is actually incredible, humans and actually all living things, having an incredible subconscious understanding of physics that is very, very difficult to pass on to machines. So I agree with you that you don't need to know the news and don't need to know extra how to solve, you know, Frontier math equations. But you definitely have to have intimate skill data, because things that for us as humans come super intuitive and are so simple. And we're like, we don't even need data for this, like we I don't go to school to know if I drop a notebook, I can catch it with this hand, like this. And yet, there's so much information this simple in this one move. I just did that machines to simply do not have currently the capacity to
to kind of transfer this information to a robot that's one two is, instead of pretending you understand math and physics, you actually need to understand math and physics, because you need to know what happens in the physical world when you manipulate it. And this to humans, you know, it's super kind of intuitive, like, I know, for example, have this notebook here. I know that if I hold this notebook here and I leave it, it's going to fall, and I can put my hand here to catch it. But this has come with millions and millions of years of evolution that humans are so able to intuitively understand that okay, if I drop this notebook, I can catch it, and I have a true understanding of the physics, and I don't even have to talk to you about what forces are being applied on this book and so on, and you know, what's the gravity vector and stuff like that, because I don't need to. But what I'm doing has so much information in it that it is simply not as it's subconscious, subconscious understanding of physics, which is actually incredible, humans and actually all living things, having an incredible subconscious understanding of physics that is very, very difficult to pass on to machines. So I agree with you that you don't need to know the news and don't need to know extra how to solve, you know, Frontier math equations. But you definitely have to have intimate skill data, because things that for us as humans come super intuitive and are so simple. And we're like, we don't even need data for this, like we I don't go to school to know if I drop a notebook, I can catch it with this hand, like this. And yet, there's so much information this simple in this one move. I just did that machines to simply do not have currently the capacity to
to kind of transfer this information to a robot that's one two is, instead of pretending you understand math and physics, you actually need to understand math and physics, because you need to know what happens in the physical world when you manipulate it. And this to humans, you know, it's super kind of intuitive, like, I know, for example, have this notebook here. I know that if I hold this notebook here and I leave it, it's going to fall, and I can put my hand here to catch it. But this has come with millions and millions of years of evolution that humans are so able to intuitively understand that okay, if I drop this notebook, I can catch it, and I have a true understanding of the physics, and I don't even have to talk to you about what forces are being applied on this book and so on, and you know, what's the gravity vector and stuff like that, because I don't need to. But what I'm doing has so much information in it that it is simply not as it's subconscious, subconscious understanding of physics, which is actually incredible, humans and actually all living things, having an incredible subconscious understanding of physics that is very, very difficult to pass on to machines. So I agree with you that you don't need to know the news and don't need to know extra how to solve, you know, Frontier math equations. But you definitely have to have intimate skill data, because things that for us as humans come super intuitive and are so simple. And we're like, we don't even need data for this, like we I don't go to school to know if I drop a notebook, I can catch it with this hand, like this. And yet, there's so much information this simple in this one move. I just did that machines to simply do not have currently the capacity to
S Speaker 115:49replicate it. Those are very good examples. Yeah, thank you. No problem. Yeah. Okay, so now what's your plan to get this data? Which is, I do you have your own custom hardware? What are different types of data that you're trying to collect? And then the other part of the I guess, as you tell that, think about which is, you know, as we engage with many companies that are building, there's there's companies that are building now I would, in fact, I would say companies that were focused on building the brains are also just most companies are going and saying that I'm going to do the vertical, I'm going to be the apple of these Robots, right, which is, I'm going to build a robot. I'm also going to build the model that runs the robot, whether it's VLA or something else, and I'm also going to go collect my own data, because that's where I think that my my note, everybody's saying that my mode is data collection, exactly. So that's a GTM question, and then maybe a tech perspective, which is, what's your perspective over there? But the first question is, how you like what's your plan to solve this problem statement that you described?
replicate it. Those are very good examples. Yeah, thank you. No problem. Yeah. Okay, so now what's your plan to get this data? Which is, I do you have your own custom hardware? What are different types of data that you're trying to collect? And then the other part of the I guess, as you tell that, think about which is, you know, as we engage with many companies that are building, there's there's companies that are building now I would, in fact, I would say companies that were focused on building the brains are also just most companies are going and saying that I'm going to do the vertical, I'm going to be the apple of these Robots, right, which is, I'm going to build a robot. I'm also going to build the model that runs the robot, whether it's VLA or something else, and I'm also going to go collect my own data, because that's where I think that my my note, everybody's saying that my mode is data collection, exactly. So that's a GTM question, and then maybe a tech perspective, which is, what's your perspective over there? But the first question is, how you like what's your plan to solve this problem statement that you described?
replicate it. Those are very good examples. Yeah, thank you. No problem. Yeah. Okay, so now what's your plan to get this data? Which is, I do you have your own custom hardware? What are different types of data that you're trying to collect? And then the other part of the I guess, as you tell that, think about which is, you know, as we engage with many companies that are building, there's there's companies that are building now I would, in fact, I would say companies that were focused on building the brains are also just most companies are going and saying that I'm going to do the vertical, I'm going to be the apple of these Robots, right, which is, I'm going to build a robot. I'm also going to build the model that runs the robot, whether it's VLA or something else, and I'm also going to go collect my own data, because that's where I think that my my note, everybody's saying that my mode is data collection, exactly. So that's a GTM question, and then maybe a tech perspective, which is, what's your perspective over there? But the first question is, how you like what's your plan to solve this problem statement that you described?
replicate it. Those are very good examples. Yeah, thank you. No problem. Yeah. Okay, so now what's your plan to get this data? Which is, I do you have your own custom hardware? What are different types of data that you're trying to collect? And then the other part of the I guess, as you tell that, think about which is, you know, as we engage with many companies that are building, there's there's companies that are building now I would, in fact, I would say companies that were focused on building the brains are also just most companies are going and saying that I'm going to do the vertical, I'm going to be the apple of these Robots, right, which is, I'm going to build a robot. I'm also going to build the model that runs the robot, whether it's VLA or something else, and I'm also going to go collect my own data, because that's where I think that my my note, everybody's saying that my mode is data collection, exactly. So that's a GTM question, and then maybe a tech perspective, which is, what's your perspective over there? But the first question is, how you like what's your plan to solve this problem statement that you described?
S Speaker 317:15So yes, we've built custom hardware and custom software to collect this data. We've built our own hand models, which are VR level hand models for hand tracking. We've built a vest that packs on board compute. It has a depth cameras. It has stereo vision cameras. It we are also now exploring actually extending the vest to capture IMU data or hands, although we're we're still considering this, because we, I do think we're able to get better accuracy by simply improving vision models, rather than adding more hardware. We have a very lightweight setup. Our vest weighs less than a kilogram. Actually, no, sorry, now it was almost a kilogram. And we are in the process of redesigning a lot of these things as well, because now we did, for example, our first run. So I was talking to you about this just in the beginning of the call. So now we were preparing a data sample for DeepMind, Google, DeepMind here in London. And so in that run, we basically, we basically recorded incredible amounts of information for these vests, and like, what works well with it, what doesn't work well with it, and now we're in the process of actually redesigning it to, instead of being a vest, it's essentially kind of like a shirt, a padded shirt, which packs the exact same hardware as before, with upgraded compute. It feels more lightweight and actually captures better, better angles more easily. This is, this is our hardware, but really the secret sauce comes not on the hardware, but rather in the software. So first of all, as I said, we have proprietary hand models, which are VR state hand models. We're not using media pipe, for example, as a lot of other companies are. And then the most important thing is that we've we're able to essentially collect and process all of this data at runtime. So when we collect the data, we're able to process at the time. We're able to do the hand prediction immediately, frame by frame for all our frames. Right? Every single frame we record is fully annotated. We collect depth we collect risk trajectories, the, I'd say, the most important thing for manipulation tasks. We collect IMU data. We collect body pose estimation as well. We just actually piloted this last week. And of course, we collect also RGB frames and then fully annotate this as well. So we do multiple steps of annotation, human annotation, which eventually, as we get more data in, will become largely automated. So the first step is obviously action notation, like pickup notebook. We do two levels of that. The first level is like Log Horizon tasks, like organized desks, and then under organized desk, you have pickup notebook, pickup and put laptop in place, and so on.
So yes, we've built custom hardware and custom software to collect this data. We've built our own hand models, which are VR level hand models for hand tracking. We've built a vest that packs on board compute. It has a depth cameras. It has stereo vision cameras. It we are also now exploring actually extending the vest to capture IMU data or hands, although we're we're still considering this, because we, I do think we're able to get better accuracy by simply improving vision models, rather than adding more hardware. We have a very lightweight setup. Our vest weighs less than a kilogram. Actually, no, sorry, now it was almost a kilogram. And we are in the process of redesigning a lot of these things as well, because now we did, for example, our first run. So I was talking to you about this just in the beginning of the call. So now we were preparing a data sample for DeepMind, Google, DeepMind here in London. And so in that run, we basically, we basically recorded incredible amounts of information for these vests, and like, what works well with it, what doesn't work well with it, and now we're in the process of actually redesigning it to, instead of being a vest, it's essentially kind of like a shirt, a padded shirt, which packs the exact same hardware as before, with upgraded compute. It feels more lightweight and actually captures better, better angles more easily. This is, this is our hardware, but really the secret sauce comes not on the hardware, but rather in the software. So first of all, as I said, we have proprietary hand models, which are VR state hand models. We're not using media pipe, for example, as a lot of other companies are. And then the most important thing is that we've we're able to essentially collect and process all of this data at runtime. So when we collect the data, we're able to process at the time. We're able to do the hand prediction immediately, frame by frame for all our frames. Right? Every single frame we record is fully annotated. We collect depth we collect risk trajectories, the, I'd say, the most important thing for manipulation tasks. We collect IMU data. We collect body pose estimation as well. We just actually piloted this last week. And of course, we collect also RGB frames and then fully annotate this as well. So we do multiple steps of annotation, human annotation, which eventually, as we get more data in, will become largely automated. So the first step is obviously action notation, like pickup notebook. We do two levels of that. The first level is like Log Horizon tasks, like organized desks, and then under organized desk, you have pickup notebook, pickup and put laptop in place, and so on.
So yes, we've built custom hardware and custom software to collect this data. We've built our own hand models, which are VR level hand models for hand tracking. We've built a vest that packs on board compute. It has a depth cameras. It has stereo vision cameras. It we are also now exploring actually extending the vest to capture IMU data or hands, although we're we're still considering this, because we, I do think we're able to get better accuracy by simply improving vision models, rather than adding more hardware. We have a very lightweight setup. Our vest weighs less than a kilogram. Actually, no, sorry, now it was almost a kilogram. And we are in the process of redesigning a lot of these things as well, because now we did, for example, our first run. So I was talking to you about this just in the beginning of the call. So now we were preparing a data sample for DeepMind, Google, DeepMind here in London. And so in that run, we basically, we basically recorded incredible amounts of information for these vests, and like, what works well with it, what doesn't work well with it, and now we're in the process of actually redesigning it to, instead of being a vest, it's essentially kind of like a shirt, a padded shirt, which packs the exact same hardware as before, with upgraded compute. It feels more lightweight and actually captures better, better angles more easily. This is, this is our hardware, but really the secret sauce comes not on the hardware, but rather in the software. So first of all, as I said, we have proprietary hand models, which are VR state hand models. We're not using media pipe, for example, as a lot of other companies are. And then the most important thing is that we've we're able to essentially collect and process all of this data at runtime. So when we collect the data, we're able to process at the time. We're able to do the hand prediction immediately, frame by frame for all our frames. Right? Every single frame we record is fully annotated. We collect depth we collect risk trajectories, the, I'd say, the most important thing for manipulation tasks. We collect IMU data. We collect body pose estimation as well. We just actually piloted this last week. And of course, we collect also RGB frames and then fully annotate this as well. So we do multiple steps of annotation, human annotation, which eventually, as we get more data in, will become largely automated. So the first step is obviously action notation, like pickup notebook. We do two levels of that. The first level is like Log Horizon tasks, like organized desks, and then under organized desk, you have pickup notebook, pickup and put laptop in place, and so on.
So yes, we've built custom hardware and custom software to collect this data. We've built our own hand models, which are VR level hand models for hand tracking. We've built a vest that packs on board compute. It has a depth cameras. It has stereo vision cameras. It we are also now exploring actually extending the vest to capture IMU data or hands, although we're we're still considering this, because we, I do think we're able to get better accuracy by simply improving vision models, rather than adding more hardware. We have a very lightweight setup. Our vest weighs less than a kilogram. Actually, no, sorry, now it was almost a kilogram. And we are in the process of redesigning a lot of these things as well, because now we did, for example, our first run. So I was talking to you about this just in the beginning of the call. So now we were preparing a data sample for DeepMind, Google, DeepMind here in London. And so in that run, we basically, we basically recorded incredible amounts of information for these vests, and like, what works well with it, what doesn't work well with it, and now we're in the process of actually redesigning it to, instead of being a vest, it's essentially kind of like a shirt, a padded shirt, which packs the exact same hardware as before, with upgraded compute. It feels more lightweight and actually captures better, better angles more easily. This is, this is our hardware, but really the secret sauce comes not on the hardware, but rather in the software. So first of all, as I said, we have proprietary hand models, which are VR state hand models. We're not using media pipe, for example, as a lot of other companies are. And then the most important thing is that we've we're able to essentially collect and process all of this data at runtime. So when we collect the data, we're able to process at the time. We're able to do the hand prediction immediately, frame by frame for all our frames. Right? Every single frame we record is fully annotated. We collect depth we collect risk trajectories, the, I'd say, the most important thing for manipulation tasks. We collect IMU data. We collect body pose estimation as well. We just actually piloted this last week. And of course, we collect also RGB frames and then fully annotate this as well. So we do multiple steps of annotation, human annotation, which eventually, as we get more data in, will become largely automated. So the first step is obviously action notation, like pickup notebook. We do two levels of that. The first level is like Log Horizon tasks, like organized desks, and then under organized desk, you have pickup notebook, pickup and put laptop in place, and so on.
20:40And then we also do
S Speaker 320:42key point annotation, like, for example, when I'm holding this essentially the key point for my for my left hand. Here is here, because basically this is the point where I'm holding and this is where we should point the attention of the robot to. And then we also do finger state. So for example, when I'm holding this notebook like this, essentially the fingers that are active are these three and the thumb, the pinky, is not active. So we basically say these are the active fingers in this in this frame. And we do this for 100%
key point annotation, like, for example, when I'm holding this essentially the key point for my for my left hand. Here is here, because basically this is the point where I'm holding and this is where we should point the attention of the robot to. And then we also do finger state. So for example, when I'm holding this notebook like this, essentially the fingers that are active are these three and the thumb, the pinky, is not active. So we basically say these are the active fingers in this in this frame. And we do this for 100%
key point annotation, like, for example, when I'm holding this essentially the key point for my for my left hand. Here is here, because basically this is the point where I'm holding and this is where we should point the attention of the robot to. And then we also do finger state. So for example, when I'm holding this notebook like this, essentially the fingers that are active are these three and the thumb, the pinky, is not active. So we basically say these are the active fingers in this in this frame. And we do this for 100%
key point annotation, like, for example, when I'm holding this essentially the key point for my for my left hand. Here is here, because basically this is the point where I'm holding and this is where we should point the attention of the robot to. And then we also do finger state. So for example, when I'm holding this notebook like this, essentially the fingers that are active are these three and the thumb, the pinky, is not active. So we basically say these are the active fingers in this in this frame. And we do this for 100%
21:18of the data that we provide,
of the data that we provide,
of the data that we provide,
of the data that we provide,
S Speaker 321:22and to do this just to before, before we also have a custom annotation suite where we do this annotation, which we built in house from scratch and and basically every hour that's annotated goes back into our model, so that it makes the next hour of annotation easier and easier and eventually automated,
and to do this just to before, before we also have a custom annotation suite where we do this annotation, which we built in house from scratch and and basically every hour that's annotated goes back into our model, so that it makes the next hour of annotation easier and easier and eventually automated,
and to do this just to before, before we also have a custom annotation suite where we do this annotation, which we built in house from scratch and and basically every hour that's annotated goes back into our model, so that it makes the next hour of annotation easier and easier and eventually automated,
and to do this just to before, before we also have a custom annotation suite where we do this annotation, which we built in house from scratch and and basically every hour that's annotated goes back into our model, so that it makes the next hour of annotation easier and easier and eventually automated,
S Speaker 121:45and with some of the examples that you gave, which is holding something
and with some of the examples that you gave, which is holding something
and with some of the examples that you gave, which is holding something
and with some of the examples that you gave, which is holding something
21:50or what was the finger state, do
or what was the finger state, do
or what was the finger state, do
or what was the finger state, do
21:55you also have like
21:56pressure sensors or force sensors
pressure sensors or force sensors
pressure sensors or force sensors
pressure sensors or force sensors
21:59to collect that data as well. No.
to collect that data as well. No.
to collect that data as well. No.
to collect that data as well. No.
S Speaker 322:03So right now we don't have forced sensors, but what we're going to do is we're now preparing to set up basically a bespoke way of data collection, where instead of actually being, actually going into real world, into the real world, and collecting this data as we currently are, we're also going to have a small team of people who are specifically collecting the highest quality of data that is possible. Because we don't care about hardware, like when you deploy in the real world, you care like you can't put too many sensors on someone because then they won't be able to do their job right. But if their job is simply to data collect, then you can put a lot more sensors per person. And you can have things like, you know, pressure sensors for sensors, stuff like that, as well as more cameras every compute, you know, leopard things like that. And you can also do custom task design, which is very useful for a lot of these robotics companies. Can you know, because as they're preparing for, like, a lot of demos, or maybe they're preparing for a specific pilot, they need the robot have certain skills which might not be widely available or widely accessible in the real world, so you have to set up a custom environment for them, essentially, to go and collect this data. And the idea is, we collect, we collect this super high quality data in house, and then we use it to essentially train models to be able to estimate all this data based on vision. So like essentially you can, you can estimate how much force someone is using to lift an object if you know what that object is. So if you have and how much force it usually takes to lift it right, laptops weigh pretty much the same across categories. MacBooks weigh exactly the same, like across kind of product lines and so on. So we're certain we can get this just with vision. And the idea is to get most of this with vision, because vision is super, super scalable. Everything else is not so, but right now, vision is not accurate enough, so we can't just launch with vision, because not accurate enough. But that's what we want. We want to do just vision, because it's the it's the most scalable thing you can possibly get to. So the idea is we collect a lot of this data, and we build models that are able to predict those things that we can't possibly launch at scale, like I can't have, you know. So you can't put gloves with like, four sensors to every person that you work with. First of all, it's super expensive, if you have, like, let's say, 10,000 20,000 100,000 data collectors. It's super expensive to literally give them four sensing gloves. This is expensive hardware, and so it's harder to scale, but also the this, these, this hardware comes with a lot of things like drift and all these kind of other components that are much, much harder to manage at scale. But if you have like a centralized environment, then obviously these are very, very manageable. And you collect this data, build the models, and then you estimate these things with vision.
So right now we don't have forced sensors, but what we're going to do is we're now preparing to set up basically a bespoke way of data collection, where instead of actually being, actually going into real world, into the real world, and collecting this data as we currently are, we're also going to have a small team of people who are specifically collecting the highest quality of data that is possible. Because we don't care about hardware, like when you deploy in the real world, you care like you can't put too many sensors on someone because then they won't be able to do their job right. But if their job is simply to data collect, then you can put a lot more sensors per person. And you can have things like, you know, pressure sensors for sensors, stuff like that, as well as more cameras every compute, you know, leopard things like that. And you can also do custom task design, which is very useful for a lot of these robotics companies. Can you know, because as they're preparing for, like, a lot of demos, or maybe they're preparing for a specific pilot, they need the robot have certain skills which might not be widely available or widely accessible in the real world, so you have to set up a custom environment for them, essentially, to go and collect this data. And the idea is, we collect, we collect this super high quality data in house, and then we use it to essentially train models to be able to estimate all this data based on vision. So like essentially you can, you can estimate how much force someone is using to lift an object if you know what that object is. So if you have and how much force it usually takes to lift it right, laptops weigh pretty much the same across categories. MacBooks weigh exactly the same, like across kind of product lines and so on. So we're certain we can get this just with vision. And the idea is to get most of this with vision, because vision is super, super scalable. Everything else is not so, but right now, vision is not accurate enough, so we can't just launch with vision, because not accurate enough. But that's what we want. We want to do just vision, because it's the it's the most scalable thing you can possibly get to. So the idea is we collect a lot of this data, and we build models that are able to predict those things that we can't possibly launch at scale, like I can't have, you know. So you can't put gloves with like, four sensors to every person that you work with. First of all, it's super expensive, if you have, like, let's say, 10,000 20,000 100,000 data collectors. It's super expensive to literally give them four sensing gloves. This is expensive hardware, and so it's harder to scale, but also the this, these, this hardware comes with a lot of things like drift and all these kind of other components that are much, much harder to manage at scale. But if you have like a centralized environment, then obviously these are very, very manageable. And you collect this data, build the models, and then you estimate these things with vision.
So right now we don't have forced sensors, but what we're going to do is we're now preparing to set up basically a bespoke way of data collection, where instead of actually being, actually going into real world, into the real world, and collecting this data as we currently are, we're also going to have a small team of people who are specifically collecting the highest quality of data that is possible. Because we don't care about hardware, like when you deploy in the real world, you care like you can't put too many sensors on someone because then they won't be able to do their job right. But if their job is simply to data collect, then you can put a lot more sensors per person. And you can have things like, you know, pressure sensors for sensors, stuff like that, as well as more cameras every compute, you know, leopard things like that. And you can also do custom task design, which is very useful for a lot of these robotics companies. Can you know, because as they're preparing for, like, a lot of demos, or maybe they're preparing for a specific pilot, they need the robot have certain skills which might not be widely available or widely accessible in the real world, so you have to set up a custom environment for them, essentially, to go and collect this data. And the idea is, we collect, we collect this super high quality data in house, and then we use it to essentially train models to be able to estimate all this data based on vision. So like essentially you can, you can estimate how much force someone is using to lift an object if you know what that object is. So if you have and how much force it usually takes to lift it right, laptops weigh pretty much the same across categories. MacBooks weigh exactly the same, like across kind of product lines and so on. So we're certain we can get this just with vision. And the idea is to get most of this with vision, because vision is super, super scalable. Everything else is not so, but right now, vision is not accurate enough, so we can't just launch with vision, because not accurate enough. But that's what we want. We want to do just vision, because it's the it's the most scalable thing you can possibly get to. So the idea is we collect a lot of this data, and we build models that are able to predict those things that we can't possibly launch at scale, like I can't have, you know. So you can't put gloves with like, four sensors to every person that you work with. First of all, it's super expensive, if you have, like, let's say, 10,000 20,000 100,000 data collectors. It's super expensive to literally give them four sensing gloves. This is expensive hardware, and so it's harder to scale, but also the this, these, this hardware comes with a lot of things like drift and all these kind of other components that are much, much harder to manage at scale. But if you have like a centralized environment, then obviously these are very, very manageable. And you collect this data, build the models, and then you estimate these things with vision.
So right now we don't have forced sensors, but what we're going to do is we're now preparing to set up basically a bespoke way of data collection, where instead of actually being, actually going into real world, into the real world, and collecting this data as we currently are, we're also going to have a small team of people who are specifically collecting the highest quality of data that is possible. Because we don't care about hardware, like when you deploy in the real world, you care like you can't put too many sensors on someone because then they won't be able to do their job right. But if their job is simply to data collect, then you can put a lot more sensors per person. And you can have things like, you know, pressure sensors for sensors, stuff like that, as well as more cameras every compute, you know, leopard things like that. And you can also do custom task design, which is very useful for a lot of these robotics companies. Can you know, because as they're preparing for, like, a lot of demos, or maybe they're preparing for a specific pilot, they need the robot have certain skills which might not be widely available or widely accessible in the real world, so you have to set up a custom environment for them, essentially, to go and collect this data. And the idea is, we collect, we collect this super high quality data in house, and then we use it to essentially train models to be able to estimate all this data based on vision. So like essentially you can, you can estimate how much force someone is using to lift an object if you know what that object is. So if you have and how much force it usually takes to lift it right, laptops weigh pretty much the same across categories. MacBooks weigh exactly the same, like across kind of product lines and so on. So we're certain we can get this just with vision. And the idea is to get most of this with vision, because vision is super, super scalable. Everything else is not so, but right now, vision is not accurate enough, so we can't just launch with vision, because not accurate enough. But that's what we want. We want to do just vision, because it's the it's the most scalable thing you can possibly get to. So the idea is we collect a lot of this data, and we build models that are able to predict those things that we can't possibly launch at scale, like I can't have, you know. So you can't put gloves with like, four sensors to every person that you work with. First of all, it's super expensive, if you have, like, let's say, 10,000 20,000 100,000 data collectors. It's super expensive to literally give them four sensing gloves. This is expensive hardware, and so it's harder to scale, but also the this, these, this hardware comes with a lot of things like drift and all these kind of other components that are much, much harder to manage at scale. But if you have like a centralized environment, then obviously these are very, very manageable. And you collect this data, build the models, and then you estimate these things with vision.
S Speaker 125:03I was going to ask. But before you get to vision only is there a dependency on because now every robot, every humanoid company, every robot company, they're trying to build their own robots with their own, in some cases, custom sensors, or in some cases, in most cases, they would have the positioning of the sensors somewhat different from one versus the other. And so does the data collection because, and this is one reason why they want to actually collect their own data as well, because they don't know themselves. This is like this field is so much in experimentation land today that v1 of their robot is so different from v2 is sort of different from v3 and by the time something that they build, something that they're going to deploy in the field, it's going to look very different. Yeah, so that's why they want to have more more control over how data collects gets collected. So stem which is, how do you address that?
I was going to ask. But before you get to vision only is there a dependency on because now every robot, every humanoid company, every robot company, they're trying to build their own robots with their own, in some cases, custom sensors, or in some cases, in most cases, they would have the positioning of the sensors somewhat different from one versus the other. And so does the data collection because, and this is one reason why they want to actually collect their own data as well, because they don't know themselves. This is like this field is so much in experimentation land today that v1 of their robot is so different from v2 is sort of different from v3 and by the time something that they build, something that they're going to deploy in the field, it's going to look very different. Yeah, so that's why they want to have more more control over how data collects gets collected. So stem which is, how do you address that?
I was going to ask. But before you get to vision only is there a dependency on because now every robot, every humanoid company, every robot company, they're trying to build their own robots with their own, in some cases, custom sensors, or in some cases, in most cases, they would have the positioning of the sensors somewhat different from one versus the other. And so does the data collection because, and this is one reason why they want to actually collect their own data as well, because they don't know themselves. This is like this field is so much in experimentation land today that v1 of their robot is so different from v2 is sort of different from v3 and by the time something that they build, something that they're going to deploy in the field, it's going to look very different. Yeah, so that's why they want to have more more control over how data collects gets collected. So stem which is, how do you address that?
I was going to ask. But before you get to vision only is there a dependency on because now every robot, every humanoid company, every robot company, they're trying to build their own robots with their own, in some cases, custom sensors, or in some cases, in most cases, they would have the positioning of the sensors somewhat different from one versus the other. And so does the data collection because, and this is one reason why they want to actually collect their own data as well, because they don't know themselves. This is like this field is so much in experimentation land today that v1 of their robot is so different from v2 is sort of different from v3 and by the time something that they build, something that they're going to deploy in the field, it's going to look very different. Yeah, so that's why they want to have more more control over how data collects gets collected. So stem which is, how do you address that?
S Speaker 326:12Actually, I would argue that that's not the reason why they collect data themselves. The reason they collected themselves is because, and it's informed by that is because they can only collect very small amounts of data compared to the data that they need to get general models. So if you can collect a very small amount of data based on compared to the data you need, obviously you need this data to be as valuable as it possibly can and as specific to your robot as it possibly can be, in order to be immediately useful, because then you're going to have to solve two problems. The one is how to make with how to get used out of very little data and make it general enough to apply to my robot, which is kind of, you know, has this particular maybe has, like, wrist cameras, for example, or some particular kind of LIDAR around it and so on. And how can I how can I generalize data that I can collect at scale to the particular embodiment of my robot? However, if you did have the internet scale data that I'm talking about, you will need this, because your model would be able to just generalize, right? It would just be able to be plugged into embodiment a, an appointment B, and embodiment C, and maybe even non humanoid environment, and would still get this understanding of the world. But because you do not have the luxury of having access to this kind of data, you must make whatever data you have as meaningful as possible, and that's why you need this. And in fact, there's another layer, even without the specific sensors that you put on humans, to collect data, and that is, collect on robot data, right? Which is, it's the most specific data you could get ever right? And it's the most expensive data you could ever get, which is you, especially, you essentially teleoperate robots. You do the task like you have a human operator to do this test, and then you're getting essentially one to one data on interacting with the real world. But this is simply like you need for every data collector, you need one robot. These robots are very expensive. They're very slow as well. So for example, a human could do a task, let's say, 300 times in an hour. A robot might be literally able to do it five so obviously, the number of examples in an hour from human and now or from robot is vastly, vastly different. And so there's a lot of these kind of issues there. But the biggest one is not that. It's just simple. It's not scalable. It's super expensive to scale. You're going to need, again, internet scale data. And the problem is, you can't make so many robots to get this internet scale data if the robots are useless in the first place because they don't provide any economic value, and it wouldn't make sense to kind of go after them. Got it.
Actually, I would argue that that's not the reason why they collect data themselves. The reason they collected themselves is because, and it's informed by that is because they can only collect very small amounts of data compared to the data that they need to get general models. So if you can collect a very small amount of data based on compared to the data you need, obviously you need this data to be as valuable as it possibly can and as specific to your robot as it possibly can be, in order to be immediately useful, because then you're going to have to solve two problems. The one is how to make with how to get used out of very little data and make it general enough to apply to my robot, which is kind of, you know, has this particular maybe has, like, wrist cameras, for example, or some particular kind of LIDAR around it and so on. And how can I how can I generalize data that I can collect at scale to the particular embodiment of my robot? However, if you did have the internet scale data that I'm talking about, you will need this, because your model would be able to just generalize, right? It would just be able to be plugged into embodiment a, an appointment B, and embodiment C, and maybe even non humanoid environment, and would still get this understanding of the world. But because you do not have the luxury of having access to this kind of data, you must make whatever data you have as meaningful as possible, and that's why you need this. And in fact, there's another layer, even without the specific sensors that you put on humans, to collect data, and that is, collect on robot data, right? Which is, it's the most specific data you could get ever right? And it's the most expensive data you could ever get, which is you, especially, you essentially teleoperate robots. You do the task like you have a human operator to do this test, and then you're getting essentially one to one data on interacting with the real world. But this is simply like you need for every data collector, you need one robot. These robots are very expensive. They're very slow as well. So for example, a human could do a task, let's say, 300 times in an hour. A robot might be literally able to do it five so obviously, the number of examples in an hour from human and now or from robot is vastly, vastly different. And so there's a lot of these kind of issues there. But the biggest one is not that. It's just simple. It's not scalable. It's super expensive to scale. You're going to need, again, internet scale data. And the problem is, you can't make so many robots to get this internet scale data if the robots are useless in the first place because they don't provide any economic value, and it wouldn't make sense to kind of go after them. Got it.
Actually, I would argue that that's not the reason why they collect data themselves. The reason they collected themselves is because, and it's informed by that is because they can only collect very small amounts of data compared to the data that they need to get general models. So if you can collect a very small amount of data based on compared to the data you need, obviously you need this data to be as valuable as it possibly can and as specific to your robot as it possibly can be, in order to be immediately useful, because then you're going to have to solve two problems. The one is how to make with how to get used out of very little data and make it general enough to apply to my robot, which is kind of, you know, has this particular maybe has, like, wrist cameras, for example, or some particular kind of LIDAR around it and so on. And how can I how can I generalize data that I can collect at scale to the particular embodiment of my robot? However, if you did have the internet scale data that I'm talking about, you will need this, because your model would be able to just generalize, right? It would just be able to be plugged into embodiment a, an appointment B, and embodiment C, and maybe even non humanoid environment, and would still get this understanding of the world. But because you do not have the luxury of having access to this kind of data, you must make whatever data you have as meaningful as possible, and that's why you need this. And in fact, there's another layer, even without the specific sensors that you put on humans, to collect data, and that is, collect on robot data, right? Which is, it's the most specific data you could get ever right? And it's the most expensive data you could ever get, which is you, especially, you essentially teleoperate robots. You do the task like you have a human operator to do this test, and then you're getting essentially one to one data on interacting with the real world. But this is simply like you need for every data collector, you need one robot. These robots are very expensive. They're very slow as well. So for example, a human could do a task, let's say, 300 times in an hour. A robot might be literally able to do it five so obviously, the number of examples in an hour from human and now or from robot is vastly, vastly different. And so there's a lot of these kind of issues there. But the biggest one is not that. It's just simple. It's not scalable. It's super expensive to scale. You're going to need, again, internet scale data. And the problem is, you can't make so many robots to get this internet scale data if the robots are useless in the first place because they don't provide any economic value, and it wouldn't make sense to kind of go after them. Got it.
Actually, I would argue that that's not the reason why they collect data themselves. The reason they collected themselves is because, and it's informed by that is because they can only collect very small amounts of data compared to the data that they need to get general models. So if you can collect a very small amount of data based on compared to the data you need, obviously you need this data to be as valuable as it possibly can and as specific to your robot as it possibly can be, in order to be immediately useful, because then you're going to have to solve two problems. The one is how to make with how to get used out of very little data and make it general enough to apply to my robot, which is kind of, you know, has this particular maybe has, like, wrist cameras, for example, or some particular kind of LIDAR around it and so on. And how can I how can I generalize data that I can collect at scale to the particular embodiment of my robot? However, if you did have the internet scale data that I'm talking about, you will need this, because your model would be able to just generalize, right? It would just be able to be plugged into embodiment a, an appointment B, and embodiment C, and maybe even non humanoid environment, and would still get this understanding of the world. But because you do not have the luxury of having access to this kind of data, you must make whatever data you have as meaningful as possible, and that's why you need this. And in fact, there's another layer, even without the specific sensors that you put on humans, to collect data, and that is, collect on robot data, right? Which is, it's the most specific data you could get ever right? And it's the most expensive data you could ever get, which is you, especially, you essentially teleoperate robots. You do the task like you have a human operator to do this test, and then you're getting essentially one to one data on interacting with the real world. But this is simply like you need for every data collector, you need one robot. These robots are very expensive. They're very slow as well. So for example, a human could do a task, let's say, 300 times in an hour. A robot might be literally able to do it five so obviously, the number of examples in an hour from human and now or from robot is vastly, vastly different. And so there's a lot of these kind of issues there. But the biggest one is not that. It's just simple. It's not scalable. It's super expensive to scale. You're going to need, again, internet scale data. And the problem is, you can't make so many robots to get this internet scale data if the robots are useless in the first place because they don't provide any economic value, and it wouldn't make sense to kind of go after them. Got it.
S Speaker 128:59And then what about the positioning of the sensors and how does that impact like your data is that easily reusable by any Yes? So robot company, yes.
And then what about the positioning of the sensors and how does that impact like your data is that easily reusable by any Yes? So robot company, yes.
And then what about the positioning of the sensors and how does that impact like your data is that easily reusable by any Yes? So robot company, yes.
And then what about the positioning of the sensors and how does that impact like your data is that easily reusable by any Yes? So robot company, yes.
S Speaker 329:14So essentially, we've opted for non head mounted cameras with whatever we've placed our cameras at kind of high chest level here. And the reason why is because there's basically no difference in the models ability to understand percept and control the world, whether the the cameras in the hand and the robot and then the chest in the in the data collector. And the reason why for this is, again, with the more data that you provide, the better the model is able to generalize. Keep in mind, a lot of these models are trained on like, internet data as well, like there are some videos, like POV videos from YouTube and stuff like that. Usually there is no impact. And this is kind of the sense we're getting from the people we're talking to customers, I mean that they don't really care about this because they in their training flow. They have three kind of stages. The first one, which is where we come in, is the understanding of the world and how to manipulate it. Then the second one is, how can I translate this understanding in a specific application, my specific hardware on my robot. This is the layer that deliberation comes in. And then the third layer is, how do I translate this, this kind of understanding of how it applies to my specific hardware in control commands sent to the actual robot monitors to move and actually perform the final action. And the idea is, though, that if you have a really, really good understanding of the world, you're going to need very little calibration data to actually build your model, because essentially all you will need your telegram data to do is to give your model an understanding of the embodiment, the deliberation does not need to tell you how to pick up a notebook, right? The understanding of the world will tell you how to do this. What the deliberation data will do is it will tell you what your hand looks like in the robot. Like is the gripper? Is it a dexterous Five Finger hand? What is it? And then, kind of what the sizes are, what are the relationships between the fingers? And how do you, as a robot, usually pick up things, not notebooks, just things. How do you do that? And that's kind of the the layer that you need. In fact, that what we've seen is you would need to let you need 99.9% less teleportation data, the more if you have a large egocentric understanding of the world, because you just need to teach the robot what looks like. That's it interesting.
So essentially, we've opted for non head mounted cameras with whatever we've placed our cameras at kind of high chest level here. And the reason why is because there's basically no difference in the models ability to understand percept and control the world, whether the the cameras in the hand and the robot and then the chest in the in the data collector. And the reason why for this is, again, with the more data that you provide, the better the model is able to generalize. Keep in mind, a lot of these models are trained on like, internet data as well, like there are some videos, like POV videos from YouTube and stuff like that. Usually there is no impact. And this is kind of the sense we're getting from the people we're talking to customers, I mean that they don't really care about this because they in their training flow. They have three kind of stages. The first one, which is where we come in, is the understanding of the world and how to manipulate it. Then the second one is, how can I translate this understanding in a specific application, my specific hardware on my robot. This is the layer that deliberation comes in. And then the third layer is, how do I translate this, this kind of understanding of how it applies to my specific hardware in control commands sent to the actual robot monitors to move and actually perform the final action. And the idea is, though, that if you have a really, really good understanding of the world, you're going to need very little calibration data to actually build your model, because essentially all you will need your telegram data to do is to give your model an understanding of the embodiment, the deliberation does not need to tell you how to pick up a notebook, right? The understanding of the world will tell you how to do this. What the deliberation data will do is it will tell you what your hand looks like in the robot. Like is the gripper? Is it a dexterous Five Finger hand? What is it? And then, kind of what the sizes are, what are the relationships between the fingers? And how do you, as a robot, usually pick up things, not notebooks, just things. How do you do that? And that's kind of the the layer that you need. In fact, that what we've seen is you would need to let you need 99.9% less teleportation data, the more if you have a large egocentric understanding of the world, because you just need to teach the robot what looks like. That's it interesting.
So essentially, we've opted for non head mounted cameras with whatever we've placed our cameras at kind of high chest level here. And the reason why is because there's basically no difference in the models ability to understand percept and control the world, whether the the cameras in the hand and the robot and then the chest in the in the data collector. And the reason why for this is, again, with the more data that you provide, the better the model is able to generalize. Keep in mind, a lot of these models are trained on like, internet data as well, like there are some videos, like POV videos from YouTube and stuff like that. Usually there is no impact. And this is kind of the sense we're getting from the people we're talking to customers, I mean that they don't really care about this because they in their training flow. They have three kind of stages. The first one, which is where we come in, is the understanding of the world and how to manipulate it. Then the second one is, how can I translate this understanding in a specific application, my specific hardware on my robot. This is the layer that deliberation comes in. And then the third layer is, how do I translate this, this kind of understanding of how it applies to my specific hardware in control commands sent to the actual robot monitors to move and actually perform the final action. And the idea is, though, that if you have a really, really good understanding of the world, you're going to need very little calibration data to actually build your model, because essentially all you will need your telegram data to do is to give your model an understanding of the embodiment, the deliberation does not need to tell you how to pick up a notebook, right? The understanding of the world will tell you how to do this. What the deliberation data will do is it will tell you what your hand looks like in the robot. Like is the gripper? Is it a dexterous Five Finger hand? What is it? And then, kind of what the sizes are, what are the relationships between the fingers? And how do you, as a robot, usually pick up things, not notebooks, just things. How do you do that? And that's kind of the the layer that you need. In fact, that what we've seen is you would need to let you need 99.9% less teleportation data, the more if you have a large egocentric understanding of the world, because you just need to teach the robot what looks like. That's it interesting.
So essentially, we've opted for non head mounted cameras with whatever we've placed our cameras at kind of high chest level here. And the reason why is because there's basically no difference in the models ability to understand percept and control the world, whether the the cameras in the hand and the robot and then the chest in the in the data collector. And the reason why for this is, again, with the more data that you provide, the better the model is able to generalize. Keep in mind, a lot of these models are trained on like, internet data as well, like there are some videos, like POV videos from YouTube and stuff like that. Usually there is no impact. And this is kind of the sense we're getting from the people we're talking to customers, I mean that they don't really care about this because they in their training flow. They have three kind of stages. The first one, which is where we come in, is the understanding of the world and how to manipulate it. Then the second one is, how can I translate this understanding in a specific application, my specific hardware on my robot. This is the layer that deliberation comes in. And then the third layer is, how do I translate this, this kind of understanding of how it applies to my specific hardware in control commands sent to the actual robot monitors to move and actually perform the final action. And the idea is, though, that if you have a really, really good understanding of the world, you're going to need very little calibration data to actually build your model, because essentially all you will need your telegram data to do is to give your model an understanding of the embodiment, the deliberation does not need to tell you how to pick up a notebook, right? The understanding of the world will tell you how to do this. What the deliberation data will do is it will tell you what your hand looks like in the robot. Like is the gripper? Is it a dexterous Five Finger hand? What is it? And then, kind of what the sizes are, what are the relationships between the fingers? And how do you, as a robot, usually pick up things, not notebooks, just things. How do you do that? And that's kind of the the layer that you need. In fact, that what we've seen is you would need to let you need 99.9% less teleportation data, the more if you have a large egocentric understanding of the world, because you just need to teach the robot what looks like. That's it interesting.
S Speaker 131:52And so, Stan, how do you so we're going to get to that other question that I had asked about as well, which is, yeah, yes. How do you address these vertical plays? But how do you scale this, which is,
And so, Stan, how do you so we're going to get to that other question that I had asked about as well, which is, yeah, yes. How do you address these vertical plays? But how do you scale this, which is,
And so, Stan, how do you so we're going to get to that other question that I had asked about as well, which is, yeah, yes. How do you address these vertical plays? But how do you scale this, which is,
And so, Stan, how do you so we're going to get to that other question that I had asked about as well, which is, yeah, yes. How do you address these vertical plays? But how do you scale this, which is,
32:10it will still require
it will still require
it will still require
it will still require
32:13lots of human operators.
lots of human operators.
lots of human operators.
lots of human operators.
32:17How do you is your
S Speaker 132:18plan to sell your hardware and software to these vertical AI companies so that they can collect their own data, or is your plan to actually sell them the data? And if your plan is to sell them the data, which is the true scale AI play
plan to sell your hardware and software to these vertical AI companies so that they can collect their own data, or is your plan to actually sell them the data? And if your plan is to sell them the data, which is the true scale AI play
plan to sell your hardware and software to these vertical AI companies so that they can collect their own data, or is your plan to actually sell them the data? And if your plan is to sell them the data, which is the true scale AI play
plan to sell your hardware and software to these vertical AI companies so that they can collect their own data, or is your plan to actually sell them the data? And if your plan is to sell them the data, which is the true scale AI play
32:37that you have figured out the
that you have figured out the
that you have figured out the
that you have figured out the
32:38operations of managing people.
operations of managing people.
operations of managing people.
operations of managing people.
S Speaker 332:42So first of all, two things. First of all, we're gonna sell them the data. We have to figure out the operations for them. We are the ones that manage them. They're top like they're world leading researchers. They don't want to have to deal with this. They just want to get the data. Get Data good, put it through their models, benchmark it, see how it works, see what doesn't tell us what data they need next. We go out, collect them, bring it back. In fact, the difference, really the difference in that business model with scale AI, is that instead of just annotating their existing data sets, we can go out and collect the data sets as well. And in fact, we've built an API, which is like reality API, which essentially, you can request real world data, let's say, pick up cop like I want demonstrations of picking up a cup through API. And you know, you send us this, you'll connect the web hook, and you'll get a notification when your data is collected, annotated, ready for you, ready for you to use. Essentially reducing the friction to zero between asking for data and getting the data and training your models. And importantly, there our idea is, and this is where the importance of our real time processing comes in. Is we want to do this. We want to get to the point where the moment data is collected, it becomes immediately available on the API to access so you can get true, real time access. What do you mean? Physical World like, what? So basically, our we have an incredible focus on making our data collection sufficient as possible, so that all the processing that comes with it, like all the annotations that we do, all the for example, hand is estimations and stuff like that happens real time. So a moment the frame is captured, the next moment this frame is fully annotated and ready for training the models. It's like the full, the full, the full kind of stack to happen real time right now.
So first of all, two things. First of all, we're gonna sell them the data. We have to figure out the operations for them. We are the ones that manage them. They're top like they're world leading researchers. They don't want to have to deal with this. They just want to get the data. Get Data good, put it through their models, benchmark it, see how it works, see what doesn't tell us what data they need next. We go out, collect them, bring it back. In fact, the difference, really the difference in that business model with scale AI, is that instead of just annotating their existing data sets, we can go out and collect the data sets as well. And in fact, we've built an API, which is like reality API, which essentially, you can request real world data, let's say, pick up cop like I want demonstrations of picking up a cup through API. And you know, you send us this, you'll connect the web hook, and you'll get a notification when your data is collected, annotated, ready for you, ready for you to use. Essentially reducing the friction to zero between asking for data and getting the data and training your models. And importantly, there our idea is, and this is where the importance of our real time processing comes in. Is we want to do this. We want to get to the point where the moment data is collected, it becomes immediately available on the API to access so you can get true, real time access. What do you mean? Physical World like, what? So basically, our we have an incredible focus on making our data collection sufficient as possible, so that all the processing that comes with it, like all the annotations that we do, all the for example, hand is estimations and stuff like that happens real time. So a moment the frame is captured, the next moment this frame is fully annotated and ready for training the models. It's like the full, the full, the full kind of stack to happen real time right now.
So first of all, two things. First of all, we're gonna sell them the data. We have to figure out the operations for them. We are the ones that manage them. They're top like they're world leading researchers. They don't want to have to deal with this. They just want to get the data. Get Data good, put it through their models, benchmark it, see how it works, see what doesn't tell us what data they need next. We go out, collect them, bring it back. In fact, the difference, really the difference in that business model with scale AI, is that instead of just annotating their existing data sets, we can go out and collect the data sets as well. And in fact, we've built an API, which is like reality API, which essentially, you can request real world data, let's say, pick up cop like I want demonstrations of picking up a cup through API. And you know, you send us this, you'll connect the web hook, and you'll get a notification when your data is collected, annotated, ready for you, ready for you to use. Essentially reducing the friction to zero between asking for data and getting the data and training your models. And importantly, there our idea is, and this is where the importance of our real time processing comes in. Is we want to do this. We want to get to the point where the moment data is collected, it becomes immediately available on the API to access so you can get true, real time access. What do you mean? Physical World like, what? So basically, our we have an incredible focus on making our data collection sufficient as possible, so that all the processing that comes with it, like all the annotations that we do, all the for example, hand is estimations and stuff like that happens real time. So a moment the frame is captured, the next moment this frame is fully annotated and ready for training the models. It's like the full, the full, the full kind of stack to happen real time right now.
So first of all, two things. First of all, we're gonna sell them the data. We have to figure out the operations for them. We are the ones that manage them. They're top like they're world leading researchers. They don't want to have to deal with this. They just want to get the data. Get Data good, put it through their models, benchmark it, see how it works, see what doesn't tell us what data they need next. We go out, collect them, bring it back. In fact, the difference, really the difference in that business model with scale AI, is that instead of just annotating their existing data sets, we can go out and collect the data sets as well. And in fact, we've built an API, which is like reality API, which essentially, you can request real world data, let's say, pick up cop like I want demonstrations of picking up a cup through API. And you know, you send us this, you'll connect the web hook, and you'll get a notification when your data is collected, annotated, ready for you, ready for you to use. Essentially reducing the friction to zero between asking for data and getting the data and training your models. And importantly, there our idea is, and this is where the importance of our real time processing comes in. Is we want to do this. We want to get to the point where the moment data is collected, it becomes immediately available on the API to access so you can get true, real time access. What do you mean? Physical World like, what? So basically, our we have an incredible focus on making our data collection sufficient as possible, so that all the processing that comes with it, like all the annotations that we do, all the for example, hand is estimations and stuff like that happens real time. So a moment the frame is captured, the next moment this frame is fully annotated and ready for training the models. It's like the full, the full, the full kind of stack to happen real time right now.
34:40What benefit does that provide?
What benefit does that provide?
What benefit does that provide?
What benefit does that provide?
S Speaker 334:42It's the idea that you can get instant access to the so they used to build the API so that you can access this data at real time, so the moment this data is collected, it can immediately become available in the API. And why is that important? Is because it essentially matches the time for data, for data availability, for these customers, it essentially maps it onto time itself, because as time, you know, progresses the data is collected, it immediately becomes available. So you know that you're truly accessing the world's data in real time, and you're able to, you know, feed this to your models and truly get what you would have as an Internet right? As the internet for the physical world, they turn this real time, right? You don't get the news three days later. You get it like almost a minute before it actually happens. You do.
It's the idea that you can get instant access to the so they used to build the API so that you can access this data at real time, so the moment this data is collected, it can immediately become available in the API. And why is that important? Is because it essentially matches the time for data, for data availability, for these customers, it essentially maps it onto time itself, because as time, you know, progresses the data is collected, it immediately becomes available. So you know that you're truly accessing the world's data in real time, and you're able to, you know, feed this to your models and truly get what you would have as an Internet right? As the internet for the physical world, they turn this real time, right? You don't get the news three days later. You get it like almost a minute before it actually happens. You do.
It's the idea that you can get instant access to the so they used to build the API so that you can access this data at real time, so the moment this data is collected, it can immediately become available in the API. And why is that important? Is because it essentially matches the time for data, for data availability, for these customers, it essentially maps it onto time itself, because as time, you know, progresses the data is collected, it immediately becomes available. So you know that you're truly accessing the world's data in real time, and you're able to, you know, feed this to your models and truly get what you would have as an Internet right? As the internet for the physical world, they turn this real time, right? You don't get the news three days later. You get it like almost a minute before it actually happens. You do.
It's the idea that you can get instant access to the so they used to build the API so that you can access this data at real time, so the moment this data is collected, it can immediately become available in the API. And why is that important? Is because it essentially matches the time for data, for data availability, for these customers, it essentially maps it onto time itself, because as time, you know, progresses the data is collected, it immediately becomes available. So you know that you're truly accessing the world's data in real time, and you're able to, you know, feed this to your models and truly get what you would have as an Internet right? As the internet for the physical world, they turn this real time, right? You don't get the news three days later. You get it like almost a minute before it actually happens. You do.
S Speaker 135:36The difference is when a commission. So now, how do you shorten? So you've shortened the path from when the data gets collected to when the data gets used to train a model, so that a researcher who's working on training model, they can essentially get that data as soon as it's collected. But, but they but what? When do they commission and then, so which is they ask for, okay, you know what? Because today it's all about experimentation. And how do I reduce time for experimentation, right? So a researcher says, You know what, I need to be able to collect this particular type of data in this particular type of a home environment for falling sheets, right? And so, but then they'll have to actually submit a ticket, and then somebody has to go do it real time. Typically means, you
The difference is when a commission. So now, how do you shorten? So you've shortened the path from when the data gets collected to when the data gets used to train a model, so that a researcher who's working on training model, they can essentially get that data as soon as it's collected. But, but they but what? When do they commission and then, so which is they ask for, okay, you know what? Because today it's all about experimentation. And how do I reduce time for experimentation, right? So a researcher says, You know what, I need to be able to collect this particular type of data in this particular type of a home environment for falling sheets, right? And so, but then they'll have to actually submit a ticket, and then somebody has to go do it real time. Typically means, you
The difference is when a commission. So now, how do you shorten? So you've shortened the path from when the data gets collected to when the data gets used to train a model, so that a researcher who's working on training model, they can essentially get that data as soon as it's collected. But, but they but what? When do they commission and then, so which is they ask for, okay, you know what? Because today it's all about experimentation. And how do I reduce time for experimentation, right? So a researcher says, You know what, I need to be able to collect this particular type of data in this particular type of a home environment for falling sheets, right? And so, but then they'll have to actually submit a ticket, and then somebody has to go do it real time. Typically means, you
The difference is when a commission. So now, how do you shorten? So you've shortened the path from when the data gets collected to when the data gets used to train a model, so that a researcher who's working on training model, they can essentially get that data as soon as it's collected. But, but they but what? When do they commission and then, so which is they ask for, okay, you know what? Because today it's all about experimentation. And how do I reduce time for experimentation, right? So a researcher says, You know what, I need to be able to collect this particular type of data in this particular type of a home environment for falling sheets, right? And so, but then they'll have to actually submit a ticket, and then somebody has to go do it real time. Typically means, you
S Speaker 336:38know, not really, because if you have enough scale, it's lacking that you already have this data. So it's already been collected, right? I if I'm in 10,000 houses, I promise you right now, there's every day many people have folding sheets in a particular kind of bedroom and so on. Like it's guaranteed simply by a matter of probabilities and the fact that you know every home, for
know, not really, because if you have enough scale, it's lacking that you already have this data. So it's already been collected, right? I if I'm in 10,000 houses, I promise you right now, there's every day many people have folding sheets in a particular kind of bedroom and so on. Like it's guaranteed simply by a matter of probabilities and the fact that you know every home, for
know, not really, because if you have enough scale, it's lacking that you already have this data. So it's already been collected, right? I if I'm in 10,000 houses, I promise you right now, there's every day many people have folding sheets in a particular kind of bedroom and so on. Like it's guaranteed simply by a matter of probabilities and the fact that you know every home, for
know, not really, because if you have enough scale, it's lacking that you already have this data. So it's already been collected, right? I if I'm in 10,000 houses, I promise you right now, there's every day many people have folding sheets in a particular kind of bedroom and so on. Like it's guaranteed simply by a matter of probabilities and the fact that you know every home, for
S Speaker 137:02example, right? Like this. Let's say this was an edge I just used a very simple example that, let's say this was some sort of an end scenario, because we're an experimentation, then a lot of it, yeah,
example, right? Like this. Let's say this was an edge I just used a very simple example that, let's say this was some sort of an end scenario, because we're an experimentation, then a lot of it, yeah,
example, right? Like this. Let's say this was an edge I just used a very simple example that, let's say this was some sort of an end scenario, because we're an experimentation, then a lot of it, yeah,
example, right? Like this. Let's say this was an edge I just used a very simple example that, let's say this was some sort of an end scenario, because we're an experimentation, then a lot of it, yeah,
S Speaker 337:12yeah, yeah. So obviously the the rarest, the edge case, the harder is to provide real time. But the idea is that for the data that you know then you need. You know that these applications are needed, and you just need to scale them out. You can have instant access to this data, and you can feed them to your models. As far as like, there's different types of experimentation, right? You'd have to experiment on what kind of data you need. You have to experiment what task, and you also have to experiment at what scale, because you might need, for example, 30,000 hours for this particular kind of task, or you might need 100,000 or a million hours. So the longer a million hours, for example, is in an incredible it's like, what 110 years of human life, something like that. Something crazy. Now, if it takes you seven months to get this data, obviously your time to experimentation now has been incredibly increased to, like maybe a year and a half, but if you can get access to the data instantly, at least that part is solved. So it's just a matter of how you train this and how you kind of benchmark it, and so on. So this is important reducing time for experimentation, because, as you said, this is very exploratory research, and you're going to have, you're going to need a lot of experimentation. But truly, how we solve this exact problem that you're describing is when we go into synthetic data. Because we're never going to be able to get every edge case real time with real world collective data. But what we can do is build the world's most true synthetic data engine where you can ask things and you can get a true representation of the physical world from our data distilled into a model, you can produce exactly what you're looking for, no matter what the edge case is, right? And this is really where we're moving toward. And to answer your question previously, our goal is not to become a full stack robot manufacturer. Our goal is to become the Android, essentially, for these robots, and to build the platform where these robots can operate on. But the difference is that a lot of these people are starting from the software. For example, we start from the data, because we think that's really what's going to make the difference at the end of the day, it's not going to be your model framework. This can be copy. This can be replicated in many ways, but your data mode, as you mentioned before, is really the most important thing. So it makes sense for you to go and instead of going from a top down approach, for example, figure AI, right? It's a top down approach. First, they made incredible hardware, then they made a good model, and now they care about the data, while obviously still working on the these other two things, the NVIDIA as well. For example, what they're doing is they are building the model. They're not building hardware, but let's say they go one step beyond the to be used, but then they built a model framework, and eventually they will start caring about data as well, even though currently they're not our idea is to go bottom up, right? So you start with the most important kind of difficult thing to solve, which is the data. Then you move to the model, and then you move to like the operating system, which is actually what goes into the into the robots eventually and so on.
yeah, yeah. So obviously the the rarest, the edge case, the harder is to provide real time. But the idea is that for the data that you know then you need. You know that these applications are needed, and you just need to scale them out. You can have instant access to this data, and you can feed them to your models. As far as like, there's different types of experimentation, right? You'd have to experiment on what kind of data you need. You have to experiment what task, and you also have to experiment at what scale, because you might need, for example, 30,000 hours for this particular kind of task, or you might need 100,000 or a million hours. So the longer a million hours, for example, is in an incredible it's like, what 110 years of human life, something like that. Something crazy. Now, if it takes you seven months to get this data, obviously your time to experimentation now has been incredibly increased to, like maybe a year and a half, but if you can get access to the data instantly, at least that part is solved. So it's just a matter of how you train this and how you kind of benchmark it, and so on. So this is important reducing time for experimentation, because, as you said, this is very exploratory research, and you're going to have, you're going to need a lot of experimentation. But truly, how we solve this exact problem that you're describing is when we go into synthetic data. Because we're never going to be able to get every edge case real time with real world collective data. But what we can do is build the world's most true synthetic data engine where you can ask things and you can get a true representation of the physical world from our data distilled into a model, you can produce exactly what you're looking for, no matter what the edge case is, right? And this is really where we're moving toward. And to answer your question previously, our goal is not to become a full stack robot manufacturer. Our goal is to become the Android, essentially, for these robots, and to build the platform where these robots can operate on. But the difference is that a lot of these people are starting from the software. For example, we start from the data, because we think that's really what's going to make the difference at the end of the day, it's not going to be your model framework. This can be copy. This can be replicated in many ways, but your data mode, as you mentioned before, is really the most important thing. So it makes sense for you to go and instead of going from a top down approach, for example, figure AI, right? It's a top down approach. First, they made incredible hardware, then they made a good model, and now they care about the data, while obviously still working on the these other two things, the NVIDIA as well. For example, what they're doing is they are building the model. They're not building hardware, but let's say they go one step beyond the to be used, but then they built a model framework, and eventually they will start caring about data as well, even though currently they're not our idea is to go bottom up, right? So you start with the most important kind of difficult thing to solve, which is the data. Then you move to the model, and then you move to like the operating system, which is actually what goes into the into the robots eventually and so on.
yeah, yeah. So obviously the the rarest, the edge case, the harder is to provide real time. But the idea is that for the data that you know then you need. You know that these applications are needed, and you just need to scale them out. You can have instant access to this data, and you can feed them to your models. As far as like, there's different types of experimentation, right? You'd have to experiment on what kind of data you need. You have to experiment what task, and you also have to experiment at what scale, because you might need, for example, 30,000 hours for this particular kind of task, or you might need 100,000 or a million hours. So the longer a million hours, for example, is in an incredible it's like, what 110 years of human life, something like that. Something crazy. Now, if it takes you seven months to get this data, obviously your time to experimentation now has been incredibly increased to, like maybe a year and a half, but if you can get access to the data instantly, at least that part is solved. So it's just a matter of how you train this and how you kind of benchmark it, and so on. So this is important reducing time for experimentation, because, as you said, this is very exploratory research, and you're going to have, you're going to need a lot of experimentation. But truly, how we solve this exact problem that you're describing is when we go into synthetic data. Because we're never going to be able to get every edge case real time with real world collective data. But what we can do is build the world's most true synthetic data engine where you can ask things and you can get a true representation of the physical world from our data distilled into a model, you can produce exactly what you're looking for, no matter what the edge case is, right? And this is really where we're moving toward. And to answer your question previously, our goal is not to become a full stack robot manufacturer. Our goal is to become the Android, essentially, for these robots, and to build the platform where these robots can operate on. But the difference is that a lot of these people are starting from the software. For example, we start from the data, because we think that's really what's going to make the difference at the end of the day, it's not going to be your model framework. This can be copy. This can be replicated in many ways, but your data mode, as you mentioned before, is really the most important thing. So it makes sense for you to go and instead of going from a top down approach, for example, figure AI, right? It's a top down approach. First, they made incredible hardware, then they made a good model, and now they care about the data, while obviously still working on the these other two things, the NVIDIA as well. For example, what they're doing is they are building the model. They're not building hardware, but let's say they go one step beyond the to be used, but then they built a model framework, and eventually they will start caring about data as well, even though currently they're not our idea is to go bottom up, right? So you start with the most important kind of difficult thing to solve, which is the data. Then you move to the model, and then you move to like the operating system, which is actually what goes into the into the robots eventually and so on.
yeah, yeah. So obviously the the rarest, the edge case, the harder is to provide real time. But the idea is that for the data that you know then you need. You know that these applications are needed, and you just need to scale them out. You can have instant access to this data, and you can feed them to your models. As far as like, there's different types of experimentation, right? You'd have to experiment on what kind of data you need. You have to experiment what task, and you also have to experiment at what scale, because you might need, for example, 30,000 hours for this particular kind of task, or you might need 100,000 or a million hours. So the longer a million hours, for example, is in an incredible it's like, what 110 years of human life, something like that. Something crazy. Now, if it takes you seven months to get this data, obviously your time to experimentation now has been incredibly increased to, like maybe a year and a half, but if you can get access to the data instantly, at least that part is solved. So it's just a matter of how you train this and how you kind of benchmark it, and so on. So this is important reducing time for experimentation, because, as you said, this is very exploratory research, and you're going to have, you're going to need a lot of experimentation. But truly, how we solve this exact problem that you're describing is when we go into synthetic data. Because we're never going to be able to get every edge case real time with real world collective data. But what we can do is build the world's most true synthetic data engine where you can ask things and you can get a true representation of the physical world from our data distilled into a model, you can produce exactly what you're looking for, no matter what the edge case is, right? And this is really where we're moving toward. And to answer your question previously, our goal is not to become a full stack robot manufacturer. Our goal is to become the Android, essentially, for these robots, and to build the platform where these robots can operate on. But the difference is that a lot of these people are starting from the software. For example, we start from the data, because we think that's really what's going to make the difference at the end of the day, it's not going to be your model framework. This can be copy. This can be replicated in many ways, but your data mode, as you mentioned before, is really the most important thing. So it makes sense for you to go and instead of going from a top down approach, for example, figure AI, right? It's a top down approach. First, they made incredible hardware, then they made a good model, and now they care about the data, while obviously still working on the these other two things, the NVIDIA as well. For example, what they're doing is they are building the model. They're not building hardware, but let's say they go one step beyond the to be used, but then they built a model framework, and eventually they will start caring about data as well, even though currently they're not our idea is to go bottom up, right? So you start with the most important kind of difficult thing to solve, which is the data. Then you move to the model, and then you move to like the operating system, which is actually what goes into the into the robots eventually and so on.
S Speaker 140:28See the operating system? Yeah, the hardware manufacturers at this point,
See the operating system? Yeah, the hardware manufacturers at this point,
See the operating system? Yeah, the hardware manufacturers at this point,
See the operating system? Yeah, the hardware manufacturers at this point,
40:36I think the operating system is so core that,
I think the operating system is so core that,
I think the operating system is so core that,
I think the operating system is so core that,
S Speaker 140:40yes, any of the hardware manufacturers, they get commoditized away. They let anybody else control the exactly so. So, no, my question is, I think over there, whether these vertical humanoid companies, whether or not they license somebody else's operating system that's DVD.
yes, any of the hardware manufacturers, they get commoditized away. They let anybody else control the exactly so. So, no, my question is, I think over there, whether these vertical humanoid companies, whether or not they license somebody else's operating system that's DVD.
yes, any of the hardware manufacturers, they get commoditized away. They let anybody else control the exactly so. So, no, my question is, I think over there, whether these vertical humanoid companies, whether or not they license somebody else's operating system that's DVD.
yes, any of the hardware manufacturers, they get commoditized away. They let anybody else control the exactly so. So, no, my question is, I think over there, whether these vertical humanoid companies, whether or not they license somebody else's operating system that's DVD.
S Speaker 341:03Sorry to interrupt you. I just want to fully put myself into perspective. It's not them that they're going to do this. If the if the estimate for humanoid robot demand is the estimates for human demands correct, you're not going to have, you basically don't have a smartphone like economy, not a car like economy, right? So this smartphone like economy means you have a lot of people who can make hardware in different parts of the world, but they because they might have some manufacturing advantage, they might have some cost advantage, and so on, but the it would take them a lot of technical skill, a lot of technical investment, maybe years and years of development until they get their product to market. If they also had to make the operating system, had to make the AI that powers this. So this is not the data is to be sold to these frontier companies like figure AI and so on. But the resulting world model and operating system is not to be sold to them, is to be sold to all the the other companies that will start popping up once the demand is validated. And I mean, they're already popping up, for example, in China and in Europe recently, where you know, you see that there are these companies that just build hardware, don't build any AI into the robots, and this will come into the West as well eventually, and you're going to see all of these companies who just want to make hardware. They they want to sell the product, they want to market the product, they want to like, like you do with smartphones, and they don't want to build this kind of thing. So just to a point there,
Sorry to interrupt you. I just want to fully put myself into perspective. It's not them that they're going to do this. If the if the estimate for humanoid robot demand is the estimates for human demands correct, you're not going to have, you basically don't have a smartphone like economy, not a car like economy, right? So this smartphone like economy means you have a lot of people who can make hardware in different parts of the world, but they because they might have some manufacturing advantage, they might have some cost advantage, and so on, but the it would take them a lot of technical skill, a lot of technical investment, maybe years and years of development until they get their product to market. If they also had to make the operating system, had to make the AI that powers this. So this is not the data is to be sold to these frontier companies like figure AI and so on. But the resulting world model and operating system is not to be sold to them, is to be sold to all the the other companies that will start popping up once the demand is validated. And I mean, they're already popping up, for example, in China and in Europe recently, where you know, you see that there are these companies that just build hardware, don't build any AI into the robots, and this will come into the West as well eventually, and you're going to see all of these companies who just want to make hardware. They they want to sell the product, they want to market the product, they want to like, like you do with smartphones, and they don't want to build this kind of thing. So just to a point there,
Sorry to interrupt you. I just want to fully put myself into perspective. It's not them that they're going to do this. If the if the estimate for humanoid robot demand is the estimates for human demands correct, you're not going to have, you basically don't have a smartphone like economy, not a car like economy, right? So this smartphone like economy means you have a lot of people who can make hardware in different parts of the world, but they because they might have some manufacturing advantage, they might have some cost advantage, and so on, but the it would take them a lot of technical skill, a lot of technical investment, maybe years and years of development until they get their product to market. If they also had to make the operating system, had to make the AI that powers this. So this is not the data is to be sold to these frontier companies like figure AI and so on. But the resulting world model and operating system is not to be sold to them, is to be sold to all the the other companies that will start popping up once the demand is validated. And I mean, they're already popping up, for example, in China and in Europe recently, where you know, you see that there are these companies that just build hardware, don't build any AI into the robots, and this will come into the West as well eventually, and you're going to see all of these companies who just want to make hardware. They they want to sell the product, they want to market the product, they want to like, like you do with smartphones, and they don't want to build this kind of thing. So just to a point there,
Sorry to interrupt you. I just want to fully put myself into perspective. It's not them that they're going to do this. If the if the estimate for humanoid robot demand is the estimates for human demands correct, you're not going to have, you basically don't have a smartphone like economy, not a car like economy, right? So this smartphone like economy means you have a lot of people who can make hardware in different parts of the world, but they because they might have some manufacturing advantage, they might have some cost advantage, and so on, but the it would take them a lot of technical skill, a lot of technical investment, maybe years and years of development until they get their product to market. If they also had to make the operating system, had to make the AI that powers this. So this is not the data is to be sold to these frontier companies like figure AI and so on. But the resulting world model and operating system is not to be sold to them, is to be sold to all the the other companies that will start popping up once the demand is validated. And I mean, they're already popping up, for example, in China and in Europe recently, where you know, you see that there are these companies that just build hardware, don't build any AI into the robots, and this will come into the West as well eventually, and you're going to see all of these companies who just want to make hardware. They they want to sell the product, they want to market the product, they want to like, like you do with smartphones, and they don't want to build this kind of thing. So just to a point there,
44:36the video? Seems
S Speaker 144:41interesting they're building a home robot, but without lens and so, so we're working with many companies, and we'd love to actually see how we can even partner. So when did you start the companies? Then just and then how much I raised?
interesting they're building a home robot, but without lens and so, so we're working with many companies, and we'd love to actually see how we can even partner. So when did you start the companies? Then just and then how much I raised?
interesting they're building a home robot, but without lens and so, so we're working with many companies, and we'd love to actually see how we can even partner. So when did you start the companies? Then just and then how much I raised?
interesting they're building a home robot, but without lens and so, so we're working with many companies, and we'd love to actually see how we can even partner. So when did you start the companies? Then just and then how much I raised?
S Speaker 344:59And originally we started year and a half ago, but we really shifted into the direction that we, you know, finalize now in five months ago, four or five months ago, we haven't raised any capital. I funded most of the business from my previous just
And originally we started year and a half ago, but we really shifted into the direction that we, you know, finalize now in five months ago, four or five months ago, we haven't raised any capital. I funded most of the business from my previous just
And originally we started year and a half ago, but we really shifted into the direction that we, you know, finalize now in five months ago, four or five months ago, we haven't raised any capital. I funded most of the business from my previous just
And originally we started year and a half ago, but we really shifted into the direction that we, you know, finalize now in five months ago, four or five months ago, we haven't raised any capital. I funded most of the business from my previous just
45:21reinvesting previous profits.
reinvesting previous profits.
reinvesting previous profits.
reinvesting previous profits.
S Speaker 345:24So far, we're preparing for a round soon. I don't know when exactly yet, because we are still putting certain things that we want to put into place before we go race.
So far, we're preparing for a round soon. I don't know when exactly yet, because we are still putting certain things that we want to put into place before we go race.
So far, we're preparing for a round soon. I don't know when exactly yet, because we are still putting certain things that we want to put into place before we go race.
So far, we're preparing for a round soon. I don't know when exactly yet, because we are still putting certain things that we want to put into place before we go race.
45:41that would be your C drum,
that would be your C drum,
that would be your C drum,
that would be your C drum,
45:44yeah, that would be our C Brown. Okay,
yeah, that would be our C Brown. Okay,
yeah, that would be our C Brown. Okay,
yeah, that would be our C Brown. Okay,
S Speaker 145:49got it. And when do you plan to move to the US?
got it. And when do you plan to move to the US?
got it. And when do you plan to move to the US?
got it. And when do you plan to move to the US?
S Speaker 345:52And this depends on a little bit of how our partnerships here in the UK evolve. It's unlikely that we'll close the UK branch entirely, because, for example, we there's the mic here in London, which is a super important client. It's like the second biggest robotics labs in the world, right? If not maybe even the biggest. Depends on who you think is is kind of working there. But we must, we like, it's a it's a very urgent matter for us to build other operations in the in the US, because we obviously want to speak the figure. When speak with 1x want to speak with all these kind of major manufacturers, and we're planning on doing this at some point in 2026
And this depends on a little bit of how our partnerships here in the UK evolve. It's unlikely that we'll close the UK branch entirely, because, for example, we there's the mic here in London, which is a super important client. It's like the second biggest robotics labs in the world, right? If not maybe even the biggest. Depends on who you think is is kind of working there. But we must, we like, it's a it's a very urgent matter for us to build other operations in the in the US, because we obviously want to speak the figure. When speak with 1x want to speak with all these kind of major manufacturers, and we're planning on doing this at some point in 2026
And this depends on a little bit of how our partnerships here in the UK evolve. It's unlikely that we'll close the UK branch entirely, because, for example, we there's the mic here in London, which is a super important client. It's like the second biggest robotics labs in the world, right? If not maybe even the biggest. Depends on who you think is is kind of working there. But we must, we like, it's a it's a very urgent matter for us to build other operations in the in the US, because we obviously want to speak the figure. When speak with 1x want to speak with all these kind of major manufacturers, and we're planning on doing this at some point in 2026
And this depends on a little bit of how our partnerships here in the UK evolve. It's unlikely that we'll close the UK branch entirely, because, for example, we there's the mic here in London, which is a super important client. It's like the second biggest robotics labs in the world, right? If not maybe even the biggest. Depends on who you think is is kind of working there. But we must, we like, it's a it's a very urgent matter for us to build other operations in the in the US, because we obviously want to speak the figure. When speak with 1x want to speak with all these kind of major manufacturers, and we're planning on doing this at some point in 2026
S Speaker 146:34Okay, all right. Steph, can you send me some materials on your stuff, and then we can have some conversations with our business units. The timing for us to invest in would most likely be when you move to the US and then after your first institutional route.
Okay, all right. Steph, can you send me some materials on your stuff, and then we can have some conversations with our business units. The timing for us to invest in would most likely be when you move to the US and then after your first institutional route.
Okay, all right. Steph, can you send me some materials on your stuff, and then we can have some conversations with our business units. The timing for us to invest in would most likely be when you move to the US and then after your first institutional route.
Okay, all right. Steph, can you send me some materials on your stuff, and then we can have some conversations with our business units. The timing for us to invest in would most likely be when you move to the US and then after your first institutional route.
S Speaker 146:59is definitely interesting. And then if, I mean, we can even start some sort of a pilot, if it's, if you're, yeah, at that stage, we are you at a stage then where you can engage with Qualcomm,
is definitely interesting. And then if, I mean, we can even start some sort of a pilot, if it's, if you're, yeah, at that stage, we are you at a stage then where you can engage with Qualcomm,
is definitely interesting. And then if, I mean, we can even start some sort of a pilot, if it's, if you're, yeah, at that stage, we are you at a stage then where you can engage with Qualcomm,
is definitely interesting. And then if, I mean, we can even start some sort of a pilot, if it's, if you're, yeah, at that stage, we are you at a stage then where you can engage with Qualcomm,
S Speaker 347:11yes, definitely. I mean, we're now, we're just a few months ago, we got an NDA from Deep Mind to enough for another pilot with that.
yes, definitely. I mean, we're now, we're just a few months ago, we got an NDA from Deep Mind to enough for another pilot with that.
yes, definitely. I mean, we're now, we're just a few months ago, we got an NDA from Deep Mind to enough for another pilot with that.
yes, definitely. I mean, we're now, we're just a few months ago, we got an NDA from Deep Mind to enough for another pilot with that.
S Speaker 147:20Got it. Okay, so good. How big is a team?
Got it. Okay, so good. How big is a team?
Got it. Okay, so good. How big is a team?
Got it. Okay, so good. How big is a team?
S Speaker 347:25So we are five people in total, plus data collection and so on, three primary founders, yes, those are contractors.
So we are five people in total, plus data collection and so on, three primary founders, yes, those are contractors.
So we are five people in total, plus data collection and so on, three primary founders, yes, those are contractors.
So we are five people in total, plus data collection and so on, three primary founders, yes, those are contractors.
47:38Okay, thank you.
47:42So any questions for me?
So any questions for me?
So any questions for me?
So any questions for me?
S Speaker 347:46Yes, just just a quick question in terms of materials, are you looking for our deck? Are you looking for more like customer, like materials, or like both kind of what information are you looking to get out of
Yes, just just a quick question in terms of materials, are you looking for our deck? Are you looking for more like customer, like materials, or like both kind of what information are you looking to get out of
Yes, just just a quick question in terms of materials, are you looking for our deck? Are you looking for more like customer, like materials, or like both kind of what information are you looking to get out of
Yes, just just a quick question in terms of materials, are you looking for our deck? Are you looking for more like customer, like materials, or like both kind of what information are you looking to get out of
S Speaker 147:58this deck, like whatever your pitch deck is, plus your customer material, which is think of us as if you were building chips and software that would go into these robots to
this deck, like whatever your pitch deck is, plus your customer material, which is think of us as if you were building chips and software that would go into these robots to
this deck, like whatever your pitch deck is, plus your customer material, which is think of us as if you were building chips and software that would go into these robots to
this deck, like whatever your pitch deck is, plus your customer material, which is think of us as if you were building chips and software that would go into these robots to
48:13to run inference on these robots, for
to run inference on these robots, for
to run inference on these robots, for
to run inference on these robots, for
S Speaker 349:10as well. Good question there. Just to clarify, don't take too much of your of your time, just so I fully understand. So are you guys building the Vlas as well?
as well. Good question there. Just to clarify, don't take too much of your of your time, just so I fully understand. So are you guys building the Vlas as well?
as well. Good question there. Just to clarify, don't take too much of your of your time, just so I fully understand. So are you guys building the Vlas as well?
as well. Good question there. Just to clarify, don't take too much of your of your time, just so I fully understand. So are you guys building the Vlas as well?
S Speaker 149:21We are building Vlas as well, okay, but these are reference Vlas that we have yes to test
We are building Vlas as well, okay, but these are reference Vlas that we have yes to test
We are building Vlas as well, okay, but these are reference Vlas that we have yes to test
We are building Vlas as well, okay, but these are reference Vlas that we have yes to test
S Speaker 349:31your Yeah, exactly, yeah. Understand, understand, okay, okay, yep, no problem,
your Yeah, exactly, yeah. Understand, understand, okay, okay, yep, no problem,
your Yeah, exactly, yeah. Understand, understand, okay, okay, yep, no problem,
your Yeah, exactly, yeah. Understand, understand, okay, okay, yep, no problem,
S Speaker 149:38yeah. Thank you. Thanks Dan,
yeah. Thank you. Thanks Dan,
yeah. Thank you. Thanks Dan,
yeah. Thank you. Thanks Dan,
S Speaker 349:41thank you guys. It was a pleasure meeting you both, and have a good day. You too. Thanks. Bye, bye, bye.
thank you guys. It was a pleasure meeting you both, and have a good day. You too. Thanks. Bye, bye, bye.
thank you guys. It was a pleasure meeting you both, and have a good day. You too. Thanks. Bye, bye, bye.
thank you guys. It was a pleasure meeting you both, and have a good day. You too. Thanks. Bye, bye, bye.