Meeting: ComputerX + Tushar
Thu, Jun 12
12:30 PM
21 min
Priyesh P
Introduction and Initial Greetings
1:22
Ove
URL: https://otter.ai/u/bths3ubCPd5ouc2Y8gVchdfysgs
Downloaded: 2025-12-21T21:59:31.399589
Method: text_extraction
============================================================

S Speaker 11:22Hey, hello, Hey, hi, Sherry. Good to see you again. Good to see you again. How are you? I'm good Tushar, the managing director at Qualcomm Ventures has also joined. Hi. Tushar, nice to meet you.
Hey, hello, Hey, hi, Sherry. Good to see you again. Good to see you again. How are you? I'm good Tushar, the managing director at Qualcomm Ventures has also joined. Hi. Tushar, nice to meet you.
Hey, hello, Hey, hi, Sherry. Good to see you again. Good to see you again. How are you? I'm good Tushar, the managing director at Qualcomm Ventures has also joined. Hi. Tushar, nice to meet you.
Hey, hello, Hey, hi, Sherry. Good to see you again. Good to see you again. How are you? I'm good Tushar, the managing director at Qualcomm Ventures has also joined. Hi. Tushar, nice to meet you.
S Speaker 21:32Nice to meet you as well. Are you? You're based out of Stanford. Is that?
Nice to meet you as well. Are you? You're based out of Stanford. Is that?
Nice to meet you as well. Are you? You're based out of Stanford. Is that?
Nice to meet you as well. Are you? You're based out of Stanford. Is that?
S Speaker 31:37Yeah, I live close to Stanford, so our office is in Palo Alto, working from home today.
Yeah, I live close to Stanford, so our office is in Palo Alto, working from home today.
Yeah, I live close to Stanford, so our office is in Palo Alto, working from home today.
Yeah, I live close to Stanford, so our office is in Palo Alto, working from home today.
S Speaker 32:26your background? Yeah, sure. Yeah. So I'm Bucha co founders and CEO of computer ads, and previously, I did my PhD in computer science from Stanford, and I work at Baidu, at Google, AI before. So I did a lot of research work on economic samples, reinforcement learning, etc, and I started this company with my co founder last year. You know, after seeing like, some recent progress last year, computer use so anthropic, you know, released their first computer use endpoint at the end of last year, and then we tried it out, really excited about it. And we see a lot of potential in this technology, because it's, you know, general purpose. It can control the user's mouse and keyboard to basically do any task a normal user could do on their computer once the model fully matures. So there's a lot of exciting opportunities going go going in this field. However, we also notice there's a lot of limitations in computer use, foundation models as well. You probably know, like some accuracy, especially on the desktop environment, different operating systems, different you know, desktop layout, you know different operations you want to perform on different desktop software, the accuracy can go down drastically. You might see like, like, for example, open eyes operator or some web agents has relatively, you know, good metrics that reported by them, but that's very well constrained environments and also in the browser. And also they have like, because of brother, like, for example, take open eyes operator for example, because the brother is opened up on their own server in their controlled virtual environment. So they can control exactly the resolution of the virtual environment, of the virtual you know, the brother. But one of the key limitations of current consider use foundation models is, if you're the inference you want to run on is not, you know, exactly same as like your training data, like, for example, your your drawing, consider use on the bit monitor, the accuracy will just, you know, drop drastically. So that's one of the key limitations, the accuracy. The other part is cost that I think the local, you know, model play could actually, you know, come in and, you know, play a huge role here. So based on our internal benchmarking data, if we just, you know, use anthropic or open AI's commercial computer use endpoint, a simple task that involves, like, 20 screenshots could easily, you know, incur a bill of $4 because one screenshot in doing computer use influence on one screenshot cost roughly, like, you know, 10 to 20 cents, yeah, and then, yeah. But if you know everything is local that you know the AI inference calls. You know that that is basically zero for the end users, but that also comes in another challenge, which is the latency. Right now, if there are two ways you know you can do the inference, either like using the screen, you take the screenshots, you send to the server, and then you do the inference on the cloud, the cost will be higher, and then the latency primarily comes from the network latency, or you just, you know, you take the screenshot, and then you do the inference entirely locally on the user's computer. So we did it Mac, a Mac Book, and for a small, you know, like a vision language model, it could takes like, 30 seconds to a minute to do the inference on one image. So if you know, if something like time sensitive, or, you know, requires fast processing speed, then doing local, you know, has some challenge. Yeah, so these are the challenges we notice and what we are working towards, you know, like, how, you know,
your background? Yeah, sure. Yeah. So I'm Bucha co founders and CEO of computer ads, and previously, I did my PhD in computer science from Stanford, and I work at Baidu, at Google, AI before. So I did a lot of research work on economic samples, reinforcement learning, etc, and I started this company with my co founder last year. You know, after seeing like, some recent progress last year, computer use so anthropic, you know, released their first computer use endpoint at the end of last year, and then we tried it out, really excited about it. And we see a lot of potential in this technology, because it's, you know, general purpose. It can control the user's mouse and keyboard to basically do any task a normal user could do on their computer once the model fully matures. So there's a lot of exciting opportunities going go going in this field. However, we also notice there's a lot of limitations in computer use, foundation models as well. You probably know, like some accuracy, especially on the desktop environment, different operating systems, different you know, desktop layout, you know different operations you want to perform on different desktop software, the accuracy can go down drastically. You might see like, like, for example, open eyes operator or some web agents has relatively, you know, good metrics that reported by them, but that's very well constrained environments and also in the browser. And also they have like, because of brother, like, for example, take open eyes operator for example, because the brother is opened up on their own server in their controlled virtual environment. So they can control exactly the resolution of the virtual environment, of the virtual you know, the brother. But one of the key limitations of current consider use foundation models is, if you're the inference you want to run on is not, you know, exactly same as like your training data, like, for example, your your drawing, consider use on the bit monitor, the accuracy will just, you know, drop drastically. So that's one of the key limitations, the accuracy. The other part is cost that I think the local, you know, model play could actually, you know, come in and, you know, play a huge role here. So based on our internal benchmarking data, if we just, you know, use anthropic or open AI's commercial computer use endpoint, a simple task that involves, like, 20 screenshots could easily, you know, incur a bill of $4 because one screenshot in doing computer use influence on one screenshot cost roughly, like, you know, 10 to 20 cents, yeah, and then, yeah. But if you know everything is local that you know the AI inference calls. You know that that is basically zero for the end users, but that also comes in another challenge, which is the latency. Right now, if there are two ways you know you can do the inference, either like using the screen, you take the screenshots, you send to the server, and then you do the inference on the cloud, the cost will be higher, and then the latency primarily comes from the network latency, or you just, you know, you take the screenshot, and then you do the inference entirely locally on the user's computer. So we did it Mac, a Mac Book, and for a small, you know, like a vision language model, it could takes like, 30 seconds to a minute to do the inference on one image. So if you know, if something like time sensitive, or, you know, requires fast processing speed, then doing local, you know, has some challenge. Yeah, so these are the challenges we notice and what we are working towards, you know, like, how, you know,
your background? Yeah, sure. Yeah. So I'm Bucha co founders and CEO of computer ads, and previously, I did my PhD in computer science from Stanford, and I work at Baidu, at Google, AI before. So I did a lot of research work on economic samples, reinforcement learning, etc, and I started this company with my co founder last year. You know, after seeing like, some recent progress last year, computer use so anthropic, you know, released their first computer use endpoint at the end of last year, and then we tried it out, really excited about it. And we see a lot of potential in this technology, because it's, you know, general purpose. It can control the user's mouse and keyboard to basically do any task a normal user could do on their computer once the model fully matures. So there's a lot of exciting opportunities going go going in this field. However, we also notice there's a lot of limitations in computer use, foundation models as well. You probably know, like some accuracy, especially on the desktop environment, different operating systems, different you know, desktop layout, you know different operations you want to perform on different desktop software, the accuracy can go down drastically. You might see like, like, for example, open eyes operator or some web agents has relatively, you know, good metrics that reported by them, but that's very well constrained environments and also in the browser. And also they have like, because of brother, like, for example, take open eyes operator for example, because the brother is opened up on their own server in their controlled virtual environment. So they can control exactly the resolution of the virtual environment, of the virtual you know, the brother. But one of the key limitations of current consider use foundation models is, if you're the inference you want to run on is not, you know, exactly same as like your training data, like, for example, your your drawing, consider use on the bit monitor, the accuracy will just, you know, drop drastically. So that's one of the key limitations, the accuracy. The other part is cost that I think the local, you know, model play could actually, you know, come in and, you know, play a huge role here. So based on our internal benchmarking data, if we just, you know, use anthropic or open AI's commercial computer use endpoint, a simple task that involves, like, 20 screenshots could easily, you know, incur a bill of $4 because one screenshot in doing computer use influence on one screenshot cost roughly, like, you know, 10 to 20 cents, yeah, and then, yeah. But if you know everything is local that you know the AI inference calls. You know that that is basically zero for the end users, but that also comes in another challenge, which is the latency. Right now, if there are two ways you know you can do the inference, either like using the screen, you take the screenshots, you send to the server, and then you do the inference on the cloud, the cost will be higher, and then the latency primarily comes from the network latency, or you just, you know, you take the screenshot, and then you do the inference entirely locally on the user's computer. So we did it Mac, a Mac Book, and for a small, you know, like a vision language model, it could takes like, 30 seconds to a minute to do the inference on one image. So if you know, if something like time sensitive, or, you know, requires fast processing speed, then doing local, you know, has some challenge. Yeah, so these are the challenges we notice and what we are working towards, you know, like, how, you know,
your background? Yeah, sure. Yeah. So I'm Bucha co founders and CEO of computer ads, and previously, I did my PhD in computer science from Stanford, and I work at Baidu, at Google, AI before. So I did a lot of research work on economic samples, reinforcement learning, etc, and I started this company with my co founder last year. You know, after seeing like, some recent progress last year, computer use so anthropic, you know, released their first computer use endpoint at the end of last year, and then we tried it out, really excited about it. And we see a lot of potential in this technology, because it's, you know, general purpose. It can control the user's mouse and keyboard to basically do any task a normal user could do on their computer once the model fully matures. So there's a lot of exciting opportunities going go going in this field. However, we also notice there's a lot of limitations in computer use, foundation models as well. You probably know, like some accuracy, especially on the desktop environment, different operating systems, different you know, desktop layout, you know different operations you want to perform on different desktop software, the accuracy can go down drastically. You might see like, like, for example, open eyes operator or some web agents has relatively, you know, good metrics that reported by them, but that's very well constrained environments and also in the browser. And also they have like, because of brother, like, for example, take open eyes operator for example, because the brother is opened up on their own server in their controlled virtual environment. So they can control exactly the resolution of the virtual environment, of the virtual you know, the brother. But one of the key limitations of current consider use foundation models is, if you're the inference you want to run on is not, you know, exactly same as like your training data, like, for example, your your drawing, consider use on the bit monitor, the accuracy will just, you know, drop drastically. So that's one of the key limitations, the accuracy. The other part is cost that I think the local, you know, model play could actually, you know, come in and, you know, play a huge role here. So based on our internal benchmarking data, if we just, you know, use anthropic or open AI's commercial computer use endpoint, a simple task that involves, like, 20 screenshots could easily, you know, incur a bill of $4 because one screenshot in doing computer use influence on one screenshot cost roughly, like, you know, 10 to 20 cents, yeah, and then, yeah. But if you know everything is local that you know the AI inference calls. You know that that is basically zero for the end users, but that also comes in another challenge, which is the latency. Right now, if there are two ways you know you can do the inference, either like using the screen, you take the screenshots, you send to the server, and then you do the inference on the cloud, the cost will be higher, and then the latency primarily comes from the network latency, or you just, you know, you take the screenshot, and then you do the inference entirely locally on the user's computer. So we did it Mac, a Mac Book, and for a small, you know, like a vision language model, it could takes like, 30 seconds to a minute to do the inference on one image. So if you know, if something like time sensitive, or, you know, requires fast processing speed, then doing local, you know, has some challenge. Yeah, so these are the challenges we notice and what we are working towards, you know, like, how, you know,
S Speaker 26:22how things like, which is cost, I get it, so I guess you can run accuracy. I guess would be the big one, dissolve, yeah. And then, how do you and then, so you are? You making use of computer use API, or
how things like, which is cost, I get it, so I guess you can run accuracy. I guess would be the big one, dissolve, yeah. And then, how do you and then, so you are? You making use of computer use API, or
how things like, which is cost, I get it, so I guess you can run accuracy. I guess would be the big one, dissolve, yeah. And then, how do you and then, so you are? You making use of computer use API, or
how things like, which is cost, I get it, so I guess you can run accuracy. I guess would be the big one, dissolve, yeah. And then, how do you and then, so you are? You making use of computer use API, or
S Speaker 36:37no, we are. We're doing our own multi model architecture. So there are two schools of excellence right now. One is that, you know, the big players, like open AI and anthropic, you know, they will, you know, increase their computer use foundation models. Maybe in six months, you know, they will release a new foundation model, which will be much better than the current one. So a lot of the technical challenges will be resolved. So that is, you know, one school of thought, you know, similar to any other foundation models, you just rely on the big players, right? The second school of thought is, there's a lot of opportunities for small startups on specific use cases. Like, for example, like a lot of our users, you know, for example, they have, like, a legacy system they want to do GUI automation. And for those specific type of software, you can collect some data yourself, maybe like 1000s of screenshots, or maybe even more, depending on the complexity of the software and operations you want to automate. And you fine tune a small model for that specific use case, and then you can get really good performance on that specific, you know, use case where software. But the risk here is that, what if, like, the next radius of the foundation model, you know, just overturned all the other fine tuning, you know, the small work you did. So that's like, the risk.
no, we are. We're doing our own multi model architecture. So there are two schools of excellence right now. One is that, you know, the big players, like open AI and anthropic, you know, they will, you know, increase their computer use foundation models. Maybe in six months, you know, they will release a new foundation model, which will be much better than the current one. So a lot of the technical challenges will be resolved. So that is, you know, one school of thought, you know, similar to any other foundation models, you just rely on the big players, right? The second school of thought is, there's a lot of opportunities for small startups on specific use cases. Like, for example, like a lot of our users, you know, for example, they have, like, a legacy system they want to do GUI automation. And for those specific type of software, you can collect some data yourself, maybe like 1000s of screenshots, or maybe even more, depending on the complexity of the software and operations you want to automate. And you fine tune a small model for that specific use case, and then you can get really good performance on that specific, you know, use case where software. But the risk here is that, what if, like, the next radius of the foundation model, you know, just overturned all the other fine tuning, you know, the small work you did. So that's like, the risk.
no, we are. We're doing our own multi model architecture. So there are two schools of excellence right now. One is that, you know, the big players, like open AI and anthropic, you know, they will, you know, increase their computer use foundation models. Maybe in six months, you know, they will release a new foundation model, which will be much better than the current one. So a lot of the technical challenges will be resolved. So that is, you know, one school of thought, you know, similar to any other foundation models, you just rely on the big players, right? The second school of thought is, there's a lot of opportunities for small startups on specific use cases. Like, for example, like a lot of our users, you know, for example, they have, like, a legacy system they want to do GUI automation. And for those specific type of software, you can collect some data yourself, maybe like 1000s of screenshots, or maybe even more, depending on the complexity of the software and operations you want to automate. And you fine tune a small model for that specific use case, and then you can get really good performance on that specific, you know, use case where software. But the risk here is that, what if, like, the next radius of the foundation model, you know, just overturned all the other fine tuning, you know, the small work you did. So that's like, the risk.
no, we are. We're doing our own multi model architecture. So there are two schools of excellence right now. One is that, you know, the big players, like open AI and anthropic, you know, they will, you know, increase their computer use foundation models. Maybe in six months, you know, they will release a new foundation model, which will be much better than the current one. So a lot of the technical challenges will be resolved. So that is, you know, one school of thought, you know, similar to any other foundation models, you just rely on the big players, right? The second school of thought is, there's a lot of opportunities for small startups on specific use cases. Like, for example, like a lot of our users, you know, for example, they have, like, a legacy system they want to do GUI automation. And for those specific type of software, you can collect some data yourself, maybe like 1000s of screenshots, or maybe even more, depending on the complexity of the software and operations you want to automate. And you fine tune a small model for that specific use case, and then you can get really good performance on that specific, you know, use case where software. But the risk here is that, what if, like, the next radius of the foundation model, you know, just overturned all the other fine tuning, you know, the small work you did. So that's like, the risk.
S Speaker 27:59Okay, so, but the which is, when you say vertical, what do you mean? Like, because, like, what would be some examples of the vertical?
Okay, so, but the which is, when you say vertical, what do you mean? Like, because, like, what would be some examples of the vertical?
Okay, so, but the which is, when you say vertical, what do you mean? Like, because, like, what would be some examples of the vertical?
Okay, so, but the which is, when you say vertical, what do you mean? Like, because, like, what would be some examples of the vertical?
S Speaker 38:08Yeah, primarily, like, software related. Because, if you want to have a good, you know, computer use model, our specific use case, then you need to have screenshots of that specific use case. Say, if you want to, like, you know, do automation for a legacy, you know, SAP software, then you just collect a lot of screenshots and a lot of like, you know, clicking, you know, typing, you know, the common operation operations people usually do for that software. And then you can, you know, 1000s of data points. I think it will be a good starting point to see how good the model could be.
Yeah, primarily, like, software related. Because, if you want to have a good, you know, computer use model, our specific use case, then you need to have screenshots of that specific use case. Say, if you want to, like, you know, do automation for a legacy, you know, SAP software, then you just collect a lot of screenshots and a lot of like, you know, clicking, you know, typing, you know, the common operation operations people usually do for that software. And then you can, you know, 1000s of data points. I think it will be a good starting point to see how good the model could be.
Yeah, primarily, like, software related. Because, if you want to have a good, you know, computer use model, our specific use case, then you need to have screenshots of that specific use case. Say, if you want to, like, you know, do automation for a legacy, you know, SAP software, then you just collect a lot of screenshots and a lot of like, you know, clicking, you know, typing, you know, the common operation operations people usually do for that software. And then you can, you know, 1000s of data points. I think it will be a good starting point to see how good the model could be.
Yeah, primarily, like, software related. Because, if you want to have a good, you know, computer use model, our specific use case, then you need to have screenshots of that specific use case. Say, if you want to, like, you know, do automation for a legacy, you know, SAP software, then you just collect a lot of screenshots and a lot of like, you know, clicking, you know, typing, you know, the common operation operations people usually do for that software. And then you can, you know, 1000s of data points. I think it will be a good starting point to see how good the model could be.
8:44But that's how you would normally optimize for it. And
But that's how you would normally optimize for it. And
But that's how you would normally optimize for it. And
But that's how you would normally optimize for it. And
S Speaker 28:47Lou, would you foresee yourself like, do you want to go after you know business users, or like the SAP use case that you mentioned? Or do you want to go? Or, you know, consumer, which is,
Lou, would you foresee yourself like, do you want to go after you know business users, or like the SAP use case that you mentioned? Or do you want to go? Or, you know, consumer, which is,
Lou, would you foresee yourself like, do you want to go after you know business users, or like the SAP use case that you mentioned? Or do you want to go? Or, you know, consumer, which is,
Lou, would you foresee yourself like, do you want to go after you know business users, or like the SAP use case that you mentioned? Or do you want to go? Or, you know, consumer, which is,
S Speaker 39:04yeah, we are. We want to go with consumers, because we know, although we heard from, like, as we we heard the SAP use cases, we're actually talking to SAP Mars, not like direct, you know, enterprise customers. So we don't so because of that. We want to remove the, you know, the middle layer and work with the end users who work with who use ourselves, who use with our product directly. And we're actually more focused on software such as, like Slack notion, you know, like Google Suite, you know, the more modern software suite. And then we want to do good automation on this type of software so that the end users can, you know, use our product directly. And
yeah, we are. We want to go with consumers, because we know, although we heard from, like, as we we heard the SAP use cases, we're actually talking to SAP Mars, not like direct, you know, enterprise customers. So we don't so because of that. We want to remove the, you know, the middle layer and work with the end users who work with who use ourselves, who use with our product directly. And we're actually more focused on software such as, like Slack notion, you know, like Google Suite, you know, the more modern software suite. And then we want to do good automation on this type of software so that the end users can, you know, use our product directly. And
yeah, we are. We want to go with consumers, because we know, although we heard from, like, as we we heard the SAP use cases, we're actually talking to SAP Mars, not like direct, you know, enterprise customers. So we don't so because of that. We want to remove the, you know, the middle layer and work with the end users who work with who use ourselves, who use with our product directly. And we're actually more focused on software such as, like Slack notion, you know, like Google Suite, you know, the more modern software suite. And then we want to do good automation on this type of software so that the end users can, you know, use our product directly. And
yeah, we are. We want to go with consumers, because we know, although we heard from, like, as we we heard the SAP use cases, we're actually talking to SAP Mars, not like direct, you know, enterprise customers. So we don't so because of that. We want to remove the, you know, the middle layer and work with the end users who work with who use ourselves, who use with our product directly. And we're actually more focused on software such as, like Slack notion, you know, like Google Suite, you know, the more modern software suite. And then we want to do good automation on this type of software so that the end users can, you know, use our product directly. And
S Speaker 29:55so essentially, you want to go after like, consumer. Yeah, consumers, for consumers. And how do you because consumer use cases, they're all over the place, right? And most times, consumers don't even know what they want, right? In the past, whenever I tried computer, and I do think like as models get better, like in a year from now, these gets very, very reliable, yeah, but whenever I've tried it, one, I found them to be very unreliable. And then two, how do you measure that computer use for a certain use case will actually be much better compared to actually the human trying and doing this, like, for example, I'll give you an example, right? If I'm trying to go travel, bike, ticket, then, then what I found is, like, I tried a few computer use past and, you know, products in the past. And then what ends up happening is, if I have to describe everything, then I might as well. It's better for me to just visually click through things on Kayak or United or what have you, than then actually telling the model what to do. So for that particular use case, the computer use doesn't really work out for me. I don't know. And then for shopping, for example, shopping and such a visual experience that I don't know if computer use would be good for shopping. So like, Where does and if I have to use computer use models for querying the web, then querying the web is like this. You already have chat, GPT or Gemini to query. So where do you think, like, what type? What are some examples of use cases where you see a lot of value, where computer use use cases for consumers would be,
so essentially, you want to go after like, consumer. Yeah, consumers, for consumers. And how do you because consumer use cases, they're all over the place, right? And most times, consumers don't even know what they want, right? In the past, whenever I tried computer, and I do think like as models get better, like in a year from now, these gets very, very reliable, yeah, but whenever I've tried it, one, I found them to be very unreliable. And then two, how do you measure that computer use for a certain use case will actually be much better compared to actually the human trying and doing this, like, for example, I'll give you an example, right? If I'm trying to go travel, bike, ticket, then, then what I found is, like, I tried a few computer use past and, you know, products in the past. And then what ends up happening is, if I have to describe everything, then I might as well. It's better for me to just visually click through things on Kayak or United or what have you, than then actually telling the model what to do. So for that particular use case, the computer use doesn't really work out for me. I don't know. And then for shopping, for example, shopping and such a visual experience that I don't know if computer use would be good for shopping. So like, Where does and if I have to use computer use models for querying the web, then querying the web is like this. You already have chat, GPT or Gemini to query. So where do you think, like, what type? What are some examples of use cases where you see a lot of value, where computer use use cases for consumers would be,
so essentially, you want to go after like, consumer. Yeah, consumers, for consumers. And how do you because consumer use cases, they're all over the place, right? And most times, consumers don't even know what they want, right? In the past, whenever I tried computer, and I do think like as models get better, like in a year from now, these gets very, very reliable, yeah, but whenever I've tried it, one, I found them to be very unreliable. And then two, how do you measure that computer use for a certain use case will actually be much better compared to actually the human trying and doing this, like, for example, I'll give you an example, right? If I'm trying to go travel, bike, ticket, then, then what I found is, like, I tried a few computer use past and, you know, products in the past. And then what ends up happening is, if I have to describe everything, then I might as well. It's better for me to just visually click through things on Kayak or United or what have you, than then actually telling the model what to do. So for that particular use case, the computer use doesn't really work out for me. I don't know. And then for shopping, for example, shopping and such a visual experience that I don't know if computer use would be good for shopping. So like, Where does and if I have to use computer use models for querying the web, then querying the web is like this. You already have chat, GPT or Gemini to query. So where do you think, like, what type? What are some examples of use cases where you see a lot of value, where computer use use cases for consumers would be,
so essentially, you want to go after like, consumer. Yeah, consumers, for consumers. And how do you because consumer use cases, they're all over the place, right? And most times, consumers don't even know what they want, right? In the past, whenever I tried computer, and I do think like as models get better, like in a year from now, these gets very, very reliable, yeah, but whenever I've tried it, one, I found them to be very unreliable. And then two, how do you measure that computer use for a certain use case will actually be much better compared to actually the human trying and doing this, like, for example, I'll give you an example, right? If I'm trying to go travel, bike, ticket, then, then what I found is, like, I tried a few computer use past and, you know, products in the past. And then what ends up happening is, if I have to describe everything, then I might as well. It's better for me to just visually click through things on Kayak or United or what have you, than then actually telling the model what to do. So for that particular use case, the computer use doesn't really work out for me. I don't know. And then for shopping, for example, shopping and such a visual experience that I don't know if computer use would be good for shopping. So like, Where does and if I have to use computer use models for querying the web, then querying the web is like this. You already have chat, GPT or Gemini to query. So where do you think, like, what type? What are some examples of use cases where you see a lot of value, where computer use use cases for consumers would be,
13:37So what would be some examples that you're going after?
So what would be some examples that you're going after?
So what would be some examples that you're going after?
So what would be some examples that you're going after?
S Speaker 313:41So some of the examples were trying, you know, really trying to improve our models performance. I will be like linking automation, like linking slack notion, like, for example, for example, like a lot of users, they they use linking like, for example, for outreach, for like, you know, for recruiting, for marketing automation. And usually, like, the next step will be, you know, you you have the data you collected from linking. You know, you either save into notion where you know you like, save to a Google sheet. And then there's like, follow up automation. You can continue. You can like, you know, build up how so once we tackle the LinkedIn use case. We there's more we can add to the automation pipeline.
So some of the examples were trying, you know, really trying to improve our models performance. I will be like linking automation, like linking slack notion, like, for example, for example, like a lot of users, they they use linking like, for example, for outreach, for like, you know, for recruiting, for marketing automation. And usually, like, the next step will be, you know, you you have the data you collected from linking. You know, you either save into notion where you know you like, save to a Google sheet. And then there's like, follow up automation. You can continue. You can like, you know, build up how so once we tackle the LinkedIn use case. We there's more we can add to the automation pipeline.
So some of the examples were trying, you know, really trying to improve our models performance. I will be like linking automation, like linking slack notion, like, for example, for example, like a lot of users, they they use linking like, for example, for outreach, for like, you know, for recruiting, for marketing automation. And usually, like, the next step will be, you know, you you have the data you collected from linking. You know, you either save into notion where you know you like, save to a Google sheet. And then there's like, follow up automation. You can continue. You can like, you know, build up how so once we tackle the LinkedIn use case. We there's more we can add to the automation pipeline.
So some of the examples were trying, you know, really trying to improve our models performance. I will be like linking automation, like linking slack notion, like, for example, for example, like a lot of users, they they use linking like, for example, for outreach, for like, you know, for recruiting, for marketing automation. And usually, like, the next step will be, you know, you you have the data you collected from linking. You know, you either save into notion where you know you like, save to a Google sheet. And then there's like, follow up automation. You can continue. You can like, you know, build up how so once we tackle the LinkedIn use case. We there's more we can add to the automation pipeline.
14:30get information, LinkedIn, create email,
get information, LinkedIn, create email,
get information, LinkedIn, create email,
get information, LinkedIn, create email,
S Speaker 314:33yeah, yeah, that's just my example.
yeah, yeah, that's just my example.
yeah, yeah, that's just my example.
yeah, yeah, that's just my example.
14:37Yeah, okay, got it.
S Speaker 114:44I was just asking for a use case like that. Would you not prefer a browser use agent against a computer use agent?
I was just asking for a use case like that. Would you not prefer a browser use agent against a computer use agent?
I was just asking for a use case like that. Would you not prefer a browser use agent against a computer use agent?
I was just asking for a use case like that. Would you not prefer a browser use agent against a computer use agent?
S Speaker 314:52Yeah. So if it's just linking automation, a web agent could definitely do that. But if you want to build a whole automation pipeline. If any of the software you know, you you have on the pipeline, you know, doesn't have a web version, then you know, you know, the the computer use can solve the issue. But for broader use, you have to, you know, integrate different, different tools for you know, have, like, a workaround. That's why, like we've seen, computer uses a more general, more flexible solution, and everything you do on a broader you can do that in a computer.
Yeah. So if it's just linking automation, a web agent could definitely do that. But if you want to build a whole automation pipeline. If any of the software you know, you you have on the pipeline, you know, doesn't have a web version, then you know, you know, the the computer use can solve the issue. But for broader use, you have to, you know, integrate different, different tools for you know, have, like, a workaround. That's why, like we've seen, computer uses a more general, more flexible solution, and everything you do on a broader you can do that in a computer.
Yeah. So if it's just linking automation, a web agent could definitely do that. But if you want to build a whole automation pipeline. If any of the software you know, you you have on the pipeline, you know, doesn't have a web version, then you know, you know, the the computer use can solve the issue. But for broader use, you have to, you know, integrate different, different tools for you know, have, like, a workaround. That's why, like we've seen, computer uses a more general, more flexible solution, and everything you do on a broader you can do that in a computer.
Yeah. So if it's just linking automation, a web agent could definitely do that. But if you want to build a whole automation pipeline. If any of the software you know, you you have on the pipeline, you know, doesn't have a web version, then you know, you know, the the computer use can solve the issue. But for broader use, you have to, you know, integrate different, different tools for you know, have, like, a workaround. That's why, like we've seen, computer uses a more general, more flexible solution, and everything you do on a broader you can do that in a computer.
S Speaker 215:31And then so when I essentially this runs as a system service and it has an access so like, for example, if I'm running outlook and PowerPoint and teams and zoom and everything, so it can switch between all these apps,
And then so when I essentially this runs as a system service and it has an access so like, for example, if I'm running outlook and PowerPoint and teams and zoom and everything, so it can switch between all these apps,
And then so when I essentially this runs as a system service and it has an access so like, for example, if I'm running outlook and PowerPoint and teams and zoom and everything, so it can switch between all these apps,
And then so when I essentially this runs as a system service and it has an access so like, for example, if I'm running outlook and PowerPoint and teams and zoom and everything, so it can switch between all these apps,
S Speaker 315:46right? Yeah, depending on the specific operating system the you know, the computer, the virtual computer is running on, and whether the software you know has support for that operating system
right? Yeah, depending on the specific operating system the you know, the computer, the virtual computer is running on, and whether the software you know has support for that operating system
right? Yeah, depending on the specific operating system the you know, the computer, the virtual computer is running on, and whether the software you know has support for that operating system
right? Yeah, depending on the specific operating system the you know, the computer, the virtual computer is running on, and whether the software you know has support for that operating system
15:58and our sort of, I guess, Mac OS,
and our sort of, I guess, Mac OS,
and our sort of, I guess, Mac OS,
and our sort of, I guess, Mac OS,
S Speaker 316:02Mac OS, yeah, we're actually like, we are prioritizing a Linux right now because there's a lot of open source packages, you know, supporting what we want to do. So, but we are, yeah,
Mac OS, yeah, we're actually like, we are prioritizing a Linux right now because there's a lot of open source packages, you know, supporting what we want to do. So, but we are, yeah,
Mac OS, yeah, we're actually like, we are prioritizing a Linux right now because there's a lot of open source packages, you know, supporting what we want to do. So, but we are, yeah,
Mac OS, yeah, we're actually like, we are prioritizing a Linux right now because there's a lot of open source packages, you know, supporting what we want to do. So, but we are, yeah,
16:14but not Mac OS, Mac, Mac OS,
but not Mac OS, Mac, Mac OS,
but not Mac OS, Mac, Mac OS,
but not Mac OS, Mac, Mac OS,
S Speaker 316:18Mac OS, we tried it. MacOS consumes more resources. So actually, let me take a step back. So we we did two launches for a computer, use our computer, use product. The first product, our agent operates the user's computer directly. And we realized, you know, our users don't like it, because usually my user has one computer. When the agent is using their computer, the user has nothing to do other than waiting. And the the even worse, they don't know when the agent will finish. Yeah. So that's like, very frustrating. And yeah. And then, so after the first launch, we got all this feedback, and then we we are developing our second product, which is, instead of for the agent to operate the users computer directly, we open up whisping up a virtual computer, yeah, a Docker, yeah. Basically, you know, a virtual instance on you this computer, and the agent you know, can, you know, operate freely in that virtual computer. And for, for Mac operating system, it's just a little bit heavier. It can, you know, takes up more resources and more, you know, compared to, like a Linux virtual environment, because the, you know, the Linux is more to shore, and the more open source support. So we can have a more, more efficient you will
Mac OS, we tried it. MacOS consumes more resources. So actually, let me take a step back. So we we did two launches for a computer, use our computer, use product. The first product, our agent operates the user's computer directly. And we realized, you know, our users don't like it, because usually my user has one computer. When the agent is using their computer, the user has nothing to do other than waiting. And the the even worse, they don't know when the agent will finish. Yeah. So that's like, very frustrating. And yeah. And then, so after the first launch, we got all this feedback, and then we we are developing our second product, which is, instead of for the agent to operate the users computer directly, we open up whisping up a virtual computer, yeah, a Docker, yeah. Basically, you know, a virtual instance on you this computer, and the agent you know, can, you know, operate freely in that virtual computer. And for, for Mac operating system, it's just a little bit heavier. It can, you know, takes up more resources and more, you know, compared to, like a Linux virtual environment, because the, you know, the Linux is more to shore, and the more open source support. So we can have a more, more efficient you will
Mac OS, we tried it. MacOS consumes more resources. So actually, let me take a step back. So we we did two launches for a computer, use our computer, use product. The first product, our agent operates the user's computer directly. And we realized, you know, our users don't like it, because usually my user has one computer. When the agent is using their computer, the user has nothing to do other than waiting. And the the even worse, they don't know when the agent will finish. Yeah. So that's like, very frustrating. And yeah. And then, so after the first launch, we got all this feedback, and then we we are developing our second product, which is, instead of for the agent to operate the users computer directly, we open up whisping up a virtual computer, yeah, a Docker, yeah. Basically, you know, a virtual instance on you this computer, and the agent you know, can, you know, operate freely in that virtual computer. And for, for Mac operating system, it's just a little bit heavier. It can, you know, takes up more resources and more, you know, compared to, like a Linux virtual environment, because the, you know, the Linux is more to shore, and the more open source support. So we can have a more, more efficient you will
Mac OS, we tried it. MacOS consumes more resources. So actually, let me take a step back. So we we did two launches for a computer, use our computer, use product. The first product, our agent operates the user's computer directly. And we realized, you know, our users don't like it, because usually my user has one computer. When the agent is using their computer, the user has nothing to do other than waiting. And the the even worse, they don't know when the agent will finish. Yeah. So that's like, very frustrating. And yeah. And then, so after the first launch, we got all this feedback, and then we we are developing our second product, which is, instead of for the agent to operate the users computer directly, we open up whisping up a virtual computer, yeah, a Docker, yeah. Basically, you know, a virtual instance on you this computer, and the agent you know, can, you know, operate freely in that virtual computer. And for, for Mac operating system, it's just a little bit heavier. It can, you know, takes up more resources and more, you know, compared to, like a Linux virtual environment, because the, you know, the Linux is more to shore, and the more open source support. So we can have a more, more efficient you will
S Speaker 217:40have to develop a Mac and a window, because that's where most of the users are. Yeah, for sure. Okay, so right now for testing or beta or alpha, you're doing it on Linux, yeah. Okay, got it, yeah. And so Sherry funding wise, what's the like you're raising a seed round? Is that
have to develop a Mac and a window, because that's where most of the users are. Yeah, for sure. Okay, so right now for testing or beta or alpha, you're doing it on Linux, yeah. Okay, got it, yeah. And so Sherry funding wise, what's the like you're raising a seed round? Is that
have to develop a Mac and a window, because that's where most of the users are. Yeah, for sure. Okay, so right now for testing or beta or alpha, you're doing it on Linux, yeah. Okay, got it, yeah. And so Sherry funding wise, what's the like you're raising a seed round? Is that
have to develop a Mac and a window, because that's where most of the users are. Yeah, for sure. Okay, so right now for testing or beta or alpha, you're doing it on Linux, yeah. Okay, got it, yeah. And so Sherry funding wise, what's the like you're raising a seed round? Is that
S Speaker 318:05we are, yeah? So last year we raised a priest seed round, yeah. And then we're a team of five people right now, and we are thinking about maybe reading this year we'll see. Yeah, we're still, you know, working on our second iteration, and then trying to, you know, differentiate and, you know, really find finding our niche.
we are, yeah? So last year we raised a priest seed round, yeah. And then we're a team of five people right now, and we are thinking about maybe reading this year we'll see. Yeah, we're still, you know, working on our second iteration, and then trying to, you know, differentiate and, you know, really find finding our niche.
we are, yeah? So last year we raised a priest seed round, yeah. And then we're a team of five people right now, and we are thinking about maybe reading this year we'll see. Yeah, we're still, you know, working on our second iteration, and then trying to, you know, differentiate and, you know, really find finding our niche.
we are, yeah? So last year we raised a priest seed round, yeah. And then we're a team of five people right now, and we are thinking about maybe reading this year we'll see. Yeah, we're still, you know, working on our second iteration, and then trying to, you know, differentiate and, you know, really find finding our niche.
S Speaker 218:31Yeah, I mean, there's ways to work with us, but I think, I guess, are you ready for Windows? Yeah, or not yet?
Yeah, I mean, there's ways to work with us, but I think, I guess, are you ready for Windows? Yeah, or not yet?
Yeah, I mean, there's ways to work with us, but I think, I guess, are you ready for Windows? Yeah, or not yet?
Yeah, I mean, there's ways to work with us, but I think, I guess, are you ready for Windows? Yeah, or not yet?
S Speaker 318:38No, we tried Mac OS, and we're focusing on Linux. We haven't really tried Windows yet.
No, we tried Mac OS, and we're focusing on Linux. We haven't really tried Windows yet.
No, we tried Mac OS, and we're focusing on Linux. We haven't really tried Windows yet.
No, we tried Mac OS, and we're focusing on Linux. We haven't really tried Windows yet.
18:43Yeah, okay, so, yeah, we're
Yeah, okay, so, yeah, we're
Yeah, okay, so, yeah, we're
Yeah, okay, so, yeah, we're
S Speaker 218:48Windows on ARM, so that's the key. At some point it'll be good to work with you, but I guess it's better for you. Also make sure a little bit to work with Qualcomm, because it's like, are you ready to scale? Raise the product. Ready to scale?
Windows on ARM, so that's the key. At some point it'll be good to work with you, but I guess it's better for you. Also make sure a little bit to work with Qualcomm, because it's like, are you ready to scale? Raise the product. Ready to scale?
Windows on ARM, so that's the key. At some point it'll be good to work with you, but I guess it's better for you. Also make sure a little bit to work with Qualcomm, because it's like, are you ready to scale? Raise the product. Ready to scale?
Windows on ARM, so that's the key. At some point it'll be good to work with you, but I guess it's better for you. Also make sure a little bit to work with Qualcomm, because it's like, are you ready to scale? Raise the product. Ready to scale?
S Speaker 319:05Yeah. Usually like, what is the stage of the startup you work with? We do
Yeah. Usually like, what is the stage of the startup you work with? We do
Yeah. Usually like, what is the stage of the startup you work with? We do
Yeah. Usually like, what is the stage of the startup you work with? We do
S Speaker 219:11early stage as well, like we invested in a startup called there's some overlap with you. Recently called on text
early stage as well, like we invested in a startup called there's some overlap with you. Recently called on text
early stage as well, like we invested in a startup called there's some overlap with you. Recently called on text
early stage as well, like we invested in a startup called there's some overlap with you. Recently called on text
19:20context. Context,
S Speaker 219:24and then. So we with them, we launched a whole bunch of things they're building, like this, AI productivity suite, kind of like madness, right? So which is, but they do a whole bunch of things on edges. Once we did a partnership with them, we launched with them on a PCS and everything. But they were like, they're Sea Grant seeds to startup. So, but, but their product was ready to run on edge and everything. So we do it, but then you sort of have to be ready for that. I think for for you, I would, I don't know. I mean, like, I think we need some metrics of users using this product, and then product viability across some verticals before making a decision. But I mean, it's broadly speaking, what you're doing is definitely makes sense, and I agree with you. Like, use So, and it's a fine balance between figuring out,
and then. So we with them, we launched a whole bunch of things they're building, like this, AI productivity suite, kind of like madness, right? So which is, but they do a whole bunch of things on edges. Once we did a partnership with them, we launched with them on a PCS and everything. But they were like, they're Sea Grant seeds to startup. So, but, but their product was ready to run on edge and everything. So we do it, but then you sort of have to be ready for that. I think for for you, I would, I don't know. I mean, like, I think we need some metrics of users using this product, and then product viability across some verticals before making a decision. But I mean, it's broadly speaking, what you're doing is definitely makes sense, and I agree with you. Like, use So, and it's a fine balance between figuring out,
and then. So we with them, we launched a whole bunch of things they're building, like this, AI productivity suite, kind of like madness, right? So which is, but they do a whole bunch of things on edges. Once we did a partnership with them, we launched with them on a PCS and everything. But they were like, they're Sea Grant seeds to startup. So, but, but their product was ready to run on edge and everything. So we do it, but then you sort of have to be ready for that. I think for for you, I would, I don't know. I mean, like, I think we need some metrics of users using this product, and then product viability across some verticals before making a decision. But I mean, it's broadly speaking, what you're doing is definitely makes sense, and I agree with you. Like, use So, and it's a fine balance between figuring out,
and then. So we with them, we launched a whole bunch of things they're building, like this, AI productivity suite, kind of like madness, right? So which is, but they do a whole bunch of things on edges. Once we did a partnership with them, we launched with them on a PCS and everything. But they were like, they're Sea Grant seeds to startup. So, but, but their product was ready to run on edge and everything. So we do it, but then you sort of have to be ready for that. I think for for you, I would, I don't know. I mean, like, I think we need some metrics of users using this product, and then product viability across some verticals before making a decision. But I mean, it's broadly speaking, what you're doing is definitely makes sense, and I agree with you. Like, use So, and it's a fine balance between figuring out,
20:26you know, computer use
you know, computer use
you know, computer use
you know, computer use
S Speaker 220:30for some certain use cases, what's the right? UX, because the users don't want to give up control of their mouse and keyboard altogether, all of those things, right? Like, so, so, and then you have to it on all the platform, the best way to do it. And maybe computer, maybe browser use ends up becoming better for for some of these, yeah, it's
for some certain use cases, what's the right? UX, because the users don't want to give up control of their mouse and keyboard altogether, all of those things, right? Like, so, so, and then you have to it on all the platform, the best way to do it. And maybe computer, maybe browser use ends up becoming better for for some of these, yeah, it's
for some certain use cases, what's the right? UX, because the users don't want to give up control of their mouse and keyboard altogether, all of those things, right? Like, so, so, and then you have to it on all the platform, the best way to do it. And maybe computer, maybe browser use ends up becoming better for for some of these, yeah, it's
for some certain use cases, what's the right? UX, because the users don't want to give up control of their mouse and keyboard altogether, all of those things, right? Like, so, so, and then you have to it on all the platform, the best way to do it. And maybe computer, maybe browser use ends up becoming better for for some of these, yeah, it's
20:50a better isolated environment. Yeah, maybe.
a better isolated environment. Yeah, maybe.
a better isolated environment. Yeah, maybe.
a better isolated environment. Yeah, maybe.
20:55Anyways, that's good. Thanks Sherry, this way.
Anyways, that's good. Thanks Sherry, this way.
Anyways, that's good. Thanks Sherry, this way.
Anyways, that's good. Thanks Sherry, this way.
S Speaker 221:01So maybe perhaps we should keep in touch for a future, as you develop the product and then as you have better you know when you launch, actually let us know when you've launched the product. Link for like, you know, operating systems. Dr, Chad, okay. Sure
So maybe perhaps we should keep in touch for a future, as you develop the product and then as you have better you know when you launch, actually let us know when you've launched the product. Link for like, you know, operating systems. Dr, Chad, okay. Sure
So maybe perhaps we should keep in touch for a future, as you develop the product and then as you have better you know when you launch, actually let us know when you've launched the product. Link for like, you know, operating systems. Dr, Chad, okay. Sure
So maybe perhaps we should keep in touch for a future, as you develop the product and then as you have better you know when you launch, actually let us know when you've launched the product. Link for like, you know, operating systems. Dr, Chad, okay. Sure