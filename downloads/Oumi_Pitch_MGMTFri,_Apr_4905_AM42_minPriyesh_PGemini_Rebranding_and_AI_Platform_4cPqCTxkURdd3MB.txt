Meeting: Oumi Pitch MGMT
Fri, Apr 4
9:05 AM
42 min
Priyesh P
Gemini Rebranding and AI Platform Development
0:
URL: https://otter.ai/u/4cPqCTxkURdd3MB3ePaSyyPy4I4
Downloaded: 2025-12-22T12:23:51.674802
Method: text_extraction
============================================================

S Speaker 10:00All the way from again a couple months before GPT testability comes out in November until May of the next year of 23 that we launched in general availability. And then, as you may have heard on the news, it was decided that these efforts were moving to Deep Mind rebranded as Gemini. You know, for many strategic reasons, where I continue to be involved, certainly the alignment and safety of Gemini. And that was Google. Before Google, I was at a startup, meta, working from a seasonal AI Microsoft, where, again, the last effort, it was working with someone the chatability type of system. And before that, I was to think of a PhD. Spent first three years at Princeton, last year at MIT, where I think some of the relevant discussion we had last time feedback was actually doing research on the very early days of what later was called on device. Ai already intensive. So it's like, hey, we can stop reading all these intensive devices. Intensive devices.
All the way from again a couple months before GPT testability comes out in November until May of the next year of 23 that we launched in general availability. And then, as you may have heard on the news, it was decided that these efforts were moving to Deep Mind rebranded as Gemini. You know, for many strategic reasons, where I continue to be involved, certainly the alignment and safety of Gemini. And that was Google. Before Google, I was at a startup, meta, working from a seasonal AI Microsoft, where, again, the last effort, it was working with someone the chatability type of system. And before that, I was to think of a PhD. Spent first three years at Princeton, last year at MIT, where I think some of the relevant discussion we had last time feedback was actually doing research on the very early days of what later was called on device. Ai already intensive. So it's like, hey, we can stop reading all these intensive devices. Intensive devices.
All the way from again a couple months before GPT testability comes out in November until May of the next year of 23 that we launched in general availability. And then, as you may have heard on the news, it was decided that these efforts were moving to Deep Mind rebranded as Gemini. You know, for many strategic reasons, where I continue to be involved, certainly the alignment and safety of Gemini. And that was Google. Before Google, I was at a startup, meta, working from a seasonal AI Microsoft, where, again, the last effort, it was working with someone the chatability type of system. And before that, I was to think of a PhD. Spent first three years at Princeton, last year at MIT, where I think some of the relevant discussion we had last time feedback was actually doing research on the very early days of what later was called on device. Ai already intensive. So it's like, hey, we can stop reading all these intensive devices. Intensive devices.
All the way from again a couple months before GPT testability comes out in November until May of the next year of 23 that we launched in general availability. And then, as you may have heard on the news, it was decided that these efforts were moving to Deep Mind rebranded as Gemini. You know, for many strategic reasons, where I continue to be involved, certainly the alignment and safety of Gemini. And that was Google. Before Google, I was at a startup, meta, working from a seasonal AI Microsoft, where, again, the last effort, it was working with someone the chatability type of system. And before that, I was to think of a PhD. Spent first three years at Princeton, last year at MIT, where I think some of the relevant discussion we had last time feedback was actually doing research on the very early days of what later was called on device. Ai already intensive. So it's like, hey, we can stop reading all these intensive devices. Intensive devices.
S Speaker 20:51Here's why we do this. So one of the co founders I was working at Apple when was
Here's why we do this. So one of the co founders I was working at Apple when was
Here's why we do this. So one of the co founders I was working at Apple when was
Here's why we do this. So one of the co founders I was working at Apple when was
S Speaker 30:56leading the Health Foundation models team, so specifically working on building foundation models for the health vertical working with different data streams, from example, sensors from the Apple Watch or text images, structured data, electronic health records, and working all across the stacks and for pre training, the role model to instruction, fine tuning, safety, on device optimization, etc. Before that, I used to be particularly the Twitter AI to Microsoft. We are working on shipping various large scale problems, mostly in NLP, computer vision and recommendation systems.
leading the Health Foundation models team, so specifically working on building foundation models for the health vertical working with different data streams, from example, sensors from the Apple Watch or text images, structured data, electronic health records, and working all across the stacks and for pre training, the role model to instruction, fine tuning, safety, on device optimization, etc. Before that, I used to be particularly the Twitter AI to Microsoft. We are working on shipping various large scale problems, mostly in NLP, computer vision and recommendation systems.
leading the Health Foundation models team, so specifically working on building foundation models for the health vertical working with different data streams, from example, sensors from the Apple Watch or text images, structured data, electronic health records, and working all across the stacks and for pre training, the role model to instruction, fine tuning, safety, on device optimization, etc. Before that, I used to be particularly the Twitter AI to Microsoft. We are working on shipping various large scale problems, mostly in NLP, computer vision and recommendation systems.
leading the Health Foundation models team, so specifically working on building foundation models for the health vertical working with different data streams, from example, sensors from the Apple Watch or text images, structured data, electronic health records, and working all across the stacks and for pre training, the role model to instruction, fine tuning, safety, on device optimization, etc. Before that, I used to be particularly the Twitter AI to Microsoft. We are working on shipping various large scale problems, mostly in NLP, computer vision and recommendation systems.
1:31Okay? And with that said, we go down to the deck that
Okay? And with that said, we go down to the deck that
Okay? And with that said, we go down to the deck that
Okay? And with that said, we go down to the deck that
1:37will be the right one. Okay,
will be the right one. Okay,
will be the right one. Okay,
will be the right one. Okay,
S Speaker 11:40to see the deck, right? Okay, yes, we can. Thank you. Okay, so
to see the deck, right? Okay, yes, we can. Thank you. Okay, so
to see the deck, right? Okay, yes, we can. Thank you. Okay, so
to see the deck, right? Okay, yes, we can. Thank you. Okay, so
1:47I like to start this with a question
I like to start this with a question
I like to start this with a question
I like to start this with a question
S Speaker 11:50between Elon Musk, Sam Altman and the like, who would you entrust to hold the keys to the future of AI? I think some of you may be struggling to answer this question exactly. Won't have to be any of them, because the future of AI is going to be open to be the same way. You know, with many complex technologies, from databases like PostgreSQL, you're going to have bigger markets and Oracle databases, operating systems, you know, Linux compared to Unix, and the same thing is going to be happening. Going to be happening again with AI. And to make sure that this happens to give AI Linux moment, the platform that it was missing. That's why we built with me. It's like the one all in one to go and develop end to end. The next steps. They give the wheel, or take any of the existing models, as if they are in enterprise or researchers. They can take any of the existing open models and fully customize them to their domain and needs, and then also deploy them, very lively to production. I will elaborate later what I mean, very lively. The key thing here is open. Because that was our conviction over a year ago, that the future of AI for enterprises and beyond will be open. It was quite contrary. And a year back, I was elected, you know, when we're like discussing this with PCs and others, they're like, You know what? Why are you guys doing this chat? GPT will dominate everything. Just build an app without GPT will fund you have an amazing team. So building an original platform to enable enterprise for open models was just not so didn't seem as reasonable back then, but now it's becoming almost, I don't know if it's a consensus, but it's even clearer to many, many more people and many more customers that we talk to. They say, You know what? Now we get it. We know that we can take an open model, and the moment we start customizing with our own data, it becomes better than the black folks as it comes out of anthropic Gemini, what was looking at Google or open AI, and the same time, we have the full transparency, flexibility, right? Can be controlled on Destiny, on the IP, also higher privacy, because where we deploy this on our own, on prem, on device, or whatever that means for them, and on the second alert cost and then, you know, quite often, it gives us that all the models are becoming a better option. But what about the future? And what we tell is that the future is only going to be looking better. This is a trend that's also how fast deep sea close the gap with closed models. If you look at current Lama, it's very similar in our thesis that eventually open models will pass the quality of closed ones, especially when you don't look at it as just a single model, but of a whole compound system that you build around the models. Again, same argument about why has happened with many other ones. We have also seen a very fast growing trend of enterprise moving to open technologies, obviously, report by McKinsey again, for us. You know, we might think that three calls, even from month two of chatgpt coming out, month two of chatgpt coming out, I was working with customers that were telling me, can I distill pal so they can have full control over PAL and have my own version and full control over it? And of course, we tell them that it's unfortunately not allowed to do this. But again, there's been all the signals, starting from the very early days that have been only increasing the last year, months or especially weeks. And overall, you know the what's what's unfolding in front of us. It should be no surprise. It's just out of the same thing has happened with many such complex technology history. For example, you know what happened with Linux compared to closer Unix? When Unix came out, it was people felt the same way as they felt with chatgpt, or thus they felt with OpenAI, even until a year ago, or perhaps even until a couple months ago, where they said, You know what, this is the best technology developed by best engineers. Other companies, others don't have the capability to do something similar. So that's the future. And then when Linux came out, it was not as good. Arguably, could be, where you could argue now with open models compared to closed ones. But many enterprises said, You know what? It's open source. So I have the full flex it, customize it to my needs, and I can make it as good, if not better, than the black box, than Unix, and then have all the other benefits of open source, the full flexibility, transparency, private security, lower cost. So that's how more and more enterprises started using Linux, and that's why it ended up becoming a much stronger operating system, with all the questions and a much better ecosystem. And now pretty much everything runs on Linux, including AI itself. And what we thought was the same thing is going to be happening with AI, and actually it's already happening, but in a much more profound and accelerated pace. Increasingly, the last few months, that has few weeks, the community has more powerful tools that they can use to keep advancing open models themselves. All in all, looking ahead, we see a massive opportunity, because they the uniques of AI, the black matches
between Elon Musk, Sam Altman and the like, who would you entrust to hold the keys to the future of AI? I think some of you may be struggling to answer this question exactly. Won't have to be any of them, because the future of AI is going to be open to be the same way. You know, with many complex technologies, from databases like PostgreSQL, you're going to have bigger markets and Oracle databases, operating systems, you know, Linux compared to Unix, and the same thing is going to be happening. Going to be happening again with AI. And to make sure that this happens to give AI Linux moment, the platform that it was missing. That's why we built with me. It's like the one all in one to go and develop end to end. The next steps. They give the wheel, or take any of the existing models, as if they are in enterprise or researchers. They can take any of the existing open models and fully customize them to their domain and needs, and then also deploy them, very lively to production. I will elaborate later what I mean, very lively. The key thing here is open. Because that was our conviction over a year ago, that the future of AI for enterprises and beyond will be open. It was quite contrary. And a year back, I was elected, you know, when we're like discussing this with PCs and others, they're like, You know what? Why are you guys doing this chat? GPT will dominate everything. Just build an app without GPT will fund you have an amazing team. So building an original platform to enable enterprise for open models was just not so didn't seem as reasonable back then, but now it's becoming almost, I don't know if it's a consensus, but it's even clearer to many, many more people and many more customers that we talk to. They say, You know what? Now we get it. We know that we can take an open model, and the moment we start customizing with our own data, it becomes better than the black folks as it comes out of anthropic Gemini, what was looking at Google or open AI, and the same time, we have the full transparency, flexibility, right? Can be controlled on Destiny, on the IP, also higher privacy, because where we deploy this on our own, on prem, on device, or whatever that means for them, and on the second alert cost and then, you know, quite often, it gives us that all the models are becoming a better option. But what about the future? And what we tell is that the future is only going to be looking better. This is a trend that's also how fast deep sea close the gap with closed models. If you look at current Lama, it's very similar in our thesis that eventually open models will pass the quality of closed ones, especially when you don't look at it as just a single model, but of a whole compound system that you build around the models. Again, same argument about why has happened with many other ones. We have also seen a very fast growing trend of enterprise moving to open technologies, obviously, report by McKinsey again, for us. You know, we might think that three calls, even from month two of chatgpt coming out, month two of chatgpt coming out, I was working with customers that were telling me, can I distill pal so they can have full control over PAL and have my own version and full control over it? And of course, we tell them that it's unfortunately not allowed to do this. But again, there's been all the signals, starting from the very early days that have been only increasing the last year, months or especially weeks. And overall, you know the what's what's unfolding in front of us. It should be no surprise. It's just out of the same thing has happened with many such complex technology history. For example, you know what happened with Linux compared to closer Unix? When Unix came out, it was people felt the same way as they felt with chatgpt, or thus they felt with OpenAI, even until a year ago, or perhaps even until a couple months ago, where they said, You know what, this is the best technology developed by best engineers. Other companies, others don't have the capability to do something similar. So that's the future. And then when Linux came out, it was not as good. Arguably, could be, where you could argue now with open models compared to closed ones. But many enterprises said, You know what? It's open source. So I have the full flex it, customize it to my needs, and I can make it as good, if not better, than the black box, than Unix, and then have all the other benefits of open source, the full flexibility, transparency, private security, lower cost. So that's how more and more enterprises started using Linux, and that's why it ended up becoming a much stronger operating system, with all the questions and a much better ecosystem. And now pretty much everything runs on Linux, including AI itself. And what we thought was the same thing is going to be happening with AI, and actually it's already happening, but in a much more profound and accelerated pace. Increasingly, the last few months, that has few weeks, the community has more powerful tools that they can use to keep advancing open models themselves. All in all, looking ahead, we see a massive opportunity, because they the uniques of AI, the black matches
between Elon Musk, Sam Altman and the like, who would you entrust to hold the keys to the future of AI? I think some of you may be struggling to answer this question exactly. Won't have to be any of them, because the future of AI is going to be open to be the same way. You know, with many complex technologies, from databases like PostgreSQL, you're going to have bigger markets and Oracle databases, operating systems, you know, Linux compared to Unix, and the same thing is going to be happening. Going to be happening again with AI. And to make sure that this happens to give AI Linux moment, the platform that it was missing. That's why we built with me. It's like the one all in one to go and develop end to end. The next steps. They give the wheel, or take any of the existing models, as if they are in enterprise or researchers. They can take any of the existing open models and fully customize them to their domain and needs, and then also deploy them, very lively to production. I will elaborate later what I mean, very lively. The key thing here is open. Because that was our conviction over a year ago, that the future of AI for enterprises and beyond will be open. It was quite contrary. And a year back, I was elected, you know, when we're like discussing this with PCs and others, they're like, You know what? Why are you guys doing this chat? GPT will dominate everything. Just build an app without GPT will fund you have an amazing team. So building an original platform to enable enterprise for open models was just not so didn't seem as reasonable back then, but now it's becoming almost, I don't know if it's a consensus, but it's even clearer to many, many more people and many more customers that we talk to. They say, You know what? Now we get it. We know that we can take an open model, and the moment we start customizing with our own data, it becomes better than the black folks as it comes out of anthropic Gemini, what was looking at Google or open AI, and the same time, we have the full transparency, flexibility, right? Can be controlled on Destiny, on the IP, also higher privacy, because where we deploy this on our own, on prem, on device, or whatever that means for them, and on the second alert cost and then, you know, quite often, it gives us that all the models are becoming a better option. But what about the future? And what we tell is that the future is only going to be looking better. This is a trend that's also how fast deep sea close the gap with closed models. If you look at current Lama, it's very similar in our thesis that eventually open models will pass the quality of closed ones, especially when you don't look at it as just a single model, but of a whole compound system that you build around the models. Again, same argument about why has happened with many other ones. We have also seen a very fast growing trend of enterprise moving to open technologies, obviously, report by McKinsey again, for us. You know, we might think that three calls, even from month two of chatgpt coming out, month two of chatgpt coming out, I was working with customers that were telling me, can I distill pal so they can have full control over PAL and have my own version and full control over it? And of course, we tell them that it's unfortunately not allowed to do this. But again, there's been all the signals, starting from the very early days that have been only increasing the last year, months or especially weeks. And overall, you know the what's what's unfolding in front of us. It should be no surprise. It's just out of the same thing has happened with many such complex technology history. For example, you know what happened with Linux compared to closer Unix? When Unix came out, it was people felt the same way as they felt with chatgpt, or thus they felt with OpenAI, even until a year ago, or perhaps even until a couple months ago, where they said, You know what, this is the best technology developed by best engineers. Other companies, others don't have the capability to do something similar. So that's the future. And then when Linux came out, it was not as good. Arguably, could be, where you could argue now with open models compared to closed ones. But many enterprises said, You know what? It's open source. So I have the full flex it, customize it to my needs, and I can make it as good, if not better, than the black box, than Unix, and then have all the other benefits of open source, the full flexibility, transparency, private security, lower cost. So that's how more and more enterprises started using Linux, and that's why it ended up becoming a much stronger operating system, with all the questions and a much better ecosystem. And now pretty much everything runs on Linux, including AI itself. And what we thought was the same thing is going to be happening with AI, and actually it's already happening, but in a much more profound and accelerated pace. Increasingly, the last few months, that has few weeks, the community has more powerful tools that they can use to keep advancing open models themselves. All in all, looking ahead, we see a massive opportunity, because they the uniques of AI, the black matches
between Elon Musk, Sam Altman and the like, who would you entrust to hold the keys to the future of AI? I think some of you may be struggling to answer this question exactly. Won't have to be any of them, because the future of AI is going to be open to be the same way. You know, with many complex technologies, from databases like PostgreSQL, you're going to have bigger markets and Oracle databases, operating systems, you know, Linux compared to Unix, and the same thing is going to be happening. Going to be happening again with AI. And to make sure that this happens to give AI Linux moment, the platform that it was missing. That's why we built with me. It's like the one all in one to go and develop end to end. The next steps. They give the wheel, or take any of the existing models, as if they are in enterprise or researchers. They can take any of the existing open models and fully customize them to their domain and needs, and then also deploy them, very lively to production. I will elaborate later what I mean, very lively. The key thing here is open. Because that was our conviction over a year ago, that the future of AI for enterprises and beyond will be open. It was quite contrary. And a year back, I was elected, you know, when we're like discussing this with PCs and others, they're like, You know what? Why are you guys doing this chat? GPT will dominate everything. Just build an app without GPT will fund you have an amazing team. So building an original platform to enable enterprise for open models was just not so didn't seem as reasonable back then, but now it's becoming almost, I don't know if it's a consensus, but it's even clearer to many, many more people and many more customers that we talk to. They say, You know what? Now we get it. We know that we can take an open model, and the moment we start customizing with our own data, it becomes better than the black folks as it comes out of anthropic Gemini, what was looking at Google or open AI, and the same time, we have the full transparency, flexibility, right? Can be controlled on Destiny, on the IP, also higher privacy, because where we deploy this on our own, on prem, on device, or whatever that means for them, and on the second alert cost and then, you know, quite often, it gives us that all the models are becoming a better option. But what about the future? And what we tell is that the future is only going to be looking better. This is a trend that's also how fast deep sea close the gap with closed models. If you look at current Lama, it's very similar in our thesis that eventually open models will pass the quality of closed ones, especially when you don't look at it as just a single model, but of a whole compound system that you build around the models. Again, same argument about why has happened with many other ones. We have also seen a very fast growing trend of enterprise moving to open technologies, obviously, report by McKinsey again, for us. You know, we might think that three calls, even from month two of chatgpt coming out, month two of chatgpt coming out, I was working with customers that were telling me, can I distill pal so they can have full control over PAL and have my own version and full control over it? And of course, we tell them that it's unfortunately not allowed to do this. But again, there's been all the signals, starting from the very early days that have been only increasing the last year, months or especially weeks. And overall, you know the what's what's unfolding in front of us. It should be no surprise. It's just out of the same thing has happened with many such complex technology history. For example, you know what happened with Linux compared to closer Unix? When Unix came out, it was people felt the same way as they felt with chatgpt, or thus they felt with OpenAI, even until a year ago, or perhaps even until a couple months ago, where they said, You know what, this is the best technology developed by best engineers. Other companies, others don't have the capability to do something similar. So that's the future. And then when Linux came out, it was not as good. Arguably, could be, where you could argue now with open models compared to closed ones. But many enterprises said, You know what? It's open source. So I have the full flex it, customize it to my needs, and I can make it as good, if not better, than the black box, than Unix, and then have all the other benefits of open source, the full flexibility, transparency, private security, lower cost. So that's how more and more enterprises started using Linux, and that's why it ended up becoming a much stronger operating system, with all the questions and a much better ecosystem. And now pretty much everything runs on Linux, including AI itself. And what we thought was the same thing is going to be happening with AI, and actually it's already happening, but in a much more profound and accelerated pace. Increasingly, the last few months, that has few weeks, the community has more powerful tools that they can use to keep advancing open models themselves. All in all, looking ahead, we see a massive opportunity, because they the uniques of AI, the black matches
S Speaker 46:37question on the previous slide. So if you go like, do enterprises really care about open or do they care about the features you've talked about, which is flexibility, transparency, privacy and security,
question on the previous slide. So if you go like, do enterprises really care about open or do they care about the features you've talked about, which is flexibility, transparency, privacy and security,
question on the previous slide. So if you go like, do enterprises really care about open or do they care about the features you've talked about, which is flexibility, transparency, privacy and security,
question on the previous slide. So if you go like, do enterprises really care about open or do they care about the features you've talked about, which is flexibility, transparency, privacy and security,
S Speaker 48:07the reason I'm asking is because the larger AI Labs also have a plan, and some of them already do on prem deployments like the tropics of the world, right? So does that solve and then we'll eventually, when they have an open source model as well as they've been talking about, they'll give that flexibility to that you're talking about. And the transparency you can choose. You can choose a closed model which is more accurate, and you can choose an open model that is less accurate, but more flexible and more transparent. My question is, actually a legal firm, or you go to a financial firm like you were taking some banking examples, right? They want to do financial modeling and and they care about the I mean, they care about financial modeling. They do. They really care about how you're delivering that, as long as you deliver flexibility, transparency, whether it's open model, closed model, they could all offer that, right eventually, yeah.
the reason I'm asking is because the larger AI Labs also have a plan, and some of them already do on prem deployments like the tropics of the world, right? So does that solve and then we'll eventually, when they have an open source model as well as they've been talking about, they'll give that flexibility to that you're talking about. And the transparency you can choose. You can choose a closed model which is more accurate, and you can choose an open model that is less accurate, but more flexible and more transparent. My question is, actually a legal firm, or you go to a financial firm like you were taking some banking examples, right? They want to do financial modeling and and they care about the I mean, they care about financial modeling. They do. They really care about how you're delivering that, as long as you deliver flexibility, transparency, whether it's open model, closed model, they could all offer that, right eventually, yeah.
the reason I'm asking is because the larger AI Labs also have a plan, and some of them already do on prem deployments like the tropics of the world, right? So does that solve and then we'll eventually, when they have an open source model as well as they've been talking about, they'll give that flexibility to that you're talking about. And the transparency you can choose. You can choose a closed model which is more accurate, and you can choose an open model that is less accurate, but more flexible and more transparent. My question is, actually a legal firm, or you go to a financial firm like you were taking some banking examples, right? They want to do financial modeling and and they care about the I mean, they care about financial modeling. They do. They really care about how you're delivering that, as long as you deliver flexibility, transparency, whether it's open model, closed model, they could all offer that, right eventually, yeah.
the reason I'm asking is because the larger AI Labs also have a plan, and some of them already do on prem deployments like the tropics of the world, right? So does that solve and then we'll eventually, when they have an open source model as well as they've been talking about, they'll give that flexibility to that you're talking about. And the transparency you can choose. You can choose a closed model which is more accurate, and you can choose an open model that is less accurate, but more flexible and more transparent. My question is, actually a legal firm, or you go to a financial firm like you were taking some banking examples, right? They want to do financial modeling and and they care about the I mean, they care about financial modeling. They do. They really care about how you're delivering that, as long as you deliver flexibility, transparency, whether it's open model, closed model, they could all offer that, right eventually, yeah.
S Speaker 19:11So to some extent, for example, you know, my team was building Gemini, and actually we also built APIs for Gemini, and they're extremely limited. We offer an API, and the same thing also open air does for Lora. You can parameters, but that's it. Do you want to start testing some of the latest like GRP, oh, and many other techniques that exist? You're out of luck if you want to do anything so that you can control of your own destiny and experiment and advance things the way you want that might be very specific to your field, you're out of luck, because you can only do those providers let you do behind their API. So they're very limited in many ways. And that, what we tell them, what we hear from many enterprises, is that, you know, because we know AI is going to be a key thing for our enterprise. It's not just a side technology. Is going to be the key differentiation. We need to have that control. We can just yield to any such comment like open. Ai, there's also a very common thing that many enterprises know, many enterprise one day, they don't want to have the full hardware, software lock into specific vendor. They want to have that flexibility and control. I just
So to some extent, for example, you know, my team was building Gemini, and actually we also built APIs for Gemini, and they're extremely limited. We offer an API, and the same thing also open air does for Lora. You can parameters, but that's it. Do you want to start testing some of the latest like GRP, oh, and many other techniques that exist? You're out of luck if you want to do anything so that you can control of your own destiny and experiment and advance things the way you want that might be very specific to your field, you're out of luck, because you can only do those providers let you do behind their API. So they're very limited in many ways. And that, what we tell them, what we hear from many enterprises, is that, you know, because we know AI is going to be a key thing for our enterprise. It's not just a side technology. Is going to be the key differentiation. We need to have that control. We can just yield to any such comment like open. Ai, there's also a very common thing that many enterprises know, many enterprise one day, they don't want to have the full hardware, software lock into specific vendor. They want to have that flexibility and control. I just
So to some extent, for example, you know, my team was building Gemini, and actually we also built APIs for Gemini, and they're extremely limited. We offer an API, and the same thing also open air does for Lora. You can parameters, but that's it. Do you want to start testing some of the latest like GRP, oh, and many other techniques that exist? You're out of luck if you want to do anything so that you can control of your own destiny and experiment and advance things the way you want that might be very specific to your field, you're out of luck, because you can only do those providers let you do behind their API. So they're very limited in many ways. And that, what we tell them, what we hear from many enterprises, is that, you know, because we know AI is going to be a key thing for our enterprise. It's not just a side technology. Is going to be the key differentiation. We need to have that control. We can just yield to any such comment like open. Ai, there's also a very common thing that many enterprises know, many enterprise one day, they don't want to have the full hardware, software lock into specific vendor. They want to have that flexibility and control. I just
So to some extent, for example, you know, my team was building Gemini, and actually we also built APIs for Gemini, and they're extremely limited. We offer an API, and the same thing also open air does for Lora. You can parameters, but that's it. Do you want to start testing some of the latest like GRP, oh, and many other techniques that exist? You're out of luck if you want to do anything so that you can control of your own destiny and experiment and advance things the way you want that might be very specific to your field, you're out of luck, because you can only do those providers let you do behind their API. So they're very limited in many ways. And that, what we tell them, what we hear from many enterprises, is that, you know, because we know AI is going to be a key thing for our enterprise. It's not just a side technology. Is going to be the key differentiation. We need to have that control. We can just yield to any such comment like open. Ai, there's also a very common thing that many enterprises know, many enterprise one day, they don't want to have the full hardware, software lock into specific vendor. They want to have that flexibility and control. I just
S Speaker 310:19wanted to add here to sorry you mentioned potentially open air releasing their model. I won't say that happens that news for us. If they open with their model, we're going to be the best place for people to use it, to customize it, to evaluate it, to influence with it. The more we have good open models, whether they come from deep sea or from our pumi or from open AI. The better platform is because people want to customize them for their use cases. Sell them to a smaller device. We benefit a lot from the labs open sway to a model like that's a
wanted to add here to sorry you mentioned potentially open air releasing their model. I won't say that happens that news for us. If they open with their model, we're going to be the best place for people to use it, to customize it, to evaluate it, to influence with it. The more we have good open models, whether they come from deep sea or from our pumi or from open AI. The better platform is because people want to customize them for their use cases. Sell them to a smaller device. We benefit a lot from the labs open sway to a model like that's a
wanted to add here to sorry you mentioned potentially open air releasing their model. I won't say that happens that news for us. If they open with their model, we're going to be the best place for people to use it, to customize it, to evaluate it, to influence with it. The more we have good open models, whether they come from deep sea or from our pumi or from open AI. The better platform is because people want to customize them for their use cases. Sell them to a smaller device. We benefit a lot from the labs open sway to a model like that's a
wanted to add here to sorry you mentioned potentially open air releasing their model. I won't say that happens that news for us. If they open with their model, we're going to be the best place for people to use it, to customize it, to evaluate it, to influence with it. The more we have good open models, whether they come from deep sea or from our pumi or from open AI. The better platform is because people want to customize them for their use cases. Sell them to a smaller device. We benefit a lot from the labs open sway to a model like that's a
10:53huge boost. Yeah,
S Speaker 110:54very good point summer. Because, as someone mentioned, you know, for any of these open models, or even, as we may discuss later, we even plan to support, some extent, even some close ones, just for the sake of comparison, it's going to be easiest and best way to experiment with
very good point summer. Because, as someone mentioned, you know, for any of these open models, or even, as we may discuss later, we even plan to support, some extent, even some close ones, just for the sake of comparison, it's going to be easiest and best way to experiment with
very good point summer. Because, as someone mentioned, you know, for any of these open models, or even, as we may discuss later, we even plan to support, some extent, even some close ones, just for the sake of comparison, it's going to be easiest and best way to experiment with
very good point summer. Because, as someone mentioned, you know, for any of these open models, or even, as we may discuss later, we even plan to support, some extent, even some close ones, just for the sake of comparison, it's going to be easiest and best way to experiment with
S Speaker 511:12sorry, can I interrupt just one quick question? So in your mind, when you say open versus closed, right? So just trying to understand clearly, like, you know, my open, a AI is, is closed, like Islam, open, your opinion. And then the also the Linux versus Unix analogy, like, in this case, is the model itself? Is the analogy, or is it the kind of things around it, if you will? Unix is like operating system, right? Linux is an open source version of the operating system in this case, like, what, how, how should we think about it? Now?
sorry, can I interrupt just one quick question? So in your mind, when you say open versus closed, right? So just trying to understand clearly, like, you know, my open, a AI is, is closed, like Islam, open, your opinion. And then the also the Linux versus Unix analogy, like, in this case, is the model itself? Is the analogy, or is it the kind of things around it, if you will? Unix is like operating system, right? Linux is an open source version of the operating system in this case, like, what, how, how should we think about it? Now?
sorry, can I interrupt just one quick question? So in your mind, when you say open versus closed, right? So just trying to understand clearly, like, you know, my open, a AI is, is closed, like Islam, open, your opinion. And then the also the Linux versus Unix analogy, like, in this case, is the model itself? Is the analogy, or is it the kind of things around it, if you will? Unix is like operating system, right? Linux is an open source version of the operating system in this case, like, what, how, how should we think about it? Now?
sorry, can I interrupt just one quick question? So in your mind, when you say open versus closed, right? So just trying to understand clearly, like, you know, my open, a AI is, is closed, like Islam, open, your opinion. And then the also the Linux versus Unix analogy, like, in this case, is the model itself? Is the analogy, or is it the kind of things around it, if you will? Unix is like operating system, right? Linux is an open source version of the operating system in this case, like, what, how, how should we think about it? Now?
11:46Great point going so,
Great point going so,
Great point going so,
Great point going so,
S Speaker 111:49so open AI anthropic and most of the competitive models by Google, they're all closed. Google has Gemma, yes. Open the, you know, the smaller ones, but yeah, all of these models
so open AI anthropic and most of the competitive models by Google, they're all closed. Google has Gemma, yes. Open the, you know, the smaller ones, but yeah, all of these models
so open AI anthropic and most of the competitive models by Google, they're all closed. Google has Gemma, yes. Open the, you know, the smaller ones, but yeah, all of these models
so open AI anthropic and most of the competitive models by Google, they're all closed. Google has Gemma, yes. Open the, you know, the smaller ones, but yeah, all of these models
12:03that are closed, whereas when they're open,
that are closed, whereas when they're open,
that are closed, whereas when they're open,
that are closed, whereas when they're open,
S Speaker 512:42So you you are developing open source models yourself, or not the model,
So you you are developing open source models yourself, or not the model,
So you you are developing open source models yourself, or not the model,
So you you are developing open source models yourself, or not the model,
14:03meet or surpass the part of the close ones.
meet or surpass the part of the close ones.
meet or surpass the part of the close ones.
meet or surpass the part of the close ones.
S Speaker 514:04Okay. Okay, so existing open source models to make it better, be more competitive to the close models, if you
Okay. Okay, so existing open source models to make it better, be more competitive to the close models, if you
Okay. Okay, so existing open source models to make it better, be more competitive to the close models, if you
Okay. Okay, so existing open source models to make it better, be more competitive to the close models, if you
26:49leaderboard, Gemini and Mr. Alerts.
leaderboard, Gemini and Mr. Alerts.
leaderboard, Gemini and Mr. Alerts.
leaderboard, Gemini and Mr. Alerts.
32:24this thing to three questions over there. So
this thing to three questions over there. So
this thing to three questions over there. So
this thing to three questions over there. So
S Speaker 432:28in some ways, is this similar, like the developer approach, and then the Linux for AI and Umi for enterprise with open source? Is that? How does this differentiate from Lama index and Lang chain and and then even the likes of like your TI the little is for defense companies that
in some ways, is this similar, like the developer approach, and then the Linux for AI and Umi for enterprise with open source? Is that? How does this differentiate from Lama index and Lang chain and and then even the likes of like your TI the little is for defense companies that
in some ways, is this similar, like the developer approach, and then the Linux for AI and Umi for enterprise with open source? Is that? How does this differentiate from Lama index and Lang chain and and then even the likes of like your TI the little is for defense companies that
in some ways, is this similar, like the developer approach, and then the Linux for AI and Umi for enterprise with open source? Is that? How does this differentiate from Lama index and Lang chain and and then even the likes of like your TI the little is for defense companies that
S Speaker 432:52his approach similar, and which is fine, the market is Big, but if it's similar,
his approach similar, and which is fine, the market is Big, but if it's similar,
his approach similar, and which is fine, the market is Big, but if it's similar,
his approach similar, and which is fine, the market is Big, but if it's similar,
32:59than differentiate,
S Speaker 635:20there's also a follow up question, because for most of enterprise customers they care about deploying to their specification, right? Example, service, data intensive sectors, and they also need fine tuning model, for example, business model, right? So in that perspective, how would you compare yourself with companies
there's also a follow up question, because for most of enterprise customers they care about deploying to their specification, right? Example, service, data intensive sectors, and they also need fine tuning model, for example, business model, right? So in that perspective, how would you compare yourself with companies
there's also a follow up question, because for most of enterprise customers they care about deploying to their specification, right? Example, service, data intensive sectors, and they also need fine tuning model, for example, business model, right? So in that perspective, how would you compare yourself with companies
there's also a follow up question, because for most of enterprise customers they care about deploying to their specification, right? Example, service, data intensive sectors, and they also need fine tuning model, for example, business model, right? So in that perspective, how would you compare yourself with companies
S Speaker 135:36yes, yes. So we So for one, you know, what we build is a fully open source and end to end. I am not sure. Actually, some algorithm here, I don't think laminate provides all the end to end across the data curation and all the other capabilities that we provide. And let's say anyway, so this one, one is end to end, fully open source. And let's see what else also. You know, we have a lot of research efforts that we're doing with academia and all the academic backing to make the platform itself better. Because it could be, you could have been building a closed platform ourselves, or something that is open, but it's not only developed by only us, but that's not going to be the end state. That's going to be the best solution. It needs to be the point of AI, something that is developed by the whole community. And that's, that's the likeliest winner in the you know, in
yes, yes. So we So for one, you know, what we build is a fully open source and end to end. I am not sure. Actually, some algorithm here, I don't think laminate provides all the end to end across the data curation and all the other capabilities that we provide. And let's say anyway, so this one, one is end to end, fully open source. And let's see what else also. You know, we have a lot of research efforts that we're doing with academia and all the academic backing to make the platform itself better. Because it could be, you could have been building a closed platform ourselves, or something that is open, but it's not only developed by only us, but that's not going to be the end state. That's going to be the best solution. It needs to be the point of AI, something that is developed by the whole community. And that's, that's the likeliest winner in the you know, in
yes, yes. So we So for one, you know, what we build is a fully open source and end to end. I am not sure. Actually, some algorithm here, I don't think laminate provides all the end to end across the data curation and all the other capabilities that we provide. And let's say anyway, so this one, one is end to end, fully open source. And let's see what else also. You know, we have a lot of research efforts that we're doing with academia and all the academic backing to make the platform itself better. Because it could be, you could have been building a closed platform ourselves, or something that is open, but it's not only developed by only us, but that's not going to be the end state. That's going to be the best solution. It needs to be the point of AI, something that is developed by the whole community. And that's, that's the likeliest winner in the you know, in
yes, yes. So we So for one, you know, what we build is a fully open source and end to end. I am not sure. Actually, some algorithm here, I don't think laminate provides all the end to end across the data curation and all the other capabilities that we provide. And let's say anyway, so this one, one is end to end, fully open source. And let's see what else also. You know, we have a lot of research efforts that we're doing with academia and all the academic backing to make the platform itself better. Because it could be, you could have been building a closed platform ourselves, or something that is open, but it's not only developed by only us, but that's not going to be the end state. That's going to be the best solution. It needs to be the point of AI, something that is developed by the whole community. And that's, that's the likeliest winner in the you know, in
S Speaker 636:25dominating what you say that you're doing this end to end. You're given a more open end to end platform to enterprise users, right? We
dominating what you say that you're doing this end to end. You're given a more open end to end platform to enterprise users, right? We
dominating what you say that you're doing this end to end. You're given a more open end to end platform to enterprise users, right? We
dominating what you say that you're doing this end to end. You're given a more open end to end platform to enterprise users, right? We
S Speaker 537:55Just quick question on the reliability for these regulated enterprises, right? Totally understand the point, but no hallucination, all that stuff. But how do you make sure that you do that? You need to hold your hands right to do this. Otherwise, if you just say, Hey, I have this platform. There's bunch of open source models. You guys go, go have fun with it. It's not gonna work. Yeah.
Just quick question on the reliability for these regulated enterprises, right? Totally understand the point, but no hallucination, all that stuff. But how do you make sure that you do that? You need to hold your hands right to do this. Otherwise, if you just say, Hey, I have this platform. There's bunch of open source models. You guys go, go have fun with it. It's not gonna work. Yeah.
Just quick question on the reliability for these regulated enterprises, right? Totally understand the point, but no hallucination, all that stuff. But how do you make sure that you do that? You need to hold your hands right to do this. Otherwise, if you just say, Hey, I have this platform. There's bunch of open source models. You guys go, go have fun with it. It's not gonna work. Yeah.
Just quick question on the reliability for these regulated enterprises, right? Totally understand the point, but no hallucination, all that stuff. But how do you make sure that you do that? You need to hold your hands right to do this. Otherwise, if you just say, Hey, I have this platform. There's bunch of open source models. You guys go, go have fun with it. It's not gonna work. Yeah.
S Speaker 138:16So actually, this is very timely. Just two days ago we launched this model. We call it Halloween from hallucination, and only Okay, best in the industry hallucination, take care that exists. Give you an idea how it works. I mean, maybe, maybe, let me see if I can find quickly that also
So actually, this is very timely. Just two days ago we launched this model. We call it Halloween from hallucination, and only Okay, best in the industry hallucination, take care that exists. Give you an idea how it works. I mean, maybe, maybe, let me see if I can find quickly that also
So actually, this is very timely. Just two days ago we launched this model. We call it Halloween from hallucination, and only Okay, best in the industry hallucination, take care that exists. Give you an idea how it works. I mean, maybe, maybe, let me see if I can find quickly that also
So actually, this is very timely. Just two days ago we launched this model. We call it Halloween from hallucination, and only Okay, best in the industry hallucination, take care that exists. Give you an idea how it works. I mean, maybe, maybe, let me see if I can find quickly that also
S Speaker 538:35the law so people can use this to check whether the models are also they need or not, basically,
the law so people can use this to check whether the models are also they need or not, basically,
the law so people can use this to check whether the models are also they need or not, basically,
the law so people can use this to check whether the models are also they need or not, basically,
S Speaker 138:39exactly, exactly. And you know, this is just the results. I say, You know what? It's way better quality than even the it's only eight, but it's way better quality than any of the other open model, sorry, any of the other state of the art models.
exactly, exactly. And you know, this is just the results. I say, You know what? It's way better quality than even the it's only eight, but it's way better quality than any of the other open model, sorry, any of the other state of the art models.
exactly, exactly. And you know, this is just the results. I say, You know what? It's way better quality than even the it's only eight, but it's way better quality than any of the other open model, sorry, any of the other state of the art models.
exactly, exactly. And you know, this is just the results. I say, You know what? It's way better quality than even the it's only eight, but it's way better quality than any of the other open model, sorry, any of the other state of the art models.
S Speaker 538:51But this, this, hello, this thing is your model, or this is yes,
But this, this, hello, this thing is your model, or this is yes,
But this, this, hello, this thing is your model, or this is yes,
But this, this, hello, this thing is your model, or this is yes,
38:55yes. And we released it just,
yes. And we released it just,
yes. And we released it just,
yes. And we released it just,
39:00just on Wednesday, just two, three days ago.
just on Wednesday, just two, three days ago.
just on Wednesday, just two, three days ago.
just on Wednesday, just two, three days ago.
S Speaker 539:02To give you an idea like a foundational model or modified it's built on top of llama. Well, built on top of llama, okay, got into some improved version of llama.
To give you an idea like a foundational model or modified it's built on top of llama. Well, built on top of llama, okay, got into some improved version of llama.
To give you an idea like a foundational model or modified it's built on top of llama. Well, built on top of llama, okay, got into some improved version of llama.
To give you an idea like a foundational model or modified it's built on top of llama. Well, built on top of llama, okay, got into some improved version of llama.
S Speaker 742:07question, you know a lot of focus has been on what I call as cloud AI, right, when you talk about AGI, or Linux for edge like.
question, you know a lot of focus has been on what I call as cloud AI, right, when you talk about AGI, or Linux for edge like.
question, you know a lot of focus has been on what I call as cloud AI, right, when you talk about AGI, or Linux for edge like.
question, you know a lot of focus has been on what I call as cloud AI, right, when you talk about AGI, or Linux for edge like.