Meeting: Fish Audio Pitch
Thu, Aug 22, 2024
1:01 PM
34 min
Priyesh P
Introduction and Initial Catch-Up
0:09
O
URL: https://otter.ai/u/2G_Y6EIv84yrNWNtKyfeqVWFKQs
Downloaded: 2025-12-22T14:48:07.472713
Method: text_extraction
============================================================

S Speaker 10:18I'm doing great. Thanks a lot for asking, great, we could connect. Am I pronouncing your name right? Is it siege here? Yeah, okay, okay, just wanted to get that clearly, okay, yeah, tell me. How's the week been? I think we met last week in the SFS step conference. How has the progress been? So far, we are working on improving the stability, and probably announced the agent with left turn in this week, Friday or early next week. Yeah, to some I mean, road users next week. Yeah, that's pretty exciting. Okay, teacher, remind me, are you based out of the Bay Area? Are you in SF? Where are you? I'm in Santa Clara, no, you're in Santa Clara, that's interesting. We could have met in person. In that case, like, Qualcomm's office is also in Santa Clara, so
I'm doing great. Thanks a lot for asking, great, we could connect. Am I pronouncing your name right? Is it siege here? Yeah, okay, okay, just wanted to get that clearly, okay, yeah, tell me. How's the week been? I think we met last week in the SFS step conference. How has the progress been? So far, we are working on improving the stability, and probably announced the agent with left turn in this week, Friday or early next week. Yeah, to some I mean, road users next week. Yeah, that's pretty exciting. Okay, teacher, remind me, are you based out of the Bay Area? Are you in SF? Where are you? I'm in Santa Clara, no, you're in Santa Clara, that's interesting. We could have met in person. In that case, like, Qualcomm's office is also in Santa Clara, so
I'm doing great. Thanks a lot for asking, great, we could connect. Am I pronouncing your name right? Is it siege here? Yeah, okay, okay, just wanted to get that clearly, okay, yeah, tell me. How's the week been? I think we met last week in the SFS step conference. How has the progress been? So far, we are working on improving the stability, and probably announced the agent with left turn in this week, Friday or early next week. Yeah, to some I mean, road users next week. Yeah, that's pretty exciting. Okay, teacher, remind me, are you based out of the Bay Area? Are you in SF? Where are you? I'm in Santa Clara, no, you're in Santa Clara, that's interesting. We could have met in person. In that case, like, Qualcomm's office is also in Santa Clara, so
I'm doing great. Thanks a lot for asking, great, we could connect. Am I pronouncing your name right? Is it siege here? Yeah, okay, okay, just wanted to get that clearly, okay, yeah, tell me. How's the week been? I think we met last week in the SFS step conference. How has the progress been? So far, we are working on improving the stability, and probably announced the agent with left turn in this week, Friday or early next week. Yeah, to some I mean, road users next week. Yeah, that's pretty exciting. Okay, teacher, remind me, are you based out of the Bay Area? Are you in SF? Where are you? I'm in Santa Clara, no, you're in Santa Clara, that's interesting. We could have met in person. In that case, like, Qualcomm's office is also in Santa Clara, so
S Speaker 11:20Oh, that's interesting. Okay, probably next time, but that's great. Yeah, good to connect. I think I just wanted to catch up again and sort of understand better what you guys are building and what's the plan from here on. And then also give you an introduction about Qualcomm ventures. Try to see if there are any possibilities for us to explore synergies across Qualcomm or Qualcomm ventures, and down the line, if there is a fundraising opportunity, happy to also evaluate that. Yeah, we are raising our seed around currently. Yeah, okay, okay, that's, that's, that's good context, sure. Okay, so maybe I can start by giving you a brief about Qualcomm ventures and how we operate as a fund, and then I'll pass it over to you for your background, and then we can deep dive on fish audio.
Oh, that's interesting. Okay, probably next time, but that's great. Yeah, good to connect. I think I just wanted to catch up again and sort of understand better what you guys are building and what's the plan from here on. And then also give you an introduction about Qualcomm ventures. Try to see if there are any possibilities for us to explore synergies across Qualcomm or Qualcomm ventures, and down the line, if there is a fundraising opportunity, happy to also evaluate that. Yeah, we are raising our seed around currently. Yeah, okay, okay, that's, that's, that's good context, sure. Okay, so maybe I can start by giving you a brief about Qualcomm ventures and how we operate as a fund, and then I'll pass it over to you for your background, and then we can deep dive on fish audio.
Oh, that's interesting. Okay, probably next time, but that's great. Yeah, good to connect. I think I just wanted to catch up again and sort of understand better what you guys are building and what's the plan from here on. And then also give you an introduction about Qualcomm ventures. Try to see if there are any possibilities for us to explore synergies across Qualcomm or Qualcomm ventures, and down the line, if there is a fundraising opportunity, happy to also evaluate that. Yeah, we are raising our seed around currently. Yeah, okay, okay, that's, that's, that's good context, sure. Okay, so maybe I can start by giving you a brief about Qualcomm ventures and how we operate as a fund, and then I'll pass it over to you for your background, and then we can deep dive on fish audio.
Oh, that's interesting. Okay, probably next time, but that's great. Yeah, good to connect. I think I just wanted to catch up again and sort of understand better what you guys are building and what's the plan from here on. And then also give you an introduction about Qualcomm ventures. Try to see if there are any possibilities for us to explore synergies across Qualcomm or Qualcomm ventures, and down the line, if there is a fundraising opportunity, happy to also evaluate that. Yeah, we are raising our seed around currently. Yeah, okay, okay, that's, that's, that's good context, sure. Okay, so maybe I can start by giving you a brief about Qualcomm ventures and how we operate as a fund, and then I'll pass it over to you for your background, and then we can deep dive on fish audio.
S Speaker 12:14absolutely. So srija, good to meet you. I am priyesh. P I currently work as a Senior Associate at Qualcomm ventures. I have been with the team for about four months. I've had a background in operating across different early stage ventures from different industries, including social media, FinTech, edtech, and then I've worked with a clean tech company, seen it grow from very early stages to finally achieving a unicorn status. Recently completed my MBA from UCLA, and now I'm joined, and I've joined the Qualcomm ventures team. The fund itself, Qualcomm Ventures is the corporate arm of Qualcomm, and we have been there. I guess the team has been there for the last 26 years. We have more than 250 plus active portfolio companies. We invest out of the balance sheet of Qualcomm. So we typically aim to invest somewhere between 150 to one $80 million a year in us, and then we have offices in Brazil, Israel, China and India, separately, with different team, the US team is about eight members, people, most of us based out of the Bay Area, some of us still in San Diego. And we prefer to write checks, I checks of size somewhere between two to ten million at times. We lead rounds at times. We also just participate in rounds. And our thesis is broadly around areas which intersect with Qualcomm's broader mandate of compute and connectivity. So we have had investments in the entire IoT, ar, VR space, AI, the infrastructure stack and also the application stack. We have had investments in automobile cameras. So you get the broader sense, that's where we add any specific questions. For me on the front side.
absolutely. So srija, good to meet you. I am priyesh. P I currently work as a Senior Associate at Qualcomm ventures. I have been with the team for about four months. I've had a background in operating across different early stage ventures from different industries, including social media, FinTech, edtech, and then I've worked with a clean tech company, seen it grow from very early stages to finally achieving a unicorn status. Recently completed my MBA from UCLA, and now I'm joined, and I've joined the Qualcomm ventures team. The fund itself, Qualcomm Ventures is the corporate arm of Qualcomm, and we have been there. I guess the team has been there for the last 26 years. We have more than 250 plus active portfolio companies. We invest out of the balance sheet of Qualcomm. So we typically aim to invest somewhere between 150 to one $80 million a year in us, and then we have offices in Brazil, Israel, China and India, separately, with different team, the US team is about eight members, people, most of us based out of the Bay Area, some of us still in San Diego. And we prefer to write checks, I checks of size somewhere between two to ten million at times. We lead rounds at times. We also just participate in rounds. And our thesis is broadly around areas which intersect with Qualcomm's broader mandate of compute and connectivity. So we have had investments in the entire IoT, ar, VR space, AI, the infrastructure stack and also the application stack. We have had investments in automobile cameras. So you get the broader sense, that's where we add any specific questions. For me on the front side.
absolutely. So srija, good to meet you. I am priyesh. P I currently work as a Senior Associate at Qualcomm ventures. I have been with the team for about four months. I've had a background in operating across different early stage ventures from different industries, including social media, FinTech, edtech, and then I've worked with a clean tech company, seen it grow from very early stages to finally achieving a unicorn status. Recently completed my MBA from UCLA, and now I'm joined, and I've joined the Qualcomm ventures team. The fund itself, Qualcomm Ventures is the corporate arm of Qualcomm, and we have been there. I guess the team has been there for the last 26 years. We have more than 250 plus active portfolio companies. We invest out of the balance sheet of Qualcomm. So we typically aim to invest somewhere between 150 to one $80 million a year in us, and then we have offices in Brazil, Israel, China and India, separately, with different team, the US team is about eight members, people, most of us based out of the Bay Area, some of us still in San Diego. And we prefer to write checks, I checks of size somewhere between two to ten million at times. We lead rounds at times. We also just participate in rounds. And our thesis is broadly around areas which intersect with Qualcomm's broader mandate of compute and connectivity. So we have had investments in the entire IoT, ar, VR space, AI, the infrastructure stack and also the application stack. We have had investments in automobile cameras. So you get the broader sense, that's where we add any specific questions. For me on the front side.
absolutely. So srija, good to meet you. I am priyesh. P I currently work as a Senior Associate at Qualcomm ventures. I have been with the team for about four months. I've had a background in operating across different early stage ventures from different industries, including social media, FinTech, edtech, and then I've worked with a clean tech company, seen it grow from very early stages to finally achieving a unicorn status. Recently completed my MBA from UCLA, and now I'm joined, and I've joined the Qualcomm ventures team. The fund itself, Qualcomm Ventures is the corporate arm of Qualcomm, and we have been there. I guess the team has been there for the last 26 years. We have more than 250 plus active portfolio companies. We invest out of the balance sheet of Qualcomm. So we typically aim to invest somewhere between 150 to one $80 million a year in us, and then we have offices in Brazil, Israel, China and India, separately, with different team, the US team is about eight members, people, most of us based out of the Bay Area, some of us still in San Diego. And we prefer to write checks, I checks of size somewhere between two to ten million at times. We lead rounds at times. We also just participate in rounds. And our thesis is broadly around areas which intersect with Qualcomm's broader mandate of compute and connectivity. So we have had investments in the entire IoT, ar, VR space, AI, the infrastructure stack and also the application stack. We have had investments in automobile cameras. So you get the broader sense, that's where we add any specific questions. For me on the front side.
S Speaker 14:29I don't know if Qualcomm are more focused on some model that can be run on edge, on device, or it's more like a general AI in front. So yes, we do evaluate. We like to partner with companies which can run AI on edge. As you know, Qualcomm has a very strong push in edge computing space. But that does not really stop us from investing in companies which run their entire inference on cloud as well. So both work, but however, we do prefer partnering with companies which have an edge play with them. Yeah, that sounds good. So basically, what we currently have, we have smaller TTS model that can be run 05, fast or on device like 100 m, 100 million parameters. And we also have larger model. We are currently around server, yeah, and later on we are, say, we are moving to the voice agent area, yeah. Our model, typically, the size, will be two to 3 billion, which is, like, you know, it's not a sweet point, but it's still available to run our own device, yeah. We are able to make it in real time, in accurate interaction with users, okay, okay, that's a good overview. Xi, Jin, I guess while the team has had investments in the text to speech areas, we have an investment in, well said labs, if you know about them, it would be great if you can give me an understanding of how the overall market operates right now. How is your text to speech model different from that of well said labs or 1111 labs, and you mentioned you have a smaller model and a larger model. Can you give me a broader understanding of both of them?
I don't know if Qualcomm are more focused on some model that can be run on edge, on device, or it's more like a general AI in front. So yes, we do evaluate. We like to partner with companies which can run AI on edge. As you know, Qualcomm has a very strong push in edge computing space. But that does not really stop us from investing in companies which run their entire inference on cloud as well. So both work, but however, we do prefer partnering with companies which have an edge play with them. Yeah, that sounds good. So basically, what we currently have, we have smaller TTS model that can be run 05, fast or on device like 100 m, 100 million parameters. And we also have larger model. We are currently around server, yeah, and later on we are, say, we are moving to the voice agent area, yeah. Our model, typically, the size, will be two to 3 billion, which is, like, you know, it's not a sweet point, but it's still available to run our own device, yeah. We are able to make it in real time, in accurate interaction with users, okay, okay, that's a good overview. Xi, Jin, I guess while the team has had investments in the text to speech areas, we have an investment in, well said labs, if you know about them, it would be great if you can give me an understanding of how the overall market operates right now. How is your text to speech model different from that of well said labs or 1111 labs, and you mentioned you have a smaller model and a larger model. Can you give me a broader understanding of both of them?
I don't know if Qualcomm are more focused on some model that can be run on edge, on device, or it's more like a general AI in front. So yes, we do evaluate. We like to partner with companies which can run AI on edge. As you know, Qualcomm has a very strong push in edge computing space. But that does not really stop us from investing in companies which run their entire inference on cloud as well. So both work, but however, we do prefer partnering with companies which have an edge play with them. Yeah, that sounds good. So basically, what we currently have, we have smaller TTS model that can be run 05, fast or on device like 100 m, 100 million parameters. And we also have larger model. We are currently around server, yeah, and later on we are, say, we are moving to the voice agent area, yeah. Our model, typically, the size, will be two to 3 billion, which is, like, you know, it's not a sweet point, but it's still available to run our own device, yeah. We are able to make it in real time, in accurate interaction with users, okay, okay, that's a good overview. Xi, Jin, I guess while the team has had investments in the text to speech areas, we have an investment in, well said labs, if you know about them, it would be great if you can give me an understanding of how the overall market operates right now. How is your text to speech model different from that of well said labs or 1111 labs, and you mentioned you have a smaller model and a larger model. Can you give me a broader understanding of both of them?
I don't know if Qualcomm are more focused on some model that can be run on edge, on device, or it's more like a general AI in front. So yes, we do evaluate. We like to partner with companies which can run AI on edge. As you know, Qualcomm has a very strong push in edge computing space. But that does not really stop us from investing in companies which run their entire inference on cloud as well. So both work, but however, we do prefer partnering with companies which have an edge play with them. Yeah, that sounds good. So basically, what we currently have, we have smaller TTS model that can be run 05, fast or on device like 100 m, 100 million parameters. And we also have larger model. We are currently around server, yeah, and later on we are, say, we are moving to the voice agent area, yeah. Our model, typically, the size, will be two to 3 billion, which is, like, you know, it's not a sweet point, but it's still available to run our own device, yeah. We are able to make it in real time, in accurate interaction with users, okay, okay, that's a good overview. Xi, Jin, I guess while the team has had investments in the text to speech areas, we have an investment in, well said labs, if you know about them, it would be great if you can give me an understanding of how the overall market operates right now. How is your text to speech model different from that of well said labs or 1111 labs, and you mentioned you have a smaller model and a larger model. Can you give me a broader understanding of both of them?
S Speaker 16:29So let me answer the question you have first so to begin with is, what's the difference between us and 11 apps this kind of company? Generally, this kind of a company doesn't provide you a very strong personalization experience. So in the TKS area, you can customize 11 s. 11 s doesn't mean but you can't customize that and use that instantly. Say you want to use parent thumb to tell story to children. It's impossible to upload the 1 billion parents to the to a cloud, and I mean, store them and load them directly into your model, dynamic oriented models, but in visual audio, because we do inconvenience learning, you can use any voice cloning instantly. So basically give a 32nd clip.
So let me answer the question you have first so to begin with is, what's the difference between us and 11 apps this kind of company? Generally, this kind of a company doesn't provide you a very strong personalization experience. So in the TKS area, you can customize 11 s. 11 s doesn't mean but you can't customize that and use that instantly. Say you want to use parent thumb to tell story to children. It's impossible to upload the 1 billion parents to the to a cloud, and I mean, store them and load them directly into your model, dynamic oriented models, but in visual audio, because we do inconvenience learning, you can use any voice cloning instantly. So basically give a 32nd clip.
So let me answer the question you have first so to begin with is, what's the difference between us and 11 apps this kind of company? Generally, this kind of a company doesn't provide you a very strong personalization experience. So in the TKS area, you can customize 11 s. 11 s doesn't mean but you can't customize that and use that instantly. Say you want to use parent thumb to tell story to children. It's impossible to upload the 1 billion parents to the to a cloud, and I mean, store them and load them directly into your model, dynamic oriented models, but in visual audio, because we do inconvenience learning, you can use any voice cloning instantly. So basically give a 32nd clip.
So let me answer the question you have first so to begin with is, what's the difference between us and 11 apps this kind of company? Generally, this kind of a company doesn't provide you a very strong personalization experience. So in the TKS area, you can customize 11 s. 11 s doesn't mean but you can't customize that and use that instantly. Say you want to use parent thumb to tell story to children. It's impossible to upload the 1 billion parents to the to a cloud, and I mean, store them and load them directly into your model, dynamic oriented models, but in visual audio, because we do inconvenience learning, you can use any voice cloning instantly. So basically give a 32nd clip.
S Speaker 17:25Yeah, and what was the term you used? What? What? What kind of learning will you use
Yeah, and what was the term you used? What? What? What kind of learning will you use
Yeah, and what was the term you used? What? What? What kind of learning will you use
Yeah, and what was the term you used? What? What? What kind of learning will you use
S Speaker 17:40The next point is, we provide a knowledge solution. But everyone say that we are providing low energy. But as far as now, we see they are not truly low latency. The reason is, you see 11 labs data center is the US Central. And everywhere in the world, if you want to use the API, need to go to their data center. You need to go to their API first. So because we are a strong engineering team behind what we are doing is that we are building a distributed system. We have data centers in us, east west, in China, in Japan, currently building Japan and in Singapore. So once we have this data center ready the user's traffic go to their center directly, instead of going out into our server first. This can greatly cut the latency. I mean, worldwide. Yeah, got it. The last point is cheap, because our model is optimizing size. We provide a private 10% of the 11 apps, list price, interesting. So how are you able to achieve, let's say, the same level of quality, such a low latency and also a very low price altogether? It's more like optimization in the language model signs. We do lots of heavy optimization inference, like dynamic batching, tree caching, blah, blah, blah. We do lots of organization, just engineering side. Another thing is that price is not always depends on how much you spend on GPU resources, especially GTS area, we estimate like 11 apps have like 99% or even 99.9% of
The next point is, we provide a knowledge solution. But everyone say that we are providing low energy. But as far as now, we see they are not truly low latency. The reason is, you see 11 labs data center is the US Central. And everywhere in the world, if you want to use the API, need to go to their data center. You need to go to their API first. So because we are a strong engineering team behind what we are doing is that we are building a distributed system. We have data centers in us, east west, in China, in Japan, currently building Japan and in Singapore. So once we have this data center ready the user's traffic go to their center directly, instead of going out into our server first. This can greatly cut the latency. I mean, worldwide. Yeah, got it. The last point is cheap, because our model is optimizing size. We provide a private 10% of the 11 apps, list price, interesting. So how are you able to achieve, let's say, the same level of quality, such a low latency and also a very low price altogether? It's more like optimization in the language model signs. We do lots of heavy optimization inference, like dynamic batching, tree caching, blah, blah, blah. We do lots of organization, just engineering side. Another thing is that price is not always depends on how much you spend on GPU resources, especially GTS area, we estimate like 11 apps have like 99% or even 99.9% of
The next point is, we provide a knowledge solution. But everyone say that we are providing low energy. But as far as now, we see they are not truly low latency. The reason is, you see 11 labs data center is the US Central. And everywhere in the world, if you want to use the API, need to go to their data center. You need to go to their API first. So because we are a strong engineering team behind what we are doing is that we are building a distributed system. We have data centers in us, east west, in China, in Japan, currently building Japan and in Singapore. So once we have this data center ready the user's traffic go to their center directly, instead of going out into our server first. This can greatly cut the latency. I mean, worldwide. Yeah, got it. The last point is cheap, because our model is optimizing size. We provide a private 10% of the 11 apps, list price, interesting. So how are you able to achieve, let's say, the same level of quality, such a low latency and also a very low price altogether? It's more like optimization in the language model signs. We do lots of heavy optimization inference, like dynamic batching, tree caching, blah, blah, blah. We do lots of organization, just engineering side. Another thing is that price is not always depends on how much you spend on GPU resources, especially GTS area, we estimate like 11 apps have like 99% or even 99.9% of
The next point is, we provide a knowledge solution. But everyone say that we are providing low energy. But as far as now, we see they are not truly low latency. The reason is, you see 11 labs data center is the US Central. And everywhere in the world, if you want to use the API, need to go to their data center. You need to go to their API first. So because we are a strong engineering team behind what we are doing is that we are building a distributed system. We have data centers in us, east west, in China, in Japan, currently building Japan and in Singapore. So once we have this data center ready the user's traffic go to their center directly, instead of going out into our server first. This can greatly cut the latency. I mean, worldwide. Yeah, got it. The last point is cheap, because our model is optimizing size. We provide a private 10% of the 11 apps, list price, interesting. So how are you able to achieve, let's say, the same level of quality, such a low latency and also a very low price altogether? It's more like optimization in the language model signs. We do lots of heavy optimization inference, like dynamic batching, tree caching, blah, blah, blah. We do lots of organization, just engineering side. Another thing is that price is not always depends on how much you spend on GPU resources, especially GTS area, we estimate like 11 apps have like 99% or even 99.9% of
S Speaker 19:34yeah, so cutting that into 10% still give us like 99% of profit. Now I didn't, I didn't get that math. So let's say if 11 lab has a 99% gross margin, you cut the price by 10% by 90% so you are, your prices are currently 10% of that of 11 labs, then your gross margin also shrinks by the same number, right? There are also some team operating marketing, but this kind of different stuff. So currently, our growth monitoring is still connecting interesting. And how are you serving your technology right now? Is it through an API? Yes.
yeah, so cutting that into 10% still give us like 99% of profit. Now I didn't, I didn't get that math. So let's say if 11 lab has a 99% gross margin, you cut the price by 10% by 90% so you are, your prices are currently 10% of that of 11 labs, then your gross margin also shrinks by the same number, right? There are also some team operating marketing, but this kind of different stuff. So currently, our growth monitoring is still connecting interesting. And how are you serving your technology right now? Is it through an API? Yes.
yeah, so cutting that into 10% still give us like 99% of profit. Now I didn't, I didn't get that math. So let's say if 11 lab has a 99% gross margin, you cut the price by 10% by 90% so you are, your prices are currently 10% of that of 11 labs, then your gross margin also shrinks by the same number, right? There are also some team operating marketing, but this kind of different stuff. So currently, our growth monitoring is still connecting interesting. And how are you serving your technology right now? Is it through an API? Yes.
yeah, so cutting that into 10% still give us like 99% of profit. Now I didn't, I didn't get that math. So let's say if 11 lab has a 99% gross margin, you cut the price by 10% by 90% so you are, your prices are currently 10% of that of 11 labs, then your gross margin also shrinks by the same number, right? There are also some team operating marketing, but this kind of different stuff. So currently, our growth monitoring is still connecting interesting. And how are you serving your technology right now? Is it through an API? Yes.
S Speaker 110:22How has the traction been so far? Like, do you have revenue around it? Do how many users do you serve? Things like that? We don't have a digital, static data or API yet. We are having the many business customer and POC, especially AI companion, AI toy, these two area, and they are actually testing the model, and we expect to deliver, I mean, sign, first, two, three offer in the upcoming one two weeks. Yeah. Okay, okay, that's that's interesting. So you said that these companies, or the enterprises trying your API right now are mostly in the companion space. Do you see any reason why these kind of companies are mostly using your API? Companion is a very large it's not only about older and girlfriend. Yeah. It's also like, use parents voice to tell children. Tell children story and carrying elder people, there are many kind of usage in AI companion. We see different usage from both of them. Besides that, we also see we also have customer reaching out. They are testing our for the audiobook or the sorry. Go ahead. Convenient. We also have aI coach. Ai trainer is a different kind of stuff. Yeah, this kind of customers, yeah. So their company is not a very small market, and I believe in most startup, aigc Now, when we talk to them, most of them need a voice API,
How has the traction been so far? Like, do you have revenue around it? Do how many users do you serve? Things like that? We don't have a digital, static data or API yet. We are having the many business customer and POC, especially AI companion, AI toy, these two area, and they are actually testing the model, and we expect to deliver, I mean, sign, first, two, three offer in the upcoming one two weeks. Yeah. Okay, okay, that's that's interesting. So you said that these companies, or the enterprises trying your API right now are mostly in the companion space. Do you see any reason why these kind of companies are mostly using your API? Companion is a very large it's not only about older and girlfriend. Yeah. It's also like, use parents voice to tell children. Tell children story and carrying elder people, there are many kind of usage in AI companion. We see different usage from both of them. Besides that, we also see we also have customer reaching out. They are testing our for the audiobook or the sorry. Go ahead. Convenient. We also have aI coach. Ai trainer is a different kind of stuff. Yeah, this kind of customers, yeah. So their company is not a very small market, and I believe in most startup, aigc Now, when we talk to them, most of them need a voice API,
How has the traction been so far? Like, do you have revenue around it? Do how many users do you serve? Things like that? We don't have a digital, static data or API yet. We are having the many business customer and POC, especially AI companion, AI toy, these two area, and they are actually testing the model, and we expect to deliver, I mean, sign, first, two, three offer in the upcoming one two weeks. Yeah. Okay, okay, that's that's interesting. So you said that these companies, or the enterprises trying your API right now are mostly in the companion space. Do you see any reason why these kind of companies are mostly using your API? Companion is a very large it's not only about older and girlfriend. Yeah. It's also like, use parents voice to tell children. Tell children story and carrying elder people, there are many kind of usage in AI companion. We see different usage from both of them. Besides that, we also see we also have customer reaching out. They are testing our for the audiobook or the sorry. Go ahead. Convenient. We also have aI coach. Ai trainer is a different kind of stuff. Yeah, this kind of customers, yeah. So their company is not a very small market, and I believe in most startup, aigc Now, when we talk to them, most of them need a voice API,
How has the traction been so far? Like, do you have revenue around it? Do how many users do you serve? Things like that? We don't have a digital, static data or API yet. We are having the many business customer and POC, especially AI companion, AI toy, these two area, and they are actually testing the model, and we expect to deliver, I mean, sign, first, two, three offer in the upcoming one two weeks. Yeah. Okay, okay, that's that's interesting. So you said that these companies, or the enterprises trying your API right now are mostly in the companion space. Do you see any reason why these kind of companies are mostly using your API? Companion is a very large it's not only about older and girlfriend. Yeah. It's also like, use parents voice to tell children. Tell children story and carrying elder people, there are many kind of usage in AI companion. We see different usage from both of them. Besides that, we also see we also have customer reaching out. They are testing our for the audiobook or the sorry. Go ahead. Convenient. We also have aI coach. Ai trainer is a different kind of stuff. Yeah, this kind of customers, yeah. So their company is not a very small market, and I believe in most startup, aigc Now, when we talk to them, most of them need a voice API,
S Speaker 112:08That's interesting. And are these come? How are these companies finding you right now? So some are from our open source project, right? And some from some our videos or Twitter. Basically we joined, announced that with some other cloud provider like lepton and yeah, others, we get in touch, maybe in person, like we go to this booth, we go to this event, power, meet, yeah. Okay, okay, pretty interesting, but yeah, so far, you haven't had a very strong, let's say, a marketing push towards it. We are still trying to see how much can customer pay in each market. Yeah, we are still doing this kind of test. Want to see how much can they pay. We don't have the data because our API, I mean, the integration between us and them is not online yet. Got it so as of now, you do not have any deployed customers in production. No, we don't have any deployed customers in production. We only have some to see customer play in the playground. That's not a big part of our revenue. So we don't need to Understood, understood, and how long has it been since the company was formed? And yeah, so we have working on audio around two years ago and working on TTS since December last year. Yeah, and how many was formed,
That's interesting. And are these come? How are these companies finding you right now? So some are from our open source project, right? And some from some our videos or Twitter. Basically we joined, announced that with some other cloud provider like lepton and yeah, others, we get in touch, maybe in person, like we go to this booth, we go to this event, power, meet, yeah. Okay, okay, pretty interesting, but yeah, so far, you haven't had a very strong, let's say, a marketing push towards it. We are still trying to see how much can customer pay in each market. Yeah, we are still doing this kind of test. Want to see how much can they pay. We don't have the data because our API, I mean, the integration between us and them is not online yet. Got it so as of now, you do not have any deployed customers in production. No, we don't have any deployed customers in production. We only have some to see customer play in the playground. That's not a big part of our revenue. So we don't need to Understood, understood, and how long has it been since the company was formed? And yeah, so we have working on audio around two years ago and working on TTS since December last year. Yeah, and how many was formed,
That's interesting. And are these come? How are these companies finding you right now? So some are from our open source project, right? And some from some our videos or Twitter. Basically we joined, announced that with some other cloud provider like lepton and yeah, others, we get in touch, maybe in person, like we go to this booth, we go to this event, power, meet, yeah. Okay, okay, pretty interesting, but yeah, so far, you haven't had a very strong, let's say, a marketing push towards it. We are still trying to see how much can customer pay in each market. Yeah, we are still doing this kind of test. Want to see how much can they pay. We don't have the data because our API, I mean, the integration between us and them is not online yet. Got it so as of now, you do not have any deployed customers in production. No, we don't have any deployed customers in production. We only have some to see customer play in the playground. That's not a big part of our revenue. So we don't need to Understood, understood, and how long has it been since the company was formed? And yeah, so we have working on audio around two years ago and working on TTS since December last year. Yeah, and how many was formed,
That's interesting. And are these come? How are these companies finding you right now? So some are from our open source project, right? And some from some our videos or Twitter. Basically we joined, announced that with some other cloud provider like lepton and yeah, others, we get in touch, maybe in person, like we go to this booth, we go to this event, power, meet, yeah. Okay, okay, pretty interesting, but yeah, so far, you haven't had a very strong, let's say, a marketing push towards it. We are still trying to see how much can customer pay in each market. Yeah, we are still doing this kind of test. Want to see how much can they pay. We don't have the data because our API, I mean, the integration between us and them is not online yet. Got it so as of now, you do not have any deployed customers in production. No, we don't have any deployed customers in production. We only have some to see customer play in the playground. That's not a big part of our revenue. So we don't need to Understood, understood, and how long has it been since the company was formed? And yeah, so we have working on audio around two years ago and working on TTS since December last year. Yeah, and how many was formed,
S Speaker 113:50Got it? Got it so. So you said that you've been working on DTS, which means, like, was it through fish audio? Was it through a separate part time project. How was that source? Fish audio, the open source project for speech, right?
Got it? Got it so. So you said that you've been working on DTS, which means, like, was it through fish audio? Was it through a separate part time project. How was that source? Fish audio, the open source project for speech, right?
Got it? Got it so. So you said that you've been working on DTS, which means, like, was it through fish audio? Was it through a separate part time project. How was that source? Fish audio, the open source project for speech, right?
Got it? Got it so. So you said that you've been working on DTS, which means, like, was it through fish audio? Was it through a separate part time project. How was that source? Fish audio, the open source project for speech, right?
S Speaker 114:10seven months ago. But other projects, TTS project, like purchase, which is also made by us, it also has a lot of attraction, attraction of Git, it was way back, like, around one year ago. Yeah. What is this project again? Can you tell me a little bit more about our like, older TTS model? Okay, it's using Bert and vrts to improve the performance on emotion. It runs pretty fast, but the outer bond is lower than current model, so we are moving to current model. Current models architecture is also designed for our next generation voice agent. Yeah,
seven months ago. But other projects, TTS project, like purchase, which is also made by us, it also has a lot of attraction, attraction of Git, it was way back, like, around one year ago. Yeah. What is this project again? Can you tell me a little bit more about our like, older TTS model? Okay, it's using Bert and vrts to improve the performance on emotion. It runs pretty fast, but the outer bond is lower than current model, so we are moving to current model. Current models architecture is also designed for our next generation voice agent. Yeah,
seven months ago. But other projects, TTS project, like purchase, which is also made by us, it also has a lot of attraction, attraction of Git, it was way back, like, around one year ago. Yeah. What is this project again? Can you tell me a little bit more about our like, older TTS model? Okay, it's using Bert and vrts to improve the performance on emotion. It runs pretty fast, but the outer bond is lower than current model, so we are moving to current model. Current models architecture is also designed for our next generation voice agent. Yeah,
seven months ago. But other projects, TTS project, like purchase, which is also made by us, it also has a lot of attraction, attraction of Git, it was way back, like, around one year ago. Yeah. What is this project again? Can you tell me a little bit more about our like, older TTS model? Okay, it's using Bert and vrts to improve the performance on emotion. It runs pretty fast, but the outer bond is lower than current model, so we are moving to current model. Current models architecture is also designed for our next generation voice agent. Yeah,
S Speaker 114:53understood. I'm quite curious about the voice agent, part of it, seizure, but let's wait before we discuss that. You also mentioned you have a smaller model and a larger model. How is can you tell me a little bit more about both each of these, and how do they compare against the existing models from 11 labs? Well said labs and other companies. So I don't know the exact size 11 app turbo is kind of model is, yeah, but I would say for us, the large model currently is around 1 billion it supports, currently, when we are training a model support seven languages and but smaller model is generally like we see, it's like 100 million parameters, and it only supports like one or two languages, right? And which, which, how many and which languages does the larger model support. I need to check my list because, yeah, I know that Chinese, English, Japanese and Korean. These four are supported right?
understood. I'm quite curious about the voice agent, part of it, seizure, but let's wait before we discuss that. You also mentioned you have a smaller model and a larger model. How is can you tell me a little bit more about both each of these, and how do they compare against the existing models from 11 labs? Well said labs and other companies. So I don't know the exact size 11 app turbo is kind of model is, yeah, but I would say for us, the large model currently is around 1 billion it supports, currently, when we are training a model support seven languages and but smaller model is generally like we see, it's like 100 million parameters, and it only supports like one or two languages, right? And which, which, how many and which languages does the larger model support. I need to check my list because, yeah, I know that Chinese, English, Japanese and Korean. These four are supported right?
understood. I'm quite curious about the voice agent, part of it, seizure, but let's wait before we discuss that. You also mentioned you have a smaller model and a larger model. How is can you tell me a little bit more about both each of these, and how do they compare against the existing models from 11 labs? Well said labs and other companies. So I don't know the exact size 11 app turbo is kind of model is, yeah, but I would say for us, the large model currently is around 1 billion it supports, currently, when we are training a model support seven languages and but smaller model is generally like we see, it's like 100 million parameters, and it only supports like one or two languages, right? And which, which, how many and which languages does the larger model support. I need to check my list because, yeah, I know that Chinese, English, Japanese and Korean. These four are supported right?
understood. I'm quite curious about the voice agent, part of it, seizure, but let's wait before we discuss that. You also mentioned you have a smaller model and a larger model. How is can you tell me a little bit more about both each of these, and how do they compare against the existing models from 11 labs? Well said labs and other companies. So I don't know the exact size 11 app turbo is kind of model is, yeah, but I would say for us, the large model currently is around 1 billion it supports, currently, when we are training a model support seven languages and but smaller model is generally like we see, it's like 100 million parameters, and it only supports like one or two languages, right? And which, which, how many and which languages does the larger model support. I need to check my list because, yeah, I know that Chinese, English, Japanese and Korean. These four are supported right?
S Speaker 115:59his Spanish and which one? And Portugal? Yeah, produce is supported, yeah, I think there are still two remaining, but I didn't remember, I think there's Arabic or, yeah, interesting, interesting. That's, that's quite interesting. And, yeah, can you? Can you tell me a little bit about how your plans are with the voice agent space? What are you trying to build around there? So we want to build a smaller model that can interact and we can call tools. So basically, when you are talking to the model, the model instantly respond you, and it also triggered a phone call if you, if you need a very heavy reasoning process like planning the trip sort of stuff, it use a asynchronized server to run maybe so, for example, my idea and finish the planning, and merge the results into the into the speech screen, so you can feel you are real time interactive. But it's actually, it can have more heavy, heavy process, I think, do you know the apple intelligence, which announced recently, yeah, yeah, yeah. Well, our model, like we designed something similar that few months ago, two months ago, I think, yeah, basically it's just you have a smaller model sensitive process your lightweight data that doesn't require so much reasoning, and then you have a cloud model that is much larger and can process the data, the planning, the reasoning. Yeah, very interesting. Got it so you have your voice API running to collect data, and then you convert that speech into text. You use open AI models for reasoning and how, what, what models and what architecture are you using for tool calling? Let me there is a bit so there are some misunderstanding here. Let me describe it again. So, so here we have the real time engagement. So you see everyone besides open AI is as that is what you said. We what do we currently have? Yeah, in this paradigm, it's very easy to do phone call, right? Is just ask LLM to give us a JSON. The next stage, which is currently open AI and we are working on, is end to end model. So the model takes the voice in and output the voice. There is no ASR, no TTS process anymore. Okay.
his Spanish and which one? And Portugal? Yeah, produce is supported, yeah, I think there are still two remaining, but I didn't remember, I think there's Arabic or, yeah, interesting, interesting. That's, that's quite interesting. And, yeah, can you? Can you tell me a little bit about how your plans are with the voice agent space? What are you trying to build around there? So we want to build a smaller model that can interact and we can call tools. So basically, when you are talking to the model, the model instantly respond you, and it also triggered a phone call if you, if you need a very heavy reasoning process like planning the trip sort of stuff, it use a asynchronized server to run maybe so, for example, my idea and finish the planning, and merge the results into the into the speech screen, so you can feel you are real time interactive. But it's actually, it can have more heavy, heavy process, I think, do you know the apple intelligence, which announced recently, yeah, yeah, yeah. Well, our model, like we designed something similar that few months ago, two months ago, I think, yeah, basically it's just you have a smaller model sensitive process your lightweight data that doesn't require so much reasoning, and then you have a cloud model that is much larger and can process the data, the planning, the reasoning. Yeah, very interesting. Got it so you have your voice API running to collect data, and then you convert that speech into text. You use open AI models for reasoning and how, what, what models and what architecture are you using for tool calling? Let me there is a bit so there are some misunderstanding here. Let me describe it again. So, so here we have the real time engagement. So you see everyone besides open AI is as that is what you said. We what do we currently have? Yeah, in this paradigm, it's very easy to do phone call, right? Is just ask LLM to give us a JSON. The next stage, which is currently open AI and we are working on, is end to end model. So the model takes the voice in and output the voice. There is no ASR, no TTS process anymore. Okay.
his Spanish and which one? And Portugal? Yeah, produce is supported, yeah, I think there are still two remaining, but I didn't remember, I think there's Arabic or, yeah, interesting, interesting. That's, that's quite interesting. And, yeah, can you? Can you tell me a little bit about how your plans are with the voice agent space? What are you trying to build around there? So we want to build a smaller model that can interact and we can call tools. So basically, when you are talking to the model, the model instantly respond you, and it also triggered a phone call if you, if you need a very heavy reasoning process like planning the trip sort of stuff, it use a asynchronized server to run maybe so, for example, my idea and finish the planning, and merge the results into the into the speech screen, so you can feel you are real time interactive. But it's actually, it can have more heavy, heavy process, I think, do you know the apple intelligence, which announced recently, yeah, yeah, yeah. Well, our model, like we designed something similar that few months ago, two months ago, I think, yeah, basically it's just you have a smaller model sensitive process your lightweight data that doesn't require so much reasoning, and then you have a cloud model that is much larger and can process the data, the planning, the reasoning. Yeah, very interesting. Got it so you have your voice API running to collect data, and then you convert that speech into text. You use open AI models for reasoning and how, what, what models and what architecture are you using for tool calling? Let me there is a bit so there are some misunderstanding here. Let me describe it again. So, so here we have the real time engagement. So you see everyone besides open AI is as that is what you said. We what do we currently have? Yeah, in this paradigm, it's very easy to do phone call, right? Is just ask LLM to give us a JSON. The next stage, which is currently open AI and we are working on, is end to end model. So the model takes the voice in and output the voice. There is no ASR, no TTS process anymore. Okay.
his Spanish and which one? And Portugal? Yeah, produce is supported, yeah, I think there are still two remaining, but I didn't remember, I think there's Arabic or, yeah, interesting, interesting. That's, that's quite interesting. And, yeah, can you? Can you tell me a little bit about how your plans are with the voice agent space? What are you trying to build around there? So we want to build a smaller model that can interact and we can call tools. So basically, when you are talking to the model, the model instantly respond you, and it also triggered a phone call if you, if you need a very heavy reasoning process like planning the trip sort of stuff, it use a asynchronized server to run maybe so, for example, my idea and finish the planning, and merge the results into the into the speech screen, so you can feel you are real time interactive. But it's actually, it can have more heavy, heavy process, I think, do you know the apple intelligence, which announced recently, yeah, yeah, yeah. Well, our model, like we designed something similar that few months ago, two months ago, I think, yeah, basically it's just you have a smaller model sensitive process your lightweight data that doesn't require so much reasoning, and then you have a cloud model that is much larger and can process the data, the planning, the reasoning. Yeah, very interesting. Got it so you have your voice API running to collect data, and then you convert that speech into text. You use open AI models for reasoning and how, what, what models and what architecture are you using for tool calling? Let me there is a bit so there are some misunderstanding here. Let me describe it again. So, so here we have the real time engagement. So you see everyone besides open AI is as that is what you said. We what do we currently have? Yeah, in this paradigm, it's very easy to do phone call, right? Is just ask LLM to give us a JSON. The next stage, which is currently open AI and we are working on, is end to end model. So the model takes the voice in and output the voice. There is no ASR, no TTS process anymore. Okay.
S Speaker 118:38audio tokens and function call focus like a general text to do the final call to the use JSON to call the functions. Yeah, interesting. And you're able to achieve a pretty fast latency on that is what I'm seeing. That's, that's quite exciting. Yeah, that's what we are planning, yeah. And also we want to another, I mean, the most interesting part of our voice agent, that differs are from open AI products kind of company is we especially care about personalized we, as you see last time, we show that if you clone a voice, right? Yeah, you do the timber tone with speed, customized or clone. The next step is the personality behavior and the knowledge customization of the voice agent. So based on our three data engines, we are able to collect customers real time feedback, real time feedback, like if the user is satisfied with the result in real time. Use it to do both in connect learning and use it to function the model. So as the model, use the as the users the model for longer time, the model will fit users behavior and try to say what users want to hear and act like what the user prefers. And also it also has an aligned process say, if our use our agent in some calling center, or this kind of scenario, and the agent is interrupted by a human agent. Yeah, you think that's not correct. Okay, so our model will collect this kind of data, the NGO collect this kind of data and use it to fine tune the original model. So it's an online learning process. So the preference and knowledge is the online learning process. One is for users. Preference one is for users. Knowledge,
audio tokens and function call focus like a general text to do the final call to the use JSON to call the functions. Yeah, interesting. And you're able to achieve a pretty fast latency on that is what I'm seeing. That's, that's quite exciting. Yeah, that's what we are planning, yeah. And also we want to another, I mean, the most interesting part of our voice agent, that differs are from open AI products kind of company is we especially care about personalized we, as you see last time, we show that if you clone a voice, right? Yeah, you do the timber tone with speed, customized or clone. The next step is the personality behavior and the knowledge customization of the voice agent. So based on our three data engines, we are able to collect customers real time feedback, real time feedback, like if the user is satisfied with the result in real time. Use it to do both in connect learning and use it to function the model. So as the model, use the as the users the model for longer time, the model will fit users behavior and try to say what users want to hear and act like what the user prefers. And also it also has an aligned process say, if our use our agent in some calling center, or this kind of scenario, and the agent is interrupted by a human agent. Yeah, you think that's not correct. Okay, so our model will collect this kind of data, the NGO collect this kind of data and use it to fine tune the original model. So it's an online learning process. So the preference and knowledge is the online learning process. One is for users. Preference one is for users. Knowledge,
audio tokens and function call focus like a general text to do the final call to the use JSON to call the functions. Yeah, interesting. And you're able to achieve a pretty fast latency on that is what I'm seeing. That's, that's quite exciting. Yeah, that's what we are planning, yeah. And also we want to another, I mean, the most interesting part of our voice agent, that differs are from open AI products kind of company is we especially care about personalized we, as you see last time, we show that if you clone a voice, right? Yeah, you do the timber tone with speed, customized or clone. The next step is the personality behavior and the knowledge customization of the voice agent. So based on our three data engines, we are able to collect customers real time feedback, real time feedback, like if the user is satisfied with the result in real time. Use it to do both in connect learning and use it to function the model. So as the model, use the as the users the model for longer time, the model will fit users behavior and try to say what users want to hear and act like what the user prefers. And also it also has an aligned process say, if our use our agent in some calling center, or this kind of scenario, and the agent is interrupted by a human agent. Yeah, you think that's not correct. Okay, so our model will collect this kind of data, the NGO collect this kind of data and use it to fine tune the original model. So it's an online learning process. So the preference and knowledge is the online learning process. One is for users. Preference one is for users. Knowledge,
audio tokens and function call focus like a general text to do the final call to the use JSON to call the functions. Yeah, interesting. And you're able to achieve a pretty fast latency on that is what I'm seeing. That's, that's quite exciting. Yeah, that's what we are planning, yeah. And also we want to another, I mean, the most interesting part of our voice agent, that differs are from open AI products kind of company is we especially care about personalized we, as you see last time, we show that if you clone a voice, right? Yeah, you do the timber tone with speed, customized or clone. The next step is the personality behavior and the knowledge customization of the voice agent. So based on our three data engines, we are able to collect customers real time feedback, real time feedback, like if the user is satisfied with the result in real time. Use it to do both in connect learning and use it to function the model. So as the model, use the as the users the model for longer time, the model will fit users behavior and try to say what users want to hear and act like what the user prefers. And also it also has an aligned process say, if our use our agent in some calling center, or this kind of scenario, and the agent is interrupted by a human agent. Yeah, you think that's not correct. Okay, so our model will collect this kind of data, the NGO collect this kind of data and use it to fine tune the original model. So it's an online learning process. So the preference and knowledge is the online learning process. One is for users. Preference one is for users. Knowledge,
S Speaker 120:34the synthesizer is another part that allows you to create a voiceover in a much cheaper price, like if you hire someone to record a video, if it's a famous I'm not super famous, like $100 per minute is the general price, yeah. But if you use fish audio, you just get it is five minutes of audio, and then you can use fish audio to synthesize, again, $1 per minute.
the synthesizer is another part that allows you to create a voiceover in a much cheaper price, like if you hire someone to record a video, if it's a famous I'm not super famous, like $100 per minute is the general price, yeah. But if you use fish audio, you just get it is five minutes of audio, and then you can use fish audio to synthesize, again, $1 per minute.
the synthesizer is another part that allows you to create a voiceover in a much cheaper price, like if you hire someone to record a video, if it's a famous I'm not super famous, like $100 per minute is the general price, yeah. But if you use fish audio, you just get it is five minutes of audio, and then you can use fish audio to synthesize, again, $1 per minute.
the synthesizer is another part that allows you to create a voiceover in a much cheaper price, like if you hire someone to record a video, if it's a famous I'm not super famous, like $100 per minute is the general price, yeah. But if you use fish audio, you just get it is five minutes of audio, and then you can use fish audio to synthesize, again, $1 per minute.
S Speaker 121:03see this use case in audiobook, especially so this kind of pro, pro level, people who do the generally read out the books. Do that, this kind of podcast or recording asking them to read one book, take generally more than 20k yeah. But say you have one hour of his data, and you use some annotator, no matter in which country, annotate if audio quality is correct and choose the best one. Yeah, yeah. It only takes you 1k interesting and are you? Are you seeing more, let's say, signals from the market, that something like this in the audiobook space is quite exciting. Is the market sort of pulling you towards it this kind of market? But you know, audiobook is kind of existing market, yeah. So we are trying to convert. We are getting in touch with many big audiobooks companies in China, we didn't reach out to us audio books yet. We see they have great potential to pay, but we didn't see a very large room for continuous getting benefits from them. Yeah. Say AI to convert that. We can convert 10 books a day, right? But I mean the others, the top others who write books, and the writers, they don't, they can't write 10 books a day, yeah, yeah. So we are targeting the new market, the AI GC startup. This is what we are targeting on so this is we can see growing market, and they are, since they are growing revenue, yeah, yes, yes, I understand that. She did. You also showed me in the previous slide. You You explained the stage two. Can you explain me the next stage and how are you thinking? Sorry, yes, you explained stage one. How are you thinking about stage two? And what will that be? The Stage Two will be, keep a live connection open between you and the agent. It's full double click, so apparently you can see you can interrupt open. Error, yeah, by the way, you are speaking open and never speak. It doesn't say, Ah, yeah, anything supportive? No, because it's still turn based. So if you speak, I need to shut. I can't, I can't speak anything the term base, but in the next stage, the AI and the human can interrupt each other. So it will be a truly interactive say. If you say yes, a true This doesn't mean to interrupt the model. The model say that. It doesn't means to interrupt you too. So there's a kind of next stage, basically, chat, native, elect, human, interactive. Yeah, that's that's pretty interesting. Is there any technological blockage right now that open? Ai models cannot do that. This is one. This is the expense. The expense will be open. Ai, apparently don't want to use so many GPUs to I mean, if you want to make it low latency and keep it connected to prepare, for example, one GPU for maybe 10 or 16 customers. I mean, one, one, no, in their model size, and this is hard to afford. And another thing is it's harder to do alignment on this kind because it's a two way stream. It's not like a term base that easier, that easy to create the supervised data. Yeah, it's two way data, so it's much harder to collect. Yeah, got it, got it. That's pretty interesting. Yeah, I can imagine a lot of new use cases being open to that, and that's certainly a limitation with the models right now. I didn't think about it that way, but that's that's quite exciting, yeah, yeah, but
see this use case in audiobook, especially so this kind of pro, pro level, people who do the generally read out the books. Do that, this kind of podcast or recording asking them to read one book, take generally more than 20k yeah. But say you have one hour of his data, and you use some annotator, no matter in which country, annotate if audio quality is correct and choose the best one. Yeah, yeah. It only takes you 1k interesting and are you? Are you seeing more, let's say, signals from the market, that something like this in the audiobook space is quite exciting. Is the market sort of pulling you towards it this kind of market? But you know, audiobook is kind of existing market, yeah. So we are trying to convert. We are getting in touch with many big audiobooks companies in China, we didn't reach out to us audio books yet. We see they have great potential to pay, but we didn't see a very large room for continuous getting benefits from them. Yeah. Say AI to convert that. We can convert 10 books a day, right? But I mean the others, the top others who write books, and the writers, they don't, they can't write 10 books a day, yeah, yeah. So we are targeting the new market, the AI GC startup. This is what we are targeting on so this is we can see growing market, and they are, since they are growing revenue, yeah, yes, yes, I understand that. She did. You also showed me in the previous slide. You You explained the stage two. Can you explain me the next stage and how are you thinking? Sorry, yes, you explained stage one. How are you thinking about stage two? And what will that be? The Stage Two will be, keep a live connection open between you and the agent. It's full double click, so apparently you can see you can interrupt open. Error, yeah, by the way, you are speaking open and never speak. It doesn't say, Ah, yeah, anything supportive? No, because it's still turn based. So if you speak, I need to shut. I can't, I can't speak anything the term base, but in the next stage, the AI and the human can interrupt each other. So it will be a truly interactive say. If you say yes, a true This doesn't mean to interrupt the model. The model say that. It doesn't means to interrupt you too. So there's a kind of next stage, basically, chat, native, elect, human, interactive. Yeah, that's that's pretty interesting. Is there any technological blockage right now that open? Ai models cannot do that. This is one. This is the expense. The expense will be open. Ai, apparently don't want to use so many GPUs to I mean, if you want to make it low latency and keep it connected to prepare, for example, one GPU for maybe 10 or 16 customers. I mean, one, one, no, in their model size, and this is hard to afford. And another thing is it's harder to do alignment on this kind because it's a two way stream. It's not like a term base that easier, that easy to create the supervised data. Yeah, it's two way data, so it's much harder to collect. Yeah, got it, got it. That's pretty interesting. Yeah, I can imagine a lot of new use cases being open to that, and that's certainly a limitation with the models right now. I didn't think about it that way, but that's that's quite exciting, yeah, yeah, but
see this use case in audiobook, especially so this kind of pro, pro level, people who do the generally read out the books. Do that, this kind of podcast or recording asking them to read one book, take generally more than 20k yeah. But say you have one hour of his data, and you use some annotator, no matter in which country, annotate if audio quality is correct and choose the best one. Yeah, yeah. It only takes you 1k interesting and are you? Are you seeing more, let's say, signals from the market, that something like this in the audiobook space is quite exciting. Is the market sort of pulling you towards it this kind of market? But you know, audiobook is kind of existing market, yeah. So we are trying to convert. We are getting in touch with many big audiobooks companies in China, we didn't reach out to us audio books yet. We see they have great potential to pay, but we didn't see a very large room for continuous getting benefits from them. Yeah. Say AI to convert that. We can convert 10 books a day, right? But I mean the others, the top others who write books, and the writers, they don't, they can't write 10 books a day, yeah, yeah. So we are targeting the new market, the AI GC startup. This is what we are targeting on so this is we can see growing market, and they are, since they are growing revenue, yeah, yes, yes, I understand that. She did. You also showed me in the previous slide. You You explained the stage two. Can you explain me the next stage and how are you thinking? Sorry, yes, you explained stage one. How are you thinking about stage two? And what will that be? The Stage Two will be, keep a live connection open between you and the agent. It's full double click, so apparently you can see you can interrupt open. Error, yeah, by the way, you are speaking open and never speak. It doesn't say, Ah, yeah, anything supportive? No, because it's still turn based. So if you speak, I need to shut. I can't, I can't speak anything the term base, but in the next stage, the AI and the human can interrupt each other. So it will be a truly interactive say. If you say yes, a true This doesn't mean to interrupt the model. The model say that. It doesn't means to interrupt you too. So there's a kind of next stage, basically, chat, native, elect, human, interactive. Yeah, that's that's pretty interesting. Is there any technological blockage right now that open? Ai models cannot do that. This is one. This is the expense. The expense will be open. Ai, apparently don't want to use so many GPUs to I mean, if you want to make it low latency and keep it connected to prepare, for example, one GPU for maybe 10 or 16 customers. I mean, one, one, no, in their model size, and this is hard to afford. And another thing is it's harder to do alignment on this kind because it's a two way stream. It's not like a term base that easier, that easy to create the supervised data. Yeah, it's two way data, so it's much harder to collect. Yeah, got it, got it. That's pretty interesting. Yeah, I can imagine a lot of new use cases being open to that, and that's certainly a limitation with the models right now. I didn't think about it that way, but that's that's quite exciting, yeah, yeah, but
see this use case in audiobook, especially so this kind of pro, pro level, people who do the generally read out the books. Do that, this kind of podcast or recording asking them to read one book, take generally more than 20k yeah. But say you have one hour of his data, and you use some annotator, no matter in which country, annotate if audio quality is correct and choose the best one. Yeah, yeah. It only takes you 1k interesting and are you? Are you seeing more, let's say, signals from the market, that something like this in the audiobook space is quite exciting. Is the market sort of pulling you towards it this kind of market? But you know, audiobook is kind of existing market, yeah. So we are trying to convert. We are getting in touch with many big audiobooks companies in China, we didn't reach out to us audio books yet. We see they have great potential to pay, but we didn't see a very large room for continuous getting benefits from them. Yeah. Say AI to convert that. We can convert 10 books a day, right? But I mean the others, the top others who write books, and the writers, they don't, they can't write 10 books a day, yeah, yeah. So we are targeting the new market, the AI GC startup. This is what we are targeting on so this is we can see growing market, and they are, since they are growing revenue, yeah, yes, yes, I understand that. She did. You also showed me in the previous slide. You You explained the stage two. Can you explain me the next stage and how are you thinking? Sorry, yes, you explained stage one. How are you thinking about stage two? And what will that be? The Stage Two will be, keep a live connection open between you and the agent. It's full double click, so apparently you can see you can interrupt open. Error, yeah, by the way, you are speaking open and never speak. It doesn't say, Ah, yeah, anything supportive? No, because it's still turn based. So if you speak, I need to shut. I can't, I can't speak anything the term base, but in the next stage, the AI and the human can interrupt each other. So it will be a truly interactive say. If you say yes, a true This doesn't mean to interrupt the model. The model say that. It doesn't means to interrupt you too. So there's a kind of next stage, basically, chat, native, elect, human, interactive. Yeah, that's that's pretty interesting. Is there any technological blockage right now that open? Ai models cannot do that. This is one. This is the expense. The expense will be open. Ai, apparently don't want to use so many GPUs to I mean, if you want to make it low latency and keep it connected to prepare, for example, one GPU for maybe 10 or 16 customers. I mean, one, one, no, in their model size, and this is hard to afford. And another thing is it's harder to do alignment on this kind because it's a two way stream. It's not like a term base that easier, that easy to create the supervised data. Yeah, it's two way data, so it's much harder to collect. Yeah, got it, got it. That's pretty interesting. Yeah, I can imagine a lot of new use cases being open to that, and that's certainly a limitation with the models right now. I didn't think about it that way, but that's that's quite exciting, yeah, yeah, but
24:48say if we have a 2 billion model can be deployed
S Speaker 124:52on customers, device, mobile phones. I think it should be possible, because you don't use a central data center,
on customers, device, mobile phones. I think it should be possible, because you don't use a central data center,
on customers, device, mobile phones. I think it should be possible, because you don't use a central data center,
on customers, device, mobile phones. I think it should be possible, because you don't use a central data center,
S Speaker 125:06right? Got it? Do you have plans to launch your models on edge, and have you taken any steps towards that? So far, I think for TTS, it can be fast if we see some company, for example, Qualcomm, we are working together to toward that to be fast, because our team doesn't have lots of experiment in deploying LM to to mobile devices. Yeah, but I know Hong Kong has my
right? Got it? Do you have plans to launch your models on edge, and have you taken any steps towards that? So far, I think for TTS, it can be fast if we see some company, for example, Qualcomm, we are working together to toward that to be fast, because our team doesn't have lots of experiment in deploying LM to to mobile devices. Yeah, but I know Hong Kong has my
right? Got it? Do you have plans to launch your models on edge, and have you taken any steps towards that? So far, I think for TTS, it can be fast if we see some company, for example, Qualcomm, we are working together to toward that to be fast, because our team doesn't have lots of experiment in deploying LM to to mobile devices. Yeah, but I know Hong Kong has my
right? Got it? Do you have plans to launch your models on edge, and have you taken any steps towards that? So far, I think for TTS, it can be fast if we see some company, for example, Qualcomm, we are working together to toward that to be fast, because our team doesn't have lots of experiment in deploying LM to to mobile devices. Yeah, but I know Hong Kong has my
S Speaker 125:48in this area? Yeah, yeah, yeah, absolutely, yes, yes, yes. For sure, there is a lot of lot of, I would say, research going internally to make sure that several models are compatible with edge if you research about Qualcomm's AI model hub, you'll see that the team internally has worked with a lot of model developers to make sure that their models are compatible for edge deployment. So certainly I see synergies around that space. Yeah, and then is the end to end stage, which is term based. The model will be around two, 3 billion, which is also runable on edge devices. It's overall, still a transformer model and not super. I mean, there's no huge difference between it and llama. The only difference is like the decoder part, like we decode a continuous latent, or at least four to eight tokens, parallely, yeah, yeah, two, two. Because say, text, you can have one word in one token, yeah. But audio is much harder, because you have a timbre, you have your current Tom, you have your peach, you have different characteristics. You can't include all of them in one token. Audio, we in fish audio, we predict 48 tokens for each friend directly, yeah, for each token for each you said, sorry I missed
in this area? Yeah, yeah, yeah, absolutely, yes, yes, yes. For sure, there is a lot of lot of, I would say, research going internally to make sure that several models are compatible with edge if you research about Qualcomm's AI model hub, you'll see that the team internally has worked with a lot of model developers to make sure that their models are compatible for edge deployment. So certainly I see synergies around that space. Yeah, and then is the end to end stage, which is term based. The model will be around two, 3 billion, which is also runable on edge devices. It's overall, still a transformer model and not super. I mean, there's no huge difference between it and llama. The only difference is like the decoder part, like we decode a continuous latent, or at least four to eight tokens, parallely, yeah, yeah, two, two. Because say, text, you can have one word in one token, yeah. But audio is much harder, because you have a timbre, you have your current Tom, you have your peach, you have different characteristics. You can't include all of them in one token. Audio, we in fish audio, we predict 48 tokens for each friend directly, yeah, for each token for each you said, sorry I missed
in this area? Yeah, yeah, yeah, absolutely, yes, yes, yes. For sure, there is a lot of lot of, I would say, research going internally to make sure that several models are compatible with edge if you research about Qualcomm's AI model hub, you'll see that the team internally has worked with a lot of model developers to make sure that their models are compatible for edge deployment. So certainly I see synergies around that space. Yeah, and then is the end to end stage, which is term based. The model will be around two, 3 billion, which is also runable on edge devices. It's overall, still a transformer model and not super. I mean, there's no huge difference between it and llama. The only difference is like the decoder part, like we decode a continuous latent, or at least four to eight tokens, parallely, yeah, yeah, two, two. Because say, text, you can have one word in one token, yeah. But audio is much harder, because you have a timbre, you have your current Tom, you have your peach, you have different characteristics. You can't include all of them in one token. Audio, we in fish audio, we predict 48 tokens for each friend directly, yeah, for each token for each you said, sorry I missed
in this area? Yeah, yeah, yeah, absolutely, yes, yes, yes. For sure, there is a lot of lot of, I would say, research going internally to make sure that several models are compatible with edge if you research about Qualcomm's AI model hub, you'll see that the team internally has worked with a lot of model developers to make sure that their models are compatible for edge deployment. So certainly I see synergies around that space. Yeah, and then is the end to end stage, which is term based. The model will be around two, 3 billion, which is also runable on edge devices. It's overall, still a transformer model and not super. I mean, there's no huge difference between it and llama. The only difference is like the decoder part, like we decode a continuous latent, or at least four to eight tokens, parallely, yeah, yeah, two, two. Because say, text, you can have one word in one token, yeah. But audio is much harder, because you have a timbre, you have your current Tom, you have your peach, you have different characteristics. You can't include all of them in one token. Audio, we in fish audio, we predict 48 tokens for each friend directly, yeah, for each token for each you said, sorry I missed
27:18it, right? So basically, we have 20 frames per second, yeah, yeah.
it, right? So basically, we have 20 frames per second, yeah, yeah.
it, right? So basically, we have 20 frames per second, yeah, yeah.
it, right? So basically, we have 20 frames per second, yeah, yeah.
S Speaker 127:23Understood. Understood. That's that's interesting and yes, yes, what do you have for me on the deck that remains? I guess this is quite exciting. Yeah, I think what's remaining for us is the I think we, we didn't introduce our team yet, so let me do a brief introduction. Absolutely. Oh, so first, so the first three are tech co founders from open source community, the leftmost me, I build free speech to be fusion and contribute to birds with two. So it SVC and TV saw it. But this is my personal experience, and before that, I begin working on computer vision back to seven years when I was in high school. Yeah, that time I was building computer vision and joined open source, and I built my own computer vision platform that time. And when I go to study UMB in major in CS and math, this platform occurs my spans. It gives me like 300k us, dollar per year revenue. That's what I got previously and during my study, I go to meta to do computer vision intern, but it's covid time, so nobody gets them to offer their year.
Understood. Understood. That's that's interesting and yes, yes, what do you have for me on the deck that remains? I guess this is quite exciting. Yeah, I think what's remaining for us is the I think we, we didn't introduce our team yet, so let me do a brief introduction. Absolutely. Oh, so first, so the first three are tech co founders from open source community, the leftmost me, I build free speech to be fusion and contribute to birds with two. So it SVC and TV saw it. But this is my personal experience, and before that, I begin working on computer vision back to seven years when I was in high school. Yeah, that time I was building computer vision and joined open source, and I built my own computer vision platform that time. And when I go to study UMB in major in CS and math, this platform occurs my spans. It gives me like 300k us, dollar per year revenue. That's what I got previously and during my study, I go to meta to do computer vision intern, but it's covid time, so nobody gets them to offer their year.
Understood. Understood. That's that's interesting and yes, yes, what do you have for me on the deck that remains? I guess this is quite exciting. Yeah, I think what's remaining for us is the I think we, we didn't introduce our team yet, so let me do a brief introduction. Absolutely. Oh, so first, so the first three are tech co founders from open source community, the leftmost me, I build free speech to be fusion and contribute to birds with two. So it SVC and TV saw it. But this is my personal experience, and before that, I begin working on computer vision back to seven years when I was in high school. Yeah, that time I was building computer vision and joined open source, and I built my own computer vision platform that time. And when I go to study UMB in major in CS and math, this platform occurs my spans. It gives me like 300k us, dollar per year revenue. That's what I got previously and during my study, I go to meta to do computer vision intern, but it's covid time, so nobody gets them to offer their year.
Understood. Understood. That's that's interesting and yes, yes, what do you have for me on the deck that remains? I guess this is quite exciting. Yeah, I think what's remaining for us is the I think we, we didn't introduce our team yet, so let me do a brief introduction. Absolutely. Oh, so first, so the first three are tech co founders from open source community, the leftmost me, I build free speech to be fusion and contribute to birds with two. So it SVC and TV saw it. But this is my personal experience, and before that, I begin working on computer vision back to seven years when I was in high school. Yeah, that time I was building computer vision and joined open source, and I built my own computer vision platform that time. And when I go to study UMB in major in CS and math, this platform occurs my spans. It gives me like 300k us, dollar per year revenue. That's what I got previously and during my study, I go to meta to do computer vision intern, but it's covid time, so nobody gets them to offer their year.
S Speaker 128:47I go to Nvidia, and also CMU offer. And after working in Nvidia, I mean working on three months on the I build the MV dynamic to lead the data, both the data and the model of the MV dynamic two. And after I it's hiring freeze that I got a return offer. Then after return, I am working on a weekly also delivered by me, currently wider use internally, and also contribute to Lita and the VR development. So basically I was working on visual language model and visual foundation model. At that time. Got it, yeah. But I begin, like, joined participating in the open source audio community, like around two years ago, and the meet, yeah, start us and ask at that time. So start us is another technical component from us. He is the current core maintainer of birth with two and three speech. He's mostly focusing on our experiment and data side. So basically, he's questioning a more detailed data recipe and tuning do a lot of hyper parameter searching to make sure we can deliver a better model, right? Yeah. RCL is our I would say research lead, yeah, yeah. He is the creator of service as we see, which get him. The service as we see, is shown as top 200 AI app, I think it's even a top 100 in time.
I go to Nvidia, and also CMU offer. And after working in Nvidia, I mean working on three months on the I build the MV dynamic to lead the data, both the data and the model of the MV dynamic two. And after I it's hiring freeze that I got a return offer. Then after return, I am working on a weekly also delivered by me, currently wider use internally, and also contribute to Lita and the VR development. So basically I was working on visual language model and visual foundation model. At that time. Got it, yeah. But I begin, like, joined participating in the open source audio community, like around two years ago, and the meet, yeah, start us and ask at that time. So start us is another technical component from us. He is the current core maintainer of birth with two and three speech. He's mostly focusing on our experiment and data side. So basically, he's questioning a more detailed data recipe and tuning do a lot of hyper parameter searching to make sure we can deliver a better model, right? Yeah. RCL is our I would say research lead, yeah, yeah. He is the creator of service as we see, which get him. The service as we see, is shown as top 200 AI app, I think it's even a top 100 in time.
I go to Nvidia, and also CMU offer. And after working in Nvidia, I mean working on three months on the I build the MV dynamic to lead the data, both the data and the model of the MV dynamic two. And after I it's hiring freeze that I got a return offer. Then after return, I am working on a weekly also delivered by me, currently wider use internally, and also contribute to Lita and the VR development. So basically I was working on visual language model and visual foundation model. At that time. Got it, yeah. But I begin, like, joined participating in the open source audio community, like around two years ago, and the meet, yeah, start us and ask at that time. So start us is another technical component from us. He is the current core maintainer of birth with two and three speech. He's mostly focusing on our experiment and data side. So basically, he's questioning a more detailed data recipe and tuning do a lot of hyper parameter searching to make sure we can deliver a better model, right? Yeah. RCL is our I would say research lead, yeah, yeah. He is the creator of service as we see, which get him. The service as we see, is shown as top 200 AI app, I think it's even a top 100 in time.
I go to Nvidia, and also CMU offer. And after working in Nvidia, I mean working on three months on the I build the MV dynamic to lead the data, both the data and the model of the MV dynamic two. And after I it's hiring freeze that I got a return offer. Then after return, I am working on a weekly also delivered by me, currently wider use internally, and also contribute to Lita and the VR development. So basically I was working on visual language model and visual foundation model. At that time. Got it, yeah. But I begin, like, joined participating in the open source audio community, like around two years ago, and the meet, yeah, start us and ask at that time. So start us is another technical component from us. He is the current core maintainer of birth with two and three speech. He's mostly focusing on our experiment and data side. So basically, he's questioning a more detailed data recipe and tuning do a lot of hyper parameter searching to make sure we can deliver a better model, right? Yeah. RCL is our I would say research lead, yeah, yeah. He is the creator of service as we see, which get him. The service as we see, is shown as top 200 AI app, I think it's even a top 100 in time.
S Speaker 130:25I think it's around 250 k star, 25k stars. And he also built versus two and GP Soviets. Which one is the versus two is the AK, seven, 8k star project and another stupid service initially built im, which is another 30 KST project. Yeah, he's very passionate in independent voice research and Yeah, and he's currently both stardust. Stardust is working in Alibaba audio group, audio algorithm RCL is currently working in another China, large language models, companies, audio side. Yeah, right. And is the entire team based out of us right now? Or are they distributed? We are distributed, yeah. So currently me and the right most which we use, we also see us in the last I last session or Chinese name we are. We are us. Way on is our focusing on our VP at BD and the financial He has lots of experience in as and conductor and AI music. Yeah.
I think it's around 250 k star, 25k stars. And he also built versus two and GP Soviets. Which one is the versus two is the AK, seven, 8k star project and another stupid service initially built im, which is another 30 KST project. Yeah, he's very passionate in independent voice research and Yeah, and he's currently both stardust. Stardust is working in Alibaba audio group, audio algorithm RCL is currently working in another China, large language models, companies, audio side. Yeah, right. And is the entire team based out of us right now? Or are they distributed? We are distributed, yeah. So currently me and the right most which we use, we also see us in the last I last session or Chinese name we are. We are us. Way on is our focusing on our VP at BD and the financial He has lots of experience in as and conductor and AI music. Yeah.
I think it's around 250 k star, 25k stars. And he also built versus two and GP Soviets. Which one is the versus two is the AK, seven, 8k star project and another stupid service initially built im, which is another 30 KST project. Yeah, he's very passionate in independent voice research and Yeah, and he's currently both stardust. Stardust is working in Alibaba audio group, audio algorithm RCL is currently working in another China, large language models, companies, audio side. Yeah, right. And is the entire team based out of us right now? Or are they distributed? We are distributed, yeah. So currently me and the right most which we use, we also see us in the last I last session or Chinese name we are. We are us. Way on is our focusing on our VP at BD and the financial He has lots of experience in as and conductor and AI music. Yeah.
I think it's around 250 k star, 25k stars. And he also built versus two and GP Soviets. Which one is the versus two is the AK, seven, 8k star project and another stupid service initially built im, which is another 30 KST project. Yeah, he's very passionate in independent voice research and Yeah, and he's currently both stardust. Stardust is working in Alibaba audio group, audio algorithm RCL is currently working in another China, large language models, companies, audio side. Yeah, right. And is the entire team based out of us right now? Or are they distributed? We are distributed, yeah. So currently me and the right most which we use, we also see us in the last I last session or Chinese name we are. We are us. Way on is our focusing on our VP at BD and the financial He has lots of experience in as and conductor and AI music. Yeah.
S Speaker 131:40graduated from Qinghai University around 10 years ago. Yeah, 10 years ago. And go to Chicago for master it statistic, yeah, after that, yeah, got it. Got it. That's exciting. Shijia, and can you tell me a little bit more about the round that you're raising right now?
graduated from Qinghai University around 10 years ago. Yeah, 10 years ago. And go to Chicago for master it statistic, yeah, after that, yeah, got it. Got it. That's exciting. Shijia, and can you tell me a little bit more about the round that you're raising right now?
graduated from Qinghai University around 10 years ago. Yeah, 10 years ago. And go to Chicago for master it statistic, yeah, after that, yeah, got it. Got it. That's exciting. Shijia, and can you tell me a little bit more about the round that you're raising right now?
graduated from Qinghai University around 10 years ago. Yeah, 10 years ago. And go to Chicago for master it statistic, yeah, after that, yeah, got it. Got it. That's exciting. Shijia, and can you tell me a little bit more about the round that you're raising right now?
S Speaker 134:20okay, sounds good. Then I'll stay in touch. I'll come back sometime next week, and looking forward to our next chat. Yeah, looking forward to that. Thanks a lot for your time, man. I appreciate it. That's, that's some really good projects that you guys have built really exciting. Thanks for TAP. Absolutely.
okay, sounds good. Then I'll stay in touch. I'll come back sometime next week, and looking forward to our next chat. Yeah, looking forward to that. Thanks a lot for your time, man. I appreciate it. That's, that's some really good projects that you guys have built really exciting. Thanks for TAP. Absolutely.
okay, sounds good. Then I'll stay in touch. I'll come back sometime next week, and looking forward to our next chat. Yeah, looking forward to that. Thanks a lot for your time, man. I appreciate it. That's, that's some really good projects that you guys have built really exciting. Thanks for TAP. Absolutely.
okay, sounds good. Then I'll stay in touch. I'll come back sometime next week, and looking forward to our next chat. Yeah, looking forward to that. Thanks a lot for your time, man. I appreciate it. That's, that's some really good projects that you guys have built really exciting. Thanks for TAP. Absolutely.