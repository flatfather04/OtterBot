Meeting: Adaptive ML + Tushar
Thu, May 1
12:00 PM
51 min
Priyesh P
Introduction and Backgrounds
0:28
Overview
URL: https://otter.ai/u/RXpNYf32Mi2ecX9xFLC-u7-GXuw
Downloaded: 2025-12-22T11:31:20.912452
Method: text_extraction
============================================================

0:28They keep joining you.
They keep joining you.
They keep joining you.
They keep joining you.
0:59Hello, guys, sorry. Bye. switching to
Hello, guys, sorry. Bye. switching to
Hello, guys, sorry. Bye. switching to
Hello, guys, sorry. Bye. switching to
1:09next to zoom to teams.
next to zoom to teams.
next to zoom to teams.
next to zoom to teams.
S Speaker 11:13What's that? Are tools. What's that you've
What's that? Are tools. What's that you've
What's that? Are tools. What's that you've
What's that? Are tools. What's that you've
S Speaker 21:19got all the video conferencing tools you have to decide
got all the video conferencing tools you have to decide
got all the video conferencing tools you have to decide
got all the video conferencing tools you have to decide
S Speaker 11:24exactly, you know, the good news is there's no more. What is that Cisco tool? And then for a short duration, there was also that Amazon chime. I don't know if you ever got on an Amazon chime is still
exactly, you know, the good news is there's no more. What is that Cisco tool? And then for a short duration, there was also that Amazon chime. I don't know if you ever got on an Amazon chime is still
exactly, you know, the good news is there's no more. What is that Cisco tool? And then for a short duration, there was also that Amazon chime. I don't know if you ever got on an Amazon chime is still
exactly, you know, the good news is there's no more. What is that Cisco tool? And then for a short duration, there was also that Amazon chime. I don't know if you ever got on an Amazon chime is still
S Speaker 21:41kicking around. Unfortunately, there's
kicking around. Unfortunately, there's
kicking around. Unfortunately, there's
kicking around. Unfortunately, there's
1:48also things. Yes, we actually
also things. Yes, we actually
also things. Yes, we actually
also things. Yes, we actually
S Speaker 31:52have a relationship with AWS, as
have a relationship with AWS, as
have a relationship with AWS, as
have a relationship with AWS, as
S Speaker 21:57you might imagine, they let us know the good news a couple
you might imagine, they let us know the good news a couple
you might imagine, they let us know the good news a couple
you might imagine, they let us know the good news a couple
2:01of weeks ago, Recently,
of weeks ago, Recently,
of weeks ago, Recently,
of weeks ago, Recently,
S Speaker 12:05yeah? So Amazon meeting needs to be Yeah.
yeah? So Amazon meeting needs to be Yeah.
yeah? So Amazon meeting needs to be Yeah.
yeah? So Amazon meeting needs to be Yeah.
S Speaker 12:18So hey, thanks, guys. So we do I get a quick introduction so on our side, and then you met Priyesh already. And so my background is in engineering, product, computer vision. Background, actually, and then, and then, I currently run north america investments team at Qualcomm. And so Qualcomm Ventures is strategic investment arm for Qualcomm. We've been around for a long, long time. We've been around for 25 years, the ventures team, 25 years, and then so the broad thesis of investment for us is that we invest in areas that are strategically relevant. And it's really broadly speaking, is technologies that push the boundaries of compute and connectivity. So technology, the biggest technology that's pushing the boundary of connectivity and compute today is AI, so it's a big area of investment for us. We've been investing in like, several AI info companies like we've done many investments across companies like hugging face and anthropic and scale and bits and biases, Occam, these ml ops infra companies, and then, More recently, we've been doing many application companies as well. The idea is, like Qualcomm, we ship, actually close to 2 billion ships every year into platforms such as mobile and PCs, and even we have a data center business now and automotive and IoT and the likes. And then for each one of these, we've been shipping AI compute in each one of these. So our vision of the world is that we're going to be living in a hybrid AI compute world, where you have models running in the cloud, and you have models running on the edge, and then you pick and choose between your orchestra, between those two as and when the need arises, because there's, I mean the same constraints of latency, cost, speed and power. Those constraints determine where you and accuracy, of course, determine where you want to run the model and a and eventually, I think there will be a good trade off of use cases where you actually want to run model closer to the user. So that's the lens that we look at. And then we work with many startups. Maybe we take them to market. We work with them on go to market partnerships across different business units, and in some cases we also become their customers. So that's the like big lens that we use for investing. We invest about one $50 million every year, so and about 80 of those are in the US, and then the rest is we have bigger investment, six regions. So
So hey, thanks, guys. So we do I get a quick introduction so on our side, and then you met Priyesh already. And so my background is in engineering, product, computer vision. Background, actually, and then, and then, I currently run north america investments team at Qualcomm. And so Qualcomm Ventures is strategic investment arm for Qualcomm. We've been around for a long, long time. We've been around for 25 years, the ventures team, 25 years, and then so the broad thesis of investment for us is that we invest in areas that are strategically relevant. And it's really broadly speaking, is technologies that push the boundaries of compute and connectivity. So technology, the biggest technology that's pushing the boundary of connectivity and compute today is AI, so it's a big area of investment for us. We've been investing in like, several AI info companies like we've done many investments across companies like hugging face and anthropic and scale and bits and biases, Occam, these ml ops infra companies, and then, More recently, we've been doing many application companies as well. The idea is, like Qualcomm, we ship, actually close to 2 billion ships every year into platforms such as mobile and PCs, and even we have a data center business now and automotive and IoT and the likes. And then for each one of these, we've been shipping AI compute in each one of these. So our vision of the world is that we're going to be living in a hybrid AI compute world, where you have models running in the cloud, and you have models running on the edge, and then you pick and choose between your orchestra, between those two as and when the need arises, because there's, I mean the same constraints of latency, cost, speed and power. Those constraints determine where you and accuracy, of course, determine where you want to run the model and a and eventually, I think there will be a good trade off of use cases where you actually want to run model closer to the user. So that's the lens that we look at. And then we work with many startups. Maybe we take them to market. We work with them on go to market partnerships across different business units, and in some cases we also become their customers. So that's the like big lens that we use for investing. We invest about one $50 million every year, so and about 80 of those are in the US, and then the rest is we have bigger investment, six regions. So
So hey, thanks, guys. So we do I get a quick introduction so on our side, and then you met Priyesh already. And so my background is in engineering, product, computer vision. Background, actually, and then, and then, I currently run north america investments team at Qualcomm. And so Qualcomm Ventures is strategic investment arm for Qualcomm. We've been around for a long, long time. We've been around for 25 years, the ventures team, 25 years, and then so the broad thesis of investment for us is that we invest in areas that are strategically relevant. And it's really broadly speaking, is technologies that push the boundaries of compute and connectivity. So technology, the biggest technology that's pushing the boundary of connectivity and compute today is AI, so it's a big area of investment for us. We've been investing in like, several AI info companies like we've done many investments across companies like hugging face and anthropic and scale and bits and biases, Occam, these ml ops infra companies, and then, More recently, we've been doing many application companies as well. The idea is, like Qualcomm, we ship, actually close to 2 billion ships every year into platforms such as mobile and PCs, and even we have a data center business now and automotive and IoT and the likes. And then for each one of these, we've been shipping AI compute in each one of these. So our vision of the world is that we're going to be living in a hybrid AI compute world, where you have models running in the cloud, and you have models running on the edge, and then you pick and choose between your orchestra, between those two as and when the need arises, because there's, I mean the same constraints of latency, cost, speed and power. Those constraints determine where you and accuracy, of course, determine where you want to run the model and a and eventually, I think there will be a good trade off of use cases where you actually want to run model closer to the user. So that's the lens that we look at. And then we work with many startups. Maybe we take them to market. We work with them on go to market partnerships across different business units, and in some cases we also become their customers. So that's the like big lens that we use for investing. We invest about one $50 million every year, so and about 80 of those are in the US, and then the rest is we have bigger investment, six regions. So
So hey, thanks, guys. So we do I get a quick introduction so on our side, and then you met Priyesh already. And so my background is in engineering, product, computer vision. Background, actually, and then, and then, I currently run north america investments team at Qualcomm. And so Qualcomm Ventures is strategic investment arm for Qualcomm. We've been around for a long, long time. We've been around for 25 years, the ventures team, 25 years, and then so the broad thesis of investment for us is that we invest in areas that are strategically relevant. And it's really broadly speaking, is technologies that push the boundaries of compute and connectivity. So technology, the biggest technology that's pushing the boundary of connectivity and compute today is AI, so it's a big area of investment for us. We've been investing in like, several AI info companies like we've done many investments across companies like hugging face and anthropic and scale and bits and biases, Occam, these ml ops infra companies, and then, More recently, we've been doing many application companies as well. The idea is, like Qualcomm, we ship, actually close to 2 billion ships every year into platforms such as mobile and PCs, and even we have a data center business now and automotive and IoT and the likes. And then for each one of these, we've been shipping AI compute in each one of these. So our vision of the world is that we're going to be living in a hybrid AI compute world, where you have models running in the cloud, and you have models running on the edge, and then you pick and choose between your orchestra, between those two as and when the need arises, because there's, I mean the same constraints of latency, cost, speed and power. Those constraints determine where you and accuracy, of course, determine where you want to run the model and a and eventually, I think there will be a good trade off of use cases where you actually want to run model closer to the user. So that's the lens that we look at. And then we work with many startups. Maybe we take them to market. We work with them on go to market partnerships across different business units, and in some cases we also become their customers. So that's the like big lens that we use for investing. We invest about one $50 million every year, so and about 80 of those are in the US, and then the rest is we have bigger investment, six regions. So
5:47that's the 30,000
S Speaker 15:52brief on on us. And happy to answer any questions. Yeah.
brief on on us. And happy to answer any questions. Yeah.
brief on on us. And happy to answer any questions. Yeah.
brief on on us. And happy to answer any questions. Yeah.
S Speaker 25:59That's a good overview. Thank you. Maybe we can just do quick intros on our side, and then we can agree exactly what we want to talk about. I have my preferences, but I won't force them upon you necessarily. Julian joins a quick intro on your side.
That's a good overview. Thank you. Maybe we can just do quick intros on our side, and then we can agree exactly what we want to talk about. I have my preferences, but I won't force them upon you necessarily. Julian joins a quick intro on your side.
That's a good overview. Thank you. Maybe we can just do quick intros on our side, and then we can agree exactly what we want to talk about. I have my preferences, but I won't force them upon you necessarily. Julian joins a quick intro on your side.
That's a good overview. Thank you. Maybe we can just do quick intros on our side, and then we can agree exactly what we want to talk about. I have my preferences, but I won't force them upon you necessarily. Julian joins a quick intro on your side.
9:31Thanks for the overview. Julian, so this is
Thanks for the overview. Julian, so this is
Thanks for the overview. Julian, so this is
Thanks for the overview. Julian, so this is
S Speaker 19:36so and then. So, how do you see in a world of like MCP and function calling, where every company was trying to build agent tech workflows they're using, you know, some combination of MCP servers and function calls to be able to dynamically figure out, okay, what tool they want to use. Like, and there's this, like, I understand there's a different
so and then. So, how do you see in a world of like MCP and function calling, where every company was trying to build agent tech workflows they're using, you know, some combination of MCP servers and function calls to be able to dynamically figure out, okay, what tool they want to use. Like, and there's this, like, I understand there's a different
so and then. So, how do you see in a world of like MCP and function calling, where every company was trying to build agent tech workflows they're using, you know, some combination of MCP servers and function calls to be able to dynamically figure out, okay, what tool they want to use. Like, and there's this, like, I understand there's a different
so and then. So, how do you see in a world of like MCP and function calling, where every company was trying to build agent tech workflows they're using, you know, some combination of MCP servers and function calls to be able to dynamically figure out, okay, what tool they want to use. Like, and there's this, like, I understand there's a different
S Speaker 111:31so right now you essentially, do you already have a beta product, or, like, a product,
so right now you essentially, do you already have a beta product, or, like, a product,
so right now you essentially, do you already have a beta product, or, like, a product,
so right now you essentially, do you already have a beta product, or, like, a product,
S Speaker 311:38yeah? Totally, yeah. We have a, we have a product that runs well, that our customers are using, you know, at 18, for instance.
yeah? Totally, yeah. We have a, we have a product that runs well, that our customers are using, you know, at 18, for instance.
yeah? Totally, yeah. We have a, we have a product that runs well, that our customers are using, you know, at 18, for instance.
yeah? Totally, yeah. We have a, we have a product that runs well, that our customers are using, you know, at 18, for instance.
S Speaker 111:46So it's it for Scott, you said 18, did you say
So it's it for Scott, you said 18, did you say
So it's it for Scott, you said 18, did you say
So it's it for Scott, you said 18, did you say
S Speaker 311:54so, yeah, the data science team at atnt is one of our customers. They use our the user product, the applications you know that they use it. It's quite broad, because the data sensitive does a lot of stuff, sorry, as you might expect, but the big ones, sorry, just give us a
so, yeah, the data science team at atnt is one of our customers. They use our the user product, the applications you know that they use it. It's quite broad, because the data sensitive does a lot of stuff, sorry, as you might expect, but the big ones, sorry, just give us a
so, yeah, the data science team at atnt is one of our customers. They use our the user product, the applications you know that they use it. It's quite broad, because the data sensitive does a lot of stuff, sorry, as you might expect, but the big ones, sorry, just give us a
so, yeah, the data science team at atnt is one of our customers. They use our the user product, the applications you know that they use it. It's quite broad, because the data sensitive does a lot of stuff, sorry, as you might expect, but the big ones, sorry, just give us a
S Speaker 212:10yeah, sorry, Julian, we've taken our flu that I brought to our company off site.
yeah, sorry, Julian, we've taken our flu that I brought to our company off site.
yeah, sorry, Julian, we've taken our flu that I brought to our company off site.
yeah, sorry, Julian, we've taken our flu that I brought to our company off site.
12:17Half of our company
S Speaker 312:19has been a difficult past week, I think, for everyone. But yes, what is the kind of use case that they right now? You know, for instance, they are exploring stuff like text to SQL was there's a lot of benefit from reinforcement learning in terms of execution feedback and in terms of building more complex pipelines. They do also a lot on like the simpler stuff. They do a lot on call summarization that for kind of as like, customer support, information extraction, that sort of stuff. A lot of a lot of things on that. Yeah, and it's kind of continuously expanding, you know, as as we get more and more traction with their teams, like we are running another boot camp, you know, in a few weeks in in New York, you know, with to onboard some of their new data scientists. So it's kind of like taking a life of its own, which is very great to see.
has been a difficult past week, I think, for everyone. But yes, what is the kind of use case that they right now? You know, for instance, they are exploring stuff like text to SQL was there's a lot of benefit from reinforcement learning in terms of execution feedback and in terms of building more complex pipelines. They do also a lot on like the simpler stuff. They do a lot on call summarization that for kind of as like, customer support, information extraction, that sort of stuff. A lot of a lot of things on that. Yeah, and it's kind of continuously expanding, you know, as as we get more and more traction with their teams, like we are running another boot camp, you know, in a few weeks in in New York, you know, with to onboard some of their new data scientists. So it's kind of like taking a life of its own, which is very great to see.
has been a difficult past week, I think, for everyone. But yes, what is the kind of use case that they right now? You know, for instance, they are exploring stuff like text to SQL was there's a lot of benefit from reinforcement learning in terms of execution feedback and in terms of building more complex pipelines. They do also a lot on like the simpler stuff. They do a lot on call summarization that for kind of as like, customer support, information extraction, that sort of stuff. A lot of a lot of things on that. Yeah, and it's kind of continuously expanding, you know, as as we get more and more traction with their teams, like we are running another boot camp, you know, in a few weeks in in New York, you know, with to onboard some of their new data scientists. So it's kind of like taking a life of its own, which is very great to see.
has been a difficult past week, I think, for everyone. But yes, what is the kind of use case that they right now? You know, for instance, they are exploring stuff like text to SQL was there's a lot of benefit from reinforcement learning in terms of execution feedback and in terms of building more complex pipelines. They do also a lot on like the simpler stuff. They do a lot on call summarization that for kind of as like, customer support, information extraction, that sort of stuff. A lot of a lot of things on that. Yeah, and it's kind of continuously expanding, you know, as as we get more and more traction with their teams, like we are running another boot camp, you know, in a few weeks in in New York, you know, with to onboard some of their new data scientists. So it's kind of like taking a life of its own, which is very great to see.
S Speaker 213:05Yeah, I would just add on to that. So like, at&t is a special case, because they're very hands on the platform, and they're exploring lots of different things, but across all of the people that we engage with, I think you can, like, broadly, put the use cases into like, three big buckets, two of which are the ones that are most popular by far. So the first one would be like some form of rag with enterprise search. So I just like document management, knowledge management, being able to pass that and search it using llms. So that's the first big group, and then the second use case, or a group of use cases, which is probably equivalent in terms of size, is some form of customer support agent. Like, it could be like a fully autonomous agent. It could be agent assist. Could be like automated email response, like something in that group. Those are the two big areas where we see people really wanting to find two models, whether that's for like, better performance or reduced cost, lower latency, depends. And then the third bucket, which is slightly less common and in some cases a little linked by search, would be structured data analysis, which is typically like a text to code, like a text to see, like, those are the three big areas. There's sometimes, like, a little bit of like a gray area in between where, like, I think you have use cases yourself, which are like customer facing, but a more like enterprise search, where you're giving your customers the ability to search through technical documentation, so that's definitely something that
Yeah, I would just add on to that. So like, at&t is a special case, because they're very hands on the platform, and they're exploring lots of different things, but across all of the people that we engage with, I think you can, like, broadly, put the use cases into like, three big buckets, two of which are the ones that are most popular by far. So the first one would be like some form of rag with enterprise search. So I just like document management, knowledge management, being able to pass that and search it using llms. So that's the first big group, and then the second use case, or a group of use cases, which is probably equivalent in terms of size, is some form of customer support agent. Like, it could be like a fully autonomous agent. It could be agent assist. Could be like automated email response, like something in that group. Those are the two big areas where we see people really wanting to find two models, whether that's for like, better performance or reduced cost, lower latency, depends. And then the third bucket, which is slightly less common and in some cases a little linked by search, would be structured data analysis, which is typically like a text to code, like a text to see, like, those are the three big areas. There's sometimes, like, a little bit of like a gray area in between where, like, I think you have use cases yourself, which are like customer facing, but a more like enterprise search, where you're giving your customers the ability to search through technical documentation, so that's definitely something that
Yeah, I would just add on to that. So like, at&t is a special case, because they're very hands on the platform, and they're exploring lots of different things, but across all of the people that we engage with, I think you can, like, broadly, put the use cases into like, three big buckets, two of which are the ones that are most popular by far. So the first one would be like some form of rag with enterprise search. So I just like document management, knowledge management, being able to pass that and search it using llms. So that's the first big group, and then the second use case, or a group of use cases, which is probably equivalent in terms of size, is some form of customer support agent. Like, it could be like a fully autonomous agent. It could be agent assist. Could be like automated email response, like something in that group. Those are the two big areas where we see people really wanting to find two models, whether that's for like, better performance or reduced cost, lower latency, depends. And then the third bucket, which is slightly less common and in some cases a little linked by search, would be structured data analysis, which is typically like a text to code, like a text to see, like, those are the three big areas. There's sometimes, like, a little bit of like a gray area in between where, like, I think you have use cases yourself, which are like customer facing, but a more like enterprise search, where you're giving your customers the ability to search through technical documentation, so that's definitely something that
Yeah, I would just add on to that. So like, at&t is a special case, because they're very hands on the platform, and they're exploring lots of different things, but across all of the people that we engage with, I think you can, like, broadly, put the use cases into like, three big buckets, two of which are the ones that are most popular by far. So the first one would be like some form of rag with enterprise search. So I just like document management, knowledge management, being able to pass that and search it using llms. So that's the first big group, and then the second use case, or a group of use cases, which is probably equivalent in terms of size, is some form of customer support agent. Like, it could be like a fully autonomous agent. It could be agent assist. Could be like automated email response, like something in that group. Those are the two big areas where we see people really wanting to find two models, whether that's for like, better performance or reduced cost, lower latency, depends. And then the third bucket, which is slightly less common and in some cases a little linked by search, would be structured data analysis, which is typically like a text to code, like a text to see, like, those are the three big areas. There's sometimes, like, a little bit of like a gray area in between where, like, I think you have use cases yourself, which are like customer facing, but a more like enterprise search, where you're giving your customers the ability to search through technical documentation, so that's definitely something that
15:02What do you mean by this, figuring
What do you mean by this, figuring
What do you mean by this, figuring
What do you mean by this, figuring
S Speaker 115:04out which tool to use, which is mostly like enterprise search, for example, right like, right now, like, what do you are trying to figure out? Okay, what is the appropriate tool to call?
out which tool to use, which is mostly like enterprise search, for example, right like, right now, like, what do you are trying to figure out? Okay, what is the appropriate tool to call?
out which tool to use, which is mostly like enterprise search, for example, right like, right now, like, what do you are trying to figure out? Okay, what is the appropriate tool to call?
out which tool to use, which is mostly like enterprise search, for example, right like, right now, like, what do you are trying to figure out? Okay, what is the appropriate tool to call?
S Speaker 117:42that then I think that makes sense. So how do you like typically, when you approach customers right, like you like enterprise search. For example, if you're going to put enterprise search right, you've got glee? Who is the you know? I mean, I get it. It's what green is offering is fundamentally a different product. But Green's known for enterprise search, right? Like they've got 1400 logos. Are you I? Do you go up against glean? Or do you sip? We can be a companion,
that then I think that makes sense. So how do you like typically, when you approach customers right, like you like enterprise search. For example, if you're going to put enterprise search right, you've got glee? Who is the you know? I mean, I get it. It's what green is offering is fundamentally a different product. But Green's known for enterprise search, right? Like they've got 1400 logos. Are you I? Do you go up against glean? Or do you sip? We can be a companion,
that then I think that makes sense. So how do you like typically, when you approach customers right, like you like enterprise search. For example, if you're going to put enterprise search right, you've got glee? Who is the you know? I mean, I get it. It's what green is offering is fundamentally a different product. But Green's known for enterprise search, right? Like they've got 1400 logos. Are you I? Do you go up against glean? Or do you sip? We can be a companion,
that then I think that makes sense. So how do you like typically, when you approach customers right, like you like enterprise search. For example, if you're going to put enterprise search right, you've got glee? Who is the you know? I mean, I get it. It's what green is offering is fundamentally a different product. But Green's known for enterprise search, right? Like they've got 1400 logos. Are you I? Do you go up against glean? Or do you sip? We can be a companion,
S Speaker 318:20not really. So we are more like we sell tools. So glean could be a customer. Like, for instance, you could imagine that glean could be a customer for like, improving their systems to companies we will deal with, with regard, companies that don't want to use, a company that want to roll out their own things. A good example of this is at&t. At&t is very opinionated about this sort of tools, like green or like others, you know, for customer support, and they much prefer to build internally for, you know, a myriad of philosophical reasons. And so for them, you know, essentially we are tool for their data science team to build this thing. So really, you know, we don't really have a verticalized offering on this. So I wouldn't consider as a competitor. I would consider as more like a tool that, you know, you could use to be something similar to be in that you could use other competitor to be in good use as well. It's like, it's perfectly obvious, yeah, it's a layer beneath, I get it, but then I guess you could use it. So you would sell to, let's say, for example, you would tend to sell to a power company team. And they could be internal, clean themselves, right?
not really. So we are more like we sell tools. So glean could be a customer. Like, for instance, you could imagine that glean could be a customer for like, improving their systems to companies we will deal with, with regard, companies that don't want to use, a company that want to roll out their own things. A good example of this is at&t. At&t is very opinionated about this sort of tools, like green or like others, you know, for customer support, and they much prefer to build internally for, you know, a myriad of philosophical reasons. And so for them, you know, essentially we are tool for their data science team to build this thing. So really, you know, we don't really have a verticalized offering on this. So I wouldn't consider as a competitor. I would consider as more like a tool that, you know, you could use to be something similar to be in that you could use other competitor to be in good use as well. It's like, it's perfectly obvious, yeah, it's a layer beneath, I get it, but then I guess you could use it. So you would sell to, let's say, for example, you would tend to sell to a power company team. And they could be internal, clean themselves, right?
not really. So we are more like we sell tools. So glean could be a customer. Like, for instance, you could imagine that glean could be a customer for like, improving their systems to companies we will deal with, with regard, companies that don't want to use, a company that want to roll out their own things. A good example of this is at&t. At&t is very opinionated about this sort of tools, like green or like others, you know, for customer support, and they much prefer to build internally for, you know, a myriad of philosophical reasons. And so for them, you know, essentially we are tool for their data science team to build this thing. So really, you know, we don't really have a verticalized offering on this. So I wouldn't consider as a competitor. I would consider as more like a tool that, you know, you could use to be something similar to be in that you could use other competitor to be in good use as well. It's like, it's perfectly obvious, yeah, it's a layer beneath, I get it, but then I guess you could use it. So you would sell to, let's say, for example, you would tend to sell to a power company team. And they could be internal, clean themselves, right?
not really. So we are more like we sell tools. So glean could be a customer. Like, for instance, you could imagine that glean could be a customer for like, improving their systems to companies we will deal with, with regard, companies that don't want to use, a company that want to roll out their own things. A good example of this is at&t. At&t is very opinionated about this sort of tools, like green or like others, you know, for customer support, and they much prefer to build internally for, you know, a myriad of philosophical reasons. And so for them, you know, essentially we are tool for their data science team to build this thing. So really, you know, we don't really have a verticalized offering on this. So I wouldn't consider as a competitor. I would consider as more like a tool that, you know, you could use to be something similar to be in that you could use other competitor to be in good use as well. It's like, it's perfectly obvious, yeah, it's a layer beneath, I get it, but then I guess you could use it. So you would sell to, let's say, for example, you would tend to sell to a power company team. And they could be internal, clean themselves, right?
19:19So essentially, you are not building
So essentially, you are not building
So essentially, you are not building
So essentially, you are not building
S Speaker 119:23end to end, like UI, per se, or Enterprise Search you're going to enable, you're enabling other people to go build these things, right? Yeah,
end to end, like UI, per se, or Enterprise Search you're going to enable, you're enabling other people to go build these things, right? Yeah,
end to end, like UI, per se, or Enterprise Search you're going to enable, you're enabling other people to go build these things, right? Yeah,
end to end, like UI, per se, or Enterprise Search you're going to enable, you're enabling other people to go build these things, right? Yeah,
S Speaker 319:33we do, like, we do have a UI for, like, the simple flow, so that people can easily build some of the basics, like, for instance, for our galastation reduction, since this is very often asked, we can just kick through and get it done. But we don't have, like, a UI for document search or
we do, like, we do have a UI for, like, the simple flow, so that people can easily build some of the basics, like, for instance, for our galastation reduction, since this is very often asked, we can just kick through and get it done. But we don't have, like, a UI for document search or
we do, like, we do have a UI for, like, the simple flow, so that people can easily build some of the basics, like, for instance, for our galastation reduction, since this is very often asked, we can just kick through and get it done. But we don't have, like, a UI for document search or
we do, like, we do have a UI for, like, the simple flow, so that people can easily build some of the basics, like, for instance, for our galastation reduction, since this is very often asked, we can just kick through and get it done. But we don't have, like, a UI for document search or
S Speaker 119:47that sort of stuff. How many number of tools currently, like, Is there, like, a threshold of number of pools that you can search through and
that sort of stuff. How many number of tools currently, like, Is there, like, a threshold of number of pools that you can search through and
that sort of stuff. How many number of tools currently, like, Is there, like, a threshold of number of pools that you can search through and
that sort of stuff. How many number of tools currently, like, Is there, like, a threshold of number of pools that you can search through and
S Speaker 220:38Honestly, you mentioned enterprise search, like a couple of times, and this also came up with Priyesh when we spoke previously. I think you've been working with few companies in that particular problem space. Is that your priority, like, is that the thing that Qualcomm is trying to solve for in terms of Ji use cases? Or are also other things that you're working on
Honestly, you mentioned enterprise search, like a couple of times, and this also came up with Priyesh when we spoke previously. I think you've been working with few companies in that particular problem space. Is that your priority, like, is that the thing that Qualcomm is trying to solve for in terms of Ji use cases? Or are also other things that you're working on
Honestly, you mentioned enterprise search, like a couple of times, and this also came up with Priyesh when we spoke previously. I think you've been working with few companies in that particular problem space. Is that your priority, like, is that the thing that Qualcomm is trying to solve for in terms of Ji use cases? Or are also other things that you're working on
Honestly, you mentioned enterprise search, like a couple of times, and this also came up with Priyesh when we spoke previously. I think you've been working with few companies in that particular problem space. Is that your priority, like, is that the thing that Qualcomm is trying to solve for in terms of Ji use cases? Or are also other things that you're working on
S Speaker 223:24it can be. It depends on what your specific use case is and the requirements. And I think the fundamental value proposition is enabling you to have better, faster or cheaper language models for your particular problem, whatever bubbles, if it is a challenge that requires dynamic tool use, then yes, that is something that we can do, and it's also something that Reinforcement learning is quite well suited to, but isn't necessarily always the problem that people have.
it can be. It depends on what your specific use case is and the requirements. And I think the fundamental value proposition is enabling you to have better, faster or cheaper language models for your particular problem, whatever bubbles, if it is a challenge that requires dynamic tool use, then yes, that is something that we can do, and it's also something that Reinforcement learning is quite well suited to, but isn't necessarily always the problem that people have.
it can be. It depends on what your specific use case is and the requirements. And I think the fundamental value proposition is enabling you to have better, faster or cheaper language models for your particular problem, whatever bubbles, if it is a challenge that requires dynamic tool use, then yes, that is something that we can do, and it's also something that Reinforcement learning is quite well suited to, but isn't necessarily always the problem that people have.
it can be. It depends on what your specific use case is and the requirements. And I think the fundamental value proposition is enabling you to have better, faster or cheaper language models for your particular problem, whatever bubbles, if it is a challenge that requires dynamic tool use, then yes, that is something that we can do, and it's also something that Reinforcement learning is quite well suited to, but isn't necessarily always the problem that people have.
27:00your costs for that same problem.
your costs for that same problem.
your costs for that same problem.
your costs for that same problem.
S Speaker 127:04That's a Okay, so and, and what about once it's in deployment? Do you like? How do you manage and then is this done through like so let's say, for example, at&t has deployed something with on prem. Do you have an on prem customer today?
That's a Okay, so and, and what about once it's in deployment? Do you like? How do you manage and then is this done through like so let's say, for example, at&t has deployed something with on prem. Do you have an on prem customer today?
That's a Okay, so and, and what about once it's in deployment? Do you like? How do you manage and then is this done through like so let's say, for example, at&t has deployed something with on prem. Do you have an on prem customer today?
That's a Okay, so and, and what about once it's in deployment? Do you like? How do you manage and then is this done through like so let's say, for example, at&t has deployed something with on prem. Do you have an on prem customer today?
27:24Yes, at&t is on prem.
Yes, at&t is on prem.
Yes, at&t is on prem.
Yes, at&t is on prem.
S Speaker 127:26Okay, on prem. And then, based on your benchmarks as of today, you have said that this is the best model that runs for this particular use case, do you give them an optionality, even then, like, like, in the in a in a world where we have the best model of the day that Do you still give them, continue to give them that option or, like, how does that work? Or say that, okay, you know what? We're good for the next six months?
Okay, on prem. And then, based on your benchmarks as of today, you have said that this is the best model that runs for this particular use case, do you give them an optionality, even then, like, like, in the in a in a world where we have the best model of the day that Do you still give them, continue to give them that option or, like, how does that work? Or say that, okay, you know what? We're good for the next six months?
Okay, on prem. And then, based on your benchmarks as of today, you have said that this is the best model that runs for this particular use case, do you give them an optionality, even then, like, like, in the in a in a world where we have the best model of the day that Do you still give them, continue to give them that option or, like, how does that work? Or say that, okay, you know what? We're good for the next six months?
Okay, on prem. And then, based on your benchmarks as of today, you have said that this is the best model that runs for this particular use case, do you give them an optionality, even then, like, like, in the in a in a world where we have the best model of the day that Do you still give them, continue to give them that option or, like, how does that work? Or say that, okay, you know what? We're good for the next six months?
S Speaker 227:56Yeah, absolutely. So one of the things that we do through adaptive is we actually make it very easy for you to connect to closed model providers, so you can manage anthropic and open AI and even Nvidia NIMS through adaptive engine, so that you can compare the performance. And if there's a new model that's released and is better for your task and you want to switch, you can do that very easily while still managing everything through the platform. So we don't really lock you in. Of course, there are, like, some commercial constraints, because we do have to have some kind of functioning business, but we definitely don't lock you into using a particular model. You can move to other providers, or you could also retrain the existing model that you have and improve
Yeah, absolutely. So one of the things that we do through adaptive is we actually make it very easy for you to connect to closed model providers, so you can manage anthropic and open AI and even Nvidia NIMS through adaptive engine, so that you can compare the performance. And if there's a new model that's released and is better for your task and you want to switch, you can do that very easily while still managing everything through the platform. So we don't really lock you in. Of course, there are, like, some commercial constraints, because we do have to have some kind of functioning business, but we definitely don't lock you into using a particular model. You can move to other providers, or you could also retrain the existing model that you have and improve
Yeah, absolutely. So one of the things that we do through adaptive is we actually make it very easy for you to connect to closed model providers, so you can manage anthropic and open AI and even Nvidia NIMS through adaptive engine, so that you can compare the performance. And if there's a new model that's released and is better for your task and you want to switch, you can do that very easily while still managing everything through the platform. So we don't really lock you in. Of course, there are, like, some commercial constraints, because we do have to have some kind of functioning business, but we definitely don't lock you into using a particular model. You can move to other providers, or you could also retrain the existing model that you have and improve
Yeah, absolutely. So one of the things that we do through adaptive is we actually make it very easy for you to connect to closed model providers, so you can manage anthropic and open AI and even Nvidia NIMS through adaptive engine, so that you can compare the performance. And if there's a new model that's released and is better for your task and you want to switch, you can do that very easily while still managing everything through the platform. So we don't really lock you in. Of course, there are, like, some commercial constraints, because we do have to have some kind of functioning business, but we definitely don't lock you into using a particular model. You can move to other providers, or you could also retrain the existing model that you have and improve
S Speaker 128:38it. Got it? Okay, thanks. It's actually very interesting because so we were going with Qualcomm in general is going very serious with on prem deployments, what we've called it, we've started building AI appliances and everything that we're going to customers. So we've got some large customers, even internationally, in the Middle East, for example, that are interested in this. However, we've got the appliance, but we need the the you guys could fit pretty well from that standpoint on that, and not just that, it's actually even the automotive guys. It's the IoT guys and
it. Got it? Okay, thanks. It's actually very interesting because so we were going with Qualcomm in general is going very serious with on prem deployments, what we've called it, we've started building AI appliances and everything that we're going to customers. So we've got some large customers, even internationally, in the Middle East, for example, that are interested in this. However, we've got the appliance, but we need the the you guys could fit pretty well from that standpoint on that, and not just that, it's actually even the automotive guys. It's the IoT guys and
it. Got it? Okay, thanks. It's actually very interesting because so we were going with Qualcomm in general is going very serious with on prem deployments, what we've called it, we've started building AI appliances and everything that we're going to customers. So we've got some large customers, even internationally, in the Middle East, for example, that are interested in this. However, we've got the appliance, but we need the the you guys could fit pretty well from that standpoint on that, and not just that, it's actually even the automotive guys. It's the IoT guys and
it. Got it? Okay, thanks. It's actually very interesting because so we were going with Qualcomm in general is going very serious with on prem deployments, what we've called it, we've started building AI appliances and everything that we're going to customers. So we've got some large customers, even internationally, in the Middle East, for example, that are interested in this. However, we've got the appliance, but we need the the you guys could fit pretty well from that standpoint on that, and not just that, it's actually even the automotive guys. It's the IoT guys and
29:31so typically, like,
S Speaker 129:35what's the like across so, so do you have, like, a segmentation of the kind of deployments that you've targeted? And then, because I'm the reason I'm asking is, what's the footprint for each one of these, and I'm guessing for on prem, that's also a consideration.
what's the like across so, so do you have, like, a segmentation of the kind of deployments that you've targeted? And then, because I'm the reason I'm asking is, what's the footprint for each one of these, and I'm guessing for on prem, that's also a consideration.
what's the like across so, so do you have, like, a segmentation of the kind of deployments that you've targeted? And then, because I'm the reason I'm asking is, what's the footprint for each one of these, and I'm guessing for on prem, that's also a consideration.
what's the like across so, so do you have, like, a segmentation of the kind of deployments that you've targeted? And then, because I'm the reason I'm asking is, what's the footprint for each one of these, and I'm guessing for on prem, that's also a consideration.
S Speaker 229:54So in terms of, like, the GPU footprint, of course, it like, depends on, like, the few things is like problem you're trying to solve and how difficult that is, which then has like, an impact on, maybe the model size that you would want to use. And then once you're actually in production and you've trained the model, like, what is your throughput? It's a very high throughput use case that, of course, will then change, like, what your GPU footprint needs to be. Typically, what we see is for like, large enterprise customers like yourself, you would start probably with like, a single node of a 100 or h1 100 GPUs, and that's going to give you enough capacity that you can train models of a variety of sizes. So you could train 70 B's, you could train eight B's, and then, depending on your use cases are, we'd need to have, like, a discussion on throughput, I think for internal use cases, honestly, throughput is actually not that significant. So you probably would not need to increase the number of GPUs you have versus what you want just for training. Anyway, if you're starting to look externally and you have a high volume of customers and lots of interactions, then you might need to scale your GPUs, but at that point, the cost of doing that is still going to be significantly less than would be if you were making API calls to, like, anthropic or open AI. So, like, the cost isn't really an issue,
So in terms of, like, the GPU footprint, of course, it like, depends on, like, the few things is like problem you're trying to solve and how difficult that is, which then has like, an impact on, maybe the model size that you would want to use. And then once you're actually in production and you've trained the model, like, what is your throughput? It's a very high throughput use case that, of course, will then change, like, what your GPU footprint needs to be. Typically, what we see is for like, large enterprise customers like yourself, you would start probably with like, a single node of a 100 or h1 100 GPUs, and that's going to give you enough capacity that you can train models of a variety of sizes. So you could train 70 B's, you could train eight B's, and then, depending on your use cases are, we'd need to have, like, a discussion on throughput, I think for internal use cases, honestly, throughput is actually not that significant. So you probably would not need to increase the number of GPUs you have versus what you want just for training. Anyway, if you're starting to look externally and you have a high volume of customers and lots of interactions, then you might need to scale your GPUs, but at that point, the cost of doing that is still going to be significantly less than would be if you were making API calls to, like, anthropic or open AI. So, like, the cost isn't really an issue,
So in terms of, like, the GPU footprint, of course, it like, depends on, like, the few things is like problem you're trying to solve and how difficult that is, which then has like, an impact on, maybe the model size that you would want to use. And then once you're actually in production and you've trained the model, like, what is your throughput? It's a very high throughput use case that, of course, will then change, like, what your GPU footprint needs to be. Typically, what we see is for like, large enterprise customers like yourself, you would start probably with like, a single node of a 100 or h1 100 GPUs, and that's going to give you enough capacity that you can train models of a variety of sizes. So you could train 70 B's, you could train eight B's, and then, depending on your use cases are, we'd need to have, like, a discussion on throughput, I think for internal use cases, honestly, throughput is actually not that significant. So you probably would not need to increase the number of GPUs you have versus what you want just for training. Anyway, if you're starting to look externally and you have a high volume of customers and lots of interactions, then you might need to scale your GPUs, but at that point, the cost of doing that is still going to be significantly less than would be if you were making API calls to, like, anthropic or open AI. So, like, the cost isn't really an issue,
So in terms of, like, the GPU footprint, of course, it like, depends on, like, the few things is like problem you're trying to solve and how difficult that is, which then has like, an impact on, maybe the model size that you would want to use. And then once you're actually in production and you've trained the model, like, what is your throughput? It's a very high throughput use case that, of course, will then change, like, what your GPU footprint needs to be. Typically, what we see is for like, large enterprise customers like yourself, you would start probably with like, a single node of a 100 or h1 100 GPUs, and that's going to give you enough capacity that you can train models of a variety of sizes. So you could train 70 B's, you could train eight B's, and then, depending on your use cases are, we'd need to have, like, a discussion on throughput, I think for internal use cases, honestly, throughput is actually not that significant. So you probably would not need to increase the number of GPUs you have versus what you want just for training. Anyway, if you're starting to look externally and you have a high volume of customers and lots of interactions, then you might need to scale your GPUs, but at that point, the cost of doing that is still going to be significantly less than would be if you were making API calls to, like, anthropic or open AI. So, like, the cost isn't really an issue,
S Speaker 131:24but that cost, like the cloud cost, and I guess is that, like a pass through.
but that cost, like the cloud cost, and I guess is that, like a pass through.
but that cost, like the cloud cost, and I guess is that, like a pass through.
but that cost, like the cloud cost, and I guess is that, like a pass through.
S Speaker 231:32Yeah, so we wouldn't charge you. You would be in control of, like, your own infrastructure. So, like, you would procure your own GPUs, and we wouldn't be involved in that necessarily. I mean, we could, and we do have some customers that are but I would assume Qualcomm you would want to manage yourselves.
Yeah, so we wouldn't charge you. You would be in control of, like, your own infrastructure. So, like, you would procure your own GPUs, and we wouldn't be involved in that necessarily. I mean, we could, and we do have some customers that are but I would assume Qualcomm you would want to manage yourselves.
Yeah, so we wouldn't charge you. You would be in control of, like, your own infrastructure. So, like, you would procure your own GPUs, and we wouldn't be involved in that necessarily. I mean, we could, and we do have some customers that are but I would assume Qualcomm you would want to manage yourselves.
Yeah, so we wouldn't charge you. You would be in control of, like, your own infrastructure. So, like, you would procure your own GPUs, and we wouldn't be involved in that necessarily. I mean, we could, and we do have some customers that are but I would assume Qualcomm you would want to manage yourselves.
S Speaker 131:48And the reinforcement learning part on your end is to actually, it's, it's the orchestrator, if you will, of this entire system. Is that right?
And the reinforcement learning part on your end is to actually, it's, it's the orchestrator, if you will, of this entire system. Is that right?
And the reinforcement learning part on your end is to actually, it's, it's the orchestrator, if you will, of this entire system. Is that right?
And the reinforcement learning part on your end is to actually, it's, it's the orchestrator, if you will, of this entire system. Is that right?
S Speaker 232:01We can use RL to train a model that can act as an orchestrator of different models, or we can also use RL to train a model that is able to dynamically do function calling. Again, this kind of comes down specifically to your individual use case, because another use case that isn't really related to orchestrating could just be like fixed rag, where you have a static workflow, but you're still getting hallucination. And actually, this is another one of the problems that we solved for ADP. They were doing rag Q and A on some highly technical telco documents, and even the frontier models were still hallucinating to the point where the answers were not sufficiently reliable, and so in that case, we used RL just to reduce the hallucination of models on that particular rag q&a task. So it wasn't about function calling, it wasn't about orchestrating.
We can use RL to train a model that can act as an orchestrator of different models, or we can also use RL to train a model that is able to dynamically do function calling. Again, this kind of comes down specifically to your individual use case, because another use case that isn't really related to orchestrating could just be like fixed rag, where you have a static workflow, but you're still getting hallucination. And actually, this is another one of the problems that we solved for ADP. They were doing rag Q and A on some highly technical telco documents, and even the frontier models were still hallucinating to the point where the answers were not sufficiently reliable, and so in that case, we used RL just to reduce the hallucination of models on that particular rag q&a task. So it wasn't about function calling, it wasn't about orchestrating.
We can use RL to train a model that can act as an orchestrator of different models, or we can also use RL to train a model that is able to dynamically do function calling. Again, this kind of comes down specifically to your individual use case, because another use case that isn't really related to orchestrating could just be like fixed rag, where you have a static workflow, but you're still getting hallucination. And actually, this is another one of the problems that we solved for ADP. They were doing rag Q and A on some highly technical telco documents, and even the frontier models were still hallucinating to the point where the answers were not sufficiently reliable, and so in that case, we used RL just to reduce the hallucination of models on that particular rag q&a task. So it wasn't about function calling, it wasn't about orchestrating.
We can use RL to train a model that can act as an orchestrator of different models, or we can also use RL to train a model that is able to dynamically do function calling. Again, this kind of comes down specifically to your individual use case, because another use case that isn't really related to orchestrating could just be like fixed rag, where you have a static workflow, but you're still getting hallucination. And actually, this is another one of the problems that we solved for ADP. They were doing rag Q and A on some highly technical telco documents, and even the frontier models were still hallucinating to the point where the answers were not sufficiently reliable, and so in that case, we used RL just to reduce the hallucination of models on that particular rag q&a task. So it wasn't about function calling, it wasn't about orchestrating.
S Speaker 132:51So Andrew, has that been solved? Like, like you probably. I'm guessing, you already know our partner, that we work with ragspace contextual. It's public knowledge, and then that we do it with a contact, like a customer support use case, customer engineering use case. And when we initially started looking at these types of solutions, so the like at our company, even for that use case, let's say there was a build versus buy decision and and then there were internal teams trying to build rags, call three APIs and build a rag system internally and then see if that works. And then the accuracies with those were pretty bad, and then we use the budget vendors in order to address that. And then the biggest challenge
So Andrew, has that been solved? Like, like you probably. I'm guessing, you already know our partner, that we work with ragspace contextual. It's public knowledge, and then that we do it with a contact, like a customer support use case, customer engineering use case. And when we initially started looking at these types of solutions, so the like at our company, even for that use case, let's say there was a build versus buy decision and and then there were internal teams trying to build rags, call three APIs and build a rag system internally and then see if that works. And then the accuracies with those were pretty bad, and then we use the budget vendors in order to address that. And then the biggest challenge
So Andrew, has that been solved? Like, like you probably. I'm guessing, you already know our partner, that we work with ragspace contextual. It's public knowledge, and then that we do it with a contact, like a customer support use case, customer engineering use case. And when we initially started looking at these types of solutions, so the like at our company, even for that use case, let's say there was a build versus buy decision and and then there were internal teams trying to build rags, call three APIs and build a rag system internally and then see if that works. And then the accuracies with those were pretty bad, and then we use the budget vendors in order to address that. And then the biggest challenge
So Andrew, has that been solved? Like, like you probably. I'm guessing, you already know our partner, that we work with ragspace contextual. It's public knowledge, and then that we do it with a contact, like a customer support use case, customer engineering use case. And when we initially started looking at these types of solutions, so the like at our company, even for that use case, let's say there was a build versus buy decision and and then there were internal teams trying to build rags, call three APIs and build a rag system internally and then see if that works. And then the accuracies with those were pretty bad, and then we use the budget vendors in order to address that. And then the biggest challenge
33:49late last year was hallucination.
late last year was hallucination.
late last year was hallucination.
late last year was hallucination.
S Speaker 133:54And then the accuracy was just poor, but as now, as I look at the results now that come out from like the contextual that hallucination is no longer a big challenge. There are other challenges of, you know, larger context, windows and everything, but hallucination is no longer a challenge like so we have about 100 engineers who test it every day. Test is in use these systems every day. And then now we've exposed these systems to external customers, and then, so that's not a problem that when I checked with the person who was leading that project internally recently, they said hallucination is no longer a problem. We've got other problems. Other problems, we've got other use cases. We've got other problems, but we definitely don't have hallucination as a problem. Which was interesting to me, because I was like, Okay, this like, in a few months now, we're we're definitely not seeing hallucination as a bigger problem. So do you see that as a big problem still, because the bigger problem, I'll tell you, as I said, is the bigger problem is contextual. Yeah,
And then the accuracy was just poor, but as now, as I look at the results now that come out from like the contextual that hallucination is no longer a big challenge. There are other challenges of, you know, larger context, windows and everything, but hallucination is no longer a challenge like so we have about 100 engineers who test it every day. Test is in use these systems every day. And then now we've exposed these systems to external customers, and then, so that's not a problem that when I checked with the person who was leading that project internally recently, they said hallucination is no longer a problem. We've got other problems. Other problems, we've got other use cases. We've got other problems, but we definitely don't have hallucination as a problem. Which was interesting to me, because I was like, Okay, this like, in a few months now, we're we're definitely not seeing hallucination as a bigger problem. So do you see that as a big problem still, because the bigger problem, I'll tell you, as I said, is the bigger problem is contextual. Yeah,
And then the accuracy was just poor, but as now, as I look at the results now that come out from like the contextual that hallucination is no longer a big challenge. There are other challenges of, you know, larger context, windows and everything, but hallucination is no longer a challenge like so we have about 100 engineers who test it every day. Test is in use these systems every day. And then now we've exposed these systems to external customers, and then, so that's not a problem that when I checked with the person who was leading that project internally recently, they said hallucination is no longer a problem. We've got other problems. Other problems, we've got other use cases. We've got other problems, but we definitely don't have hallucination as a problem. Which was interesting to me, because I was like, Okay, this like, in a few months now, we're we're definitely not seeing hallucination as a bigger problem. So do you see that as a big problem still, because the bigger problem, I'll tell you, as I said, is the bigger problem is contextual. Yeah,
And then the accuracy was just poor, but as now, as I look at the results now that come out from like the contextual that hallucination is no longer a big challenge. There are other challenges of, you know, larger context, windows and everything, but hallucination is no longer a challenge like so we have about 100 engineers who test it every day. Test is in use these systems every day. And then now we've exposed these systems to external customers, and then, so that's not a problem that when I checked with the person who was leading that project internally recently, they said hallucination is no longer a problem. We've got other problems. Other problems, we've got other use cases. We've got other problems, but we definitely don't have hallucination as a problem. Which was interesting to me, because I was like, Okay, this like, in a few months now, we're we're definitely not seeing hallucination as a bigger problem. So do you see that as a big problem still, because the bigger problem, I'll tell you, as I said, is the bigger problem is contextual. Yeah,
S Speaker 235:02so hallucination can still sometimes be a problem again, as specific to your use case, and it's also hard to comment on your particular task, because I don't know what contextual was doing, like they may well,
so hallucination can still sometimes be a problem again, as specific to your use case, and it's also hard to comment on your particular task, because I don't know what contextual was doing, like they may well,
so hallucination can still sometimes be a problem again, as specific to your use case, and it's also hard to comment on your particular task, because I don't know what contextual was doing, like they may well,
so hallucination can still sometimes be a problem again, as specific to your use case, and it's also hard to comment on your particular task, because I don't know what contextual was doing, like they may well,
S Speaker 135:16actually, when you do with the docs, it's super easy, it's public, everything. So, but do you can take a look at that so, and it's a similar problem that we would want to work with, like, if you as we take these appliances, so forget Qualcomm as a customer for a second, right? Like Qualcomm and the customers also has other problems within the organization, and we could be interesting, but Qualcomm as a partner, where we take you to customers like every all the other customers that I've heard from our guys is similar set of problems. It's just that input documents are different, or like an inductor company, and they're different for like a handset OEM. They're different for an automotive manufacturer. They're different for, like, just, just
actually, when you do with the docs, it's super easy, it's public, everything. So, but do you can take a look at that so, and it's a similar problem that we would want to work with, like, if you as we take these appliances, so forget Qualcomm as a customer for a second, right? Like Qualcomm and the customers also has other problems within the organization, and we could be interesting, but Qualcomm as a partner, where we take you to customers like every all the other customers that I've heard from our guys is similar set of problems. It's just that input documents are different, or like an inductor company, and they're different for like a handset OEM. They're different for an automotive manufacturer. They're different for, like, just, just
actually, when you do with the docs, it's super easy, it's public, everything. So, but do you can take a look at that so, and it's a similar problem that we would want to work with, like, if you as we take these appliances, so forget Qualcomm as a customer for a second, right? Like Qualcomm and the customers also has other problems within the organization, and we could be interesting, but Qualcomm as a partner, where we take you to customers like every all the other customers that I've heard from our guys is similar set of problems. It's just that input documents are different, or like an inductor company, and they're different for like a handset OEM. They're different for an automotive manufacturer. They're different for, like, just, just
actually, when you do with the docs, it's super easy, it's public, everything. So, but do you can take a look at that so, and it's a similar problem that we would want to work with, like, if you as we take these appliances, so forget Qualcomm as a customer for a second, right? Like Qualcomm and the customers also has other problems within the organization, and we could be interesting, but Qualcomm as a partner, where we take you to customers like every all the other customers that I've heard from our guys is similar set of problems. It's just that input documents are different, or like an inductor company, and they're different for like a handset OEM. They're different for an automotive manufacturer. They're different for, like, just, just
36:09slight differences, but,
slight differences, but,
slight differences, but,
slight differences, but,
S Speaker 136:12but what I guess said is somebody can solve a problem with our dogs, then they can probably live
but what I guess said is somebody can solve a problem with our dogs, then they can probably live
but what I guess said is somebody can solve a problem with our dogs, then they can probably live
but what I guess said is somebody can solve a problem with our dogs, then they can probably live
S Speaker 236:19with anybody. Maybe the way that I rephrase this is hallucination can still be a problem for people that are doing rag Q and A tasks when they're trying to use just a standard off the shelf model. I can't speak to what contextual did. I don't know if they find true models on your documents or not. What we see is that we can solve that problem, pretty much by taking an approach where we're specializing models with RL for that particular set of documents, using RL and synthetic data. So is it a problem anymore? Yes and no. Like I think it is solved in the sense that we can solve for it, and perhaps others are as well. But I think a lot of people are still trying to solve this using off the shelf models, and in that case, I think no one's actually not solved. Like, I was talking to somebody at a large US Bank yesterday, and they were talking about how everyone's been trying to do this for 10 days, for like, five years. But still, he finds that there's, like, hallucinations on tabular data and, like, particular financial values where it's not reliable enough. So, like, a little bit of nuance, and it's two different stories, but this is just like one of the things that we focus on, generally, the better way to think about why you would use adaptive is performance overall hallucination is just like one example of a performance problem, like another one could be the reliability of your LLM in a more multi turn scenario, you know, where, like, you're asking it again and again, and it starts to go off track, and maybe it doesn't quite follow the correct formatting, or, like, after A certain number of times it starts behaving weirdly, like in the example Julian was giving earlier around, like the head to tech that we're working with that's like a situation where it's a very significant context, like people are asking questions, you need to make sure that it's still behaving in a way that is coherent with the beginning of that chat Session. Like these are more the kind of issues that we're seeing.
with anybody. Maybe the way that I rephrase this is hallucination can still be a problem for people that are doing rag Q and A tasks when they're trying to use just a standard off the shelf model. I can't speak to what contextual did. I don't know if they find true models on your documents or not. What we see is that we can solve that problem, pretty much by taking an approach where we're specializing models with RL for that particular set of documents, using RL and synthetic data. So is it a problem anymore? Yes and no. Like I think it is solved in the sense that we can solve for it, and perhaps others are as well. But I think a lot of people are still trying to solve this using off the shelf models, and in that case, I think no one's actually not solved. Like, I was talking to somebody at a large US Bank yesterday, and they were talking about how everyone's been trying to do this for 10 days, for like, five years. But still, he finds that there's, like, hallucinations on tabular data and, like, particular financial values where it's not reliable enough. So, like, a little bit of nuance, and it's two different stories, but this is just like one of the things that we focus on, generally, the better way to think about why you would use adaptive is performance overall hallucination is just like one example of a performance problem, like another one could be the reliability of your LLM in a more multi turn scenario, you know, where, like, you're asking it again and again, and it starts to go off track, and maybe it doesn't quite follow the correct formatting, or, like, after A certain number of times it starts behaving weirdly, like in the example Julian was giving earlier around, like the head to tech that we're working with that's like a situation where it's a very significant context, like people are asking questions, you need to make sure that it's still behaving in a way that is coherent with the beginning of that chat Session. Like these are more the kind of issues that we're seeing.
with anybody. Maybe the way that I rephrase this is hallucination can still be a problem for people that are doing rag Q and A tasks when they're trying to use just a standard off the shelf model. I can't speak to what contextual did. I don't know if they find true models on your documents or not. What we see is that we can solve that problem, pretty much by taking an approach where we're specializing models with RL for that particular set of documents, using RL and synthetic data. So is it a problem anymore? Yes and no. Like I think it is solved in the sense that we can solve for it, and perhaps others are as well. But I think a lot of people are still trying to solve this using off the shelf models, and in that case, I think no one's actually not solved. Like, I was talking to somebody at a large US Bank yesterday, and they were talking about how everyone's been trying to do this for 10 days, for like, five years. But still, he finds that there's, like, hallucinations on tabular data and, like, particular financial values where it's not reliable enough. So, like, a little bit of nuance, and it's two different stories, but this is just like one of the things that we focus on, generally, the better way to think about why you would use adaptive is performance overall hallucination is just like one example of a performance problem, like another one could be the reliability of your LLM in a more multi turn scenario, you know, where, like, you're asking it again and again, and it starts to go off track, and maybe it doesn't quite follow the correct formatting, or, like, after A certain number of times it starts behaving weirdly, like in the example Julian was giving earlier around, like the head to tech that we're working with that's like a situation where it's a very significant context, like people are asking questions, you need to make sure that it's still behaving in a way that is coherent with the beginning of that chat Session. Like these are more the kind of issues that we're seeing.
with anybody. Maybe the way that I rephrase this is hallucination can still be a problem for people that are doing rag Q and A tasks when they're trying to use just a standard off the shelf model. I can't speak to what contextual did. I don't know if they find true models on your documents or not. What we see is that we can solve that problem, pretty much by taking an approach where we're specializing models with RL for that particular set of documents, using RL and synthetic data. So is it a problem anymore? Yes and no. Like I think it is solved in the sense that we can solve for it, and perhaps others are as well. But I think a lot of people are still trying to solve this using off the shelf models, and in that case, I think no one's actually not solved. Like, I was talking to somebody at a large US Bank yesterday, and they were talking about how everyone's been trying to do this for 10 days, for like, five years. But still, he finds that there's, like, hallucinations on tabular data and, like, particular financial values where it's not reliable enough. So, like, a little bit of nuance, and it's two different stories, but this is just like one of the things that we focus on, generally, the better way to think about why you would use adaptive is performance overall hallucination is just like one example of a performance problem, like another one could be the reliability of your LLM in a more multi turn scenario, you know, where, like, you're asking it again and again, and it starts to go off track, and maybe it doesn't quite follow the correct formatting, or, like, after A certain number of times it starts behaving weirdly, like in the example Julian was giving earlier around, like the head to tech that we're working with that's like a situation where it's a very significant context, like people are asking questions, you need to make sure that it's still behaving in a way that is coherent with the beginning of that chat Session. Like these are more the kind of issues that we're seeing.
38:24Yeah, and what about like,
Yeah, and what about like,
Yeah, and what about like,
Yeah, and what about like,
38:30the other problem where there's one more problem
the other problem where there's one more problem
the other problem where there's one more problem
the other problem where there's one more problem
38:35statement that comes across.
statement that comes across.
statement that comes across.
statement that comes across.
S Speaker 138:40Look at it and see what they list out all the problems that they're facing.
Look at it and see what they list out all the problems that they're facing.
Look at it and see what they list out all the problems that they're facing.
Look at it and see what they list out all the problems that they're facing.
S Speaker 238:52If you have teams that have problems related to Gen AI, we would be very happy to speak
If you have teams that have problems related to Gen AI, we would be very happy to speak
If you have teams that have problems related to Gen AI, we would be very happy to speak
If you have teams that have problems related to Gen AI, we would be very happy to speak
S Speaker 139:00with them. Oh, yeah. Now remember, okay. So the other one is, you know what, what people hate is. So every time, let's say you have a new problem statement, anytime engage with a partner, they say, oh, we need to work with you. And then we need to get these question answer prayers, and we need to get, you know, we need to get more from your guys, thumbs up, thumbs down and everything, right? We're not in the business of that. We're not in the business supporting Samsung and Apple and everything. So is that is the solution to that problem,
with them. Oh, yeah. Now remember, okay. So the other one is, you know what, what people hate is. So every time, let's say you have a new problem statement, anytime engage with a partner, they say, oh, we need to work with you. And then we need to get these question answer prayers, and we need to get, you know, we need to get more from your guys, thumbs up, thumbs down and everything, right? We're not in the business of that. We're not in the business supporting Samsung and Apple and everything. So is that is the solution to that problem,
with them. Oh, yeah. Now remember, okay. So the other one is, you know what, what people hate is. So every time, let's say you have a new problem statement, anytime engage with a partner, they say, oh, we need to work with you. And then we need to get these question answer prayers, and we need to get, you know, we need to get more from your guys, thumbs up, thumbs down and everything, right? We're not in the business of that. We're not in the business supporting Samsung and Apple and everything. So is that is the solution to that problem,
with them. Oh, yeah. Now remember, okay. So the other one is, you know what, what people hate is. So every time, let's say you have a new problem statement, anytime engage with a partner, they say, oh, we need to work with you. And then we need to get these question answer prayers, and we need to get, you know, we need to get more from your guys, thumbs up, thumbs down and everything, right? We're not in the business of that. We're not in the business supporting Samsung and Apple and everything. So is that is the solution to that problem,
S Speaker 239:39absolutely. So all that particular issue, like the lack of training data, what we do is put a very significant emphasis on synthetic data. So
absolutely. So all that particular issue, like the lack of training data, what we do is put a very significant emphasis on synthetic data. So
absolutely. So all that particular issue, like the lack of training data, what we do is put a very significant emphasis on synthetic data. So
absolutely. So all that particular issue, like the lack of training data, what we do is put a very significant emphasis on synthetic data. So
39:51you kept saying,
S Speaker 339:56Yeah, so what you mentioned is only something we feel like, off like that. Need for data, and companies, you know, not not having the time to provide it, or the structured quality, because it's very annoying to point the classical way. What we really focus on with data, synthetic data, is on time, a few expert annotation. So obviously, it doesn't work in a black box. It doesn't work out of nowhere. We do need, like, a few things, like a few high quality annotations from experts. But when I say a few, I genuinely mean a few. So in one of the Ed Tech project recently, in the entire pipeline, there is 55 zero expert auditions, which are critics and a lot of the conversation. But 50 is very reasonable. You can, I mean, I'm sure you know, your experts can find the time to sit down to give 50 of these critics, even if you take them a few hours. That's like, I think it's very it's a very reasonable amount.
Yeah, so what you mentioned is only something we feel like, off like that. Need for data, and companies, you know, not not having the time to provide it, or the structured quality, because it's very annoying to point the classical way. What we really focus on with data, synthetic data, is on time, a few expert annotation. So obviously, it doesn't work in a black box. It doesn't work out of nowhere. We do need, like, a few things, like a few high quality annotations from experts. But when I say a few, I genuinely mean a few. So in one of the Ed Tech project recently, in the entire pipeline, there is 55 zero expert auditions, which are critics and a lot of the conversation. But 50 is very reasonable. You can, I mean, I'm sure you know, your experts can find the time to sit down to give 50 of these critics, even if you take them a few hours. That's like, I think it's very it's a very reasonable amount.
Yeah, so what you mentioned is only something we feel like, off like that. Need for data, and companies, you know, not not having the time to provide it, or the structured quality, because it's very annoying to point the classical way. What we really focus on with data, synthetic data, is on time, a few expert annotation. So obviously, it doesn't work in a black box. It doesn't work out of nowhere. We do need, like, a few things, like a few high quality annotations from experts. But when I say a few, I genuinely mean a few. So in one of the Ed Tech project recently, in the entire pipeline, there is 55 zero expert auditions, which are critics and a lot of the conversation. But 50 is very reasonable. You can, I mean, I'm sure you know, your experts can find the time to sit down to give 50 of these critics, even if you take them a few hours. That's like, I think it's very it's a very reasonable amount.
Yeah, so what you mentioned is only something we feel like, off like that. Need for data, and companies, you know, not not having the time to provide it, or the structured quality, because it's very annoying to point the classical way. What we really focus on with data, synthetic data, is on time, a few expert annotation. So obviously, it doesn't work in a black box. It doesn't work out of nowhere. We do need, like, a few things, like a few high quality annotations from experts. But when I say a few, I genuinely mean a few. So in one of the Ed Tech project recently, in the entire pipeline, there is 55 zero expert auditions, which are critics and a lot of the conversation. But 50 is very reasonable. You can, I mean, I'm sure you know, your experts can find the time to sit down to give 50 of these critics, even if you take them a few hours. That's like, I think it's very it's a very reasonable amount.
40:47And from this 50 we generate 80,000
And from this 50 we generate 80,000
And from this 50 we generate 80,000
And from this 50 we generate 80,000
S Speaker 340:52synthetic conversation. So, like, we multiply this 50 into 80,000 we self play synthetic conversation. So this is like 300 magnitude, essentially. So we can't provide, you know, to remove Ontario implementation, but we really focus where we ask you for higher quality like in the platform, you are going to be leviating higher quality annotations like the product ask for you to hide like, to have this higher quantization that come from experts that are like, very thoughtfully made, but then these annotation are used, like, multiply, like multiplied across the entire pipeline to really get the best results possible. And actually, the funny thing is that I think the experts much prefer this way of working as well, because they see more of the impact that they have on the model training process. You know, if you give like, 1000 thumbs up, thumbs down, it's kind of, you know, dehumanizing, like you don't really understand the impact that you have. But since the annotation we asked for critics and rewrites, you cannot directly feel like it's kind of like, I don't want to say it's almost like teaching someone, but it's, you know, it feels more like that of explicitly providing the feedback and in natural language and, and every writing the answer, I think it's like, we have had, like, good reviews from people that are, like, non technical use the platform to do this thing, like, oh, you know, actually, this is great. Like, I actually like working like this. I think it feels, it feels better. So yeah, so be something on which we put a lot of emphasis.
synthetic conversation. So, like, we multiply this 50 into 80,000 we self play synthetic conversation. So this is like 300 magnitude, essentially. So we can't provide, you know, to remove Ontario implementation, but we really focus where we ask you for higher quality like in the platform, you are going to be leviating higher quality annotations like the product ask for you to hide like, to have this higher quantization that come from experts that are like, very thoughtfully made, but then these annotation are used, like, multiply, like multiplied across the entire pipeline to really get the best results possible. And actually, the funny thing is that I think the experts much prefer this way of working as well, because they see more of the impact that they have on the model training process. You know, if you give like, 1000 thumbs up, thumbs down, it's kind of, you know, dehumanizing, like you don't really understand the impact that you have. But since the annotation we asked for critics and rewrites, you cannot directly feel like it's kind of like, I don't want to say it's almost like teaching someone, but it's, you know, it feels more like that of explicitly providing the feedback and in natural language and, and every writing the answer, I think it's like, we have had, like, good reviews from people that are, like, non technical use the platform to do this thing, like, oh, you know, actually, this is great. Like, I actually like working like this. I think it feels, it feels better. So yeah, so be something on which we put a lot of emphasis.
synthetic conversation. So, like, we multiply this 50 into 80,000 we self play synthetic conversation. So this is like 300 magnitude, essentially. So we can't provide, you know, to remove Ontario implementation, but we really focus where we ask you for higher quality like in the platform, you are going to be leviating higher quality annotations like the product ask for you to hide like, to have this higher quantization that come from experts that are like, very thoughtfully made, but then these annotation are used, like, multiply, like multiplied across the entire pipeline to really get the best results possible. And actually, the funny thing is that I think the experts much prefer this way of working as well, because they see more of the impact that they have on the model training process. You know, if you give like, 1000 thumbs up, thumbs down, it's kind of, you know, dehumanizing, like you don't really understand the impact that you have. But since the annotation we asked for critics and rewrites, you cannot directly feel like it's kind of like, I don't want to say it's almost like teaching someone, but it's, you know, it feels more like that of explicitly providing the feedback and in natural language and, and every writing the answer, I think it's like, we have had, like, good reviews from people that are, like, non technical use the platform to do this thing, like, oh, you know, actually, this is great. Like, I actually like working like this. I think it feels, it feels better. So yeah, so be something on which we put a lot of emphasis.
synthetic conversation. So, like, we multiply this 50 into 80,000 we self play synthetic conversation. So this is like 300 magnitude, essentially. So we can't provide, you know, to remove Ontario implementation, but we really focus where we ask you for higher quality like in the platform, you are going to be leviating higher quality annotations like the product ask for you to hide like, to have this higher quantization that come from experts that are like, very thoughtfully made, but then these annotation are used, like, multiply, like multiplied across the entire pipeline to really get the best results possible. And actually, the funny thing is that I think the experts much prefer this way of working as well, because they see more of the impact that they have on the model training process. You know, if you give like, 1000 thumbs up, thumbs down, it's kind of, you know, dehumanizing, like you don't really understand the impact that you have. But since the annotation we asked for critics and rewrites, you cannot directly feel like it's kind of like, I don't want to say it's almost like teaching someone, but it's, you know, it feels more like that of explicitly providing the feedback and in natural language and, and every writing the answer, I think it's like, we have had, like, good reviews from people that are, like, non technical use the platform to do this thing, like, oh, you know, actually, this is great. Like, I actually like working like this. I think it feels, it feels better. So yeah, so be something on which we put a lot of emphasis.
42:23Okay, so I'll be curious to then
Okay, so I'll be curious to then
Okay, so I'll be curious to then
Okay, so I'll be curious to then
42:27get some of our guys to actually
get some of our guys to actually
get some of our guys to actually
get some of our guys to actually
S Speaker 142:31take a look and and even evaluate, right, like because, so let's, I think we will follow up on this end to actually have some conversations around both, which is Qualcomm as a partner and a customer. I think it'll be interesting, definitely, to work with you guys.
take a look and and even evaluate, right, like because, so let's, I think we will follow up on this end to actually have some conversations around both, which is Qualcomm as a partner and a customer. I think it'll be interesting, definitely, to work with you guys.
take a look and and even evaluate, right, like because, so let's, I think we will follow up on this end to actually have some conversations around both, which is Qualcomm as a partner and a customer. I think it'll be interesting, definitely, to work with you guys.
take a look and and even evaluate, right, like because, so let's, I think we will follow up on this end to actually have some conversations around both, which is Qualcomm as a partner and a customer. I think it'll be interesting, definitely, to work with you guys.
42:54What's your like
S Speaker 142:57in terms of traction, when did you launch the product? When did you, like, you started about early last year, right?
in terms of traction, when did you launch the product? When did you, like, you started about early last year, right?
in terms of traction, when did you launch the product? When did you, like, you started about early last year, right?
in terms of traction, when did you launch the product? When did you, like, you started about early last year, right?
S Speaker 343:05Yeah. So, I mean, we started the company, like, about a year, like it was October, like 31st, of October, 2023, so really, end of October. But, you know, we started commercializing, or still last summer, like we started and, you know, we have been iterating a lot on the product. The product is evolving very fast. I will see, like we are still in a phase of fast evolution, where we are constantly bringing new flows, new features. You know, we work very closely with our customers, in the sense that if there is something the product doesn't do yet, we'll first do it ourselves, and then we will add it, you know, kind of take this this way, because, to us, you know, the real challenge is not building an API for reinforcement learning or for that, because that's easy to do, like that's anyone can, can do this, but it's building the right building blocks around it to really make it something that's high leverage for your teams and that they can use, You know, autonomously. And that's, and that's something you know, that I think, is, is that credit company we are very, working really hard on. So, yeah, so, and now, you know, we are, you know, deployed in a few places, as latex, as at&t. There are other, you know, other customers that do like stuff more, like hug. We have a we have a bit of like, different use cases across the board, and we have some more, some more deployments coming, yeah, that's
Yeah. So, I mean, we started the company, like, about a year, like it was October, like 31st, of October, 2023, so really, end of October. But, you know, we started commercializing, or still last summer, like we started and, you know, we have been iterating a lot on the product. The product is evolving very fast. I will see, like we are still in a phase of fast evolution, where we are constantly bringing new flows, new features. You know, we work very closely with our customers, in the sense that if there is something the product doesn't do yet, we'll first do it ourselves, and then we will add it, you know, kind of take this this way, because, to us, you know, the real challenge is not building an API for reinforcement learning or for that, because that's easy to do, like that's anyone can, can do this, but it's building the right building blocks around it to really make it something that's high leverage for your teams and that they can use, You know, autonomously. And that's, and that's something you know, that I think, is, is that credit company we are very, working really hard on. So, yeah, so, and now, you know, we are, you know, deployed in a few places, as latex, as at&t. There are other, you know, other customers that do like stuff more, like hug. We have a we have a bit of like, different use cases across the board, and we have some more, some more deployments coming, yeah, that's
Yeah. So, I mean, we started the company, like, about a year, like it was October, like 31st, of October, 2023, so really, end of October. But, you know, we started commercializing, or still last summer, like we started and, you know, we have been iterating a lot on the product. The product is evolving very fast. I will see, like we are still in a phase of fast evolution, where we are constantly bringing new flows, new features. You know, we work very closely with our customers, in the sense that if there is something the product doesn't do yet, we'll first do it ourselves, and then we will add it, you know, kind of take this this way, because, to us, you know, the real challenge is not building an API for reinforcement learning or for that, because that's easy to do, like that's anyone can, can do this, but it's building the right building blocks around it to really make it something that's high leverage for your teams and that they can use, You know, autonomously. And that's, and that's something you know, that I think, is, is that credit company we are very, working really hard on. So, yeah, so, and now, you know, we are, you know, deployed in a few places, as latex, as at&t. There are other, you know, other customers that do like stuff more, like hug. We have a we have a bit of like, different use cases across the board, and we have some more, some more deployments coming, yeah, that's
Yeah. So, I mean, we started the company, like, about a year, like it was October, like 31st, of October, 2023, so really, end of October. But, you know, we started commercializing, or still last summer, like we started and, you know, we have been iterating a lot on the product. The product is evolving very fast. I will see, like we are still in a phase of fast evolution, where we are constantly bringing new flows, new features. You know, we work very closely with our customers, in the sense that if there is something the product doesn't do yet, we'll first do it ourselves, and then we will add it, you know, kind of take this this way, because, to us, you know, the real challenge is not building an API for reinforcement learning or for that, because that's easy to do, like that's anyone can, can do this, but it's building the right building blocks around it to really make it something that's high leverage for your teams and that they can use, You know, autonomously. And that's, and that's something you know, that I think, is, is that credit company we are very, working really hard on. So, yeah, so, and now, you know, we are, you know, deployed in a few places, as latex, as at&t. There are other, you know, other customers that do like stuff more, like hug. We have a we have a bit of like, different use cases across the board, and we have some more, some more deployments coming, yeah, that's
S Speaker 144:22kind of the high level. Okay, and then so,
kind of the high level. Okay, and then so,
kind of the high level. Okay, and then so,
kind of the high level. Okay, and then so,
44:28Arr, wise, revenue wise, where are
Arr, wise, revenue wise, where are
Arr, wise, revenue wise, where are
Arr, wise, revenue wise, where are
S Speaker 344:29you today? It's we don't so we don't totally disclose exact figures to external to external people. But, you know, it's like the contracts around few 100k some of them are scaling to be present to a million. You know, again, can kind of do the math by yourself. I mean, like nothing very, very surprising, essentially, in in the range
you today? It's we don't so we don't totally disclose exact figures to external to external people. But, you know, it's like the contracts around few 100k some of them are scaling to be present to a million. You know, again, can kind of do the math by yourself. I mean, like nothing very, very surprising, essentially, in in the range
you today? It's we don't so we don't totally disclose exact figures to external to external people. But, you know, it's like the contracts around few 100k some of them are scaling to be present to a million. You know, again, can kind of do the math by yourself. I mean, like nothing very, very surprising, essentially, in in the range
you today? It's we don't so we don't totally disclose exact figures to external to external people. But, you know, it's like the contracts around few 100k some of them are scaling to be present to a million. You know, again, can kind of do the math by yourself. I mean, like nothing very, very surprising, essentially, in in the range
S Speaker 144:48of where we are, okay? And then you, you've raised a seed round. Yeah,
of where we are, okay? And then you, you've raised a seed round. Yeah,
of where we are, okay? And then you, you've raised a seed round. Yeah,
of where we are, okay? And then you, you've raised a seed round. Yeah,
44:55million, okay? And then
million, okay? And then
million, okay? And then
million, okay? And then
44:59are you looking to raise someone as well? Or
are you looking to raise someone as well? Or
are you looking to raise someone as well? Or
are you looking to raise someone as well? Or
S Speaker 345:03probably not, I will say, probably a bit later this year. We are still, you know, we are still very heads down on on product building and commercial expansion. We think it's like, it's where the best time like to spend is right now. We also are quite comfortable in Tower of roadway. But there is, like, your point this year where we'd be like, Look, it would make sense to rise, to double down, essentially, to be able to, like, hire more people in the Timothy rule, that sort of stuff. So, yeah, I think it's, it's likely to be this year. It's like, not, you know, nothing is very set in stone yet is that our will present it.
probably not, I will say, probably a bit later this year. We are still, you know, we are still very heads down on on product building and commercial expansion. We think it's like, it's where the best time like to spend is right now. We also are quite comfortable in Tower of roadway. But there is, like, your point this year where we'd be like, Look, it would make sense to rise, to double down, essentially, to be able to, like, hire more people in the Timothy rule, that sort of stuff. So, yeah, I think it's, it's likely to be this year. It's like, not, you know, nothing is very set in stone yet is that our will present it.
probably not, I will say, probably a bit later this year. We are still, you know, we are still very heads down on on product building and commercial expansion. We think it's like, it's where the best time like to spend is right now. We also are quite comfortable in Tower of roadway. But there is, like, your point this year where we'd be like, Look, it would make sense to rise, to double down, essentially, to be able to, like, hire more people in the Timothy rule, that sort of stuff. So, yeah, I think it's, it's likely to be this year. It's like, not, you know, nothing is very set in stone yet is that our will present it.
probably not, I will say, probably a bit later this year. We are still, you know, we are still very heads down on on product building and commercial expansion. We think it's like, it's where the best time like to spend is right now. We also are quite comfortable in Tower of roadway. But there is, like, your point this year where we'd be like, Look, it would make sense to rise, to double down, essentially, to be able to, like, hire more people in the Timothy rule, that sort of stuff. So, yeah, I think it's, it's likely to be this year. It's like, not, you know, nothing is very set in stone yet is that our will present it.
S Speaker 145:37How many people do you have right now? Went to and you guys are based where it's
How many people do you have right now? Went to and you guys are based where it's
How many people do you have right now? Went to and you guys are based where it's
How many people do you have right now? Went to and you guys are based where it's
S Speaker 345:44a bit it's a mix. We are split between Europe and the US. So we have people in Toronto, New York and Paris, predominantly, and then a few people remote as well, across Europe.
a bit it's a mix. We are split between Europe and the US. So we have people in Toronto, New York and Paris, predominantly, and then a few people remote as well, across Europe.
a bit it's a mix. We are split between Europe and the US. So we have people in Toronto, New York and Paris, predominantly, and then a few people remote as well, across Europe.
a bit it's a mix. We are split between Europe and the US. So we have people in Toronto, New York and Paris, predominantly, and then a few people remote as well, across Europe.
45:57And so the headquarter
And so the headquarter
And so the headquarter
And so the headquarter
S Speaker 345:59is us. We have a US, company with a friendship, yeah,
is us. We have a US, company with a friendship, yeah,
is us. We have a US, company with a friendship, yeah,
is us. We have a US, company with a friendship, yeah,
46:05okay. And then so the investors are also us, like
okay. And then so the investors are also us, like
okay. And then so the investors are also us, like
okay. And then so the investors are also us, like
46:09it's Index Ventures and iconic,
it's Index Ventures and iconic,
it's Index Ventures and iconic,
it's Index Ventures and iconic,
S Speaker 146:12who iconic and index? Okay?
who iconic and index? Okay?
who iconic and index? Okay?
who iconic and index? Okay?
S Speaker 246:20Just, I think so I got to prepare for interview with a candidate. But I just one quick question for you before we got I know you mentioned that you'd like to set up a conversation with, I guess, some of your engineering teams to explore where we could be useful to Qualcomm. Are there any specific use cases that you had in mind when you said that?
Just, I think so I got to prepare for interview with a candidate. But I just one quick question for you before we got I know you mentioned that you'd like to set up a conversation with, I guess, some of your engineering teams to explore where we could be useful to Qualcomm. Are there any specific use cases that you had in mind when you said that?
Just, I think so I got to prepare for interview with a candidate. But I just one quick question for you before we got I know you mentioned that you'd like to set up a conversation with, I guess, some of your engineering teams to explore where we could be useful to Qualcomm. Are there any specific use cases that you had in mind when you said that?
Just, I think so I got to prepare for interview with a candidate. But I just one quick question for you before we got I know you mentioned that you'd like to set up a conversation with, I guess, some of your engineering teams to explore where we could be useful to Qualcomm. Are there any specific use cases that you had in mind when you said that?
S Speaker 146:43Why don't so there's a there's two aspects to this. One is what I talked about, this AI appliance team. And then AI appliance teams, they can talk to you about more specific use cases. But then they're going into environments offer on prem deployments for AI systems across similar enterprise, search, rag type of use cases. And then, then also they're going into like industrial so there's, there's those two, and then the second one is, you had a smile. So it seems weird.
Why don't so there's a there's two aspects to this. One is what I talked about, this AI appliance team. And then AI appliance teams, they can talk to you about more specific use cases. But then they're going into environments offer on prem deployments for AI systems across similar enterprise, search, rag type of use cases. And then, then also they're going into like industrial so there's, there's those two, and then the second one is, you had a smile. So it seems weird.
Why don't so there's a there's two aspects to this. One is what I talked about, this AI appliance team. And then AI appliance teams, they can talk to you about more specific use cases. But then they're going into environments offer on prem deployments for AI systems across similar enterprise, search, rag type of use cases. And then, then also they're going into like industrial so there's, there's those two, and then the second one is, you had a smile. So it seems weird.
Why don't so there's a there's two aspects to this. One is what I talked about, this AI appliance team. And then AI appliance teams, they can talk to you about more specific use cases. But then they're going into environments offer on prem deployments for AI systems across similar enterprise, search, rag type of use cases. And then, then also they're going into like industrial so there's, there's those two, and then the second one is, you had a smile. So it seems weird.
S Speaker 247:20No, I was laughing at Julian because he was complaining about how he's come downhill with the illness, like Come work. So Julian, Julian was mocking me yesterday because I had a fever a few days ago, and he was saying that I was a week old man because I'm a little bit older than Julian, and he was, I can get over the illness, no problem. But he's since succumbed to illness, and he's not feeling too good.
No, I was laughing at Julian because he was complaining about how he's come downhill with the illness, like Come work. So Julian, Julian was mocking me yesterday because I had a fever a few days ago, and he was saying that I was a week old man because I'm a little bit older than Julian, and he was, I can get over the illness, no problem. But he's since succumbed to illness, and he's not feeling too good.
No, I was laughing at Julian because he was complaining about how he's come downhill with the illness, like Come work. So Julian, Julian was mocking me yesterday because I had a fever a few days ago, and he was saying that I was a week old man because I'm a little bit older than Julian, and he was, I can get over the illness, no problem. But he's since succumbed to illness, and he's not feeling too good.
No, I was laughing at Julian because he was complaining about how he's come downhill with the illness, like Come work. So Julian, Julian was mocking me yesterday because I had a fever a few days ago, and he was saying that I was a week old man because I'm a little bit older than Julian, and he was, I can get over the illness, no problem. But he's since succumbed to illness, and he's not feeling too good.
S Speaker 347:49This has taken a lot of people at the company, it's been a bit of a big company wide, private joke in the past year, in the past and being sick. So next time, that joke as well, but,
This has taken a lot of people at the company, it's been a bit of a big company wide, private joke in the past year, in the past and being sick. So next time, that joke as well, but,
This has taken a lot of people at the company, it's been a bit of a big company wide, private joke in the past year, in the past and being sick. So next time, that joke as well, but,
This has taken a lot of people at the company, it's been a bit of a big company wide, private joke in the past year, in the past and being sick. So next time, that joke as well, but,
S Speaker 148:05and then the second one, Andrew, to your question, is this, we have this AI PC effort, which is so the world of PCs, the way it works is, we sell to the audience, but we also sell through, which is, we go to, we go to, you know, enterprises, we go to school, we go to whoever, different types of industries, and then we go sell through to these. So there's a lot of demand for like we've got 40 tops of NPU compute, AI compute, and you compute on these laptops, and then these enterprises all want similar use cases of Enterprise Search and all of those for privacy, security,
and then the second one, Andrew, to your question, is this, we have this AI PC effort, which is so the world of PCs, the way it works is, we sell to the audience, but we also sell through, which is, we go to, we go to, you know, enterprises, we go to school, we go to whoever, different types of industries, and then we go sell through to these. So there's a lot of demand for like we've got 40 tops of NPU compute, AI compute, and you compute on these laptops, and then these enterprises all want similar use cases of Enterprise Search and all of those for privacy, security,
and then the second one, Andrew, to your question, is this, we have this AI PC effort, which is so the world of PCs, the way it works is, we sell to the audience, but we also sell through, which is, we go to, we go to, you know, enterprises, we go to school, we go to whoever, different types of industries, and then we go sell through to these. So there's a lot of demand for like we've got 40 tops of NPU compute, AI compute, and you compute on these laptops, and then these enterprises all want similar use cases of Enterprise Search and all of those for privacy, security,
and then the second one, Andrew, to your question, is this, we have this AI PC effort, which is so the world of PCs, the way it works is, we sell to the audience, but we also sell through, which is, we go to, we go to, you know, enterprises, we go to school, we go to whoever, different types of industries, and then we go sell through to these. So there's a lot of demand for like we've got 40 tops of NPU compute, AI compute, and you compute on these laptops, and then these enterprises all want similar use cases of Enterprise Search and all of those for privacy, security,
48:53latency in some cases and cost reasons,
latency in some cases and cost reasons,
latency in some cases and cost reasons,
latency in some cases and cost reasons,
S Speaker 148:57different industries have different reasons. Like the Department of Defense has a different reason, and which is privacy and security and and, and then some creatives have, like creative industries have latency as a reason, and everybody's got different reasons. But so on those platforms, those are the two, I would say, to work with, and then so they will have different use cases, but they all fall in similar bucket,
different industries have different reasons. Like the Department of Defense has a different reason, and which is privacy and security and and, and then some creatives have, like creative industries have latency as a reason, and everybody's got different reasons. But so on those platforms, those are the two, I would say, to work with, and then so they will have different use cases, but they all fall in similar bucket,
different industries have different reasons. Like the Department of Defense has a different reason, and which is privacy and security and and, and then some creatives have, like creative industries have latency as a reason, and everybody's got different reasons. But so on those platforms, those are the two, I would say, to work with, and then so they will have different use cases, but they all fall in similar bucket,
different industries have different reasons. Like the Department of Defense has a different reason, and which is privacy and security and and, and then some creatives have, like creative industries have latency as a reason, and everybody's got different reasons. But so on those platforms, those are the two, I would say, to work with, and then so they will have different use cases, but they all fall in similar bucket,
49:29I would say, that's really
I would say, that's really
I would say, that's really
I would say, that's really
S Speaker 149:38hey, yeah. So we will follow up. Great chatting with you guys. We'll follow up and then set up. The only thing is next week is, like we've got this next week is we've got this big event and everything. So, so give us some time. Next week is our offside and event and everything. So, so we'll
hey, yeah. So we will follow up. Great chatting with you guys. We'll follow up and then set up. The only thing is next week is, like we've got this next week is we've got this big event and everything. So, so give us some time. Next week is our offside and event and everything. So, so we'll
hey, yeah. So we will follow up. Great chatting with you guys. We'll follow up and then set up. The only thing is next week is, like we've got this next week is we've got this big event and everything. So, so give us some time. Next week is our offside and event and everything. So, so we'll
hey, yeah. So we will follow up. Great chatting with you guys. We'll follow up and then set up. The only thing is next week is, like we've got this next week is we've got this big event and everything. So, so give us some time. Next week is our offside and event and everything. So, so we'll
49:57look for a follow up for the week after
look for a follow up for the week after
look for a follow up for the week after
look for a follow up for the week after
S Speaker 249:58Okay, sounds good. Thank
Okay, sounds good. Thank
Okay, sounds good. Thank
Okay, sounds good. Thank
50:09you very much. Interesting. Next calling It
you very much. Interesting. Next calling It
you very much. Interesting. Next calling It
you very much. Interesting. Next calling It