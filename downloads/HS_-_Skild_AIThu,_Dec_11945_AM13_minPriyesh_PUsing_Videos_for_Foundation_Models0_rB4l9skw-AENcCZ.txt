Meeting: HS - Skild AI
Thu, Dec 11
9:45 AM
13 min
Priyesh P
Using Videos for Foundation Models
0:00
Different
URL: https://otter.ai/u/rB4l9skw-AENcCZ9QJC7oCpigHM
Downloaded: 2025-12-21T19:16:41.759247
Method: text_extraction
============================================================

S Speaker 10:00I go forward, I just want to use a minute to talk about how we can use videos, because this is the most dark art in terms of creating these foundation models for the model, because no one knows how to use videos yet. And so if you think, if you're trying to learn a foundation model you want, you're taking input as pixels, this model makes some reasoning, and output is motor commands, and this model is to learn visual representation, convert pixels into something meaningful, dynamics. How does the world change with forces? And then finally, what action it should take to solve this
I go forward, I just want to use a minute to talk about how we can use videos, because this is the most dark art in terms of creating these foundation models for the model, because no one knows how to use videos yet. And so if you think, if you're trying to learn a foundation model you want, you're taking input as pixels, this model makes some reasoning, and output is motor commands, and this model is to learn visual representation, convert pixels into something meaningful, dynamics. How does the world change with forces? And then finally, what action it should take to solve this
I go forward, I just want to use a minute to talk about how we can use videos, because this is the most dark art in terms of creating these foundation models for the model, because no one knows how to use videos yet. And so if you think, if you're trying to learn a foundation model you want, you're taking input as pixels, this model makes some reasoning, and output is motor commands, and this model is to learn visual representation, convert pixels into something meaningful, dynamics. How does the world change with forces? And then finally, what action it should take to solve this
I go forward, I just want to use a minute to talk about how we can use videos, because this is the most dark art in terms of creating these foundation models for the model, because no one knows how to use videos yet. And so if you think, if you're trying to learn a foundation model you want, you're taking input as pixels, this model makes some reasoning, and output is motor commands, and this model is to learn visual representation, convert pixels into something meaningful, dynamics. How does the world change with forces? And then finally, what action it should take to solve this
0:31task. And so what you can do is
task. And so what you can do is
task. And so what you can do is
task. And so what you can do is
S Speaker 10:33you can modularize your learning. You can first have the model only learn visual representation from videos, and then a separate robot data model, which uses to learn dynamics in action. So this is one way you can use videos, and this is what we have been trying a lot in our literature as well, where we basically train visual representations using affordances, like, where should human hands go, and what are the hands poses? And then basically use the robotics data on the top. Or you can actually use videos to learn both representation and dynamics. This is the world model approach that we are hearing a lot. Let's build a world model and then solve the robotics. And again, in this area as well, we have tried lot of things. For example, we are building world models in the space of segmentations tracking points, so you are predicting the future in space of human segmentations. And then you convert these visual plans into robot interactions using some amount of labor data. And last, but not the least, you can also use videos policy. Here you can just predict actions using videos. These will not be the best actions, but then you can bring your model end to end. Not to summarize, what we've seen is four different forms of data variable simulation are nearly scalable, but poor, right? They're not. They don't have four data, they don't have central data, and so on and but they're very complementary, if you think both simulation is reduced to each other. So simulation, you can get but the problem is simple. Here in videos, we need a lot of diversity. In simulation, diversity is really hard. Telling about requirements are basically the best form of data, rich, but just not scalable. And so the stage approach is actually very, very interesting. What we are doing is we realize that pre training does not need clean form of data, so we pre train using
you can modularize your learning. You can first have the model only learn visual representation from videos, and then a separate robot data model, which uses to learn dynamics in action. So this is one way you can use videos, and this is what we have been trying a lot in our literature as well, where we basically train visual representations using affordances, like, where should human hands go, and what are the hands poses? And then basically use the robotics data on the top. Or you can actually use videos to learn both representation and dynamics. This is the world model approach that we are hearing a lot. Let's build a world model and then solve the robotics. And again, in this area as well, we have tried lot of things. For example, we are building world models in the space of segmentations tracking points, so you are predicting the future in space of human segmentations. And then you convert these visual plans into robot interactions using some amount of labor data. And last, but not the least, you can also use videos policy. Here you can just predict actions using videos. These will not be the best actions, but then you can bring your model end to end. Not to summarize, what we've seen is four different forms of data variable simulation are nearly scalable, but poor, right? They're not. They don't have four data, they don't have central data, and so on and but they're very complementary, if you think both simulation is reduced to each other. So simulation, you can get but the problem is simple. Here in videos, we need a lot of diversity. In simulation, diversity is really hard. Telling about requirements are basically the best form of data, rich, but just not scalable. And so the stage approach is actually very, very interesting. What we are doing is we realize that pre training does not need clean form of data, so we pre train using
you can modularize your learning. You can first have the model only learn visual representation from videos, and then a separate robot data model, which uses to learn dynamics in action. So this is one way you can use videos, and this is what we have been trying a lot in our literature as well, where we basically train visual representations using affordances, like, where should human hands go, and what are the hands poses? And then basically use the robotics data on the top. Or you can actually use videos to learn both representation and dynamics. This is the world model approach that we are hearing a lot. Let's build a world model and then solve the robotics. And again, in this area as well, we have tried lot of things. For example, we are building world models in the space of segmentations tracking points, so you are predicting the future in space of human segmentations. And then you convert these visual plans into robot interactions using some amount of labor data. And last, but not the least, you can also use videos policy. Here you can just predict actions using videos. These will not be the best actions, but then you can bring your model end to end. Not to summarize, what we've seen is four different forms of data variable simulation are nearly scalable, but poor, right? They're not. They don't have four data, they don't have central data, and so on and but they're very complementary, if you think both simulation is reduced to each other. So simulation, you can get but the problem is simple. Here in videos, we need a lot of diversity. In simulation, diversity is really hard. Telling about requirements are basically the best form of data, rich, but just not scalable. And so the stage approach is actually very, very interesting. What we are doing is we realize that pre training does not need clean form of data, so we pre train using
you can modularize your learning. You can first have the model only learn visual representation from videos, and then a separate robot data model, which uses to learn dynamics in action. So this is one way you can use videos, and this is what we have been trying a lot in our literature as well, where we basically train visual representations using affordances, like, where should human hands go, and what are the hands poses? And then basically use the robotics data on the top. Or you can actually use videos to learn both representation and dynamics. This is the world model approach that we are hearing a lot. Let's build a world model and then solve the robotics. And again, in this area as well, we have tried lot of things. For example, we are building world models in the space of segmentations tracking points, so you are predicting the future in space of human segmentations. And then you convert these visual plans into robot interactions using some amount of labor data. And last, but not the least, you can also use videos policy. Here you can just predict actions using videos. These will not be the best actions, but then you can bring your model end to end. Not to summarize, what we've seen is four different forms of data variable simulation are nearly scalable, but poor, right? They're not. They don't have four data, they don't have central data, and so on and but they're very complementary, if you think both simulation is reduced to each other. So simulation, you can get but the problem is simple. Here in videos, we need a lot of diversity. In simulation, diversity is really hard. Telling about requirements are basically the best form of data, rich, but just not scalable. And so the stage approach is actually very, very interesting. What we are doing is we realize that pre training does not need clean form of data, so we pre train using
2:24large amounts of simulation
large amounts of simulation
large amounts of simulation
large amounts of simulation
2:26and human videos. Now this does not need to solve the problem.
and human videos. Now this does not need to solve the problem.
and human videos. Now this does not need to solve the problem.
and human videos. Now this does not need to solve the problem.
S Speaker 12:29All this needs to do is learn the different configurations, different behaviors need to emerge out of this, and then we post it using the penny of data and the deployment data. And so let me now show you some of the best. Of the slides are going to be mostly videos. And so we basically learn my videos, how the skill bit works. And so first, let's see that this is the pre trained modularity, not fine tune, with very less amount of data for this whole task. And this is in 1x like not speed it up. Lot of videos you see are you speed up 10x if you will, close your little corner, and you can see that it can do these tasks very nicely by picking up all these objects, different objects, on the table, and putting it but the important thing is not to solve it in one hole. You can actually almost collect lots of examples in one room and then make it work by almost retraining the trajectories. That's a problem with robotics. What? What is exciting about robotics? Videos is also a curse, right? It's very easy to create videos, but it's very hard to actually make it work. And so here's so what we are going to do is we are going to show you the same task on very different scenarios. So now this is different homes and different objects, untrained objects that you've never trained before, they are also basically going to be shown. And again, the idea to see that, will it generalize? Will it generates different backgrounds? Will it generate different objects?
All this needs to do is learn the different configurations, different behaviors need to emerge out of this, and then we post it using the penny of data and the deployment data. And so let me now show you some of the best. Of the slides are going to be mostly videos. And so we basically learn my videos, how the skill bit works. And so first, let's see that this is the pre trained modularity, not fine tune, with very less amount of data for this whole task. And this is in 1x like not speed it up. Lot of videos you see are you speed up 10x if you will, close your little corner, and you can see that it can do these tasks very nicely by picking up all these objects, different objects, on the table, and putting it but the important thing is not to solve it in one hole. You can actually almost collect lots of examples in one room and then make it work by almost retraining the trajectories. That's a problem with robotics. What? What is exciting about robotics? Videos is also a curse, right? It's very easy to create videos, but it's very hard to actually make it work. And so here's so what we are going to do is we are going to show you the same task on very different scenarios. So now this is different homes and different objects, untrained objects that you've never trained before, they are also basically going to be shown. And again, the idea to see that, will it generalize? Will it generates different backgrounds? Will it generate different objects?
All this needs to do is learn the different configurations, different behaviors need to emerge out of this, and then we post it using the penny of data and the deployment data. And so let me now show you some of the best. Of the slides are going to be mostly videos. And so we basically learn my videos, how the skill bit works. And so first, let's see that this is the pre trained modularity, not fine tune, with very less amount of data for this whole task. And this is in 1x like not speed it up. Lot of videos you see are you speed up 10x if you will, close your little corner, and you can see that it can do these tasks very nicely by picking up all these objects, different objects, on the table, and putting it but the important thing is not to solve it in one hole. You can actually almost collect lots of examples in one room and then make it work by almost retraining the trajectories. That's a problem with robotics. What? What is exciting about robotics? Videos is also a curse, right? It's very easy to create videos, but it's very hard to actually make it work. And so here's so what we are going to do is we are going to show you the same task on very different scenarios. So now this is different homes and different objects, untrained objects that you've never trained before, they are also basically going to be shown. And again, the idea to see that, will it generalize? Will it generates different backgrounds? Will it generate different objects?
All this needs to do is learn the different configurations, different behaviors need to emerge out of this, and then we post it using the penny of data and the deployment data. And so let me now show you some of the best. Of the slides are going to be mostly videos. And so we basically learn my videos, how the skill bit works. And so first, let's see that this is the pre trained modularity, not fine tune, with very less amount of data for this whole task. And this is in 1x like not speed it up. Lot of videos you see are you speed up 10x if you will, close your little corner, and you can see that it can do these tasks very nicely by picking up all these objects, different objects, on the table, and putting it but the important thing is not to solve it in one hole. You can actually almost collect lots of examples in one room and then make it work by almost retraining the trajectories. That's a problem with robotics. What? What is exciting about robotics? Videos is also a curse, right? It's very easy to create videos, but it's very hard to actually make it work. And so here's so what we are going to do is we are going to show you the same task on very different scenarios. So now this is different homes and different objects, untrained objects that you've never trained before, they are also basically going to be shown. And again, the idea to see that, will it generalize? Will it generates different backgrounds? Will it generate different objects?
4:02And so on? One thing people always ask
And so on? One thing people always ask
And so on? One thing people always ask
And so on? One thing people always ask
S Speaker 14:04me, Are videos by itself only useful? So we also basically tried to explore just using videos and do not, do not do any fine tuning. How far you can go with such an approach? And so here are these examples. In these examples, we did not fine tune and using any tele operation data. This is just from videos we are making that you want to do these different kind of tasks, for example, opening doors. So while the data is noisy, I completely agree that data is noisy, still it has lot of power, and that's what we want to show, that just videos and simulation by themselves will have lot of power to solve a lot of manipulation tasks that we might think are really hard and we might be teleported data for. And this is kind of what, again, is very, very critical. The power of studio, especially when you're large scale, is huge, still. And not just these one task, but you can see all these different tasks are only based on video data, not using any solution, not using any post trading data.
me, Are videos by itself only useful? So we also basically tried to explore just using videos and do not, do not do any fine tuning. How far you can go with such an approach? And so here are these examples. In these examples, we did not fine tune and using any tele operation data. This is just from videos we are making that you want to do these different kind of tasks, for example, opening doors. So while the data is noisy, I completely agree that data is noisy, still it has lot of power, and that's what we want to show, that just videos and simulation by themselves will have lot of power to solve a lot of manipulation tasks that we might think are really hard and we might be teleported data for. And this is kind of what, again, is very, very critical. The power of studio, especially when you're large scale, is huge, still. And not just these one task, but you can see all these different tasks are only based on video data, not using any solution, not using any post trading data.
me, Are videos by itself only useful? So we also basically tried to explore just using videos and do not, do not do any fine tuning. How far you can go with such an approach? And so here are these examples. In these examples, we did not fine tune and using any tele operation data. This is just from videos we are making that you want to do these different kind of tasks, for example, opening doors. So while the data is noisy, I completely agree that data is noisy, still it has lot of power, and that's what we want to show, that just videos and simulation by themselves will have lot of power to solve a lot of manipulation tasks that we might think are really hard and we might be teleported data for. And this is kind of what, again, is very, very critical. The power of studio, especially when you're large scale, is huge, still. And not just these one task, but you can see all these different tasks are only based on video data, not using any solution, not using any post trading data.
me, Are videos by itself only useful? So we also basically tried to explore just using videos and do not, do not do any fine tuning. How far you can go with such an approach? And so here are these examples. In these examples, we did not fine tune and using any tele operation data. This is just from videos we are making that you want to do these different kind of tasks, for example, opening doors. So while the data is noisy, I completely agree that data is noisy, still it has lot of power, and that's what we want to show, that just videos and simulation by themselves will have lot of power to solve a lot of manipulation tasks that we might think are really hard and we might be teleported data for. And this is kind of what, again, is very, very critical. The power of studio, especially when you're large scale, is huge, still. And not just these one task, but you can see all these different tasks are only based on video data, not using any solution, not using any post trading data.
5:04So our recipe is to pre train and then
So our recipe is to pre train and then
So our recipe is to pre train and then
So our recipe is to pre train and then
S Speaker 15:07find people using post trading. So what does this post trading build and what does why do we need to pre train? Let's see this. So what we get lot of people are showing these tasks of folding cloths, clothes. I want to again emphasize, from robotics perspective, folding clothes is the easiest task because it's easy to grasp. It's a deformable object. Then no one cares about the quality of folding. It's very easy to pick up from the center and just go towards the center of the cloth. It will always fold it out. What is hard is when you choose fine grained tasks. For example, in this one here, you have to basically insert these airports into airport cover. Millimeter precision needs to be there. And these kind of where these are very slipping. But what makes it even harder are these that this is a fake airport from Telugu, so this is, this does not have those magnetic characters that will basically pull the thing towards it. So it's a really, really hard task and but you can still do it if you train on large scale data and you fine tune it with using posterior data. And that's kind of what you want to show that these tasks are much harder. But of course, people argue that deformable objects is where you should be kind of focusing on because deformable objects are hard. I agree, deformable objects are hard, but when you are making them do very, very precise deformable objects, for example, something like this. Now, here's a deformable object, a buyer, and now you have to insert it in these internet tables. And you can again see this one is hard to it's hard to predict the wire configurations. How do they change? And so on, but our model is able to do that with various amounts of tenure of data, mostly basically trading approaches, and these are internet cables. But we can also do audio fiber cables and transceivers, and with this seamless speed, as humans, in fact, they're able to almost beat humans at this task itself,
find people using post trading. So what does this post trading build and what does why do we need to pre train? Let's see this. So what we get lot of people are showing these tasks of folding cloths, clothes. I want to again emphasize, from robotics perspective, folding clothes is the easiest task because it's easy to grasp. It's a deformable object. Then no one cares about the quality of folding. It's very easy to pick up from the center and just go towards the center of the cloth. It will always fold it out. What is hard is when you choose fine grained tasks. For example, in this one here, you have to basically insert these airports into airport cover. Millimeter precision needs to be there. And these kind of where these are very slipping. But what makes it even harder are these that this is a fake airport from Telugu, so this is, this does not have those magnetic characters that will basically pull the thing towards it. So it's a really, really hard task and but you can still do it if you train on large scale data and you fine tune it with using posterior data. And that's kind of what you want to show that these tasks are much harder. But of course, people argue that deformable objects is where you should be kind of focusing on because deformable objects are hard. I agree, deformable objects are hard, but when you are making them do very, very precise deformable objects, for example, something like this. Now, here's a deformable object, a buyer, and now you have to insert it in these internet tables. And you can again see this one is hard to it's hard to predict the wire configurations. How do they change? And so on, but our model is able to do that with various amounts of tenure of data, mostly basically trading approaches, and these are internet cables. But we can also do audio fiber cables and transceivers, and with this seamless speed, as humans, in fact, they're able to almost beat humans at this task itself,
find people using post trading. So what does this post trading build and what does why do we need to pre train? Let's see this. So what we get lot of people are showing these tasks of folding cloths, clothes. I want to again emphasize, from robotics perspective, folding clothes is the easiest task because it's easy to grasp. It's a deformable object. Then no one cares about the quality of folding. It's very easy to pick up from the center and just go towards the center of the cloth. It will always fold it out. What is hard is when you choose fine grained tasks. For example, in this one here, you have to basically insert these airports into airport cover. Millimeter precision needs to be there. And these kind of where these are very slipping. But what makes it even harder are these that this is a fake airport from Telugu, so this is, this does not have those magnetic characters that will basically pull the thing towards it. So it's a really, really hard task and but you can still do it if you train on large scale data and you fine tune it with using posterior data. And that's kind of what you want to show that these tasks are much harder. But of course, people argue that deformable objects is where you should be kind of focusing on because deformable objects are hard. I agree, deformable objects are hard, but when you are making them do very, very precise deformable objects, for example, something like this. Now, here's a deformable object, a buyer, and now you have to insert it in these internet tables. And you can again see this one is hard to it's hard to predict the wire configurations. How do they change? And so on, but our model is able to do that with various amounts of tenure of data, mostly basically trading approaches, and these are internet cables. But we can also do audio fiber cables and transceivers, and with this seamless speed, as humans, in fact, they're able to almost beat humans at this task itself,
find people using post trading. So what does this post trading build and what does why do we need to pre train? Let's see this. So what we get lot of people are showing these tasks of folding cloths, clothes. I want to again emphasize, from robotics perspective, folding clothes is the easiest task because it's easy to grasp. It's a deformable object. Then no one cares about the quality of folding. It's very easy to pick up from the center and just go towards the center of the cloth. It will always fold it out. What is hard is when you choose fine grained tasks. For example, in this one here, you have to basically insert these airports into airport cover. Millimeter precision needs to be there. And these kind of where these are very slipping. But what makes it even harder are these that this is a fake airport from Telugu, so this is, this does not have those magnetic characters that will basically pull the thing towards it. So it's a really, really hard task and but you can still do it if you train on large scale data and you fine tune it with using posterior data. And that's kind of what you want to show that these tasks are much harder. But of course, people argue that deformable objects is where you should be kind of focusing on because deformable objects are hard. I agree, deformable objects are hard, but when you are making them do very, very precise deformable objects, for example, something like this. Now, here's a deformable object, a buyer, and now you have to insert it in these internet tables. And you can again see this one is hard to it's hard to predict the wire configurations. How do they change? And so on, but our model is able to do that with various amounts of tenure of data, mostly basically trading approaches, and these are internet cables. But we can also do audio fiber cables and transceivers, and with this seamless speed, as humans, in fact, they're able to almost beat humans at this task itself,
7:04inserting bias and capability.
inserting bias and capability.
inserting bias and capability.
inserting bias and capability.
9:19knows that there is the height coming in, and
knows that there is the height coming in, and
knows that there is the height coming in, and
knows that there is the height coming in, and
9:22it can do this really, really fast. So in this case, we are forcing
it can do this really, really fast. So in this case, we are forcing
it can do this really, really fast. So in this case, we are forcing
it can do this really, really fast. So in this case, we are forcing
S Speaker 19:29the robot to go on this so that it can be seen at the bottom of it and so on. You and
the robot to go on this so that it can be seen at the bottom of it and so on. You and
the robot to go on this so that it can be seen at the bottom of it and so on. You and
the robot to go on this so that it can be seen at the bottom of it and so on. You and
S Speaker 19:54it came to lots of different stages, whether they are fire escapes, we have hundreds of different, say, the biggest player in Pittsburgh. You can climb up and down using objects with objects in hand and so on, so forth. And then you can still do part two, because everyone loves doing it. So there are some art videos of trying interesting actions. You can see them. They might See her, because much easier you
it came to lots of different stages, whether they are fire escapes, we have hundreds of different, say, the biggest player in Pittsburgh. You can climb up and down using objects with objects in hand and so on, so forth. And then you can still do part two, because everyone loves doing it. So there are some art videos of trying interesting actions. You can see them. They might See her, because much easier you
it came to lots of different stages, whether they are fire escapes, we have hundreds of different, say, the biggest player in Pittsburgh. You can climb up and down using objects with objects in hand and so on, so forth. And then you can still do part two, because everyone loves doing it. So there are some art videos of trying interesting actions. You can see them. They might See her, because much easier you
it came to lots of different stages, whether they are fire escapes, we have hundreds of different, say, the biggest player in Pittsburgh. You can climb up and down using objects with objects in hand and so on, so forth. And then you can still do part two, because everyone loves doing it. So there are some art videos of trying interesting actions. You can see them. They might See her, because much easier you
S Speaker 110:46Okay, and let me get this last one minute. I will use it on only body we are building, one way that will work on all the different hardwares. This is very critical from safety viewpoint, because bodies break down. These machines, they have motors. They have gears. Gears break down. Motor stops working. What should happen to the humanoid in that case law showing you serious falling. But what if we actually do not fall? Better than say, falling is not falling. So if your motor goes bad, you should be able to adapt. And this is why only body is really, really critical for safety, even if your motor needs now, the hardware changes, but the robot should adapt to it. And one of the biggest property of scale is adaptation. It's reasonable. It's a whole body as well, and it adapts on the fly. And the way we are doing it through in context learning. This is only a concept in language, in other words, in context learning, for the first time, we have breaking into the context of robotics where we are actually measuring and learning this on these kind of robots, which are like stick figures. So what happens is we are running it on many different stick figures. So we uses the context window to figure out how many motors are working. So for example, in this case, we are not going to let you use the front two motors and still. So it's first figuring it out, okay, my top, my front two motors are not working, so it keeps falling down. But once it has enough in the context window, then this same quadruped will start walking at the funeral, because it's using the same brain. It has seen humanoid data, it has seen quadruped data, and it starts basically adapting using the scenario.
Okay, and let me get this last one minute. I will use it on only body we are building, one way that will work on all the different hardwares. This is very critical from safety viewpoint, because bodies break down. These machines, they have motors. They have gears. Gears break down. Motor stops working. What should happen to the humanoid in that case law showing you serious falling. But what if we actually do not fall? Better than say, falling is not falling. So if your motor goes bad, you should be able to adapt. And this is why only body is really, really critical for safety, even if your motor needs now, the hardware changes, but the robot should adapt to it. And one of the biggest property of scale is adaptation. It's reasonable. It's a whole body as well, and it adapts on the fly. And the way we are doing it through in context learning. This is only a concept in language, in other words, in context learning, for the first time, we have breaking into the context of robotics where we are actually measuring and learning this on these kind of robots, which are like stick figures. So what happens is we are running it on many different stick figures. So we uses the context window to figure out how many motors are working. So for example, in this case, we are not going to let you use the front two motors and still. So it's first figuring it out, okay, my top, my front two motors are not working, so it keeps falling down. But once it has enough in the context window, then this same quadruped will start walking at the funeral, because it's using the same brain. It has seen humanoid data, it has seen quadruped data, and it starts basically adapting using the scenario.
Okay, and let me get this last one minute. I will use it on only body we are building, one way that will work on all the different hardwares. This is very critical from safety viewpoint, because bodies break down. These machines, they have motors. They have gears. Gears break down. Motor stops working. What should happen to the humanoid in that case law showing you serious falling. But what if we actually do not fall? Better than say, falling is not falling. So if your motor goes bad, you should be able to adapt. And this is why only body is really, really critical for safety, even if your motor needs now, the hardware changes, but the robot should adapt to it. And one of the biggest property of scale is adaptation. It's reasonable. It's a whole body as well, and it adapts on the fly. And the way we are doing it through in context learning. This is only a concept in language, in other words, in context learning, for the first time, we have breaking into the context of robotics where we are actually measuring and learning this on these kind of robots, which are like stick figures. So what happens is we are running it on many different stick figures. So we uses the context window to figure out how many motors are working. So for example, in this case, we are not going to let you use the front two motors and still. So it's first figuring it out, okay, my top, my front two motors are not working, so it keeps falling down. But once it has enough in the context window, then this same quadruped will start walking at the funeral, because it's using the same brain. It has seen humanoid data, it has seen quadruped data, and it starts basically adapting using the scenario.
Okay, and let me get this last one minute. I will use it on only body we are building, one way that will work on all the different hardwares. This is very critical from safety viewpoint, because bodies break down. These machines, they have motors. They have gears. Gears break down. Motor stops working. What should happen to the humanoid in that case law showing you serious falling. But what if we actually do not fall? Better than say, falling is not falling. So if your motor goes bad, you should be able to adapt. And this is why only body is really, really critical for safety, even if your motor needs now, the hardware changes, but the robot should adapt to it. And one of the biggest property of scale is adaptation. It's reasonable. It's a whole body as well, and it adapts on the fly. And the way we are doing it through in context learning. This is only a concept in language, in other words, in context learning, for the first time, we have breaking into the context of robotics where we are actually measuring and learning this on these kind of robots, which are like stick figures. So what happens is we are running it on many different stick figures. So we uses the context window to figure out how many motors are working. So for example, in this case, we are not going to let you use the front two motors and still. So it's first figuring it out, okay, my top, my front two motors are not working, so it keeps falling down. But once it has enough in the context window, then this same quadruped will start walking at the funeral, because it's using the same brain. It has seen humanoid data, it has seen quadruped data, and it starts basically adapting using the scenario.
12:26And so here is my last video again, is
And so here is my last video again, is
And so here is my last video again, is
And so here is my last video again, is
12:32do lot of things in this way, keep lot of corner cases. For example, crack robot lanes,
do lot of things in this way, keep lot of corner cases. For example, crack robot lanes,
do lot of things in this way, keep lot of corner cases. For example, crack robot lanes,
do lot of things in this way, keep lot of corner cases. For example, crack robot lanes,
S Speaker 112:37make robot beams fail, and the robot adapt on the fly in real time to solve and still continue to do these tasks as well. And this is what we will provide safety. We think this is a critical aspect of safety that you should be able to adapt to the changes in hardware. And these are also so, for example, globally, the robot adapts in a second, and it
make robot beams fail, and the robot adapt on the fly in real time to solve and still continue to do these tasks as well. And this is what we will provide safety. We think this is a critical aspect of safety that you should be able to adapt to the changes in hardware. And these are also so, for example, globally, the robot adapts in a second, and it
make robot beams fail, and the robot adapt on the fly in real time to solve and still continue to do these tasks as well. And this is what we will provide safety. We think this is a critical aspect of safety that you should be able to adapt to the changes in hardware. And these are also so, for example, globally, the robot adapts in a second, and it
make robot beams fail, and the robot adapt on the fly in real time to solve and still continue to do these tasks as well. And this is what we will provide safety. We think this is a critical aspect of safety that you should be able to adapt to the changes in hardware. And these are also so, for example, globally, the robot adapts in a second, and it
13:00starts walking. Okay, that's it. I think I'm over time. Thank you. Okay,
starts walking. Okay, that's it. I think I'm over time. Thank you. Okay,
starts walking. Okay, that's it. I think I'm over time. Thank you. Okay,
starts walking. Okay, that's it. I think I'm over time. Thank you. Okay,
13:16so we have one more presentation for you guys. It's actually the panel discussions are.
so we have one more presentation for you guys. It's actually the panel discussions are.
so we have one more presentation for you guys. It's actually the panel discussions are.
so we have one more presentation for you guys. It's actually the panel discussions are.