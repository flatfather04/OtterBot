Meeting: HS - Field AI
Thu, Dec 11
1:55 PM
17 min
Priyesh P
Background and Company Origins
0:00
Challenges an
URL: https://otter.ai/u/vHmkxrPjbyHXZLkw4lhlzfqmdy4
Downloaded: 2025-12-21T19:12:34.332894
Method: text_extraction
============================================================

S Speaker 10:00Solutions. We layer the data driven solution with physics everywhere solutions to make it the
Solutions. We layer the data driven solution with physics everywhere solutions to make it the
Solutions. We layer the data driven solution with physics everywhere solutions to make it the
Solutions. We layer the data driven solution with physics everywhere solutions to make it the
0:05entire solution very data efficient
entire solution very data efficient
entire solution very data efficient
entire solution very data efficient
S Speaker 10:07and also safer to be able to deploy. So you see
and also safer to be able to deploy. So you see
and also safer to be able to deploy. So you see
and also safer to be able to deploy. So you see
0:11here some of the platforms you put
here some of the platforms you put
here some of the platforms you put
here some of the platforms you put
0:13there, brain on, brain on the smaller
there, brain on, brain on the smaller
there, brain on, brain on the smaller
there, brain on, brain on the smaller
S Speaker 10:16platforms and backward human noise all the way to passenger sized vehicles. So I'm going to show some some of the results and where we are and what we're doing in the company with these but a little bit about the background of the company. We come from two camps that the founding team part from come the safety aware mostly NASA and the other part come from data driven institutions like Google did mine almost from a decade ago. We have been pushing the boundaries of deploying robots in extreme environments like Mars, analog caves and birds to mines to industrial factories and so on. And you see some of some of what we have been doing many years ago, pushing and deploying these robots, winning some Docker challenges with them. As many of you know, these DARPA challenges are fully ready for us. Ai robotics people, this is a DARPA Challenge we won several years ago where we sent 11 robots, four of them walking with legs, four of them with wheels, and three of them flying, going into environments that they have never seen before. We have never seen before, and the sponsor puts cameras. All of these are the sponsored DARPA cameras, record what's happening. And these robots go in, discover the world on their own, no map, no GPS, no any sort of prior information, and they dispatch themselves. They see, oh, here's a stairs. I'm sending the walking platform to handle it. Here's a vertical shaft. I'm going to have drones take off map that come back, land, charge itself on it, a grand robot come to the rendezvous point, handle the mission as a team of robots. And as we move forward, you'll see some of the drones and all of that. But this is just to say that we have pushed, over the last, I would say, decade, from 2010 to 20. We have been pushing really hard on the x axis
platforms and backward human noise all the way to passenger sized vehicles. So I'm going to show some some of the results and where we are and what we're doing in the company with these but a little bit about the background of the company. We come from two camps that the founding team part from come the safety aware mostly NASA and the other part come from data driven institutions like Google did mine almost from a decade ago. We have been pushing the boundaries of deploying robots in extreme environments like Mars, analog caves and birds to mines to industrial factories and so on. And you see some of some of what we have been doing many years ago, pushing and deploying these robots, winning some Docker challenges with them. As many of you know, these DARPA challenges are fully ready for us. Ai robotics people, this is a DARPA Challenge we won several years ago where we sent 11 robots, four of them walking with legs, four of them with wheels, and three of them flying, going into environments that they have never seen before. We have never seen before, and the sponsor puts cameras. All of these are the sponsored DARPA cameras, record what's happening. And these robots go in, discover the world on their own, no map, no GPS, no any sort of prior information, and they dispatch themselves. They see, oh, here's a stairs. I'm sending the walking platform to handle it. Here's a vertical shaft. I'm going to have drones take off map that come back, land, charge itself on it, a grand robot come to the rendezvous point, handle the mission as a team of robots. And as we move forward, you'll see some of the drones and all of that. But this is just to say that we have pushed, over the last, I would say, decade, from 2010 to 20. We have been pushing really hard on the x axis
platforms and backward human noise all the way to passenger sized vehicles. So I'm going to show some some of the results and where we are and what we're doing in the company with these but a little bit about the background of the company. We come from two camps that the founding team part from come the safety aware mostly NASA and the other part come from data driven institutions like Google did mine almost from a decade ago. We have been pushing the boundaries of deploying robots in extreme environments like Mars, analog caves and birds to mines to industrial factories and so on. And you see some of some of what we have been doing many years ago, pushing and deploying these robots, winning some Docker challenges with them. As many of you know, these DARPA challenges are fully ready for us. Ai robotics people, this is a DARPA Challenge we won several years ago where we sent 11 robots, four of them walking with legs, four of them with wheels, and three of them flying, going into environments that they have never seen before. We have never seen before, and the sponsor puts cameras. All of these are the sponsored DARPA cameras, record what's happening. And these robots go in, discover the world on their own, no map, no GPS, no any sort of prior information, and they dispatch themselves. They see, oh, here's a stairs. I'm sending the walking platform to handle it. Here's a vertical shaft. I'm going to have drones take off map that come back, land, charge itself on it, a grand robot come to the rendezvous point, handle the mission as a team of robots. And as we move forward, you'll see some of the drones and all of that. But this is just to say that we have pushed, over the last, I would say, decade, from 2010 to 20. We have been pushing really hard on the x axis
platforms and backward human noise all the way to passenger sized vehicles. So I'm going to show some some of the results and where we are and what we're doing in the company with these but a little bit about the background of the company. We come from two camps that the founding team part from come the safety aware mostly NASA and the other part come from data driven institutions like Google did mine almost from a decade ago. We have been pushing the boundaries of deploying robots in extreme environments like Mars, analog caves and birds to mines to industrial factories and so on. And you see some of some of what we have been doing many years ago, pushing and deploying these robots, winning some Docker challenges with them. As many of you know, these DARPA challenges are fully ready for us. Ai robotics people, this is a DARPA Challenge we won several years ago where we sent 11 robots, four of them walking with legs, four of them with wheels, and three of them flying, going into environments that they have never seen before. We have never seen before, and the sponsor puts cameras. All of these are the sponsored DARPA cameras, record what's happening. And these robots go in, discover the world on their own, no map, no GPS, no any sort of prior information, and they dispatch themselves. They see, oh, here's a stairs. I'm sending the walking platform to handle it. Here's a vertical shaft. I'm going to have drones take off map that come back, land, charge itself on it, a grand robot come to the rendezvous point, handle the mission as a team of robots. And as we move forward, you'll see some of the drones and all of that. But this is just to say that we have pushed, over the last, I would say, decade, from 2010 to 20. We have been pushing really hard on the x axis
2:05of this curve. This is one
of this curve. This is one
of this curve. This is one
of this curve. This is one
S Speaker 12:06lens you can look at the models and robotic AI that you need safety and deployability of these solutions, and you need generalizability. A lot of today's focus has been on generalizability, the promise of this massive tan of the market. But what the other side of the coin is, when these models hallucinate, what would happen? How do you make a business out of these generalizable solution when it doesn't generalize, which is robotics always going to happen? Why? Because, even with trillions of tokens in chatgpt, we still have hallucinations all the time, and now in the robotics regime that the data regime is extremely different, that's going to happen always, right? We can reduce it. But what do you do at that point? And that's really as focused. So if you think about it, for the last decade, our team have been pushing very hard on on moving the solution from this corner to the right side of x axis, enabling uncertainty, awareness, risk awareness, and deploy these solutions actually in the field, not demo, not POC, not a video, but in the environment with the customer. But then downside, that's the upside you can deploy. You can be safe. But the downside is, you are single purpose. If you move it from a mines factory to 2000 parameters you need to change for it, it will take three months to adapt it to the environment. On the other side, the other part of other half of our company, coming from
lens you can look at the models and robotic AI that you need safety and deployability of these solutions, and you need generalizability. A lot of today's focus has been on generalizability, the promise of this massive tan of the market. But what the other side of the coin is, when these models hallucinate, what would happen? How do you make a business out of these generalizable solution when it doesn't generalize, which is robotics always going to happen? Why? Because, even with trillions of tokens in chatgpt, we still have hallucinations all the time, and now in the robotics regime that the data regime is extremely different, that's going to happen always, right? We can reduce it. But what do you do at that point? And that's really as focused. So if you think about it, for the last decade, our team have been pushing very hard on on moving the solution from this corner to the right side of x axis, enabling uncertainty, awareness, risk awareness, and deploy these solutions actually in the field, not demo, not POC, not a video, but in the environment with the customer. But then downside, that's the upside you can deploy. You can be safe. But the downside is, you are single purpose. If you move it from a mines factory to 2000 parameters you need to change for it, it will take three months to adapt it to the environment. On the other side, the other part of other half of our company, coming from
lens you can look at the models and robotic AI that you need safety and deployability of these solutions, and you need generalizability. A lot of today's focus has been on generalizability, the promise of this massive tan of the market. But what the other side of the coin is, when these models hallucinate, what would happen? How do you make a business out of these generalizable solution when it doesn't generalize, which is robotics always going to happen? Why? Because, even with trillions of tokens in chatgpt, we still have hallucinations all the time, and now in the robotics regime that the data regime is extremely different, that's going to happen always, right? We can reduce it. But what do you do at that point? And that's really as focused. So if you think about it, for the last decade, our team have been pushing very hard on on moving the solution from this corner to the right side of x axis, enabling uncertainty, awareness, risk awareness, and deploy these solutions actually in the field, not demo, not POC, not a video, but in the environment with the customer. But then downside, that's the upside you can deploy. You can be safe. But the downside is, you are single purpose. If you move it from a mines factory to 2000 parameters you need to change for it, it will take three months to adapt it to the environment. On the other side, the other part of other half of our company, coming from
lens you can look at the models and robotic AI that you need safety and deployability of these solutions, and you need generalizability. A lot of today's focus has been on generalizability, the promise of this massive tan of the market. But what the other side of the coin is, when these models hallucinate, what would happen? How do you make a business out of these generalizable solution when it doesn't generalize, which is robotics always going to happen? Why? Because, even with trillions of tokens in chatgpt, we still have hallucinations all the time, and now in the robotics regime that the data regime is extremely different, that's going to happen always, right? We can reduce it. But what do you do at that point? And that's really as focused. So if you think about it, for the last decade, our team have been pushing very hard on on moving the solution from this corner to the right side of x axis, enabling uncertainty, awareness, risk awareness, and deploy these solutions actually in the field, not demo, not POC, not a video, but in the environment with the customer. But then downside, that's the upside you can deploy. You can be safe. But the downside is, you are single purpose. If you move it from a mines factory to 2000 parameters you need to change for it, it will take three months to adapt it to the environment. On the other side, the other part of other half of our company, coming from
3:35DeepMind and data driven
DeepMind and data driven
DeepMind and data driven
DeepMind and data driven
S Speaker 13:37side of the world, they have been pushing and building foundation models that are very generalizable, but they could listen, and this is where filaya was born a few years ago, that when you meet them, okay, you guys need us to be able to deploy. We need you to be able to when we go to other environment, we don't touch anything, and it just handles another task another environment.
side of the world, they have been pushing and building foundation models that are very generalizable, but they could listen, and this is where filaya was born a few years ago, that when you meet them, okay, you guys need us to be able to deploy. We need you to be able to when we go to other environment, we don't touch anything, and it just handles another task another environment.
side of the world, they have been pushing and building foundation models that are very generalizable, but they could listen, and this is where filaya was born a few years ago, that when you meet them, okay, you guys need us to be able to deploy. We need you to be able to when we go to other environment, we don't touch anything, and it just handles another task another environment.
side of the world, they have been pushing and building foundation models that are very generalizable, but they could listen, and this is where filaya was born a few years ago, that when you meet them, okay, you guys need us to be able to deploy. We need you to be able to when we go to other environment, we don't touch anything, and it just handles another task another environment.
3:58So that's what we are building at filaya. We
So that's what we are building at filaya. We
So that's what we are building at filaya. We
So that's what we are building at filaya. We
S Speaker 14:00call it FFM field foundation models, a new class of foundation models that are not only generalizable, not only they build and learn based on data, but they delete
call it FFM field foundation models, a new class of foundation models that are not only generalizable, not only they build and learn based on data, but they delete
call it FFM field foundation models, a new class of foundation models that are not only generalizable, not only they build and learn based on data, but they delete
call it FFM field foundation models, a new class of foundation models that are not only generalizable, not only they build and learn based on data, but they delete
4:12all of what we have learned and have been
all of what we have learned and have been
all of what we have learned and have been
all of what we have learned and have been
4:13pushing the boundaries of the physics, first
pushing the boundaries of the physics, first
pushing the boundaries of the physics, first
pushing the boundaries of the physics, first
4:16into it to make it
S Speaker 14:18safe and also data efficient so we don't need trillions of tokens human race. If you kids, you see, we do not require billions and trillions of data. We learn differently. We learn to get a small policy that enables that policy to collect data. First thing the
safe and also data efficient so we don't need trillions of tokens human race. If you kids, you see, we do not require billions and trillions of data. We learn differently. We learn to get a small policy that enables that policy to collect data. First thing the
safe and also data efficient so we don't need trillions of tokens human race. If you kids, you see, we do not require billions and trillions of data. We learn differently. We learn to get a small policy that enables that policy to collect data. First thing the
safe and also data efficient so we don't need trillions of tokens human race. If you kids, you see, we do not require billions and trillions of data. We learn differently. We learn to get a small policy that enables that policy to collect data. First thing the
4:39kid does is not to do the task, to learn enough
kid does is not to do the task, to learn enough
kid does is not to do the task, to learn enough
kid does is not to do the task, to learn enough
S Speaker 14:42to be able to collect the richer data and then build a policy to collect another richer data to do the task. And that's a much faster trajectory. And this is where FFM, philosophically is looking at the problem, not just pure data driven, not the way conversation, chatgpt, and we believe that's the main and maybe the only way to actually build these within a few years. And we'll show some of the results of that in the field actually. So that has, at least bringing these to end of spectrum safety and foundation model, data driven, has been the backbone of what we're doing. Some of the logos of the where the team members are coming from, been very, very fortunate that we are working with this incredible team, pushing and bringing these two end of the spectrum together. So as a product, we are building what you see at that middle of the sensible box with computer sensing, as opposed to different types of robots. We have now been on more than 20 different embodiments you see here, and again, these are not in that, not POC. We are actually deployed with the customers all around the world. Now we have a little bit quiet on the market side, but
to be able to collect the richer data and then build a policy to collect another richer data to do the task. And that's a much faster trajectory. And this is where FFM, philosophically is looking at the problem, not just pure data driven, not the way conversation, chatgpt, and we believe that's the main and maybe the only way to actually build these within a few years. And we'll show some of the results of that in the field actually. So that has, at least bringing these to end of spectrum safety and foundation model, data driven, has been the backbone of what we're doing. Some of the logos of the where the team members are coming from, been very, very fortunate that we are working with this incredible team, pushing and bringing these two end of the spectrum together. So as a product, we are building what you see at that middle of the sensible box with computer sensing, as opposed to different types of robots. We have now been on more than 20 different embodiments you see here, and again, these are not in that, not POC. We are actually deployed with the customers all around the world. Now we have a little bit quiet on the market side, but
to be able to collect the richer data and then build a policy to collect another richer data to do the task. And that's a much faster trajectory. And this is where FFM, philosophically is looking at the problem, not just pure data driven, not the way conversation, chatgpt, and we believe that's the main and maybe the only way to actually build these within a few years. And we'll show some of the results of that in the field actually. So that has, at least bringing these to end of spectrum safety and foundation model, data driven, has been the backbone of what we're doing. Some of the logos of the where the team members are coming from, been very, very fortunate that we are working with this incredible team, pushing and bringing these two end of the spectrum together. So as a product, we are building what you see at that middle of the sensible box with computer sensing, as opposed to different types of robots. We have now been on more than 20 different embodiments you see here, and again, these are not in that, not POC. We are actually deployed with the customers all around the world. Now we have a little bit quiet on the market side, but
to be able to collect the richer data and then build a policy to collect another richer data to do the task. And that's a much faster trajectory. And this is where FFM, philosophically is looking at the problem, not just pure data driven, not the way conversation, chatgpt, and we believe that's the main and maybe the only way to actually build these within a few years. And we'll show some of the results of that in the field actually. So that has, at least bringing these to end of spectrum safety and foundation model, data driven, has been the backbone of what we're doing. Some of the logos of the where the team members are coming from, been very, very fortunate that we are working with this incredible team, pushing and bringing these two end of the spectrum together. So as a product, we are building what you see at that middle of the sensible box with computer sensing, as opposed to different types of robots. We have now been on more than 20 different embodiments you see here, and again, these are not in that, not POC. We are actually deployed with the customers all around the world. Now we have a little bit quiet on the market side, but
5:50we will be, we'll be talking a
we will be, we'll be talking a
we will be, we'll be talking a
we will be, we'll be talking a
S Speaker 15:51little more, but from Japan to Singapore to Europe to the US. We have as we speak, our robots are walking across industrial sectors, and we don't do typically, the
little more, but from Japan to Singapore to Europe to the US. We have as we speak, our robots are walking across industrial sectors, and we don't do typically, the
little more, but from Japan to Singapore to Europe to the US. We have as we speak, our robots are walking across industrial sectors, and we don't do typically, the
little more, but from Japan to Singapore to Europe to the US. We have as we speak, our robots are walking across industrial sectors, and we don't do typically, the
6:01home application is for
home application is for
home application is for
home application is for
S Speaker 16:04Unreal self driving. We are typically unstructured, complex industrial environments like the one you see here. So these are the places our robots have been deployed. Have been walking and very, very challenging. They change on an hourly basis. There are a lot of different tasks, from manipulation to locomotion to operations and operation is another
Unreal self driving. We are typically unstructured, complex industrial environments like the one you see here. So these are the places our robots have been deployed. Have been walking and very, very challenging. They change on an hourly basis. There are a lot of different tasks, from manipulation to locomotion to operations and operation is another
Unreal self driving. We are typically unstructured, complex industrial environments like the one you see here. So these are the places our robots have been deployed. Have been walking and very, very challenging. They change on an hourly basis. There are a lot of different tasks, from manipulation to locomotion to operations and operation is another
Unreal self driving. We are typically unstructured, complex industrial environments like the one you see here. So these are the places our robots have been deployed. Have been walking and very, very challenging. They change on an hourly basis. There are a lot of different tasks, from manipulation to locomotion to operations and operation is another
6:25angle that we haven't watched today, but we are seeing when
angle that we haven't watched today, but we are seeing when
angle that we haven't watched today, but we are seeing when
angle that we haven't watched today, but we are seeing when
S Speaker 16:28you're deploying the customers, is much more important than both manipulation and locomotion, to embed the solution into the customer workflow and understand the risks. So by the way, again, since it was just five minutes
you're deploying the customers, is much more important than both manipulation and locomotion, to embed the solution into the customer workflow and understand the risks. So by the way, again, since it was just five minutes
you're deploying the customers, is much more important than both manipulation and locomotion, to embed the solution into the customer workflow and understand the risks. So by the way, again, since it was just five minutes
you're deploying the customers, is much more important than both manipulation and locomotion, to embed the solution into the customer workflow and understand the risks. So by the way, again, since it was just five minutes
6:41ago before the talk, I learned about as much
ago before the talk, I learned about as much
ago before the talk, I learned about as much
ago before the talk, I learned about as much
S Speaker 16:43talk as this 45 minutes talk. So I'm going to skip onto the slides to be able to wrap it up at the time we have. And I don't see a timer
talk as this 45 minutes talk. So I'm going to skip onto the slides to be able to wrap it up at the time we have. And I don't see a timer
talk as this 45 minutes talk. So I'm going to skip onto the slides to be able to wrap it up at the time we have. And I don't see a timer
talk as this 45 minutes talk. So I'm going to skip onto the slides to be able to wrap it up at the time we have. And I don't see a timer
6:51here, maybe somebody can
here, maybe somebody can
here, maybe somebody can
here, maybe somebody can
6:59So here you see some of the deployments with our customers.
So here you see some of the deployments with our customers.
So here you see some of the deployments with our customers.
So here you see some of the deployments with our customers.
9:53hundreds of herds, you maintain
hundreds of herds, you maintain
hundreds of herds, you maintain
hundreds of herds, you maintain
S Speaker 19:57uncertainty awareness and risk awareness across many
uncertainty awareness and risk awareness across many
uncertainty awareness and risk awareness across many
uncertainty awareness and risk awareness across many
10:01state domains of the system, from
state domains of the system, from
state domains of the system, from
state domains of the system, from
S Speaker 110:04traversability to allocation, to localizability, communicability and so on. And finally, you know, the same payload becomes a backpack was behind humanoid you can do a variety of different tasks with it. And at the end, you'll see some of what we are doing through manipulation, with with the platform and what? When we do manipulation, we compound and expand the use cases that we have already with customers. If you're deploying and the robot is doing something but needs to open door along the way, we teach it to open doors. We if you need to pick up tools and drill something, collect something, and so on. We do so.
traversability to allocation, to localizability, communicability and so on. And finally, you know, the same payload becomes a backpack was behind humanoid you can do a variety of different tasks with it. And at the end, you'll see some of what we are doing through manipulation, with with the platform and what? When we do manipulation, we compound and expand the use cases that we have already with customers. If you're deploying and the robot is doing something but needs to open door along the way, we teach it to open doors. We if you need to pick up tools and drill something, collect something, and so on. We do so.
traversability to allocation, to localizability, communicability and so on. And finally, you know, the same payload becomes a backpack was behind humanoid you can do a variety of different tasks with it. And at the end, you'll see some of what we are doing through manipulation, with with the platform and what? When we do manipulation, we compound and expand the use cases that we have already with customers. If you're deploying and the robot is doing something but needs to open door along the way, we teach it to open doors. We if you need to pick up tools and drill something, collect something, and so on. We do so.
traversability to allocation, to localizability, communicability and so on. And finally, you know, the same payload becomes a backpack was behind humanoid you can do a variety of different tasks with it. And at the end, you'll see some of what we are doing through manipulation, with with the platform and what? When we do manipulation, we compound and expand the use cases that we have already with customers. If you're deploying and the robot is doing something but needs to open door along the way, we teach it to open doors. We if you need to pick up tools and drill something, collect something, and so on. We do so.
10:42So we go very much sort of
So we go very much sort of
So we go very much sort of
So we go very much sort of
S Speaker 110:44customer driven approach and expanding the skills of the system is one one by one, while maintaining a back end that uses all of this data across different tasks and across different embodiments and environments. But that's the physical AI layer that but it's one thing to to keep the system stable, totally another thing to embed it in the customer. And that's I think us overall, together as a community, will be, will be we'll be learning and as we deploy more and more robots. And then maybe a good visual example of it is, you remember I just showed you this a second ago. The robot walking on pallets and not falling down. That's a great thing, right? It's a reactive behavior. But when you go to customers, the first thing customers, that's amazing. I'm happy you showed me that. But can you please notice the other pallets? Right? People just don't do that. They see the palette, and that's where the operational AI comes in, that the story is now, okay, you maintain this stability, but how you look at that pallet stairs and everything and not all these are all from our customers today, around the world, in Japan, North of Japan, and Hokkaido to Europe, to the US. And all of these robots are operating in environments that are changing. No map needed, no predefined trajectories, no GPSs, and you verbally go and say them, tell the mission. Go read these 2000 gages here in this gas foiling acid pipe. Do perimeter security for me on this real estate you have arms and hands. Go open close these valves you're on a data center construction. Go build a digital twin for me
customer driven approach and expanding the skills of the system is one one by one, while maintaining a back end that uses all of this data across different tasks and across different embodiments and environments. But that's the physical AI layer that but it's one thing to to keep the system stable, totally another thing to embed it in the customer. And that's I think us overall, together as a community, will be, will be we'll be learning and as we deploy more and more robots. And then maybe a good visual example of it is, you remember I just showed you this a second ago. The robot walking on pallets and not falling down. That's a great thing, right? It's a reactive behavior. But when you go to customers, the first thing customers, that's amazing. I'm happy you showed me that. But can you please notice the other pallets? Right? People just don't do that. They see the palette, and that's where the operational AI comes in, that the story is now, okay, you maintain this stability, but how you look at that pallet stairs and everything and not all these are all from our customers today, around the world, in Japan, North of Japan, and Hokkaido to Europe, to the US. And all of these robots are operating in environments that are changing. No map needed, no predefined trajectories, no GPSs, and you verbally go and say them, tell the mission. Go read these 2000 gages here in this gas foiling acid pipe. Do perimeter security for me on this real estate you have arms and hands. Go open close these valves you're on a data center construction. Go build a digital twin for me
customer driven approach and expanding the skills of the system is one one by one, while maintaining a back end that uses all of this data across different tasks and across different embodiments and environments. But that's the physical AI layer that but it's one thing to to keep the system stable, totally another thing to embed it in the customer. And that's I think us overall, together as a community, will be, will be we'll be learning and as we deploy more and more robots. And then maybe a good visual example of it is, you remember I just showed you this a second ago. The robot walking on pallets and not falling down. That's a great thing, right? It's a reactive behavior. But when you go to customers, the first thing customers, that's amazing. I'm happy you showed me that. But can you please notice the other pallets? Right? People just don't do that. They see the palette, and that's where the operational AI comes in, that the story is now, okay, you maintain this stability, but how you look at that pallet stairs and everything and not all these are all from our customers today, around the world, in Japan, North of Japan, and Hokkaido to Europe, to the US. And all of these robots are operating in environments that are changing. No map needed, no predefined trajectories, no GPSs, and you verbally go and say them, tell the mission. Go read these 2000 gages here in this gas foiling acid pipe. Do perimeter security for me on this real estate you have arms and hands. Go open close these valves you're on a data center construction. Go build a digital twin for me
customer driven approach and expanding the skills of the system is one one by one, while maintaining a back end that uses all of this data across different tasks and across different embodiments and environments. But that's the physical AI layer that but it's one thing to to keep the system stable, totally another thing to embed it in the customer. And that's I think us overall, together as a community, will be, will be we'll be learning and as we deploy more and more robots. And then maybe a good visual example of it is, you remember I just showed you this a second ago. The robot walking on pallets and not falling down. That's a great thing, right? It's a reactive behavior. But when you go to customers, the first thing customers, that's amazing. I'm happy you showed me that. But can you please notice the other pallets? Right? People just don't do that. They see the palette, and that's where the operational AI comes in, that the story is now, okay, you maintain this stability, but how you look at that pallet stairs and everything and not all these are all from our customers today, around the world, in Japan, North of Japan, and Hokkaido to Europe, to the US. And all of these robots are operating in environments that are changing. No map needed, no predefined trajectories, no GPSs, and you verbally go and say them, tell the mission. Go read these 2000 gages here in this gas foiling acid pipe. Do perimeter security for me on this real estate you have arms and hands. Go open close these valves you're on a data center construction. Go build a digital twin for me
12:18and compare it to the
and compare it to the
and compare it to the
and compare it to the
S Speaker 112:19architect plan and tell me what has been deviated, what's not been built based on plan and so on. But that ability to do variety of different tasks across different environments with one model, without changing as you go to another environment, without asking customers to do any work, is what allows us to expand pretty quickly from one customer to another. I would pretty quickly from one customer to another. Again, we discussed a lot, we know that sort of as robotics, sort of community growing and evolvement. We deploy more robots. We heard sort of the POC, and I'm very happy that I would say we kind of passed that. Now we have got expansion partner. We started one robot, two robots. Now, okay, this customer, 150 of them deployed this, 100 this 130 and that's, I think, where we are starting to see an influx of data covered. And then very interesting part is this brain, if you build it correctly, you can deploy it on radically different vehicles. So this is now not a human, not a four legged robot. This is a robot we collect four terabytes of data per hour. It has 14 different robots, 50 different sensors, and it collects all of this multi modal data coming into the model. So of course, the size of model is a little bit bigger behind this. We have this lack of GPUs, but the last run we did, and again, this is not our plane. It's not a demo, it's not a POC, the customer in an environment that robot has not experienced before. Customer deployed. We went 36 miles, no map, no GPS, no satellite imagery over rocks, vegetation at the slopes and so on, 36 miles zero intervention we used to do with that similar customer. But two years ago, in a similar environment, we had 18 interventions for the same length of the trajectory. But in two years, the system, now zero human touch, was able to have what they said. We will again be announcing a little bit more. But you can imagine what will come after this. Maybe a human whether or dargo will come out all of this vehicle autonomously, just pass and go back, and that's sort of where this direction is. But I'll skip this one on the architecture given the time, but maybe just short to say, because there's a lot of technical people here to say that we depart from this model. So this is the conversation of the iPhone. If you have a very big, gigantic neural network, you pass the data conversations are being text, and it spits out the next
architect plan and tell me what has been deviated, what's not been built based on plan and so on. But that ability to do variety of different tasks across different environments with one model, without changing as you go to another environment, without asking customers to do any work, is what allows us to expand pretty quickly from one customer to another. I would pretty quickly from one customer to another. Again, we discussed a lot, we know that sort of as robotics, sort of community growing and evolvement. We deploy more robots. We heard sort of the POC, and I'm very happy that I would say we kind of passed that. Now we have got expansion partner. We started one robot, two robots. Now, okay, this customer, 150 of them deployed this, 100 this 130 and that's, I think, where we are starting to see an influx of data covered. And then very interesting part is this brain, if you build it correctly, you can deploy it on radically different vehicles. So this is now not a human, not a four legged robot. This is a robot we collect four terabytes of data per hour. It has 14 different robots, 50 different sensors, and it collects all of this multi modal data coming into the model. So of course, the size of model is a little bit bigger behind this. We have this lack of GPUs, but the last run we did, and again, this is not our plane. It's not a demo, it's not a POC, the customer in an environment that robot has not experienced before. Customer deployed. We went 36 miles, no map, no GPS, no satellite imagery over rocks, vegetation at the slopes and so on, 36 miles zero intervention we used to do with that similar customer. But two years ago, in a similar environment, we had 18 interventions for the same length of the trajectory. But in two years, the system, now zero human touch, was able to have what they said. We will again be announcing a little bit more. But you can imagine what will come after this. Maybe a human whether or dargo will come out all of this vehicle autonomously, just pass and go back, and that's sort of where this direction is. But I'll skip this one on the architecture given the time, but maybe just short to say, because there's a lot of technical people here to say that we depart from this model. So this is the conversation of the iPhone. If you have a very big, gigantic neural network, you pass the data conversations are being text, and it spits out the next
architect plan and tell me what has been deviated, what's not been built based on plan and so on. But that ability to do variety of different tasks across different environments with one model, without changing as you go to another environment, without asking customers to do any work, is what allows us to expand pretty quickly from one customer to another. I would pretty quickly from one customer to another. Again, we discussed a lot, we know that sort of as robotics, sort of community growing and evolvement. We deploy more robots. We heard sort of the POC, and I'm very happy that I would say we kind of passed that. Now we have got expansion partner. We started one robot, two robots. Now, okay, this customer, 150 of them deployed this, 100 this 130 and that's, I think, where we are starting to see an influx of data covered. And then very interesting part is this brain, if you build it correctly, you can deploy it on radically different vehicles. So this is now not a human, not a four legged robot. This is a robot we collect four terabytes of data per hour. It has 14 different robots, 50 different sensors, and it collects all of this multi modal data coming into the model. So of course, the size of model is a little bit bigger behind this. We have this lack of GPUs, but the last run we did, and again, this is not our plane. It's not a demo, it's not a POC, the customer in an environment that robot has not experienced before. Customer deployed. We went 36 miles, no map, no GPS, no satellite imagery over rocks, vegetation at the slopes and so on, 36 miles zero intervention we used to do with that similar customer. But two years ago, in a similar environment, we had 18 interventions for the same length of the trajectory. But in two years, the system, now zero human touch, was able to have what they said. We will again be announcing a little bit more. But you can imagine what will come after this. Maybe a human whether or dargo will come out all of this vehicle autonomously, just pass and go back, and that's sort of where this direction is. But I'll skip this one on the architecture given the time, but maybe just short to say, because there's a lot of technical people here to say that we depart from this model. So this is the conversation of the iPhone. If you have a very big, gigantic neural network, you pass the data conversations are being text, and it spits out the next
architect plan and tell me what has been deviated, what's not been built based on plan and so on. But that ability to do variety of different tasks across different environments with one model, without changing as you go to another environment, without asking customers to do any work, is what allows us to expand pretty quickly from one customer to another. I would pretty quickly from one customer to another. Again, we discussed a lot, we know that sort of as robotics, sort of community growing and evolvement. We deploy more robots. We heard sort of the POC, and I'm very happy that I would say we kind of passed that. Now we have got expansion partner. We started one robot, two robots. Now, okay, this customer, 150 of them deployed this, 100 this 130 and that's, I think, where we are starting to see an influx of data covered. And then very interesting part is this brain, if you build it correctly, you can deploy it on radically different vehicles. So this is now not a human, not a four legged robot. This is a robot we collect four terabytes of data per hour. It has 14 different robots, 50 different sensors, and it collects all of this multi modal data coming into the model. So of course, the size of model is a little bit bigger behind this. We have this lack of GPUs, but the last run we did, and again, this is not our plane. It's not a demo, it's not a POC, the customer in an environment that robot has not experienced before. Customer deployed. We went 36 miles, no map, no GPS, no satellite imagery over rocks, vegetation at the slopes and so on, 36 miles zero intervention we used to do with that similar customer. But two years ago, in a similar environment, we had 18 interventions for the same length of the trajectory. But in two years, the system, now zero human touch, was able to have what they said. We will again be announcing a little bit more. But you can imagine what will come after this. Maybe a human whether or dargo will come out all of this vehicle autonomously, just pass and go back, and that's sort of where this direction is. But I'll skip this one on the architecture given the time, but maybe just short to say, because there's a lot of technical people here to say that we depart from this model. So this is the conversation of the iPhone. If you have a very big, gigantic neural network, you pass the data conversations are being text, and it spits out the next
14:52sort of work. Here in robotics, you pass camera images,
sort of work. Here in robotics, you pass camera images,
sort of work. Here in robotics, you pass camera images,
sort of work. Here in robotics, you pass camera images,