Meeting: AI Platform Strategy Meeting
Mon, Jun 2
9:40 AM
35 min
Priyesh P
Apple ML Rotation and Multimodal Mo
URL: https://otter.ai/u/LcUj5MKLtPa6y-mSjek0GnBKOX8
Downloaded: 2025-12-22T10:03:39.296743
Method: text_extraction
============================================================

S Speaker 10:00He actually, I missed him first, and then he's the one who connected me with Neil, and then that's okay, okay. So, so I was working on ML platform technology, and I actually was in the Apple rotation, the ML rotation program back then. So I worked, I ended up on Krishnas team, which was the MLP team, working on, like a first party Michelangelo type system for Apple developers, essentially providing developers tools by which they, you know, move through the life cycle of any AI system, whether it's a computer vision system or whether it's an LLM system. At that time, it was less llms were like Transformers models, yeah, but, and so things have changed a lot. I spent a lot of time also working on the the vision Pro. So I worked in a few different capacities. One on multimodal models, which was, you know, can we use Siri plus your visual environment to answer questions? And that's was, that was more on the LLM side of things. Can we, you know, do multimodal models? And then I also worked, actually, on the hand gesture recognition models. So I was optimizing those models for the A and E. So we were, I mean, it was, how do we run that thing on device, super efficiently? And so I'm very familiar also with the pitch of running these ml models on edge and how Apple was. There was a whole mission at Apple, so, so it's cool to see also a lot of those, those people, you know, that theory team here working at Qualcomm now too. Yeah,
He actually, I missed him first, and then he's the one who connected me with Neil, and then that's okay, okay. So, so I was working on ML platform technology, and I actually was in the Apple rotation, the ML rotation program back then. So I worked, I ended up on Krishnas team, which was the MLP team, working on, like a first party Michelangelo type system for Apple developers, essentially providing developers tools by which they, you know, move through the life cycle of any AI system, whether it's a computer vision system or whether it's an LLM system. At that time, it was less llms were like Transformers models, yeah, but, and so things have changed a lot. I spent a lot of time also working on the the vision Pro. So I worked in a few different capacities. One on multimodal models, which was, you know, can we use Siri plus your visual environment to answer questions? And that's was, that was more on the LLM side of things. Can we, you know, do multimodal models? And then I also worked, actually, on the hand gesture recognition models. So I was optimizing those models for the A and E. So we were, I mean, it was, how do we run that thing on device, super efficiently? And so I'm very familiar also with the pitch of running these ml models on edge and how Apple was. There was a whole mission at Apple, so, so it's cool to see also a lot of those, those people, you know, that theory team here working at Qualcomm now too. Yeah,
He actually, I missed him first, and then he's the one who connected me with Neil, and then that's okay, okay. So, so I was working on ML platform technology, and I actually was in the Apple rotation, the ML rotation program back then. So I worked, I ended up on Krishnas team, which was the MLP team, working on, like a first party Michelangelo type system for Apple developers, essentially providing developers tools by which they, you know, move through the life cycle of any AI system, whether it's a computer vision system or whether it's an LLM system. At that time, it was less llms were like Transformers models, yeah, but, and so things have changed a lot. I spent a lot of time also working on the the vision Pro. So I worked in a few different capacities. One on multimodal models, which was, you know, can we use Siri plus your visual environment to answer questions? And that's was, that was more on the LLM side of things. Can we, you know, do multimodal models? And then I also worked, actually, on the hand gesture recognition models. So I was optimizing those models for the A and E. So we were, I mean, it was, how do we run that thing on device, super efficiently? And so I'm very familiar also with the pitch of running these ml models on edge and how Apple was. There was a whole mission at Apple, so, so it's cool to see also a lot of those, those people, you know, that theory team here working at Qualcomm now too. Yeah,
He actually, I missed him first, and then he's the one who connected me with Neil, and then that's okay, okay. So, so I was working on ML platform technology, and I actually was in the Apple rotation, the ML rotation program back then. So I worked, I ended up on Krishnas team, which was the MLP team, working on, like a first party Michelangelo type system for Apple developers, essentially providing developers tools by which they, you know, move through the life cycle of any AI system, whether it's a computer vision system or whether it's an LLM system. At that time, it was less llms were like Transformers models, yeah, but, and so things have changed a lot. I spent a lot of time also working on the the vision Pro. So I worked in a few different capacities. One on multimodal models, which was, you know, can we use Siri plus your visual environment to answer questions? And that's was, that was more on the LLM side of things. Can we, you know, do multimodal models? And then I also worked, actually, on the hand gesture recognition models. So I was optimizing those models for the A and E. So we were, I mean, it was, how do we run that thing on device, super efficiently? And so I'm very familiar also with the pitch of running these ml models on edge and how Apple was. There was a whole mission at Apple, so, so it's cool to see also a lot of those, those people, you know, that theory team here working at Qualcomm now too. Yeah,
1:36and it's the same vision.
and it's the same vision.
and it's the same vision.
and it's the same vision.
1:42Much. Further further ahead, absolutely.
Much. Further further ahead, absolutely.
Much. Further further ahead, absolutely.
Much. Further further ahead, absolutely.
S Speaker 21:48And then you fast forward another year, I think you'll start seeing a lot. Yeah,
And then you fast forward another year, I think you'll start seeing a lot. Yeah,
And then you fast forward another year, I think you'll start seeing a lot. Yeah,
And then you fast forward another year, I think you'll start seeing a lot. Yeah,
1:56super excited. So
7:54users would frequently
users would frequently
users would frequently
users would frequently
S Speaker 17:57try to game the system. So they would say, Hey, I have a, have a Doberman, and I want to be able to bring the Doberman to the to the unit and live there. And the listing, the real the landlord, has clearly said, We don't accept dogs, no pets. And so the chat bot would initially look at that response, saying, unfortunately, no pets are allowed. But here are some other listings you can look at. Then the person would say, Oh, but I really like this place, and I want to be able to come, you know, bring my dog, and then the system would be would respond, saying, Okay, I guess we can maybe make an exception. And then, so this would happen a lot, and piss off the landlords. So they needed a solution to this, and didn't really know, so they just described their problem in our like aI assistant, and it trickled it down into a test rule, which basically looked for sycophants. So it looked for sycophantic responses by their chatbot, and suddenly, in a day, they were able to catch all of those different scenarios in which this was happening and come up with a fix for it. So it's things like that where you don't you just want to describe what your problem is and have it kind of figured it out for you.
try to game the system. So they would say, Hey, I have a, have a Doberman, and I want to be able to bring the Doberman to the to the unit and live there. And the listing, the real the landlord, has clearly said, We don't accept dogs, no pets. And so the chat bot would initially look at that response, saying, unfortunately, no pets are allowed. But here are some other listings you can look at. Then the person would say, Oh, but I really like this place, and I want to be able to come, you know, bring my dog, and then the system would be would respond, saying, Okay, I guess we can maybe make an exception. And then, so this would happen a lot, and piss off the landlords. So they needed a solution to this, and didn't really know, so they just described their problem in our like aI assistant, and it trickled it down into a test rule, which basically looked for sycophants. So it looked for sycophantic responses by their chatbot, and suddenly, in a day, they were able to catch all of those different scenarios in which this was happening and come up with a fix for it. So it's things like that where you don't you just want to describe what your problem is and have it kind of figured it out for you.
try to game the system. So they would say, Hey, I have a, have a Doberman, and I want to be able to bring the Doberman to the to the unit and live there. And the listing, the real the landlord, has clearly said, We don't accept dogs, no pets. And so the chat bot would initially look at that response, saying, unfortunately, no pets are allowed. But here are some other listings you can look at. Then the person would say, Oh, but I really like this place, and I want to be able to come, you know, bring my dog, and then the system would be would respond, saying, Okay, I guess we can maybe make an exception. And then, so this would happen a lot, and piss off the landlords. So they needed a solution to this, and didn't really know, so they just described their problem in our like aI assistant, and it trickled it down into a test rule, which basically looked for sycophants. So it looked for sycophantic responses by their chatbot, and suddenly, in a day, they were able to catch all of those different scenarios in which this was happening and come up with a fix for it. So it's things like that where you don't you just want to describe what your problem is and have it kind of figured it out for you.
try to game the system. So they would say, Hey, I have a, have a Doberman, and I want to be able to bring the Doberman to the to the unit and live there. And the listing, the real the landlord, has clearly said, We don't accept dogs, no pets. And so the chat bot would initially look at that response, saying, unfortunately, no pets are allowed. But here are some other listings you can look at. Then the person would say, Oh, but I really like this place, and I want to be able to come, you know, bring my dog, and then the system would be would respond, saying, Okay, I guess we can maybe make an exception. And then, so this would happen a lot, and piss off the landlords. So they needed a solution to this, and didn't really know, so they just described their problem in our like aI assistant, and it trickled it down into a test rule, which basically looked for sycophants. So it looked for sycophantic responses by their chatbot, and suddenly, in a day, they were able to catch all of those different scenarios in which this was happening and come up with a fix for it. So it's things like that where you don't you just want to describe what your problem is and have it kind of figured it out for you.
S Speaker 29:02But single fancy is also, like, subjective, right? Like, in matters that are subjective, then how can but I get it like, pretty good. It's good
But single fancy is also, like, subjective, right? Like, in matters that are subjective, then how can but I get it like, pretty good. It's good
But single fancy is also, like, subjective, right? Like, in matters that are subjective, then how can but I get it like, pretty good. It's good
But single fancy is also, like, subjective, right? Like, in matters that are subjective, then how can but I get it like, pretty good. It's good
S Speaker 19:12at it, and it also it's because we also able to do better than us in many ways too. It's better, yes, exactly. It's better than us, and it has that continuous context, right? Like a human needs to be looking out for this all the time. But the system that you know on open air, we get user input, services, data points, flowing through, and it's able to kind of look at a lot of context and figure it out. So just giving you a sense of automation is one of our key pillars, as well as is not just being like an observability dashboard, where you can even see a bunch of graphs and data points and figure things out. So the current
at it, and it also it's because we also able to do better than us in many ways too. It's better, yes, exactly. It's better than us, and it has that continuous context, right? Like a human needs to be looking out for this all the time. But the system that you know on open air, we get user input, services, data points, flowing through, and it's able to kind of look at a lot of context and figure it out. So just giving you a sense of automation is one of our key pillars, as well as is not just being like an observability dashboard, where you can even see a bunch of graphs and data points and figure things out. So the current
at it, and it also it's because we also able to do better than us in many ways too. It's better, yes, exactly. It's better than us, and it has that continuous context, right? Like a human needs to be looking out for this all the time. But the system that you know on open air, we get user input, services, data points, flowing through, and it's able to kind of look at a lot of context and figure it out. So just giving you a sense of automation is one of our key pillars, as well as is not just being like an observability dashboard, where you can even see a bunch of graphs and data points and figure things out. So the current
at it, and it also it's because we also able to do better than us in many ways too. It's better, yes, exactly. It's better than us, and it has that continuous context, right? Like a human needs to be looking out for this all the time. But the system that you know on open air, we get user input, services, data points, flowing through, and it's able to kind of look at a lot of context and figure it out. So just giving you a sense of automation is one of our key pillars, as well as is not just being like an observability dashboard, where you can even see a bunch of graphs and data points and figure things out. So the current
S Speaker 29:49drive as you deploy with customers, like, what's there? You know, like, for example, in some cases, I don't know, how is the regulations in this evolving, which is, in some cases, they want a stamp of approval as well, right? Like, like the example would be, you know, McKinsey exists because, I mean, chat GPT can write a much better report than McKinsey can, but McKinsey still exists because executives at large companies want a stamp of McKinsey that hey, so that if anything goes wrong, we did our deed due to the genes between, yeah. So is there like a like, that's a need, right? So, so and then force and then like SOC two, compliance is a need, right?
drive as you deploy with customers, like, what's there? You know, like, for example, in some cases, I don't know, how is the regulations in this evolving, which is, in some cases, they want a stamp of approval as well, right? Like, like the example would be, you know, McKinsey exists because, I mean, chat GPT can write a much better report than McKinsey can, but McKinsey still exists because executives at large companies want a stamp of McKinsey that hey, so that if anything goes wrong, we did our deed due to the genes between, yeah. So is there like a like, that's a need, right? So, so and then force and then like SOC two, compliance is a need, right?
drive as you deploy with customers, like, what's there? You know, like, for example, in some cases, I don't know, how is the regulations in this evolving, which is, in some cases, they want a stamp of approval as well, right? Like, like the example would be, you know, McKinsey exists because, I mean, chat GPT can write a much better report than McKinsey can, but McKinsey still exists because executives at large companies want a stamp of McKinsey that hey, so that if anything goes wrong, we did our deed due to the genes between, yeah. So is there like a like, that's a need, right? So, so and then force and then like SOC two, compliance is a need, right?
drive as you deploy with customers, like, what's there? You know, like, for example, in some cases, I don't know, how is the regulations in this evolving, which is, in some cases, they want a stamp of approval as well, right? Like, like the example would be, you know, McKinsey exists because, I mean, chat GPT can write a much better report than McKinsey can, but McKinsey still exists because executives at large companies want a stamp of McKinsey that hey, so that if anything goes wrong, we did our deed due to the genes between, yeah. So is there like a like, that's a need, right? So, so and then force and then like SOC two, compliance is a need, right?
S Speaker 110:46That's a huge avenue of exploration for us as well compliance and regulation. We have another customer that's in the financial services sector, and they have done financial services you were asking about, what vertical, what industries you know, I have this pain point Services is a big one, because they actually have compliance requirements that, generally industries which have these compliance requirements ahead of, you know, the general world, they're the ones who adopt tools like this, and are the kind of like, it feels like cybersecurity requirements.
That's a huge avenue of exploration for us as well compliance and regulation. We have another customer that's in the financial services sector, and they have done financial services you were asking about, what vertical, what industries you know, I have this pain point Services is a big one, because they actually have compliance requirements that, generally industries which have these compliance requirements ahead of, you know, the general world, they're the ones who adopt tools like this, and are the kind of like, it feels like cybersecurity requirements.
That's a huge avenue of exploration for us as well compliance and regulation. We have another customer that's in the financial services sector, and they have done financial services you were asking about, what vertical, what industries you know, I have this pain point Services is a big one, because they actually have compliance requirements that, generally industries which have these compliance requirements ahead of, you know, the general world, they're the ones who adopt tools like this, and are the kind of like, it feels like cybersecurity requirements.
That's a huge avenue of exploration for us as well compliance and regulation. We have another customer that's in the financial services sector, and they have done financial services you were asking about, what vertical, what industries you know, I have this pain point Services is a big one, because they actually have compliance requirements that, generally industries which have these compliance requirements ahead of, you know, the general world, they're the ones who adopt tools like this, and are the kind of like, it feels like cybersecurity requirements.
S Speaker 211:22But is there, like, do they have some sort of, like, is there a common you know, there's a sock tool, like, SOC two is a common term, and everybody's got a universal complex. And then, is there a universal day?
But is there, like, do they have some sort of, like, is there a common you know, there's a sock tool, like, SOC two is a common term, and everybody's got a universal complex. And then, is there a universal day?
But is there, like, do they have some sort of, like, is there a common you know, there's a sock tool, like, SOC two is a common term, and everybody's got a universal complex. And then, is there a universal day?
But is there, like, do they have some sort of, like, is there a common you know, there's a sock tool, like, SOC two is a common term, and everybody's got a universal complex. And then, is there a universal day?
S Speaker 111:36Yeah, but it's happening back we just spoke to someone who's the head of the Texas what's the what's that
Yeah, but it's happening back we just spoke to someone who's the head of the Texas what's the what's that
Yeah, but it's happening back we just spoke to someone who's the head of the Texas what's the what's that
Yeah, but it's happening back we just spoke to someone who's the head of the Texas what's the what's that
11:43University of Texas Medical
University of Texas Medical
University of Texas Medical
University of Texas Medical
11:47and you want to know about it? Yeah.
and you want to know about it? Yeah.
and you want to know about it? Yeah.
and you want to know about it? Yeah.
S Speaker 112:50just things like the EU AI Act also, which we've been kind of combing through a bunch of the stuff, and a lot of it is a little bit vague and open to interpretation. But there are. It's essentially, how do you boil down massive AI regulations to a set of technical requirements? And so that's one of the things that we're also heavily focused on.
just things like the EU AI Act also, which we've been kind of combing through a bunch of the stuff, and a lot of it is a little bit vague and open to interpretation. But there are. It's essentially, how do you boil down massive AI regulations to a set of technical requirements? And so that's one of the things that we're also heavily focused on.
just things like the EU AI Act also, which we've been kind of combing through a bunch of the stuff, and a lot of it is a little bit vague and open to interpretation. But there are. It's essentially, how do you boil down massive AI regulations to a set of technical requirements? And so that's one of the things that we're also heavily focused on.
just things like the EU AI Act also, which we've been kind of combing through a bunch of the stuff, and a lot of it is a little bit vague and open to interpretation. But there are. It's essentially, how do you boil down massive AI regulations to a set of technical requirements? And so that's one of the things that we're also heavily focused on.
S Speaker 213:12Do you think that would be it's my assumption? I don't know if that that'll be true. That it's my assumption that, you know, like today, for any AI application that's getting deployed in an enterprise setting. At the very least, you need software compliance, right? Like, there's no way you're getting deployed in an enterprise without being software compliant. So the first thing everybody said, Hey, I use software. So they all rush to whatever is required to in order to get there, right? But there's got to be some form eventually, like, Hey, are you compliant with the, you know, some reliability. Ai, reliability. And then I guess at that point, because, because your sales cycles at that point significantly reduced, right? You're not educating. Do you educate? Do you have to educate the market today?
Do you think that would be it's my assumption? I don't know if that that'll be true. That it's my assumption that, you know, like today, for any AI application that's getting deployed in an enterprise setting. At the very least, you need software compliance, right? Like, there's no way you're getting deployed in an enterprise without being software compliant. So the first thing everybody said, Hey, I use software. So they all rush to whatever is required to in order to get there, right? But there's got to be some form eventually, like, Hey, are you compliant with the, you know, some reliability. Ai, reliability. And then I guess at that point, because, because your sales cycles at that point significantly reduced, right? You're not educating. Do you educate? Do you have to educate the market today?
Do you think that would be it's my assumption? I don't know if that that'll be true. That it's my assumption that, you know, like today, for any AI application that's getting deployed in an enterprise setting. At the very least, you need software compliance, right? Like, there's no way you're getting deployed in an enterprise without being software compliant. So the first thing everybody said, Hey, I use software. So they all rush to whatever is required to in order to get there, right? But there's got to be some form eventually, like, Hey, are you compliant with the, you know, some reliability. Ai, reliability. And then I guess at that point, because, because your sales cycles at that point significantly reduced, right? You're not educating. Do you educate? Do you have to educate the market today?
Do you think that would be it's my assumption? I don't know if that that'll be true. That it's my assumption that, you know, like today, for any AI application that's getting deployed in an enterprise setting. At the very least, you need software compliance, right? Like, there's no way you're getting deployed in an enterprise without being software compliant. So the first thing everybody said, Hey, I use software. So they all rush to whatever is required to in order to get there, right? But there's got to be some form eventually, like, Hey, are you compliant with the, you know, some reliability. Ai, reliability. And then I guess at that point, because, because your sales cycles at that point significantly reduced, right? You're not educating. Do you educate? Do you have to educate the market today?
S Speaker 114:05Well, so it depends on the second sector or segment of the market, financial services, I'm guessing. I guess we don't really have to educate them on the compliance requirements, but there's like desperately looking for tools to make the process of actually providing evidence easier. So with this virtual financial which is one of the financial services companies we work with, they use us to, like, disprove XYZ in the regulations that they need to adhere to. So that's kind of the approach we're taking. We what we're doing as well is kind of trying to provide these off the shelf bundles. So let's say you wanted to be compliant with some ISO framework. There is an ISO framework for AI right now, or you wanted to be, yeah, that is, it's ISO, and then forgotten what the number is. And then there's, you know, there's also things like OWASP, which is not compliance, but it's, you know, the OWASP cybersecurity, generally, it was in the software world, but now they have OWASP LLM guidelines as well. And so we're kind of packaging each of these different things and saying, yes, plug in your system, run these tests. Compliance. Perfect.
Well, so it depends on the second sector or segment of the market, financial services, I'm guessing. I guess we don't really have to educate them on the compliance requirements, but there's like desperately looking for tools to make the process of actually providing evidence easier. So with this virtual financial which is one of the financial services companies we work with, they use us to, like, disprove XYZ in the regulations that they need to adhere to. So that's kind of the approach we're taking. We what we're doing as well is kind of trying to provide these off the shelf bundles. So let's say you wanted to be compliant with some ISO framework. There is an ISO framework for AI right now, or you wanted to be, yeah, that is, it's ISO, and then forgotten what the number is. And then there's, you know, there's also things like OWASP, which is not compliance, but it's, you know, the OWASP cybersecurity, generally, it was in the software world, but now they have OWASP LLM guidelines as well. And so we're kind of packaging each of these different things and saying, yes, plug in your system, run these tests. Compliance. Perfect.
Well, so it depends on the second sector or segment of the market, financial services, I'm guessing. I guess we don't really have to educate them on the compliance requirements, but there's like desperately looking for tools to make the process of actually providing evidence easier. So with this virtual financial which is one of the financial services companies we work with, they use us to, like, disprove XYZ in the regulations that they need to adhere to. So that's kind of the approach we're taking. We what we're doing as well is kind of trying to provide these off the shelf bundles. So let's say you wanted to be compliant with some ISO framework. There is an ISO framework for AI right now, or you wanted to be, yeah, that is, it's ISO, and then forgotten what the number is. And then there's, you know, there's also things like OWASP, which is not compliance, but it's, you know, the OWASP cybersecurity, generally, it was in the software world, but now they have OWASP LLM guidelines as well. And so we're kind of packaging each of these different things and saying, yes, plug in your system, run these tests. Compliance. Perfect.
Well, so it depends on the second sector or segment of the market, financial services, I'm guessing. I guess we don't really have to educate them on the compliance requirements, but there's like desperately looking for tools to make the process of actually providing evidence easier. So with this virtual financial which is one of the financial services companies we work with, they use us to, like, disprove XYZ in the regulations that they need to adhere to. So that's kind of the approach we're taking. We what we're doing as well is kind of trying to provide these off the shelf bundles. So let's say you wanted to be compliant with some ISO framework. There is an ISO framework for AI right now, or you wanted to be, yeah, that is, it's ISO, and then forgotten what the number is. And then there's, you know, there's also things like OWASP, which is not compliance, but it's, you know, the OWASP cybersecurity, generally, it was in the software world, but now they have OWASP LLM guidelines as well. And so we're kind of packaging each of these different things and saying, yes, plug in your system, run these tests. Compliance. Perfect.
15:20Okay, so, so then
S Speaker 215:23Rishabh, like, how do you differentiate between the risers of the world, the fiddlers of the world? And then I think, is that Arthur, that New York based another company? Yeah,
Rishabh, like, how do you differentiate between the risers of the world, the fiddlers of the world? And then I think, is that Arthur, that New York based another company? Yeah,
Rishabh, like, how do you differentiate between the risers of the world, the fiddlers of the world? And then I think, is that Arthur, that New York based another company? Yeah,
Rishabh, like, how do you differentiate between the risers of the world, the fiddlers of the world? And then I think, is that Arthur, that New York based another company? Yeah,
S Speaker 115:40it's, it's these, it's these three things, right when you So the third thing that I didn't get to is also the user and developer experience. Yeah, one, you want multiple stakeholders, non technical stakeholders, to get involved in this process as well. And if you kind of gatekeep it around, being just for developers, or just for a certain, you know, data scientists, it's it doesn't create the use of universal field that enterprises tend to look for. So, so we this, this the reality. The honest answer is, AI is moving at light speed. We're able to keep up with the changes. So when multi modality comes around, multi modalities already come around, but when Gen AI came around, we didn't have to fundamentally rework our platform or create a new offering for Gen AI, and we it was just a national extension of our platform. Whereas if you look at arise and Arthur and all of these companies, what they had to do is essentially completely redefine what that platform looked like or create a separate offering like it arise Phoenix for llms. Yeah, that's one. There's there's so many different kind of pieces to this puzzle of what makes a software company differentiated. This is one of them. The the honest reality also is this is was so, like you said, we're so early in this game. Companies are now starting to realize the value and the importance of these concepts of observability evaluation, testing, and us kind of scrambling to get their act together. And so we haven't really seen we're not running into issues saying how you differentiate it with the rise in the market. We've run into it a lot when we talk to VCs, but not really in the market. That's the honest reality. Oh
it's, it's these, it's these three things, right when you So the third thing that I didn't get to is also the user and developer experience. Yeah, one, you want multiple stakeholders, non technical stakeholders, to get involved in this process as well. And if you kind of gatekeep it around, being just for developers, or just for a certain, you know, data scientists, it's it doesn't create the use of universal field that enterprises tend to look for. So, so we this, this the reality. The honest answer is, AI is moving at light speed. We're able to keep up with the changes. So when multi modality comes around, multi modalities already come around, but when Gen AI came around, we didn't have to fundamentally rework our platform or create a new offering for Gen AI, and we it was just a national extension of our platform. Whereas if you look at arise and Arthur and all of these companies, what they had to do is essentially completely redefine what that platform looked like or create a separate offering like it arise Phoenix for llms. Yeah, that's one. There's there's so many different kind of pieces to this puzzle of what makes a software company differentiated. This is one of them. The the honest reality also is this is was so, like you said, we're so early in this game. Companies are now starting to realize the value and the importance of these concepts of observability evaluation, testing, and us kind of scrambling to get their act together. And so we haven't really seen we're not running into issues saying how you differentiate it with the rise in the market. We've run into it a lot when we talk to VCs, but not really in the market. That's the honest reality. Oh
it's, it's these, it's these three things, right when you So the third thing that I didn't get to is also the user and developer experience. Yeah, one, you want multiple stakeholders, non technical stakeholders, to get involved in this process as well. And if you kind of gatekeep it around, being just for developers, or just for a certain, you know, data scientists, it's it doesn't create the use of universal field that enterprises tend to look for. So, so we this, this the reality. The honest answer is, AI is moving at light speed. We're able to keep up with the changes. So when multi modality comes around, multi modalities already come around, but when Gen AI came around, we didn't have to fundamentally rework our platform or create a new offering for Gen AI, and we it was just a national extension of our platform. Whereas if you look at arise and Arthur and all of these companies, what they had to do is essentially completely redefine what that platform looked like or create a separate offering like it arise Phoenix for llms. Yeah, that's one. There's there's so many different kind of pieces to this puzzle of what makes a software company differentiated. This is one of them. The the honest reality also is this is was so, like you said, we're so early in this game. Companies are now starting to realize the value and the importance of these concepts of observability evaluation, testing, and us kind of scrambling to get their act together. And so we haven't really seen we're not running into issues saying how you differentiate it with the rise in the market. We've run into it a lot when we talk to VCs, but not really in the market. That's the honest reality. Oh
it's, it's these, it's these three things, right when you So the third thing that I didn't get to is also the user and developer experience. Yeah, one, you want multiple stakeholders, non technical stakeholders, to get involved in this process as well. And if you kind of gatekeep it around, being just for developers, or just for a certain, you know, data scientists, it's it doesn't create the use of universal field that enterprises tend to look for. So, so we this, this the reality. The honest answer is, AI is moving at light speed. We're able to keep up with the changes. So when multi modality comes around, multi modalities already come around, but when Gen AI came around, we didn't have to fundamentally rework our platform or create a new offering for Gen AI, and we it was just a national extension of our platform. Whereas if you look at arise and Arthur and all of these companies, what they had to do is essentially completely redefine what that platform looked like or create a separate offering like it arise Phoenix for llms. Yeah, that's one. There's there's so many different kind of pieces to this puzzle of what makes a software company differentiated. This is one of them. The the honest reality also is this is was so, like you said, we're so early in this game. Companies are now starting to realize the value and the importance of these concepts of observability evaluation, testing, and us kind of scrambling to get their act together. And so we haven't really seen we're not running into issues saying how you differentiate it with the rise in the market. We've run into it a lot when we talk to VCs, but not really in the market. That's the honest reality. Oh
S Speaker 217:26I mean, you don't see them in the market because the market is big enough, and then you guys are all going after some places
I mean, you don't see them in the market because the market is big enough, and then you guys are all going after some places
I mean, you don't see them in the market because the market is big enough, and then you guys are all going after some places
I mean, you don't see them in the market because the market is big enough, and then you guys are all going after some places
S Speaker 117:36in the market. But it's often. It's often mean, so we had new bank is a good example of a customer that, yeah, was working with our eyes. And now we shouldn't be talking about a customer that we're almost at this end stages with, but on the cusp of closing the deal with them, and we're basically going to take over most of this so
in the market. But it's often. It's often mean, so we had new bank is a good example of a customer that, yeah, was working with our eyes. And now we shouldn't be talking about a customer that we're almost at this end stages with, but on the cusp of closing the deal with them, and we're basically going to take over most of this so
in the market. But it's often. It's often mean, so we had new bank is a good example of a customer that, yeah, was working with our eyes. And now we shouldn't be talking about a customer that we're almost at this end stages with, but on the cusp of closing the deal with them, and we're basically going to take over most of this so
in the market. But it's often. It's often mean, so we had new bank is a good example of a customer that, yeah, was working with our eyes. And now we shouldn't be talking about a customer that we're almost at this end stages with, but on the cusp of closing the deal with them, and we're basically going to take over most of this so
S Speaker 217:56and so, what's the like? Why did new bank is like a big customer? Yeah, so Nubank, why would? Why didn't new bank choose your platform versus why are they migrating? Yeah,
and so, what's the like? Why did new bank is like a big customer? Yeah, so Nubank, why would? Why didn't new bank choose your platform versus why are they migrating? Yeah,
and so, what's the like? Why did new bank is like a big customer? Yeah, so Nubank, why would? Why didn't new bank choose your platform versus why are they migrating? Yeah,
and so, what's the like? Why did new bank is like a big customer? Yeah, so Nubank, why would? Why didn't new bank choose your platform versus why are they migrating? Yeah,
S Speaker 118:08it's, it's a combination of factors. One is this kind of seamless continuum between data quality monitoring and ml ops, or ml monitoring and Gen AI and kind of a single place in a single platform that has it's also the framework that there really appeals to them, the test framework, the flexibility to deploy in a variety of different ways. So, so it's, it's a lot of different things that go into it. It's, it's ultimately a better product. I like to avoid saying that as much as possible, because it can feel subjective. But the reality is, it's a reality is, it's a better product. It's kind of like a it's an example of a product that has lots of features in it, but it doesn't it's not cohesive in a way that feels extensible, that feels you know, that feels like it's gonna live with you for as your needs grow. That's the
it's, it's a combination of factors. One is this kind of seamless continuum between data quality monitoring and ml ops, or ml monitoring and Gen AI and kind of a single place in a single platform that has it's also the framework that there really appeals to them, the test framework, the flexibility to deploy in a variety of different ways. So, so it's, it's a lot of different things that go into it. It's, it's ultimately a better product. I like to avoid saying that as much as possible, because it can feel subjective. But the reality is, it's a reality is, it's a better product. It's kind of like a it's an example of a product that has lots of features in it, but it doesn't it's not cohesive in a way that feels extensible, that feels you know, that feels like it's gonna live with you for as your needs grow. That's the
it's, it's a combination of factors. One is this kind of seamless continuum between data quality monitoring and ml ops, or ml monitoring and Gen AI and kind of a single place in a single platform that has it's also the framework that there really appeals to them, the test framework, the flexibility to deploy in a variety of different ways. So, so it's, it's a lot of different things that go into it. It's, it's ultimately a better product. I like to avoid saying that as much as possible, because it can feel subjective. But the reality is, it's a reality is, it's a better product. It's kind of like a it's an example of a product that has lots of features in it, but it doesn't it's not cohesive in a way that feels extensible, that feels you know, that feels like it's gonna live with you for as your needs grow. That's the
it's, it's a combination of factors. One is this kind of seamless continuum between data quality monitoring and ml ops, or ml monitoring and Gen AI and kind of a single place in a single platform that has it's also the framework that there really appeals to them, the test framework, the flexibility to deploy in a variety of different ways. So, so it's, it's a lot of different things that go into it. It's, it's ultimately a better product. I like to avoid saying that as much as possible, because it can feel subjective. But the reality is, it's a reality is, it's a better product. It's kind of like a it's an example of a product that has lots of features in it, but it doesn't it's not cohesive in a way that feels extensible, that feels you know, that feels like it's gonna live with you for as your needs grow. That's the
19:44concentric circles fast enough?
concentric circles fast enough?
concentric circles fast enough?
concentric circles fast enough?
21:38Yeah. I forgot about the name we leave, yeah, we
Yeah. I forgot about the name we leave, yeah, we
Yeah. I forgot about the name we leave, yeah, we
Yeah. I forgot about the name we leave, yeah, we
S Speaker 221:45and then they had, I don't know now, but like, you know, eight months, I think that it was only eight months ago when they had launched. We've and then, and then we've had started getting pretty good traction, but it was mostly because they were upselling with their existing customers. What's your like currently, from a sales cycle standpoint? How do you see this? Because I'm asking you this because I think, like I see this with other companies, some of your other competitors as well. That cracking that hasn't happened yet. The question is, when does it happen? And that's been my biggest question in this space, which is, how do you crack that? And maybe the answer is your product. And then we can dig in deeper as to how your product is superior in terms of automation, and then if it is superior, and maybe that's what it takes. Yeah,
and then they had, I don't know now, but like, you know, eight months, I think that it was only eight months ago when they had launched. We've and then, and then we've had started getting pretty good traction, but it was mostly because they were upselling with their existing customers. What's your like currently, from a sales cycle standpoint? How do you see this? Because I'm asking you this because I think, like I see this with other companies, some of your other competitors as well. That cracking that hasn't happened yet. The question is, when does it happen? And that's been my biggest question in this space, which is, how do you crack that? And maybe the answer is your product. And then we can dig in deeper as to how your product is superior in terms of automation, and then if it is superior, and maybe that's what it takes. Yeah,
and then they had, I don't know now, but like, you know, eight months, I think that it was only eight months ago when they had launched. We've and then, and then we've had started getting pretty good traction, but it was mostly because they were upselling with their existing customers. What's your like currently, from a sales cycle standpoint? How do you see this? Because I'm asking you this because I think, like I see this with other companies, some of your other competitors as well. That cracking that hasn't happened yet. The question is, when does it happen? And that's been my biggest question in this space, which is, how do you crack that? And maybe the answer is your product. And then we can dig in deeper as to how your product is superior in terms of automation, and then if it is superior, and maybe that's what it takes. Yeah,
and then they had, I don't know now, but like, you know, eight months, I think that it was only eight months ago when they had launched. We've and then, and then we've had started getting pretty good traction, but it was mostly because they were upselling with their existing customers. What's your like currently, from a sales cycle standpoint? How do you see this? Because I'm asking you this because I think, like I see this with other companies, some of your other competitors as well. That cracking that hasn't happened yet. The question is, when does it happen? And that's been my biggest question in this space, which is, how do you crack that? And maybe the answer is your product. And then we can dig in deeper as to how your product is superior in terms of automation, and then if it is superior, and maybe that's what it takes. Yeah,
22:41I think it's a good question. I I think that
I think it's a good question. I I think that
I think it's a good question. I I think that
I think it's a good question. I I think that
S Speaker 122:47it's a lot of its timing. I think a year or two ago, it's we've been doing this three and a half four years now, and so a year ago, it was much harder to sell than it is today. Well, there is a much bigger appetite right now than there has been in the past. And I think for us, we kind of approach it, we haven't like we're not a billion dollar company, and so we're still working our way towards that. But the way that we approach it is essentially,
it's a lot of its timing. I think a year or two ago, it's we've been doing this three and a half four years now, and so a year ago, it was much harder to sell than it is today. Well, there is a much bigger appetite right now than there has been in the past. And I think for us, we kind of approach it, we haven't like we're not a billion dollar company, and so we're still working our way towards that. But the way that we approach it is essentially,
it's a lot of its timing. I think a year or two ago, it's we've been doing this three and a half four years now, and so a year ago, it was much harder to sell than it is today. Well, there is a much bigger appetite right now than there has been in the past. And I think for us, we kind of approach it, we haven't like we're not a billion dollar company, and so we're still working our way towards that. But the way that we approach it is essentially,
it's a lot of its timing. I think a year or two ago, it's we've been doing this three and a half four years now, and so a year ago, it was much harder to sell than it is today. Well, there is a much bigger appetite right now than there has been in the past. And I think for us, we kind of approach it, we haven't like we're not a billion dollar company, and so we're still working our way towards that. But the way that we approach it is essentially,
S Speaker 223:15see, nobody got to a billion dollar revenue company in this space, let alone even $100 million company in this space, only company that got to that scale was scale AI, but
see, nobody got to a billion dollar revenue company in this space, let alone even $100 million company in this space, only company that got to that scale was scale AI, but
see, nobody got to a billion dollar revenue company in this space, let alone even $100 million company in this space, only company that got to that scale was scale AI, but
see, nobody got to a billion dollar revenue company in this space, let alone even $100 million company in this space, only company that got to that scale was scale AI, but
S Speaker 123:24it's timing. I mean, if you look at, if you look at scale, AI, the timing was perfect, right? Because they companies needed to, they needed to label data, so they were trained models and and so scale came in at the perfect time. And now that's a lot less relevant, and it's still relevant, but they have large business in that. But if you look at today, what the problem is, it's we have access to these extraordinary, powerful models off the shelf, and now it's a question of, how do we harness that as best as we can? How we ensure that shipping actually works well? So this is the problem of today. This is a big problem of today that enterprises across the world are facing. And so if we can, that's why, why right now, when we sell into organizations, it's it's weak years perk up, and it's unlike how it was about two, three years ago, where we had to explain and educate, listen, you're going to end up shipping a bunch of models. You're going to end up shipping a bunch of stuff. And it's stuff, and it's probably not going to work as expected, and then you're going to have to do rollbacks, and the velocity at which you get these models out the door is going to give you a huge edge over your competitors. So so many reasons why. Having a robust system by which you ensure the quality of AI ml systems that you're shipping matters, but it didn't resonate until today, until about the 678, months ago.
it's timing. I mean, if you look at, if you look at scale, AI, the timing was perfect, right? Because they companies needed to, they needed to label data, so they were trained models and and so scale came in at the perfect time. And now that's a lot less relevant, and it's still relevant, but they have large business in that. But if you look at today, what the problem is, it's we have access to these extraordinary, powerful models off the shelf, and now it's a question of, how do we harness that as best as we can? How we ensure that shipping actually works well? So this is the problem of today. This is a big problem of today that enterprises across the world are facing. And so if we can, that's why, why right now, when we sell into organizations, it's it's weak years perk up, and it's unlike how it was about two, three years ago, where we had to explain and educate, listen, you're going to end up shipping a bunch of models. You're going to end up shipping a bunch of stuff. And it's stuff, and it's probably not going to work as expected, and then you're going to have to do rollbacks, and the velocity at which you get these models out the door is going to give you a huge edge over your competitors. So so many reasons why. Having a robust system by which you ensure the quality of AI ml systems that you're shipping matters, but it didn't resonate until today, until about the 678, months ago.
it's timing. I mean, if you look at, if you look at scale, AI, the timing was perfect, right? Because they companies needed to, they needed to label data, so they were trained models and and so scale came in at the perfect time. And now that's a lot less relevant, and it's still relevant, but they have large business in that. But if you look at today, what the problem is, it's we have access to these extraordinary, powerful models off the shelf, and now it's a question of, how do we harness that as best as we can? How we ensure that shipping actually works well? So this is the problem of today. This is a big problem of today that enterprises across the world are facing. And so if we can, that's why, why right now, when we sell into organizations, it's it's weak years perk up, and it's unlike how it was about two, three years ago, where we had to explain and educate, listen, you're going to end up shipping a bunch of models. You're going to end up shipping a bunch of stuff. And it's stuff, and it's probably not going to work as expected, and then you're going to have to do rollbacks, and the velocity at which you get these models out the door is going to give you a huge edge over your competitors. So so many reasons why. Having a robust system by which you ensure the quality of AI ml systems that you're shipping matters, but it didn't resonate until today, until about the 678, months ago.
it's timing. I mean, if you look at, if you look at scale, AI, the timing was perfect, right? Because they companies needed to, they needed to label data, so they were trained models and and so scale came in at the perfect time. And now that's a lot less relevant, and it's still relevant, but they have large business in that. But if you look at today, what the problem is, it's we have access to these extraordinary, powerful models off the shelf, and now it's a question of, how do we harness that as best as we can? How we ensure that shipping actually works well? So this is the problem of today. This is a big problem of today that enterprises across the world are facing. And so if we can, that's why, why right now, when we sell into organizations, it's it's weak years perk up, and it's unlike how it was about two, three years ago, where we had to explain and educate, listen, you're going to end up shipping a bunch of models. You're going to end up shipping a bunch of stuff. And it's stuff, and it's probably not going to work as expected, and then you're going to have to do rollbacks, and the velocity at which you get these models out the door is going to give you a huge edge over your competitors. So so many reasons why. Having a robust system by which you ensure the quality of AI ml systems that you're shipping matters, but it didn't resonate until today, until about the 678, months ago.
S Speaker 124:56ARR right now is about a million and a half, okay? And our contract size is also growing wrong, so that's the nice thing. So we have, it's typically a little over six figure contracts, and that's part of the reason we're taking this platform approach, because we don't want to be a point solution in space. We don't want to be like one thing we want to say, Listen, you care about compliance, we're here. Care about observability. We're here. We care about CICD testing. We're here. Get about data quality monitoring. We're here. And you want to do this not just in the Gen AI world, but in your traditional ml world as well, which is still very relevant. Yeah, the world has moved on from that, and it's just Gen AI now we're here. We're here for all of these things.
ARR right now is about a million and a half, okay? And our contract size is also growing wrong, so that's the nice thing. So we have, it's typically a little over six figure contracts, and that's part of the reason we're taking this platform approach, because we don't want to be a point solution in space. We don't want to be like one thing we want to say, Listen, you care about compliance, we're here. Care about observability. We're here. We care about CICD testing. We're here. Get about data quality monitoring. We're here. And you want to do this not just in the Gen AI world, but in your traditional ml world as well, which is still very relevant. Yeah, the world has moved on from that, and it's just Gen AI now we're here. We're here for all of these things.
ARR right now is about a million and a half, okay? And our contract size is also growing wrong, so that's the nice thing. So we have, it's typically a little over six figure contracts, and that's part of the reason we're taking this platform approach, because we don't want to be a point solution in space. We don't want to be like one thing we want to say, Listen, you care about compliance, we're here. Care about observability. We're here. We care about CICD testing. We're here. Get about data quality monitoring. We're here. And you want to do this not just in the Gen AI world, but in your traditional ml world as well, which is still very relevant. Yeah, the world has moved on from that, and it's just Gen AI now we're here. We're here for all of these things.
ARR right now is about a million and a half, okay? And our contract size is also growing wrong, so that's the nice thing. So we have, it's typically a little over six figure contracts, and that's part of the reason we're taking this platform approach, because we don't want to be a point solution in space. We don't want to be like one thing we want to say, Listen, you care about compliance, we're here. Care about observability. We're here. We care about CICD testing. We're here. Get about data quality monitoring. We're here. And you want to do this not just in the Gen AI world, but in your traditional ml world as well, which is still very relevant. Yeah, the world has moved on from that, and it's just Gen AI now we're here. We're here for all of these things.
S Speaker 225:41yeah, so I think it's the, I agree with you. It's a timing thing,
yeah, so I think it's the, I agree with you. It's a timing thing,
yeah, so I think it's the, I agree with you. It's a timing thing,
yeah, so I think it's the, I agree with you. It's a timing thing,
S Speaker 225:50the time for this, like when I talk to so I've talked to many customers in this space, and so I talked to them about what's the biggest pain point right? In many ways, they describe this as a pain point, but they don't describe it as a because there's like five other pain points before this and then. So the question is, you know, the when their CFO has to approve or CIO has to approve budget for this, and how much budget, like, it's a
the time for this, like when I talk to so I've talked to many customers in this space, and so I talked to them about what's the biggest pain point right? In many ways, they describe this as a pain point, but they don't describe it as a because there's like five other pain points before this and then. So the question is, you know, the when their CFO has to approve or CIO has to approve budget for this, and how much budget, like, it's a
the time for this, like when I talk to so I've talked to many customers in this space, and so I talked to them about what's the biggest pain point right? In many ways, they describe this as a pain point, but they don't describe it as a because there's like five other pain points before this and then. So the question is, you know, the when their CFO has to approve or CIO has to approve budget for this, and how much budget, like, it's a
the time for this, like when I talk to so I've talked to many customers in this space, and so I talked to them about what's the biggest pain point right? In many ways, they describe this as a pain point, but they don't describe it as a because there's like five other pain points before this and then. So the question is, you know, the when their CFO has to approve or CIO has to approve budget for this, and how much budget, like, it's a
S Speaker 126:20it's a question of maturity. Also, like we generally tend to target companies, enterprises that have, like, a data story already in place, they've kind of cracked the because typically, if you look at what the number one priority problem is, we don't we haven't figured out our data pipelines yet, and that's kind of a mess, and that's where our focus is once that's in place, and you can actually do interesting things with that, then the question starts to become, how do we, how do we make sure that we're maximizing the potential of what we have, and so that we look for companies that are kind of coming out of The data, quality, maturation journey,
it's a question of maturity. Also, like we generally tend to target companies, enterprises that have, like, a data story already in place, they've kind of cracked the because typically, if you look at what the number one priority problem is, we don't we haven't figured out our data pipelines yet, and that's kind of a mess, and that's where our focus is once that's in place, and you can actually do interesting things with that, then the question starts to become, how do we, how do we make sure that we're maximizing the potential of what we have, and so that we look for companies that are kind of coming out of The data, quality, maturation journey,
it's a question of maturity. Also, like we generally tend to target companies, enterprises that have, like, a data story already in place, they've kind of cracked the because typically, if you look at what the number one priority problem is, we don't we haven't figured out our data pipelines yet, and that's kind of a mess, and that's where our focus is once that's in place, and you can actually do interesting things with that, then the question starts to become, how do we, how do we make sure that we're maximizing the potential of what we have, and so that we look for companies that are kind of coming out of The data, quality, maturation journey,
it's a question of maturity. Also, like we generally tend to target companies, enterprises that have, like, a data story already in place, they've kind of cracked the because typically, if you look at what the number one priority problem is, we don't we haven't figured out our data pipelines yet, and that's kind of a mess, and that's where our focus is once that's in place, and you can actually do interesting things with that, then the question starts to become, how do we, how do we make sure that we're maximizing the potential of what we have, and so that we look for companies that are kind of coming out of The data, quality, maturation journey,
27:01and do you what's the company? Do you see?
and do you what's the company? Do you see?
and do you what's the company? Do you see?
and do you what's the company? Do you see?
27:10Oh, fiddler, a lot. No,
Oh, fiddler, a lot. No,
Oh, fiddler, a lot. No,
Oh, fiddler, a lot. No,
S Speaker 127:14no. Fiddler has definitely lost a lot of steam. I think the biggest, I mean to tell you honestly, the biggest ones are biggest ones probably arise still from the ML ops lens landscape arise has got a sales army, and so they're good at go to market is strong. The product is not very strong, in my product opinion, but the go to market is fantastic. So they're good at weaponizing the sales people. But Fiddler has definitely died down a lot. I've not seen them show up a lot.
no. Fiddler has definitely lost a lot of steam. I think the biggest, I mean to tell you honestly, the biggest ones are biggest ones probably arise still from the ML ops lens landscape arise has got a sales army, and so they're good at go to market is strong. The product is not very strong, in my product opinion, but the go to market is fantastic. So they're good at weaponizing the sales people. But Fiddler has definitely died down a lot. I've not seen them show up a lot.
no. Fiddler has definitely lost a lot of steam. I think the biggest, I mean to tell you honestly, the biggest ones are biggest ones probably arise still from the ML ops lens landscape arise has got a sales army, and so they're good at go to market is strong. The product is not very strong, in my product opinion, but the go to market is fantastic. So they're good at weaponizing the sales people. But Fiddler has definitely died down a lot. I've not seen them show up a lot.
no. Fiddler has definitely lost a lot of steam. I think the biggest, I mean to tell you honestly, the biggest ones are biggest ones probably arise still from the ML ops lens landscape arise has got a sales army, and so they're good at go to market is strong. The product is not very strong, in my product opinion, but the go to market is fantastic. So they're good at weaponizing the sales people. But Fiddler has definitely died down a lot. I've not seen them show up a lot.
S Speaker 227:49Yeah, fiddler, I mean, there's others. There's one other New York based company.
Yeah, fiddler, I mean, there's others. There's one other New York based company.
Yeah, fiddler, I mean, there's others. There's one other New York based company.
Yeah, fiddler, I mean, there's others. There's one other New York based company.
27:53I forgot Arthur. Is New York based?
I forgot Arthur. Is New York based?
I forgot Arthur. Is New York based?
I forgot Arthur. Is New York based?
27:57Is that Arthur? No, that
Is that Arthur? No, that
Is that Arthur? No, that
Is that Arthur? No, that
28:02another Indian CEO.
28:06Is it a traditional ml ops player?
Is it a traditional ml ops player?
Is it a traditional ml ops player?
Is it a traditional ml ops player?
S Speaker 228:09LLM, ops. They only, they were born in this LM, I think they're only about a year and a half old, or a year
LLM, ops. They only, they were born in this LM, I think they're only about a year and a half old, or a year
LLM, ops. They only, they were born in this LM, I think they're only about a year and a half old, or a year
LLM, ops. They only, they were born in this LM, I think they're only about a year and a half old, or a year
S Speaker 128:16old. I forgot brain trust. They're not based out of New York. Which one brain trust, not brain trust. Anyways, yeah,
old. I forgot brain trust. They're not based out of New York. Which one brain trust, not brain trust. Anyways, yeah,
old. I forgot brain trust. They're not based out of New York. Which one brain trust, not brain trust. Anyways, yeah,
old. I forgot brain trust. They're not based out of New York. Which one brain trust, not brain trust. Anyways, yeah,
S Speaker 228:26I'm missing the name helicorn was another one. That was a YC company that was born in this your LLM observability. It's only LLM limited in scope. Yeah, it's a it's a small part of the host. But okay, so Neil, I'm guessing you, you're considering to partner for the
I'm missing the name helicorn was another one. That was a YC company that was born in this your LLM observability. It's only LLM limited in scope. Yeah, it's a it's a small part of the host. But okay, so Neil, I'm guessing you, you're considering to partner for the
I'm missing the name helicorn was another one. That was a YC company that was born in this your LLM observability. It's only LLM limited in scope. Yeah, it's a it's a small part of the host. But okay, so Neil, I'm guessing you, you're considering to partner for the
I'm missing the name helicorn was another one. That was a YC company that was born in this your LLM observability. It's only LLM limited in scope. Yeah, it's a it's a small part of the host. But okay, so Neil, I'm guessing you, you're considering to partner for the
S Speaker 428:49AI hub, yeah. So we're considering initially doing some comparisons on the pre quantized optimized model and the post quantized model, then thinking through what the observability looks like over time. But yeah, so that's like one thing for our external developers. I think there may be some additional pieces we can do for internal developers, but TBD right now, yeah.
AI hub, yeah. So we're considering initially doing some comparisons on the pre quantized optimized model and the post quantized model, then thinking through what the observability looks like over time. But yeah, so that's like one thing for our external developers. I think there may be some additional pieces we can do for internal developers, but TBD right now, yeah.
AI hub, yeah. So we're considering initially doing some comparisons on the pre quantized optimized model and the post quantized model, then thinking through what the observability looks like over time. But yeah, so that's like one thing for our external developers. I think there may be some additional pieces we can do for internal developers, but TBD right now, yeah.
AI hub, yeah. So we're considering initially doing some comparisons on the pre quantized optimized model and the post quantized model, then thinking through what the observability looks like over time. But yeah, so that's like one thing for our external developers. I think there may be some additional pieces we can do for internal developers, but TBD right now, yeah.
S Speaker 229:17I mean that, yeah. I think some sort of a partnership, if we can figure out, which is you, because all the developers that are deploying on these edge devices, if they want to monitor these models, then they can use open layers out of the box. That would be, I think that could be, is that what you're thinking?
I mean that, yeah. I think some sort of a partnership, if we can figure out, which is you, because all the developers that are deploying on these edge devices, if they want to monitor these models, then they can use open layers out of the box. That would be, I think that could be, is that what you're thinking?
I mean that, yeah. I think some sort of a partnership, if we can figure out, which is you, because all the developers that are deploying on these edge devices, if they want to monitor these models, then they can use open layers out of the box. That would be, I think that could be, is that what you're thinking?
I mean that, yeah. I think some sort of a partnership, if we can figure out, which is you, because all the developers that are deploying on these edge devices, if they want to monitor these models, then they can use open layers out of the box. That would be, I think that could be, is that what you're thinking?
S Speaker 429:41Yeah. I mean, if you think about what the platform looks like for us today, it's still, we take models in and we put models out, but the observability piece is something where, strategically, it's important for us to partner with folks like grish and so that that's where
Yeah. I mean, if you think about what the platform looks like for us today, it's still, we take models in and we put models out, but the observability piece is something where, strategically, it's important for us to partner with folks like grish and so that that's where
Yeah. I mean, if you think about what the platform looks like for us today, it's still, we take models in and we put models out, but the observability piece is something where, strategically, it's important for us to partner with folks like grish and so that that's where
Yeah. I mean, if you think about what the platform looks like for us today, it's still, we take models in and we put models out, but the observability piece is something where, strategically, it's important for us to partner with folks like grish and so that that's where
30:05philosophy is heading. And would like to see
philosophy is heading. And would like to see
philosophy is heading. And would like to see
philosophy is heading. And would like to see
S Speaker 430:10get a little bit more hope there in front of the views and things like that. Yeah.
get a little bit more hope there in front of the views and things like that. Yeah.
get a little bit more hope there in front of the views and things like that. Yeah.
get a little bit more hope there in front of the views and things like that. Yeah.
30:17Okay, that makes
S Speaker 230:25Let's see how we progress, like you said on this partnership end as well. Do
Let's see how we progress, like you said on this partnership end as well. Do
Let's see how we progress, like you said on this partnership end as well. Do
Let's see how we progress, like you said on this partnership end as well. Do
S Speaker 130:30you have thoughts? So one part of the reason that Neil had mentioned it could make sense to have this conversation is if there are other business units within Qualcomm that we could potentially POC, run a POC with for internal use, because Neil's team, they're not application developers, right? They're not building models themselves. They're there to kind of take in models and then ship them back with with, you know, performance guarantees and being able to run on your Qualcomm chips. So if there are, I'm presuming there are also teams within Qualcomm that are building models that have this need for ensuring model quality, and so
you have thoughts? So one part of the reason that Neil had mentioned it could make sense to have this conversation is if there are other business units within Qualcomm that we could potentially POC, run a POC with for internal use, because Neil's team, they're not application developers, right? They're not building models themselves. They're there to kind of take in models and then ship them back with with, you know, performance guarantees and being able to run on your Qualcomm chips. So if there are, I'm presuming there are also teams within Qualcomm that are building models that have this need for ensuring model quality, and so
you have thoughts? So one part of the reason that Neil had mentioned it could make sense to have this conversation is if there are other business units within Qualcomm that we could potentially POC, run a POC with for internal use, because Neil's team, they're not application developers, right? They're not building models themselves. They're there to kind of take in models and then ship them back with with, you know, performance guarantees and being able to run on your Qualcomm chips. So if there are, I'm presuming there are also teams within Qualcomm that are building models that have this need for ensuring model quality, and so
you have thoughts? So one part of the reason that Neil had mentioned it could make sense to have this conversation is if there are other business units within Qualcomm that we could potentially POC, run a POC with for internal use, because Neil's team, they're not application developers, right? They're not building models themselves. They're there to kind of take in models and then ship them back with with, you know, performance guarantees and being able to run on your Qualcomm chips. So if there are, I'm presuming there are also teams within Qualcomm that are building models that have this need for ensuring model quality, and so
S Speaker 231:07Rishabh Qualcomm is not your ideal customer. So the reason why is because we don't make money off of people running applications with like, your ideal customer is who's who really deeply cares about the quality of the model. What we care about is, yes, we care about the I mean, the parameters that we care about is tokens per second, power, performance. You know, those are the parameters that we care about we care less about, you know, observability, drift and all of those because that, those are problem statements that we don't understand. So our customers will understand the reason I we've tried this like when Qualcomm was already a weights and biases customer for experiment tracking and and then Lucas when he launched leave, and he's like, Hey, do you guys want to give this a shot? I'm going to make it available for free, right? So tried it with a bunch of teams, and essentially it wasn't like a lot of hunger to use those it's because of that reason. So your ideal customer profile is more people who are deploying like Qualcomm as a customer is less of a but Qualcomm as a partner across like, just not the even model hub, but even the like we sell to. Like, we have a big aipc business. So over there are we sell through. We don't just sell to the OEMs. We sell through. We have teams that are selling to enterprises, like we're selling to financial services. We're selling to banks, and we're selling to and then all of these customers, they want out of the box tools to run these models on device and do all sorts of things. So over there, there's a partnership opportunity, but Qualcomm as a customer, it'll be like a long sale, really long sales cycle. That
Rishabh Qualcomm is not your ideal customer. So the reason why is because we don't make money off of people running applications with like, your ideal customer is who's who really deeply cares about the quality of the model. What we care about is, yes, we care about the I mean, the parameters that we care about is tokens per second, power, performance. You know, those are the parameters that we care about we care less about, you know, observability, drift and all of those because that, those are problem statements that we don't understand. So our customers will understand the reason I we've tried this like when Qualcomm was already a weights and biases customer for experiment tracking and and then Lucas when he launched leave, and he's like, Hey, do you guys want to give this a shot? I'm going to make it available for free, right? So tried it with a bunch of teams, and essentially it wasn't like a lot of hunger to use those it's because of that reason. So your ideal customer profile is more people who are deploying like Qualcomm as a customer is less of a but Qualcomm as a partner across like, just not the even model hub, but even the like we sell to. Like, we have a big aipc business. So over there are we sell through. We don't just sell to the OEMs. We sell through. We have teams that are selling to enterprises, like we're selling to financial services. We're selling to banks, and we're selling to and then all of these customers, they want out of the box tools to run these models on device and do all sorts of things. So over there, there's a partnership opportunity, but Qualcomm as a customer, it'll be like a long sale, really long sales cycle. That
Rishabh Qualcomm is not your ideal customer. So the reason why is because we don't make money off of people running applications with like, your ideal customer is who's who really deeply cares about the quality of the model. What we care about is, yes, we care about the I mean, the parameters that we care about is tokens per second, power, performance. You know, those are the parameters that we care about we care less about, you know, observability, drift and all of those because that, those are problem statements that we don't understand. So our customers will understand the reason I we've tried this like when Qualcomm was already a weights and biases customer for experiment tracking and and then Lucas when he launched leave, and he's like, Hey, do you guys want to give this a shot? I'm going to make it available for free, right? So tried it with a bunch of teams, and essentially it wasn't like a lot of hunger to use those it's because of that reason. So your ideal customer profile is more people who are deploying like Qualcomm as a customer is less of a but Qualcomm as a partner across like, just not the even model hub, but even the like we sell to. Like, we have a big aipc business. So over there are we sell through. We don't just sell to the OEMs. We sell through. We have teams that are selling to enterprises, like we're selling to financial services. We're selling to banks, and we're selling to and then all of these customers, they want out of the box tools to run these models on device and do all sorts of things. So over there, there's a partnership opportunity, but Qualcomm as a customer, it'll be like a long sale, really long sales cycle. That
Rishabh Qualcomm is not your ideal customer. So the reason why is because we don't make money off of people running applications with like, your ideal customer is who's who really deeply cares about the quality of the model. What we care about is, yes, we care about the I mean, the parameters that we care about is tokens per second, power, performance. You know, those are the parameters that we care about we care less about, you know, observability, drift and all of those because that, those are problem statements that we don't understand. So our customers will understand the reason I we've tried this like when Qualcomm was already a weights and biases customer for experiment tracking and and then Lucas when he launched leave, and he's like, Hey, do you guys want to give this a shot? I'm going to make it available for free, right? So tried it with a bunch of teams, and essentially it wasn't like a lot of hunger to use those it's because of that reason. So your ideal customer profile is more people who are deploying like Qualcomm as a customer is less of a but Qualcomm as a partner across like, just not the even model hub, but even the like we sell to. Like, we have a big aipc business. So over there are we sell through. We don't just sell to the OEMs. We sell through. We have teams that are selling to enterprises, like we're selling to financial services. We're selling to banks, and we're selling to and then all of these customers, they want out of the box tools to run these models on device and do all sorts of things. So over there, there's a partnership opportunity, but Qualcomm as a customer, it'll be like a long sale, really long sales cycle. That
S Speaker 133:08makes sense. That's what my instinct as well was. But you never know. I mean, companies, yeah, that's right.
makes sense. That's what my instinct as well was. But you never know. I mean, companies, yeah, that's right.
makes sense. That's what my instinct as well was. But you never know. I mean, companies, yeah, that's right.
makes sense. That's what my instinct as well was. But you never know. I mean, companies, yeah, that's right.
S Speaker 233:13I mean, I think it's like Qualcomm type companies as a customer of observability tools. It's more of a like, when these things become like, further, you know, which is, there's compliance rules. Like, there's some, I don't know, you know, SOC five compliance for LLM observability, so there's some sort of a compliance requirement that everybody's got to have a stamp of approval for so then at that point, we will be heavily incentivized to get that stamp of approval to our customers that it's out of the box, it's ready, right but right now that's not
I mean, I think it's like Qualcomm type companies as a customer of observability tools. It's more of a like, when these things become like, further, you know, which is, there's compliance rules. Like, there's some, I don't know, you know, SOC five compliance for LLM observability, so there's some sort of a compliance requirement that everybody's got to have a stamp of approval for so then at that point, we will be heavily incentivized to get that stamp of approval to our customers that it's out of the box, it's ready, right but right now that's not
I mean, I think it's like Qualcomm type companies as a customer of observability tools. It's more of a like, when these things become like, further, you know, which is, there's compliance rules. Like, there's some, I don't know, you know, SOC five compliance for LLM observability, so there's some sort of a compliance requirement that everybody's got to have a stamp of approval for so then at that point, we will be heavily incentivized to get that stamp of approval to our customers that it's out of the box, it's ready, right but right now that's not
I mean, I think it's like Qualcomm type companies as a customer of observability tools. It's more of a like, when these things become like, further, you know, which is, there's compliance rules. Like, there's some, I don't know, you know, SOC five compliance for LLM observability, so there's some sort of a compliance requirement that everybody's got to have a stamp of approval for so then at that point, we will be heavily incentivized to get that stamp of approval to our customers that it's out of the box, it's ready, right but right now that's not
33:56there. That makes sense.
there. That makes sense.
there. That makes sense.
there. That makes sense.
S Speaker 133:57So what do you think so best next step for us here is Neil, we continue to work together on a plan to figure out a partnership angle here and potentially internal use as well, because you do have models that you guys are throwing into the model Zoo.
So what do you think so best next step for us here is Neil, we continue to work together on a plan to figure out a partnership angle here and potentially internal use as well, because you do have models that you guys are throwing into the model Zoo.
So what do you think so best next step for us here is Neil, we continue to work together on a plan to figure out a partnership angle here and potentially internal use as well, because you do have models that you guys are throwing into the model Zoo.
So what do you think so best next step for us here is Neil, we continue to work together on a plan to figure out a partnership angle here and potentially internal use as well, because you do have models that you guys are throwing into the model Zoo.
S Speaker 434:11Yeah, yeah. So we can, we can do that. Me, sir. Bob Megan, to get some more thoughts there, but I know we got that placeholder on Friday, so we can use that to kind of go through some feedback and then see what next steps are focused only mainly on AI.
Yeah, yeah. So we can, we can do that. Me, sir. Bob Megan, to get some more thoughts there, but I know we got that placeholder on Friday, so we can use that to kind of go through some feedback and then see what next steps are focused only mainly on AI.
Yeah, yeah. So we can, we can do that. Me, sir. Bob Megan, to get some more thoughts there, but I know we got that placeholder on Friday, so we can use that to kind of go through some feedback and then see what next steps are focused only mainly on AI.
Yeah, yeah. So we can, we can do that. Me, sir. Bob Megan, to get some more thoughts there, but I know we got that placeholder on Friday, so we can use that to kind of go through some feedback and then see what next steps are focused only mainly on AI.
S Speaker 134:33We can let it simmer and see where things take us. Then, yeah, all right, thank you. Thanks for sure. Nice meeting you. Thanks for setting this up pleasure. I seem to Priyesh my guns. You.
We can let it simmer and see where things take us. Then, yeah, all right, thank you. Thanks for sure. Nice meeting you. Thanks for setting this up pleasure. I seem to Priyesh my guns. You.
We can let it simmer and see where things take us. Then, yeah, all right, thank you. Thanks for sure. Nice meeting you. Thanks for setting this up pleasure. I seem to Priyesh my guns. You.
We can let it simmer and see where things take us. Then, yeah, all right, thank you. Thanks for sure. Nice meeting you. Thanks for setting this up pleasure. I seem to Priyesh my guns. You.
35:22I'm Andy sort of back. Delete message, but I have here, necessary.
I'm Andy sort of back. Delete message, but I have here, necessary.
I'm Andy sort of back. Delete message, but I have here, necessary.
I'm Andy sort of back. Delete message, but I have here, necessary.