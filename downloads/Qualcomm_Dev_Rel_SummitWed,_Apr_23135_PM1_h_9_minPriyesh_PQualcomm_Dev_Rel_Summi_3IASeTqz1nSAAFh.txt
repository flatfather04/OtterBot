Meeting: Qualcomm Dev Rel Summit
Wed, Apr 23
1:35 PM
1 h 9 min
Priyesh P
Qualcomm Dev Rel Summit: Introductio
URL: https://otter.ai/u/3IASeTqz1nSAAFhrTL0_GESPTds
Downloaded: 2025-12-22T11:39:17.661260
Method: text_extraction
============================================================

S Speaker 23:25so let's see. I think we're still getting this set up. Okay. So thank you very much to Rashan, to the team and for having us. Really appreciate it. They've been an excellent partner, and we actually co hosted an event at my previous company, Fauci, which was fantastic. So great to see the events continue here. It's pretty awesome.
so let's see. I think we're still getting this set up. Okay. So thank you very much to Rashan, to the team and for having us. Really appreciate it. They've been an excellent partner, and we actually co hosted an event at my previous company, Fauci, which was fantastic. So great to see the events continue here. It's pretty awesome.
so let's see. I think we're still getting this set up. Okay. So thank you very much to Rashan, to the team and for having us. Really appreciate it. They've been an excellent partner, and we actually co hosted an event at my previous company, Fauci, which was fantastic. So great to see the events continue here. It's pretty awesome.
so let's see. I think we're still getting this set up. Okay. So thank you very much to Rashan, to the team and for having us. Really appreciate it. They've been an excellent partner, and we actually co hosted an event at my previous company, Fauci, which was fantastic. So great to see the events continue here. It's pretty awesome.
S Speaker 312:09syncing with my calendar. Yes. Okay, great, yeah. So I saw some times and days during that email, I just want to make sure it's syncing with the calendar.
syncing with my calendar. Yes. Okay, great, yeah. So I saw some times and days during that email, I just want to make sure it's syncing with the calendar.
syncing with my calendar. Yes. Okay, great, yeah. So I saw some times and days during that email, I just want to make sure it's syncing with the calendar.
syncing with my calendar. Yes. Okay, great, yeah. So I saw some times and days during that email, I just want to make sure it's syncing with the calendar.
13:20Hello, everyone, Hi, I'm Mohit.
Hello, everyone, Hi, I'm Mohit.
Hello, everyone, Hi, I'm Mohit.
Hello, everyone, Hi, I'm Mohit.
S Speaker 413:25Here it is. Hi everyone. I'm take care of subject partnership, menu and coffee development test. We're going to talk about software test automation. But let me start with a flex code right here, and I promise I'll explain by the end of it, you will know why the OG tech CEO said what I said, lambdas is doing for test automation, what Kubernetes did for container orchestration, creating next level of efficiency around test automation so that people can actually focusing on testing versus Managing testing environments. Now, before we get into software testing, let's talk about today. I think since morning, we have heard so much such great things about AI software development. All of this is leading to a good conclusion that I think AI is going to do a lot of work for us, right, and we can get positive, relax, maybe go on board vacations, maybe do more what we love. Any sports fan in the room, yeah, so, because you're gonna get so much time, let's first talk about sports with me.
Here it is. Hi everyone. I'm take care of subject partnership, menu and coffee development test. We're going to talk about software test automation. But let me start with a flex code right here, and I promise I'll explain by the end of it, you will know why the OG tech CEO said what I said, lambdas is doing for test automation, what Kubernetes did for container orchestration, creating next level of efficiency around test automation so that people can actually focusing on testing versus Managing testing environments. Now, before we get into software testing, let's talk about today. I think since morning, we have heard so much such great things about AI software development. All of this is leading to a good conclusion that I think AI is going to do a lot of work for us, right, and we can get positive, relax, maybe go on board vacations, maybe do more what we love. Any sports fan in the room, yeah, so, because you're gonna get so much time, let's first talk about sports with me.
Here it is. Hi everyone. I'm take care of subject partnership, menu and coffee development test. We're going to talk about software test automation. But let me start with a flex code right here, and I promise I'll explain by the end of it, you will know why the OG tech CEO said what I said, lambdas is doing for test automation, what Kubernetes did for container orchestration, creating next level of efficiency around test automation so that people can actually focusing on testing versus Managing testing environments. Now, before we get into software testing, let's talk about today. I think since morning, we have heard so much such great things about AI software development. All of this is leading to a good conclusion that I think AI is going to do a lot of work for us, right, and we can get positive, relax, maybe go on board vacations, maybe do more what we love. Any sports fan in the room, yeah, so, because you're gonna get so much time, let's first talk about sports with me.
Here it is. Hi everyone. I'm take care of subject partnership, menu and coffee development test. We're going to talk about software test automation. But let me start with a flex code right here, and I promise I'll explain by the end of it, you will know why the OG tech CEO said what I said, lambdas is doing for test automation, what Kubernetes did for container orchestration, creating next level of efficiency around test automation so that people can actually focusing on testing versus Managing testing environments. Now, before we get into software testing, let's talk about today. I think since morning, we have heard so much such great things about AI software development. All of this is leading to a good conclusion that I think AI is going to do a lot of work for us, right, and we can get positive, relax, maybe go on board vacations, maybe do more what we love. Any sports fan in the room, yeah, so, because you're gonna get so much time, let's first talk about sports with me.
S Speaker 514:42Do more himself, changes the tire. Only four crew members, including the driver, are allowed to work on the car. It's the best time Holland stays in this scene, anxious to get away. Let's watch the
Do more himself, changes the tire. Only four crew members, including the driver, are allowed to work on the car. It's the best time Holland stays in this scene, anxious to get away. Let's watch the
Do more himself, changes the tire. Only four crew members, including the driver, are allowed to work on the car. It's the best time Holland stays in this scene, anxious to get away. Let's watch the
Do more himself, changes the tire. Only four crew members, including the driver, are allowed to work on the car. It's the best time Holland stays in this scene, anxious to get away. Let's watch the
S Speaker 515:07change the glass a grill man polish the windshield as Holland blooms away just 67 seconds after he stops you,
change the glass a grill man polish the windshield as Holland blooms away just 67 seconds after he stops you,
change the glass a grill man polish the windshield as Holland blooms away just 67 seconds after he stops you,
change the glass a grill man polish the windshield as Holland blooms away just 67 seconds after he stops you,
S Speaker 415:20it Okay, real quick in the room, you went through emotions. You did, yeah, maybe some of you were holding the nerves in the first case. But really quick, anybody what are the differences you saw in network scenarios? No technology, nothing, just little differences, like 18 people, not four people, fantastic. Love it more number of people, and maybe each of, each one of them were very, very special, with some special talent for a specific task. Great observation. Any other differences you saw in little scenarios? Sorry, power, power tools, exactly, fantastic. Speed, fast. Let's get things done fast. Let's release more features fast as of yesterday, right? This is what is happening exactly in software development as well, before I finally kind of switch gears to software development and testing, and in the last differences that you saw in the two scenarios anyway, any differences visible differences you saw in the two scenarios
it Okay, real quick in the room, you went through emotions. You did, yeah, maybe some of you were holding the nerves in the first case. But really quick, anybody what are the differences you saw in network scenarios? No technology, nothing, just little differences, like 18 people, not four people, fantastic. Love it more number of people, and maybe each of, each one of them were very, very special, with some special talent for a specific task. Great observation. Any other differences you saw in little scenarios? Sorry, power, power tools, exactly, fantastic. Speed, fast. Let's get things done fast. Let's release more features fast as of yesterday, right? This is what is happening exactly in software development as well, before I finally kind of switch gears to software development and testing, and in the last differences that you saw in the two scenarios anyway, any differences visible differences you saw in the two scenarios
it Okay, real quick in the room, you went through emotions. You did, yeah, maybe some of you were holding the nerves in the first case. But really quick, anybody what are the differences you saw in network scenarios? No technology, nothing, just little differences, like 18 people, not four people, fantastic. Love it more number of people, and maybe each of, each one of them were very, very special, with some special talent for a specific task. Great observation. Any other differences you saw in little scenarios? Sorry, power, power tools, exactly, fantastic. Speed, fast. Let's get things done fast. Let's release more features fast as of yesterday, right? This is what is happening exactly in software development as well, before I finally kind of switch gears to software development and testing, and in the last differences that you saw in the two scenarios anyway, any differences visible differences you saw in the two scenarios
it Okay, real quick in the room, you went through emotions. You did, yeah, maybe some of you were holding the nerves in the first case. But really quick, anybody what are the differences you saw in network scenarios? No technology, nothing, just little differences, like 18 people, not four people, fantastic. Love it more number of people, and maybe each of, each one of them were very, very special, with some special talent for a specific task. Great observation. Any other differences you saw in little scenarios? Sorry, power, power tools, exactly, fantastic. Speed, fast. Let's get things done fast. Let's release more features fast as of yesterday, right? This is what is happening exactly in software development as well, before I finally kind of switch gears to software development and testing, and in the last differences that you saw in the two scenarios anyway, any differences visible differences you saw in the two scenarios
16:34out here, another wardrobe, better wardrobe. Fantastic, it felt like
out here, another wardrobe, better wardrobe. Fantastic, it felt like
out here, another wardrobe, better wardrobe. Fantastic, it felt like
out here, another wardrobe, better wardrobe. Fantastic, it felt like
16:40and yeah, please go getting the tires off.
and yeah, please go getting the tires off.
and yeah, please go getting the tires off.
and yeah, please go getting the tires off.
S Speaker 426:37One is ease of access and utility of the platform, whether it's a developer or a automation engineer or a manual testers. I'll tell you a story. This is what a CTO told me that, hey, the question was, manual testers? Was the automation testers versus developers? Guess what they said. They said manual testers understand user behavior. Coders think code. Artificial people think code. Where does the business loss happens when the user experience, or intuitive user experience, is not captured in our test cases? So they said that we are going to bet big on manual testing in future. This is where products like can AI can actually equip them to move faster and intuitively break the test cases into the environment. So number one, that really is a trend, which I am seeing out there in the market. Second, everybody wants more tools, but they don't want a tooling suit. They don't want it. So they want different frameworks, but execution should happen on a unified environment, which is where platform really plays a big role. Use Appium, Selenium. Playwright. Any other feedback? But execution should happen on one, unified platform for two reasons, one by way. Second, insights are gold mine. When you do execution on isolated environment, you lose a single view of insights, and you have done it already in DevOps. It's time to just take that efficiency in test outs and last but not very security, very, very important. You all are doing private cloud deployment, also shipping SaaS, our solution can be used within your firewall. No testing data, or could go out outside your firewall. These three are the most important trends I have seen in testing, motivation and customers predict the digital
One is ease of access and utility of the platform, whether it's a developer or a automation engineer or a manual testers. I'll tell you a story. This is what a CTO told me that, hey, the question was, manual testers? Was the automation testers versus developers? Guess what they said. They said manual testers understand user behavior. Coders think code. Artificial people think code. Where does the business loss happens when the user experience, or intuitive user experience, is not captured in our test cases? So they said that we are going to bet big on manual testing in future. This is where products like can AI can actually equip them to move faster and intuitively break the test cases into the environment. So number one, that really is a trend, which I am seeing out there in the market. Second, everybody wants more tools, but they don't want a tooling suit. They don't want it. So they want different frameworks, but execution should happen on a unified environment, which is where platform really plays a big role. Use Appium, Selenium. Playwright. Any other feedback? But execution should happen on one, unified platform for two reasons, one by way. Second, insights are gold mine. When you do execution on isolated environment, you lose a single view of insights, and you have done it already in DevOps. It's time to just take that efficiency in test outs and last but not very security, very, very important. You all are doing private cloud deployment, also shipping SaaS, our solution can be used within your firewall. No testing data, or could go out outside your firewall. These three are the most important trends I have seen in testing, motivation and customers predict the digital
One is ease of access and utility of the platform, whether it's a developer or a automation engineer or a manual testers. I'll tell you a story. This is what a CTO told me that, hey, the question was, manual testers? Was the automation testers versus developers? Guess what they said. They said manual testers understand user behavior. Coders think code. Artificial people think code. Where does the business loss happens when the user experience, or intuitive user experience, is not captured in our test cases? So they said that we are going to bet big on manual testing in future. This is where products like can AI can actually equip them to move faster and intuitively break the test cases into the environment. So number one, that really is a trend, which I am seeing out there in the market. Second, everybody wants more tools, but they don't want a tooling suit. They don't want it. So they want different frameworks, but execution should happen on a unified environment, which is where platform really plays a big role. Use Appium, Selenium. Playwright. Any other feedback? But execution should happen on one, unified platform for two reasons, one by way. Second, insights are gold mine. When you do execution on isolated environment, you lose a single view of insights, and you have done it already in DevOps. It's time to just take that efficiency in test outs and last but not very security, very, very important. You all are doing private cloud deployment, also shipping SaaS, our solution can be used within your firewall. No testing data, or could go out outside your firewall. These three are the most important trends I have seen in testing, motivation and customers predict the digital
One is ease of access and utility of the platform, whether it's a developer or a automation engineer or a manual testers. I'll tell you a story. This is what a CTO told me that, hey, the question was, manual testers? Was the automation testers versus developers? Guess what they said. They said manual testers understand user behavior. Coders think code. Artificial people think code. Where does the business loss happens when the user experience, or intuitive user experience, is not captured in our test cases? So they said that we are going to bet big on manual testing in future. This is where products like can AI can actually equip them to move faster and intuitively break the test cases into the environment. So number one, that really is a trend, which I am seeing out there in the market. Second, everybody wants more tools, but they don't want a tooling suit. They don't want it. So they want different frameworks, but execution should happen on a unified environment, which is where platform really plays a big role. Use Appium, Selenium. Playwright. Any other feedback? But execution should happen on one, unified platform for two reasons, one by way. Second, insights are gold mine. When you do execution on isolated environment, you lose a single view of insights, and you have done it already in DevOps. It's time to just take that efficiency in test outs and last but not very security, very, very important. You all are doing private cloud deployment, also shipping SaaS, our solution can be used within your firewall. No testing data, or could go out outside your firewall. These three are the most important trends I have seen in testing, motivation and customers predict the digital
33:59to automotive at. AUc in some. our venture partners. So Hi. Thanks
to automotive at. AUc in some. our venture partners. So Hi. Thanks
to automotive at. AUc in some. our venture partners. So Hi. Thanks
to automotive at. AUc in some. our venture partners. So Hi. Thanks
45:23So here's another example here
So here's another example here
So here's another example here
So here's another example here
S Speaker 745:25where this is part of design, but we also can, I can also explain it in the deployment sense. So I'm pretty sure a lot of you here are familiar with the assisted labeling, right? It's the neat little UI where you just click on an object and then you can, very quickly, basically label a bunch of stuff, right, without human intervention. So we have that as well. But what we're doing in this sort of use case is leveraging the embeddings right that make this sort of assisted labeling possible. And we're sort of questioning, okay, how do you now use embeddings out in the deployment, right? So instead of just the number one sort of go to solution, which is like, Oh, you would use embeddings to just basically find a bunch of things, and then you would label them and create your model, right? But what if you trained your model and those things in the model are really good at detecting that particular thing, but when you're outside in the wild and you see something that is almost like that thing, but it's not exactly and the model is not able to pick it up with embeddings, you're able to sort of now have a mechanism where you can question, oh, like, this, is this thing that I'm trying to collect? Could be of interest, or it's very, very similar, but just so far away from the data distribution that the model is not able to detect something. So now you have this sort of compare and contrast mechanism where you can sort of answer a lot of open ended questions, like, Okay, if the model were in sort of a continuous learning environment, right? How do I get more data both that one particular thing, how do I continue to collect things of interest so that my model can continue to grow. And a lot of times, a lot of a lot of the customers come to us and say, and they say, oh, I want to fine tune. The problem with that is that customers think that they can manually come in with a very small data set of objects, and then they just basically train on just that small subset. What actually happens under the hood is that now you're just shifting the entire sort of weights and stuff that they learn from the model over to this new task. But users think that fine tuning allows you to accomplish both task A and task B very well. So they think it's adding capability instead of actually sort of shifting the capability. This sort of mechanism that we're exploring is really addressing being able to continuously add additional capability to the deployment as well, right? And also, this allows you to collect more data of that one thing, right, so that you can actually finally incorporate it into your model when you have time to retrain, redeploy, and so on so forth. I don't know why that was an empty slide, but this is the video that we're also sort of exploring in design as well, where this is sort of a Hello World example of how a user can sort of get started with creating a model. So let's say the video play. Oh no, so I'll just briefly explain it. So basically it's the ability for us to essentially partner with a lot of open source, open source images where we can grab them. So notably for this, for this demo, we basically partnered with Getty, and we're grabbing a lot of open source data online, right? Just publicly available images. And let's say you're a developer that wants to just get started, and you know that your application has the capability to or you need the capability to detect people, right? So what we've done here is sort of leveraging this assisted labeling and a little bit of natural language here where we just say, hey, detect people, or type in people. You'll look at a bunch of open source images, and then we'll use assisted labeling to just automatically go through all those images so that you basically have a starting point for your data set. And then you we basically take this data set, we throw it at some of our top performing models based on your requirements, and out comes this sort of singular model binary for you to deploy on your device. So of course, this is more of like a very beginner level, but you can see how a lot of our tools, the deeper and deeper you want to go, you can sort of unfold a lot of these layers, right and get more and more advanced. I think that's pretty much it for me. I'll hand the mic back to Graham to talk about partnerships.
where this is part of design, but we also can, I can also explain it in the deployment sense. So I'm pretty sure a lot of you here are familiar with the assisted labeling, right? It's the neat little UI where you just click on an object and then you can, very quickly, basically label a bunch of stuff, right, without human intervention. So we have that as well. But what we're doing in this sort of use case is leveraging the embeddings right that make this sort of assisted labeling possible. And we're sort of questioning, okay, how do you now use embeddings out in the deployment, right? So instead of just the number one sort of go to solution, which is like, Oh, you would use embeddings to just basically find a bunch of things, and then you would label them and create your model, right? But what if you trained your model and those things in the model are really good at detecting that particular thing, but when you're outside in the wild and you see something that is almost like that thing, but it's not exactly and the model is not able to pick it up with embeddings, you're able to sort of now have a mechanism where you can question, oh, like, this, is this thing that I'm trying to collect? Could be of interest, or it's very, very similar, but just so far away from the data distribution that the model is not able to detect something. So now you have this sort of compare and contrast mechanism where you can sort of answer a lot of open ended questions, like, Okay, if the model were in sort of a continuous learning environment, right? How do I get more data both that one particular thing, how do I continue to collect things of interest so that my model can continue to grow. And a lot of times, a lot of a lot of the customers come to us and say, and they say, oh, I want to fine tune. The problem with that is that customers think that they can manually come in with a very small data set of objects, and then they just basically train on just that small subset. What actually happens under the hood is that now you're just shifting the entire sort of weights and stuff that they learn from the model over to this new task. But users think that fine tuning allows you to accomplish both task A and task B very well. So they think it's adding capability instead of actually sort of shifting the capability. This sort of mechanism that we're exploring is really addressing being able to continuously add additional capability to the deployment as well, right? And also, this allows you to collect more data of that one thing, right, so that you can actually finally incorporate it into your model when you have time to retrain, redeploy, and so on so forth. I don't know why that was an empty slide, but this is the video that we're also sort of exploring in design as well, where this is sort of a Hello World example of how a user can sort of get started with creating a model. So let's say the video play. Oh no, so I'll just briefly explain it. So basically it's the ability for us to essentially partner with a lot of open source, open source images where we can grab them. So notably for this, for this demo, we basically partnered with Getty, and we're grabbing a lot of open source data online, right? Just publicly available images. And let's say you're a developer that wants to just get started, and you know that your application has the capability to or you need the capability to detect people, right? So what we've done here is sort of leveraging this assisted labeling and a little bit of natural language here where we just say, hey, detect people, or type in people. You'll look at a bunch of open source images, and then we'll use assisted labeling to just automatically go through all those images so that you basically have a starting point for your data set. And then you we basically take this data set, we throw it at some of our top performing models based on your requirements, and out comes this sort of singular model binary for you to deploy on your device. So of course, this is more of like a very beginner level, but you can see how a lot of our tools, the deeper and deeper you want to go, you can sort of unfold a lot of these layers, right and get more and more advanced. I think that's pretty much it for me. I'll hand the mic back to Graham to talk about partnerships.
where this is part of design, but we also can, I can also explain it in the deployment sense. So I'm pretty sure a lot of you here are familiar with the assisted labeling, right? It's the neat little UI where you just click on an object and then you can, very quickly, basically label a bunch of stuff, right, without human intervention. So we have that as well. But what we're doing in this sort of use case is leveraging the embeddings right that make this sort of assisted labeling possible. And we're sort of questioning, okay, how do you now use embeddings out in the deployment, right? So instead of just the number one sort of go to solution, which is like, Oh, you would use embeddings to just basically find a bunch of things, and then you would label them and create your model, right? But what if you trained your model and those things in the model are really good at detecting that particular thing, but when you're outside in the wild and you see something that is almost like that thing, but it's not exactly and the model is not able to pick it up with embeddings, you're able to sort of now have a mechanism where you can question, oh, like, this, is this thing that I'm trying to collect? Could be of interest, or it's very, very similar, but just so far away from the data distribution that the model is not able to detect something. So now you have this sort of compare and contrast mechanism where you can sort of answer a lot of open ended questions, like, Okay, if the model were in sort of a continuous learning environment, right? How do I get more data both that one particular thing, how do I continue to collect things of interest so that my model can continue to grow. And a lot of times, a lot of a lot of the customers come to us and say, and they say, oh, I want to fine tune. The problem with that is that customers think that they can manually come in with a very small data set of objects, and then they just basically train on just that small subset. What actually happens under the hood is that now you're just shifting the entire sort of weights and stuff that they learn from the model over to this new task. But users think that fine tuning allows you to accomplish both task A and task B very well. So they think it's adding capability instead of actually sort of shifting the capability. This sort of mechanism that we're exploring is really addressing being able to continuously add additional capability to the deployment as well, right? And also, this allows you to collect more data of that one thing, right, so that you can actually finally incorporate it into your model when you have time to retrain, redeploy, and so on so forth. I don't know why that was an empty slide, but this is the video that we're also sort of exploring in design as well, where this is sort of a Hello World example of how a user can sort of get started with creating a model. So let's say the video play. Oh no, so I'll just briefly explain it. So basically it's the ability for us to essentially partner with a lot of open source, open source images where we can grab them. So notably for this, for this demo, we basically partnered with Getty, and we're grabbing a lot of open source data online, right? Just publicly available images. And let's say you're a developer that wants to just get started, and you know that your application has the capability to or you need the capability to detect people, right? So what we've done here is sort of leveraging this assisted labeling and a little bit of natural language here where we just say, hey, detect people, or type in people. You'll look at a bunch of open source images, and then we'll use assisted labeling to just automatically go through all those images so that you basically have a starting point for your data set. And then you we basically take this data set, we throw it at some of our top performing models based on your requirements, and out comes this sort of singular model binary for you to deploy on your device. So of course, this is more of like a very beginner level, but you can see how a lot of our tools, the deeper and deeper you want to go, you can sort of unfold a lot of these layers, right and get more and more advanced. I think that's pretty much it for me. I'll hand the mic back to Graham to talk about partnerships.
where this is part of design, but we also can, I can also explain it in the deployment sense. So I'm pretty sure a lot of you here are familiar with the assisted labeling, right? It's the neat little UI where you just click on an object and then you can, very quickly, basically label a bunch of stuff, right, without human intervention. So we have that as well. But what we're doing in this sort of use case is leveraging the embeddings right that make this sort of assisted labeling possible. And we're sort of questioning, okay, how do you now use embeddings out in the deployment, right? So instead of just the number one sort of go to solution, which is like, Oh, you would use embeddings to just basically find a bunch of things, and then you would label them and create your model, right? But what if you trained your model and those things in the model are really good at detecting that particular thing, but when you're outside in the wild and you see something that is almost like that thing, but it's not exactly and the model is not able to pick it up with embeddings, you're able to sort of now have a mechanism where you can question, oh, like, this, is this thing that I'm trying to collect? Could be of interest, or it's very, very similar, but just so far away from the data distribution that the model is not able to detect something. So now you have this sort of compare and contrast mechanism where you can sort of answer a lot of open ended questions, like, Okay, if the model were in sort of a continuous learning environment, right? How do I get more data both that one particular thing, how do I continue to collect things of interest so that my model can continue to grow. And a lot of times, a lot of a lot of the customers come to us and say, and they say, oh, I want to fine tune. The problem with that is that customers think that they can manually come in with a very small data set of objects, and then they just basically train on just that small subset. What actually happens under the hood is that now you're just shifting the entire sort of weights and stuff that they learn from the model over to this new task. But users think that fine tuning allows you to accomplish both task A and task B very well. So they think it's adding capability instead of actually sort of shifting the capability. This sort of mechanism that we're exploring is really addressing being able to continuously add additional capability to the deployment as well, right? And also, this allows you to collect more data of that one thing, right, so that you can actually finally incorporate it into your model when you have time to retrain, redeploy, and so on so forth. I don't know why that was an empty slide, but this is the video that we're also sort of exploring in design as well, where this is sort of a Hello World example of how a user can sort of get started with creating a model. So let's say the video play. Oh no, so I'll just briefly explain it. So basically it's the ability for us to essentially partner with a lot of open source, open source images where we can grab them. So notably for this, for this demo, we basically partnered with Getty, and we're grabbing a lot of open source data online, right? Just publicly available images. And let's say you're a developer that wants to just get started, and you know that your application has the capability to or you need the capability to detect people, right? So what we've done here is sort of leveraging this assisted labeling and a little bit of natural language here where we just say, hey, detect people, or type in people. You'll look at a bunch of open source images, and then we'll use assisted labeling to just automatically go through all those images so that you basically have a starting point for your data set. And then you we basically take this data set, we throw it at some of our top performing models based on your requirements, and out comes this sort of singular model binary for you to deploy on your device. So of course, this is more of like a very beginner level, but you can see how a lot of our tools, the deeper and deeper you want to go, you can sort of unfold a lot of these layers, right and get more and more advanced. I think that's pretty much it for me. I'll hand the mic back to Graham to talk about partnerships.
S Speaker 649:44Thanks, awesome. Just to end that, we'll be here the rest of the day. Thanks. Like I said the beginning, we really only go to market or have solutions with partners, whether that's hardware partners, drone manufacturers, model providers, whatever it is. So one partner, come talk to me. Shout out to weights and biases and Microsoft, who are some of our partners right now on some of our projects, and that is Late in AI. You.
Thanks, awesome. Just to end that, we'll be here the rest of the day. Thanks. Like I said the beginning, we really only go to market or have solutions with partners, whether that's hardware partners, drone manufacturers, model providers, whatever it is. So one partner, come talk to me. Shout out to weights and biases and Microsoft, who are some of our partners right now on some of our projects, and that is Late in AI. You.
Thanks, awesome. Just to end that, we'll be here the rest of the day. Thanks. Like I said the beginning, we really only go to market or have solutions with partners, whether that's hardware partners, drone manufacturers, model providers, whatever it is. So one partner, come talk to me. Shout out to weights and biases and Microsoft, who are some of our partners right now on some of our projects, and that is Late in AI. You.
Thanks, awesome. Just to end that, we'll be here the rest of the day. Thanks. Like I said the beginning, we really only go to market or have solutions with partners, whether that's hardware partners, drone manufacturers, model providers, whatever it is. So one partner, come talk to me. Shout out to weights and biases and Microsoft, who are some of our partners right now on some of our projects, and that is Late in AI. You.
S Speaker 851:06Okay, hello, everyone again. My name is Mick. I'm from Microsoft. I have very low last name, but very short email. So if you need to reach out to just email, make@microsoft.com
Okay, hello, everyone again. My name is Mick. I'm from Microsoft. I have very low last name, but very short email. So if you need to reach out to just email, make@microsoft.com
Okay, hello, everyone again. My name is Mick. I'm from Microsoft. I have very low last name, but very short email. So if you need to reach out to just email, make@microsoft.com
Okay, hello, everyone again. My name is Mick. I'm from Microsoft. I have very low last name, but very short email. So if you need to reach out to just email, make@microsoft.com
51:17and it will come through.
and it will come through.
and it will come through.
and it will come through.
S Speaker 91:06:15So when we started the company in general, looking with the first concept and the idea of separating the stamps. We were like, Wait, we can apply this to run in real time. That's the dream, right? Imagine you're watching a soccer game, and now you can simply tune up or down the crowd. Let's suppose that you get disturbed about sound effects of a movie. Now you can tune down and adjust the volume to focus on the dialog. I've
So when we started the company in general, looking with the first concept and the idea of separating the stamps. We were like, Wait, we can apply this to run in real time. That's the dream, right? Imagine you're watching a soccer game, and now you can simply tune up or down the crowd. Let's suppose that you get disturbed about sound effects of a movie. Now you can tune down and adjust the volume to focus on the dialog. I've
So when we started the company in general, looking with the first concept and the idea of separating the stamps. We were like, Wait, we can apply this to run in real time. That's the dream, right? Imagine you're watching a soccer game, and now you can simply tune up or down the crowd. Let's suppose that you get disturbed about sound effects of a movie. Now you can tune down and adjust the volume to focus on the dialog. I've
So when we started the company in general, looking with the first concept and the idea of separating the stamps. We were like, Wait, we can apply this to run in real time. That's the dream, right? Imagine you're watching a soccer game, and now you can simply tune up or down the crowd. Let's suppose that you get disturbed about sound effects of a movie. Now you can tune down and adjust the volume to focus on the dialog. I've
1:06:47never heard anything like
never heard anything like
never heard anything like
never heard anything like
S Speaker 21:06:48voices was amazing when it wasn't live, but now that it's live, you know, just opens up so much innovation.
voices was amazing when it wasn't live, but now that it's live, you know, just opens up so much innovation.
voices was amazing when it wasn't live, but now that it's live, you know, just opens up so much innovation.
voices was amazing when it wasn't live, but now that it's live, you know, just opens up so much innovation.
1:06:56Voices has been a
S Speaker 71:06:58key player in the AI industry, and our AI models on the cloud has been state of the art for the past couple of years. With
key player in the AI industry, and our AI models on the cloud has been state of the art for the past couple of years. With
key player in the AI industry, and our AI models on the cloud has been state of the art for the past couple of years. With
key player in the AI industry, and our AI models on the cloud has been state of the art for the past couple of years. With
S Speaker 91:07:04these new generation of AI chips, it's not possible to run these AI models locally, doing that separation on the fly. The idea is to maximize the number of operations running on an NPU so it's faster. We can put better models, and it runs 35 times faster than the CPU.
these new generation of AI chips, it's not possible to run these AI models locally, doing that separation on the fly. The idea is to maximize the number of operations running on an NPU so it's faster. We can put better models, and it runs 35 times faster than the CPU.
these new generation of AI chips, it's not possible to run these AI models locally, doing that separation on the fly. The idea is to maximize the number of operations running on an NPU so it's faster. We can put better models, and it runs 35 times faster than the CPU.
these new generation of AI chips, it's not possible to run these AI models locally, doing that separation on the fly. The idea is to maximize the number of operations running on an NPU so it's faster. We can put better models, and it runs 35 times faster than the CPU.
S Speaker 21:07:21Let's play the song, and I'll move around some volumes, and you can see what happens in real time.
Let's play the song, and I'll move around some volumes, and you can see what happens in real time.
Let's play the song, and I'll move around some volumes, and you can see what happens in real time.
Let's play the song, and I'll move around some volumes, and you can see what happens in real time.
S Speaker 91:07:29We believe that this new technology will basically create a new world of possibilities. Consumer Electronics won't be the same. From pro audio to cars sound bar CVS, if there's audio, you can have our AI here. That's the beauty of it.
We believe that this new technology will basically create a new world of possibilities. Consumer Electronics won't be the same. From pro audio to cars sound bar CVS, if there's audio, you can have our AI here. That's the beauty of it.
We believe that this new technology will basically create a new world of possibilities. Consumer Electronics won't be the same. From pro audio to cars sound bar CVS, if there's audio, you can have our AI here. That's the beauty of it.
We believe that this new technology will basically create a new world of possibilities. Consumer Electronics won't be the same. From pro audio to cars sound bar CVS, if there's audio, you can have our AI here. That's the beauty of it.
S Speaker 21:07:44Imagine the implications for this. In any kind of audio environment that you're in, you can basically customize your own listening experience. I'm super excited about this. You
Imagine the implications for this. In any kind of audio environment that you're in, you can basically customize your own listening experience. I'm super excited about this. You
Imagine the implications for this. In any kind of audio environment that you're in, you can basically customize your own listening experience. I'm super excited about this. You
Imagine the implications for this. In any kind of audio environment that you're in, you can basically customize your own listening experience. I'm super excited about this. You
S Speaker 101:08:53We're going to take a quick break. There's coffee and some snacks in the back, and then we'll start up with meta at three. Thank you.
We're going to take a quick break. There's coffee and some snacks in the back, and then we'll start up with meta at three. Thank you.
We're going to take a quick break. There's coffee and some snacks in the back, and then we'll start up with meta at three. Thank you.
We're going to take a quick break. There's coffee and some snacks in the back, and then we'll start up with meta at three. Thank you.