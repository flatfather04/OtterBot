Meeting: Gatewayz AI
Wed, Dec 3
9:02 AM
23 min
Priyesh P
Decentralized AI project with crypto back end and en
URL: https://otter.ai/u/pCeAg8jkJ9l3uwujPmfF98ACjpg
Downloaded: 2025-12-21T19:23:36.018665
Method: text_extraction
============================================================

0:04Hey, hi on can you hear me?
Hey, hi on can you hear me?
Hey, hi on can you hear me?
Hey, hi on can you hear me?
S Speaker 10:07Yes, I can. Hey, that's it. Yes, I can hear you. That's a pretty cool retro PC at the back school.
Yes, I can. Hey, that's it. Yes, I can hear you. That's a pretty cool retro PC at the back school.
Yes, I can. Hey, that's it. Yes, I can hear you. That's a pretty cool retro PC at the back school.
Yes, I can. Hey, that's it. Yes, I can hear you. That's a pretty cool retro PC at the back school.
0:16The first cell phone. Well,
The first cell phone. Well,
The first cell phone. Well,
The first cell phone. Well,
S Speaker 10:19pretty cool one. One great to connect, and I know we had to move this one. So thanks a lot for the flexibility here. I'd love to understand a little bit more about alpaca. I don't think I have a lot of understanding today, but seems a very interesting concept. At the same time. I also want to sort of give this as a primer that crypto is slightly tight on our mandates, so I will, I'll take a look, but, but definitely have a chat with the team, but, but might be tighter for us as well.
pretty cool one. One great to connect, and I know we had to move this one. So thanks a lot for the flexibility here. I'd love to understand a little bit more about alpaca. I don't think I have a lot of understanding today, but seems a very interesting concept. At the same time. I also want to sort of give this as a primer that crypto is slightly tight on our mandates, so I will, I'll take a look, but, but definitely have a chat with the team, but, but might be tighter for us as well.
pretty cool one. One great to connect, and I know we had to move this one. So thanks a lot for the flexibility here. I'd love to understand a little bit more about alpaca. I don't think I have a lot of understanding today, but seems a very interesting concept. At the same time. I also want to sort of give this as a primer that crypto is slightly tight on our mandates, so I will, I'll take a look, but, but definitely have a chat with the team, but, but might be tighter for us as well.
pretty cool one. One great to connect, and I know we had to move this one. So thanks a lot for the flexibility here. I'd love to understand a little bit more about alpaca. I don't think I have a lot of understanding today, but seems a very interesting concept. At the same time. I also want to sort of give this as a primer that crypto is slightly tight on our mandates, so I will, I'll take a look, but, but definitely have a chat with the team, but, but might be tighter for us as well.
1:18Yes. Do all right, can you see my screen?
Yes. Do all right, can you see my screen?
Yes. Do all right, can you see my screen?
Yes. Do all right, can you see my screen?
S Speaker 11:25It's loading for now. Yes, it's up now. Okay, so,
It's loading for now. Yes, it's up now. Okay, so,
It's loading for now. Yes, it's up now. Okay, so,
It's loading for now. Yes, it's up now. Okay, so,
S Speaker 21:31so, yeah, so, so gateways is what we're building, and think of it as like a competitor to open router, if you're familiar with open so essentially, we're an inference aggregator. Okay, so we we bring all every single AI model that you can imagine into a single API from all of the different model providers, open AI anthropic, also the open source models like hugging face, yeah, and the and the other inference providers like open router into a single API and and what that means is we can offer the best prices and the best like latency everything, and you'll never really have to to integrate another AI model or or or application again, into your into your application, or, sorry, API, into your application. We also, what this enables us to do is offer up an enterprise chat GPT replacement, or an alternative that's self hosted. And the big difference here is that it's way less expensive. It's almost, you know, four or five times less expensive than what it costs for chatgpt, you get the exact same models, so the same kind of like quality and like intelligence and capabilities, as well as more models. So you can use the open source models. You can use the you know, like, let's say your legal team wants to use chatgpt, but your developers want to use Gemini, right? You can, you can make that happen. And then it's also through your own, sort of like self hosted interface, and it works, you know, seamlessly. So, I mean, this is kind of like the the context, the demand for inference, is exploding, doubling every three months. Also, there's just more and more open source models being available, coming online, and the demand for that is also increasing. If you look at the metrics about six months to a year ago, on a site like open router, the long tail of models was was roughly about 10% so the top 10 models accounted for over 90% of the inference. Today it's actually 30 or 40% so the long tail and all of these open source models are getting more and more popular, and that's something that we're really doubling down on in addition like so let's say, for instance, somebody is spending a lot of money on us with a specific model for, you know, a category of use cases. What we can do is we can work with them to fine tune a model and then drastically reduce their inference costs.
so, yeah, so, so gateways is what we're building, and think of it as like a competitor to open router, if you're familiar with open so essentially, we're an inference aggregator. Okay, so we we bring all every single AI model that you can imagine into a single API from all of the different model providers, open AI anthropic, also the open source models like hugging face, yeah, and the and the other inference providers like open router into a single API and and what that means is we can offer the best prices and the best like latency everything, and you'll never really have to to integrate another AI model or or or application again, into your into your application, or, sorry, API, into your application. We also, what this enables us to do is offer up an enterprise chat GPT replacement, or an alternative that's self hosted. And the big difference here is that it's way less expensive. It's almost, you know, four or five times less expensive than what it costs for chatgpt, you get the exact same models, so the same kind of like quality and like intelligence and capabilities, as well as more models. So you can use the open source models. You can use the you know, like, let's say your legal team wants to use chatgpt, but your developers want to use Gemini, right? You can, you can make that happen. And then it's also through your own, sort of like self hosted interface, and it works, you know, seamlessly. So, I mean, this is kind of like the the context, the demand for inference, is exploding, doubling every three months. Also, there's just more and more open source models being available, coming online, and the demand for that is also increasing. If you look at the metrics about six months to a year ago, on a site like open router, the long tail of models was was roughly about 10% so the top 10 models accounted for over 90% of the inference. Today it's actually 30 or 40% so the long tail and all of these open source models are getting more and more popular, and that's something that we're really doubling down on in addition like so let's say, for instance, somebody is spending a lot of money on us with a specific model for, you know, a category of use cases. What we can do is we can work with them to fine tune a model and then drastically reduce their inference costs.
so, yeah, so, so gateways is what we're building, and think of it as like a competitor to open router, if you're familiar with open so essentially, we're an inference aggregator. Okay, so we we bring all every single AI model that you can imagine into a single API from all of the different model providers, open AI anthropic, also the open source models like hugging face, yeah, and the and the other inference providers like open router into a single API and and what that means is we can offer the best prices and the best like latency everything, and you'll never really have to to integrate another AI model or or or application again, into your into your application, or, sorry, API, into your application. We also, what this enables us to do is offer up an enterprise chat GPT replacement, or an alternative that's self hosted. And the big difference here is that it's way less expensive. It's almost, you know, four or five times less expensive than what it costs for chatgpt, you get the exact same models, so the same kind of like quality and like intelligence and capabilities, as well as more models. So you can use the open source models. You can use the you know, like, let's say your legal team wants to use chatgpt, but your developers want to use Gemini, right? You can, you can make that happen. And then it's also through your own, sort of like self hosted interface, and it works, you know, seamlessly. So, I mean, this is kind of like the the context, the demand for inference, is exploding, doubling every three months. Also, there's just more and more open source models being available, coming online, and the demand for that is also increasing. If you look at the metrics about six months to a year ago, on a site like open router, the long tail of models was was roughly about 10% so the top 10 models accounted for over 90% of the inference. Today it's actually 30 or 40% so the long tail and all of these open source models are getting more and more popular, and that's something that we're really doubling down on in addition like so let's say, for instance, somebody is spending a lot of money on us with a specific model for, you know, a category of use cases. What we can do is we can work with them to fine tune a model and then drastically reduce their inference costs.
so, yeah, so, so gateways is what we're building, and think of it as like a competitor to open router, if you're familiar with open so essentially, we're an inference aggregator. Okay, so we we bring all every single AI model that you can imagine into a single API from all of the different model providers, open AI anthropic, also the open source models like hugging face, yeah, and the and the other inference providers like open router into a single API and and what that means is we can offer the best prices and the best like latency everything, and you'll never really have to to integrate another AI model or or or application again, into your into your application, or, sorry, API, into your application. We also, what this enables us to do is offer up an enterprise chat GPT replacement, or an alternative that's self hosted. And the big difference here is that it's way less expensive. It's almost, you know, four or five times less expensive than what it costs for chatgpt, you get the exact same models, so the same kind of like quality and like intelligence and capabilities, as well as more models. So you can use the open source models. You can use the you know, like, let's say your legal team wants to use chatgpt, but your developers want to use Gemini, right? You can, you can make that happen. And then it's also through your own, sort of like self hosted interface, and it works, you know, seamlessly. So, I mean, this is kind of like the the context, the demand for inference, is exploding, doubling every three months. Also, there's just more and more open source models being available, coming online, and the demand for that is also increasing. If you look at the metrics about six months to a year ago, on a site like open router, the long tail of models was was roughly about 10% so the top 10 models accounted for over 90% of the inference. Today it's actually 30 or 40% so the long tail and all of these open source models are getting more and more popular, and that's something that we're really doubling down on in addition like so let's say, for instance, somebody is spending a lot of money on us with a specific model for, you know, a category of use cases. What we can do is we can work with them to fine tune a model and then drastically reduce their inference costs.
S Speaker 14:13And when you say, work with them, the one is that, like, say, a service based model, or do you have same mlops kind of a platform for that.
And when you say, work with them, the one is that, like, say, a service based model, or do you have same mlops kind of a platform for that.
And when you say, work with them, the one is that, like, say, a service based model, or do you have same mlops kind of a platform for that.
And when you say, work with them, the one is that, like, say, a service based model, or do you have same mlops kind of a platform for that.
4:23So, okay, so we have,
So, okay, so we have,
So, okay, so we have,
So, okay, so we have,
S Speaker 24:26we have two products. So we have the the enterprise, like aI chat replacement. We're using open source software, like open web UI for the front end. And basically you can, like, plug in any model that you'd like, or any number of models that you would like, and it can either be self hosted or managed on our infrastructure. And then for the API we like. So the API is, you know, standard paper use or monthly subscription of like, you know, discounted volume based credits, and then for fine tuning we do right now, we are partnered up with like there's two different major providers that exist for this. One is called axelot, and the other is onslaught. So we're working with those two like products to be able to facilitate the fine tuning, and at this point, it's a service based offering, but we're working on making it more and more of a productized offering, where somebody where it can become self serve. But we're not there yet, right? Makes sense? Yeah. So we have, you know, 8000 plus models available on our platform. Like I said, you know, the really competitive in terms of pricing, and I think that's actually one of, like, the special, like, secret sauce in the back end. And that's where that, like, the benefit of crypto comes in, is that we actually, that's the reason why we're able to offer the lowest prices in the market, is because in the back end, what we've done is each model has token associated with it, and then anybody can buy and sell that token, and when they do that, they're they're paying a fee. They're paying a 1% fee. And so we use those fees, and it's actually quite extent like quite quite impressive how much, how much trading volume that generates. And so we can use those, those that fee to subsidize the cost of inference.
we have two products. So we have the the enterprise, like aI chat replacement. We're using open source software, like open web UI for the front end. And basically you can, like, plug in any model that you'd like, or any number of models that you would like, and it can either be self hosted or managed on our infrastructure. And then for the API we like. So the API is, you know, standard paper use or monthly subscription of like, you know, discounted volume based credits, and then for fine tuning we do right now, we are partnered up with like there's two different major providers that exist for this. One is called axelot, and the other is onslaught. So we're working with those two like products to be able to facilitate the fine tuning, and at this point, it's a service based offering, but we're working on making it more and more of a productized offering, where somebody where it can become self serve. But we're not there yet, right? Makes sense? Yeah. So we have, you know, 8000 plus models available on our platform. Like I said, you know, the really competitive in terms of pricing, and I think that's actually one of, like, the special, like, secret sauce in the back end. And that's where that, like, the benefit of crypto comes in, is that we actually, that's the reason why we're able to offer the lowest prices in the market, is because in the back end, what we've done is each model has token associated with it, and then anybody can buy and sell that token, and when they do that, they're they're paying a fee. They're paying a 1% fee. And so we use those fees, and it's actually quite extent like quite quite impressive how much, how much trading volume that generates. And so we can use those, those that fee to subsidize the cost of inference.
we have two products. So we have the the enterprise, like aI chat replacement. We're using open source software, like open web UI for the front end. And basically you can, like, plug in any model that you'd like, or any number of models that you would like, and it can either be self hosted or managed on our infrastructure. And then for the API we like. So the API is, you know, standard paper use or monthly subscription of like, you know, discounted volume based credits, and then for fine tuning we do right now, we are partnered up with like there's two different major providers that exist for this. One is called axelot, and the other is onslaught. So we're working with those two like products to be able to facilitate the fine tuning, and at this point, it's a service based offering, but we're working on making it more and more of a productized offering, where somebody where it can become self serve. But we're not there yet, right? Makes sense? Yeah. So we have, you know, 8000 plus models available on our platform. Like I said, you know, the really competitive in terms of pricing, and I think that's actually one of, like, the special, like, secret sauce in the back end. And that's where that, like, the benefit of crypto comes in, is that we actually, that's the reason why we're able to offer the lowest prices in the market, is because in the back end, what we've done is each model has token associated with it, and then anybody can buy and sell that token, and when they do that, they're they're paying a fee. They're paying a 1% fee. And so we use those fees, and it's actually quite extent like quite quite impressive how much, how much trading volume that generates. And so we can use those, those that fee to subsidize the cost of inference.
we have two products. So we have the the enterprise, like aI chat replacement. We're using open source software, like open web UI for the front end. And basically you can, like, plug in any model that you'd like, or any number of models that you would like, and it can either be self hosted or managed on our infrastructure. And then for the API we like. So the API is, you know, standard paper use or monthly subscription of like, you know, discounted volume based credits, and then for fine tuning we do right now, we are partnered up with like there's two different major providers that exist for this. One is called axelot, and the other is onslaught. So we're working with those two like products to be able to facilitate the fine tuning, and at this point, it's a service based offering, but we're working on making it more and more of a productized offering, where somebody where it can become self serve. But we're not there yet, right? Makes sense? Yeah. So we have, you know, 8000 plus models available on our platform. Like I said, you know, the really competitive in terms of pricing, and I think that's actually one of, like, the special, like, secret sauce in the back end. And that's where that, like, the benefit of crypto comes in, is that we actually, that's the reason why we're able to offer the lowest prices in the market, is because in the back end, what we've done is each model has token associated with it, and then anybody can buy and sell that token, and when they do that, they're they're paying a fee. They're paying a 1% fee. And so we use those fees, and it's actually quite extent like quite quite impressive how much, how much trading volume that generates. And so we can use those, those that fee to subsidize the cost of inference.
6:41thing that I've come across, and I've met so many companies,
thing that I've come across, and I've met so many companies,
thing that I've come across, and I've met so many companies,
thing that I've come across, and I've met so many companies,
S Speaker 26:47so the way that it works is basically here. Let me show you. Well, we've basically built an exchange, a decentralized exchange that's permissionless, so essentially anybody can go to that exchange, they can take a model from hugging face, and they can tokenize it. And essentially what they're doing is they're depositing the first cash into the token, and it goes through a bonding curve. Once it hits a certain amount of money deposited, then the token gets created. And I think what's also really important to mention here is that the that token is actually used to support the creators in the process. Like, basically, when the token gets created, we allocate a percentage of the supply to the token, sorry to the AI researcher for them to actually claim. And so what that means is that whereas before you know, creators and researchers would not have any benefit when they're when their models are used, now they actually get 3% of the profit that's generated from the in on the inference of their models that we sell through our platform.
so the way that it works is basically here. Let me show you. Well, we've basically built an exchange, a decentralized exchange that's permissionless, so essentially anybody can go to that exchange, they can take a model from hugging face, and they can tokenize it. And essentially what they're doing is they're depositing the first cash into the token, and it goes through a bonding curve. Once it hits a certain amount of money deposited, then the token gets created. And I think what's also really important to mention here is that the that token is actually used to support the creators in the process. Like, basically, when the token gets created, we allocate a percentage of the supply to the token, sorry to the AI researcher for them to actually claim. And so what that means is that whereas before you know, creators and researchers would not have any benefit when they're when their models are used, now they actually get 3% of the profit that's generated from the in on the inference of their models that we sell through our platform.
so the way that it works is basically here. Let me show you. Well, we've basically built an exchange, a decentralized exchange that's permissionless, so essentially anybody can go to that exchange, they can take a model from hugging face, and they can tokenize it. And essentially what they're doing is they're depositing the first cash into the token, and it goes through a bonding curve. Once it hits a certain amount of money deposited, then the token gets created. And I think what's also really important to mention here is that the that token is actually used to support the creators in the process. Like, basically, when the token gets created, we allocate a percentage of the supply to the token, sorry to the AI researcher for them to actually claim. And so what that means is that whereas before you know, creators and researchers would not have any benefit when they're when their models are used, now they actually get 3% of the profit that's generated from the in on the inference of their models that we sell through our platform.
so the way that it works is basically here. Let me show you. Well, we've basically built an exchange, a decentralized exchange that's permissionless, so essentially anybody can go to that exchange, they can take a model from hugging face, and they can tokenize it. And essentially what they're doing is they're depositing the first cash into the token, and it goes through a bonding curve. Once it hits a certain amount of money deposited, then the token gets created. And I think what's also really important to mention here is that the that token is actually used to support the creators in the process. Like, basically, when the token gets created, we allocate a percentage of the supply to the token, sorry to the AI researcher for them to actually claim. And so what that means is that whereas before you know, creators and researchers would not have any benefit when they're when their models are used, now they actually get 3% of the profit that's generated from the in on the inference of their models that we sell through our platform.
S Speaker 17:57So one does this only work for open source model in that case. And do you also have to then go on and take buy ins from all the AI creators for these models?
So one does this only work for open source model in that case. And do you also have to then go on and take buy ins from all the AI creators for these models?
So one does this only work for open source model in that case. And do you also have to then go on and take buy ins from all the AI creators for these models?
So one does this only work for open source model in that case. And do you also have to then go on and take buy ins from all the AI creators for these models?
S Speaker 28:07No, because the models are open source, they provide us a commercial license to use them. And in some cases, you know, some of those models are not commercially usable, but in most cases they are. So we don't need any buy in. But what we require is it's optional for them. So it's often so if they if they want to claim that royalty, then they have to verify through their hugging face account. But if they are not interested, then they're they just need to, like, they don't need to do anything, and they just like, basically that part of the percentage of the supply is burnt.
No, because the models are open source, they provide us a commercial license to use them. And in some cases, you know, some of those models are not commercially usable, but in most cases they are. So we don't need any buy in. But what we require is it's optional for them. So it's often so if they if they want to claim that royalty, then they have to verify through their hugging face account. But if they are not interested, then they're they just need to, like, they don't need to do anything, and they just like, basically that part of the percentage of the supply is burnt.
No, because the models are open source, they provide us a commercial license to use them. And in some cases, you know, some of those models are not commercially usable, but in most cases they are. So we don't need any buy in. But what we require is it's optional for them. So it's often so if they if they want to claim that royalty, then they have to verify through their hugging face account. But if they are not interested, then they're they just need to, like, they don't need to do anything, and they just like, basically that part of the percentage of the supply is burnt.
No, because the models are open source, they provide us a commercial license to use them. And in some cases, you know, some of those models are not commercially usable, but in most cases they are. So we don't need any buy in. But what we require is it's optional for them. So it's often so if they if they want to claim that royalty, then they have to verify through their hugging face account. But if they are not interested, then they're they just need to, like, they don't need to do anything, and they just like, basically that part of the percentage of the supply is burnt.
S Speaker 18:43And how about some some of the closed source models say, would open AI allow you to do this?
And how about some some of the closed source models say, would open AI allow you to do this?
And how about some some of the closed source models say, would open AI allow you to do this?
And how about some some of the closed source models say, would open AI allow you to do this?
S Speaker 28:49In the case of open AI, we are able like, yes, we can do this. The reason being, we're not actually tokenizing the model. What we're doing is we're tokenizing the revenue stream that we are generating through by selling access to the model. Yeah, so we're able to resell open AI inference. We're able to resell anthropic inference. So there's no reason why we wouldn't be able to do this
In the case of open AI, we are able like, yes, we can do this. The reason being, we're not actually tokenizing the model. What we're doing is we're tokenizing the revenue stream that we are generating through by selling access to the model. Yeah, so we're able to resell open AI inference. We're able to resell anthropic inference. So there's no reason why we wouldn't be able to do this
In the case of open AI, we are able like, yes, we can do this. The reason being, we're not actually tokenizing the model. What we're doing is we're tokenizing the revenue stream that we are generating through by selling access to the model. Yeah, so we're able to resell open AI inference. We're able to resell anthropic inference. So there's no reason why we wouldn't be able to do this
In the case of open AI, we are able like, yes, we can do this. The reason being, we're not actually tokenizing the model. What we're doing is we're tokenizing the revenue stream that we are generating through by selling access to the model. Yeah, so we're able to resell open AI inference. We're able to resell anthropic inference. So there's no reason why we wouldn't be able to do this
S Speaker 19:15interesting and so in that case, one is it fair to assume that the revenues are then tied back to the usage of gateways, AI's endpoint API. And so this is the marketplace and endpoint AI, seems like they would be tied up together, right? Yeah, super interesting. And how does the traction looks like? Look like today? And what kind of models are the most traded
interesting and so in that case, one is it fair to assume that the revenues are then tied back to the usage of gateways, AI's endpoint API. And so this is the marketplace and endpoint AI, seems like they would be tied up together, right? Yeah, super interesting. And how does the traction looks like? Look like today? And what kind of models are the most traded
interesting and so in that case, one is it fair to assume that the revenues are then tied back to the usage of gateways, AI's endpoint API. And so this is the marketplace and endpoint AI, seems like they would be tied up together, right? Yeah, super interesting. And how does the traction looks like? Look like today? And what kind of models are the most traded
interesting and so in that case, one is it fair to assume that the revenues are then tied back to the usage of gateways, AI's endpoint API. And so this is the marketplace and endpoint AI, seems like they would be tied up together, right? Yeah, super interesting. And how does the traction looks like? Look like today? And what kind of models are the most traded
S Speaker 29:43so we have, okay, so we have traction is we've got, we were working with over 20 different compute providers and partners we have on the Gateway enterprise chat, like, that's to me, like, that's our most lucrative revenue stream, because basically, you know, open AI sells access to enterprise for $50 a month per user. It's super expensive most users, most companies can't afford, you know, at a large scale, to roll that out to their entire user base, it adds up really, really fast. So we can offer, you know, something that that they can get for $50 we can sell the same product for $8 a month and still be make huge amount of money. So we're, we're right now, like, we really recently launched that. So we have three projects, three pilots right now with initial customers. But like sales, like is going really, really well, and that's moving fast. From a revenue standpoint, we are at 45,000 MRR, most of that is fees. So we're roughly generating those fees on it's about half a million in in volume that, that were that were that we're generating currently, and like some of the most popular models, mostly have been like deep seek. Deep seeks models have been quite popular and and there's a few others, but it's sort of like we've actually had one creator or one AI researcher who fine tuned their own custom Lora model for image generation, yeah, and they they've been working with us, and that's been a really strong partnership, and that's generated a lot of interest from the community.
so we have, okay, so we have traction is we've got, we were working with over 20 different compute providers and partners we have on the Gateway enterprise chat, like, that's to me, like, that's our most lucrative revenue stream, because basically, you know, open AI sells access to enterprise for $50 a month per user. It's super expensive most users, most companies can't afford, you know, at a large scale, to roll that out to their entire user base, it adds up really, really fast. So we can offer, you know, something that that they can get for $50 we can sell the same product for $8 a month and still be make huge amount of money. So we're, we're right now, like, we really recently launched that. So we have three projects, three pilots right now with initial customers. But like sales, like is going really, really well, and that's moving fast. From a revenue standpoint, we are at 45,000 MRR, most of that is fees. So we're roughly generating those fees on it's about half a million in in volume that, that were that were that we're generating currently, and like some of the most popular models, mostly have been like deep seek. Deep seeks models have been quite popular and and there's a few others, but it's sort of like we've actually had one creator or one AI researcher who fine tuned their own custom Lora model for image generation, yeah, and they they've been working with us, and that's been a really strong partnership, and that's generated a lot of interest from the community.
so we have, okay, so we have traction is we've got, we were working with over 20 different compute providers and partners we have on the Gateway enterprise chat, like, that's to me, like, that's our most lucrative revenue stream, because basically, you know, open AI sells access to enterprise for $50 a month per user. It's super expensive most users, most companies can't afford, you know, at a large scale, to roll that out to their entire user base, it adds up really, really fast. So we can offer, you know, something that that they can get for $50 we can sell the same product for $8 a month and still be make huge amount of money. So we're, we're right now, like, we really recently launched that. So we have three projects, three pilots right now with initial customers. But like sales, like is going really, really well, and that's moving fast. From a revenue standpoint, we are at 45,000 MRR, most of that is fees. So we're roughly generating those fees on it's about half a million in in volume that, that were that were that we're generating currently, and like some of the most popular models, mostly have been like deep seek. Deep seeks models have been quite popular and and there's a few others, but it's sort of like we've actually had one creator or one AI researcher who fine tuned their own custom Lora model for image generation, yeah, and they they've been working with us, and that's been a really strong partnership, and that's generated a lot of interest from the community.
so we have, okay, so we have traction is we've got, we were working with over 20 different compute providers and partners we have on the Gateway enterprise chat, like, that's to me, like, that's our most lucrative revenue stream, because basically, you know, open AI sells access to enterprise for $50 a month per user. It's super expensive most users, most companies can't afford, you know, at a large scale, to roll that out to their entire user base, it adds up really, really fast. So we can offer, you know, something that that they can get for $50 we can sell the same product for $8 a month and still be make huge amount of money. So we're, we're right now, like, we really recently launched that. So we have three projects, three pilots right now with initial customers. But like sales, like is going really, really well, and that's moving fast. From a revenue standpoint, we are at 45,000 MRR, most of that is fees. So we're roughly generating those fees on it's about half a million in in volume that, that were that were that we're generating currently, and like some of the most popular models, mostly have been like deep seek. Deep seeks models have been quite popular and and there's a few others, but it's sort of like we've actually had one creator or one AI researcher who fine tuned their own custom Lora model for image generation, yeah, and they they've been working with us, and that's been a really strong partnership, and that's generated a lot of interest from the community.
S Speaker 111:36One super interesting space, for sure. I'm not sure how Qualcomm would look at it being a corporate venture fund, but certainly quite intriguing. How are you thinking about go to market, right? It to me, it seems like there should. There would be a lot of community building, a lot of education to the market as well. How are you thinking about that?
One super interesting space, for sure. I'm not sure how Qualcomm would look at it being a corporate venture fund, but certainly quite intriguing. How are you thinking about go to market, right? It to me, it seems like there should. There would be a lot of community building, a lot of education to the market as well. How are you thinking about that?
One super interesting space, for sure. I'm not sure how Qualcomm would look at it being a corporate venture fund, but certainly quite intriguing. How are you thinking about go to market, right? It to me, it seems like there should. There would be a lot of community building, a lot of education to the market as well. How are you thinking about that?
One super interesting space, for sure. I'm not sure how Qualcomm would look at it being a corporate venture fund, but certainly quite intriguing. How are you thinking about go to market, right? It to me, it seems like there should. There would be a lot of community building, a lot of education to the market as well. How are you thinking about that?
S Speaker 114:27right now. Got it and for now, the API has say, 8000 models, or something that you mentioned, and there is no logic behind it today. Mostly it's on the user or the app developer to select the models that they want to pull out of the API. Yeah.
right now. Got it and for now, the API has say, 8000 models, or something that you mentioned, and there is no logic behind it today. Mostly it's on the user or the app developer to select the models that they want to pull out of the API. Yeah.
right now. Got it and for now, the API has say, 8000 models, or something that you mentioned, and there is no logic behind it today. Mostly it's on the user or the app developer to select the models that they want to pull out of the API. Yeah.
right now. Got it and for now, the API has say, 8000 models, or something that you mentioned, and there is no logic behind it today. Mostly it's on the user or the app developer to select the models that they want to pull out of the API. Yeah.
S Speaker 214:44So yeah, exactly they can. They can choose to optimize by price or by performance, like latency. So if we go to the models page here, they can decide in the API they can specify, okay, I want to like for this particular category, I want to be able to like, choose a model that is the fastest model, or the, you know, highest performance or lowest cost. So they excuse me, they can choose to do that on their end. And you can see here, like they have all the ability to, you know, filter by the different model types, different output formats they can choose. There are free models available that they can choose from. There's we also have, like, we recently brought on a provider that offers private models, private AI, so it's like, you know, fully secure, end to end. Encrypted inference that you know, you'll never be able to they'll never be able to read what what they're saying. Context window, all of these, like, different, you know, parameters, like tools or reasoning. All of that is available here for that like and this is accessible through the API. So, yeah, you can see our the providers that we have available here. So that's kind of been our strategy. We're offering. We offer up the ability for them to dynamically choose through the API right now, or they can. They can hard code and specify a specific model.
So yeah, exactly they can. They can choose to optimize by price or by performance, like latency. So if we go to the models page here, they can decide in the API they can specify, okay, I want to like for this particular category, I want to be able to like, choose a model that is the fastest model, or the, you know, highest performance or lowest cost. So they excuse me, they can choose to do that on their end. And you can see here, like they have all the ability to, you know, filter by the different model types, different output formats they can choose. There are free models available that they can choose from. There's we also have, like, we recently brought on a provider that offers private models, private AI, so it's like, you know, fully secure, end to end. Encrypted inference that you know, you'll never be able to they'll never be able to read what what they're saying. Context window, all of these, like, different, you know, parameters, like tools or reasoning. All of that is available here for that like and this is accessible through the API. So, yeah, you can see our the providers that we have available here. So that's kind of been our strategy. We're offering. We offer up the ability for them to dynamically choose through the API right now, or they can. They can hard code and specify a specific model.
So yeah, exactly they can. They can choose to optimize by price or by performance, like latency. So if we go to the models page here, they can decide in the API they can specify, okay, I want to like for this particular category, I want to be able to like, choose a model that is the fastest model, or the, you know, highest performance or lowest cost. So they excuse me, they can choose to do that on their end. And you can see here, like they have all the ability to, you know, filter by the different model types, different output formats they can choose. There are free models available that they can choose from. There's we also have, like, we recently brought on a provider that offers private models, private AI, so it's like, you know, fully secure, end to end. Encrypted inference that you know, you'll never be able to they'll never be able to read what what they're saying. Context window, all of these, like, different, you know, parameters, like tools or reasoning. All of that is available here for that like and this is accessible through the API. So, yeah, you can see our the providers that we have available here. So that's kind of been our strategy. We're offering. We offer up the ability for them to dynamically choose through the API right now, or they can. They can hard code and specify a specific model.
So yeah, exactly they can. They can choose to optimize by price or by performance, like latency. So if we go to the models page here, they can decide in the API they can specify, okay, I want to like for this particular category, I want to be able to like, choose a model that is the fastest model, or the, you know, highest performance or lowest cost. So they excuse me, they can choose to do that on their end. And you can see here, like they have all the ability to, you know, filter by the different model types, different output formats they can choose. There are free models available that they can choose from. There's we also have, like, we recently brought on a provider that offers private models, private AI, so it's like, you know, fully secure, end to end. Encrypted inference that you know, you'll never be able to they'll never be able to read what what they're saying. Context window, all of these, like, different, you know, parameters, like tools or reasoning. All of that is available here for that like and this is accessible through the API. So, yeah, you can see our the providers that we have available here. So that's kind of been our strategy. We're offering. We offer up the ability for them to dynamically choose through the API right now, or they can. They can hard code and specify a specific model.
S Speaker 116:21Makes sense. Makes sense. One super interesting. How do you see this sort of taking off from here? One like in your thoughts, say, is developing a more dynamic or say, if you, even if you develop a more dynamic orchestration logic that puts you on par with the open likes of open router today, right at the same time, you have a trading back end, which is allowing you to provide influence at cheaper cost. Where do you see the Alpha coming in from? How do you see this sort of actually take getting a big traction and, frankly, slightly saturated market today.
Makes sense. Makes sense. One super interesting. How do you see this sort of taking off from here? One like in your thoughts, say, is developing a more dynamic or say, if you, even if you develop a more dynamic orchestration logic that puts you on par with the open likes of open router today, right at the same time, you have a trading back end, which is allowing you to provide influence at cheaper cost. Where do you see the Alpha coming in from? How do you see this sort of actually take getting a big traction and, frankly, slightly saturated market today.
Makes sense. Makes sense. One super interesting. How do you see this sort of taking off from here? One like in your thoughts, say, is developing a more dynamic or say, if you, even if you develop a more dynamic orchestration logic that puts you on par with the open likes of open router today, right at the same time, you have a trading back end, which is allowing you to provide influence at cheaper cost. Where do you see the Alpha coming in from? How do you see this sort of actually take getting a big traction and, frankly, slightly saturated market today.
Makes sense. Makes sense. One super interesting. How do you see this sort of taking off from here? One like in your thoughts, say, is developing a more dynamic or say, if you, even if you develop a more dynamic orchestration logic that puts you on par with the open likes of open router today, right at the same time, you have a trading back end, which is allowing you to provide influence at cheaper cost. Where do you see the Alpha coming in from? How do you see this sort of actually take getting a big traction and, frankly, slightly saturated market today.
S Speaker 217:04Yeah, so there are a lot of providers. The thing that really works in our favor is that most people are not even aware that solutions like open router exist. So there's still a lack a huge lack of awareness amongst enterprise customers that they can that there are alternatives for them to run their own kind of like private, self hosted version of Chachi BT that has the same, the same user experience, but it comes with, you know, a whole bunch of benefits. Like there's, you know, one at one area that we haven't even explored at this point is giving, giving the like the admin, the ability to understand and quantify, how are their How are their users actually putting AI to work, and what are some of like, the gains, or the performance improvements or productivity gains that they're seeing within the company, like, you know, is this, is this, is there somebody who is, like, wildly more productive than everybody else, and is there a way to kind of, like, distribute and disseminate that, that in that knowledge throughout the organization, right? Those kinds of insights are, are not at all available through chatgpt At this point, and that's something that is totally under explored. And it's like that's a massive opportunity in and of itself. So those are things that we're exploring right now internally. And I think that's kind of from an enterprise chat perspective, obviously, having access to the best models at the best prices. I mean, there's, you know, just in and of itself, that's like a big opportunity. And then going, you know, further differentiating with our own router model to drive the costs down even lower, and then also making, you know, fine tuned models readily available. And then finally quantifying the ways in which, you know, users are actually, you know, putting the AI to work. That's a huge lease sought after, demand that companies are not well served for right now.
Yeah, so there are a lot of providers. The thing that really works in our favor is that most people are not even aware that solutions like open router exist. So there's still a lack a huge lack of awareness amongst enterprise customers that they can that there are alternatives for them to run their own kind of like private, self hosted version of Chachi BT that has the same, the same user experience, but it comes with, you know, a whole bunch of benefits. Like there's, you know, one at one area that we haven't even explored at this point is giving, giving the like the admin, the ability to understand and quantify, how are their How are their users actually putting AI to work, and what are some of like, the gains, or the performance improvements or productivity gains that they're seeing within the company, like, you know, is this, is this, is there somebody who is, like, wildly more productive than everybody else, and is there a way to kind of, like, distribute and disseminate that, that in that knowledge throughout the organization, right? Those kinds of insights are, are not at all available through chatgpt At this point, and that's something that is totally under explored. And it's like that's a massive opportunity in and of itself. So those are things that we're exploring right now internally. And I think that's kind of from an enterprise chat perspective, obviously, having access to the best models at the best prices. I mean, there's, you know, just in and of itself, that's like a big opportunity. And then going, you know, further differentiating with our own router model to drive the costs down even lower, and then also making, you know, fine tuned models readily available. And then finally quantifying the ways in which, you know, users are actually, you know, putting the AI to work. That's a huge lease sought after, demand that companies are not well served for right now.
Yeah, so there are a lot of providers. The thing that really works in our favor is that most people are not even aware that solutions like open router exist. So there's still a lack a huge lack of awareness amongst enterprise customers that they can that there are alternatives for them to run their own kind of like private, self hosted version of Chachi BT that has the same, the same user experience, but it comes with, you know, a whole bunch of benefits. Like there's, you know, one at one area that we haven't even explored at this point is giving, giving the like the admin, the ability to understand and quantify, how are their How are their users actually putting AI to work, and what are some of like, the gains, or the performance improvements or productivity gains that they're seeing within the company, like, you know, is this, is this, is there somebody who is, like, wildly more productive than everybody else, and is there a way to kind of, like, distribute and disseminate that, that in that knowledge throughout the organization, right? Those kinds of insights are, are not at all available through chatgpt At this point, and that's something that is totally under explored. And it's like that's a massive opportunity in and of itself. So those are things that we're exploring right now internally. And I think that's kind of from an enterprise chat perspective, obviously, having access to the best models at the best prices. I mean, there's, you know, just in and of itself, that's like a big opportunity. And then going, you know, further differentiating with our own router model to drive the costs down even lower, and then also making, you know, fine tuned models readily available. And then finally quantifying the ways in which, you know, users are actually, you know, putting the AI to work. That's a huge lease sought after, demand that companies are not well served for right now.
Yeah, so there are a lot of providers. The thing that really works in our favor is that most people are not even aware that solutions like open router exist. So there's still a lack a huge lack of awareness amongst enterprise customers that they can that there are alternatives for them to run their own kind of like private, self hosted version of Chachi BT that has the same, the same user experience, but it comes with, you know, a whole bunch of benefits. Like there's, you know, one at one area that we haven't even explored at this point is giving, giving the like the admin, the ability to understand and quantify, how are their How are their users actually putting AI to work, and what are some of like, the gains, or the performance improvements or productivity gains that they're seeing within the company, like, you know, is this, is this, is there somebody who is, like, wildly more productive than everybody else, and is there a way to kind of, like, distribute and disseminate that, that in that knowledge throughout the organization, right? Those kinds of insights are, are not at all available through chatgpt At this point, and that's something that is totally under explored. And it's like that's a massive opportunity in and of itself. So those are things that we're exploring right now internally. And I think that's kind of from an enterprise chat perspective, obviously, having access to the best models at the best prices. I mean, there's, you know, just in and of itself, that's like a big opportunity. And then going, you know, further differentiating with our own router model to drive the costs down even lower, and then also making, you know, fine tuned models readily available. And then finally quantifying the ways in which, you know, users are actually, you know, putting the AI to work. That's a huge lease sought after, demand that companies are not well served for right now.
S Speaker 119:08Agree, agree with you on that. One, how big a round Are you raising for now? One, and what could be the runway that that gives you?
Agree, agree with you on that. One, how big a round Are you raising for now? One, and what could be the runway that that gives you?
Agree, agree with you on that. One, how big a round Are you raising for now? One, and what could be the runway that that gives you?
Agree, agree with you on that. One, how big a round Are you raising for now? One, and what could be the runway that that gives you?
S Speaker 119:50And so one from the chart here, you're roughly at, say, 550, 600k ARR today. Do you expect to end the year at 1.5 1.5
And so one from the chart here, you're roughly at, say, 550, 600k ARR today. Do you expect to end the year at 1.5 1.5
And so one from the chart here, you're roughly at, say, 550, 600k ARR today. Do you expect to end the year at 1.5 1.5
And so one from the chart here, you're roughly at, say, 550, 600k ARR today. Do you expect to end the year at 1.5 1.5
20:045.5 sorry, 5.5 I
S Speaker 120:08was saying 550 K Arr, roughly speaking, 550 K Arr, today, do you expect to end it at 1.5 million by 2025?
was saying 550 K Arr, roughly speaking, 550 K Arr, today, do you expect to end it at 1.5 million by 2025?
was saying 550 K Arr, roughly speaking, 550 K Arr, today, do you expect to end it at 1.5 million by 2025?
was saying 550 K Arr, roughly speaking, 550 K Arr, today, do you expect to end it at 1.5 million by 2025?
20:20Yeah, it'll be somewhere between one and 1.5
Yeah, it'll be somewhere between one and 1.5
Yeah, it'll be somewhere between one and 1.5
Yeah, it'll be somewhere between one and 1.5
S Speaker 120:24Interesting, interesting, and most of that would still come from inference fees,
Interesting, interesting, and most of that would still come from inference fees,
Interesting, interesting, and most of that would still come from inference fees,
Interesting, interesting, and most of that would still come from inference fees,
20:29yeah, and subscriptions,
yeah, and subscriptions,
yeah, and subscriptions,
yeah, and subscriptions,
S Speaker 220:32like the subscription, like user user licenses for chatgpt, like gateways, enterprise chat.
like the subscription, like user user licenses for chatgpt, like gateways, enterprise chat.
like the subscription, like user user licenses for chatgpt, like gateways, enterprise chat.
like the subscription, like user user licenses for chatgpt, like gateways, enterprise chat.
S Speaker 120:42Got it? Got it one interesting enough. We'd love to dig deeper. Can you send me the deck? I will have to do a bit of research on my end and come back to you with some questions. We can tackle that async. I can just sort of send it to you over email, and then we can take it from there, sure.
Got it? Got it one interesting enough. We'd love to dig deeper. Can you send me the deck? I will have to do a bit of research on my end and come back to you with some questions. We can tackle that async. I can just sort of send it to you over email, and then we can take it from there, sure.
Got it? Got it one interesting enough. We'd love to dig deeper. Can you send me the deck? I will have to do a bit of research on my end and come back to you with some questions. We can tackle that async. I can just sort of send it to you over email, and then we can take it from there, sure.
Got it? Got it one interesting enough. We'd love to dig deeper. Can you send me the deck? I will have to do a bit of research on my end and come back to you with some questions. We can tackle that async. I can just sort of send it to you over email, and then we can take it from there, sure.
S Speaker 121:16That would be amazing one we can set up, say, the next, next set of follow up calls by mid of next week. Timeline wise, does that work for you? Sure? Yeah. Sounds great. Sounds great. One interesting, certainly quite intriguing. I love to sort of also go through the Marketplace side of things and see, haven't thought about it that way, but takes me, to some extent, back to the Silicon Valley sitcom, where they eventually launched their own crypto coin for their models.
That would be amazing one we can set up, say, the next, next set of follow up calls by mid of next week. Timeline wise, does that work for you? Sure? Yeah. Sounds great. Sounds great. One interesting, certainly quite intriguing. I love to sort of also go through the Marketplace side of things and see, haven't thought about it that way, but takes me, to some extent, back to the Silicon Valley sitcom, where they eventually launched their own crypto coin for their models.
That would be amazing one we can set up, say, the next, next set of follow up calls by mid of next week. Timeline wise, does that work for you? Sure? Yeah. Sounds great. Sounds great. One interesting, certainly quite intriguing. I love to sort of also go through the Marketplace side of things and see, haven't thought about it that way, but takes me, to some extent, back to the Silicon Valley sitcom, where they eventually launched their own crypto coin for their models.
That would be amazing one we can set up, say, the next, next set of follow up calls by mid of next week. Timeline wise, does that work for you? Sure? Yeah. Sounds great. Sounds great. One interesting, certainly quite intriguing. I love to sort of also go through the Marketplace side of things and see, haven't thought about it that way, but takes me, to some extent, back to the Silicon Valley sitcom, where they eventually launched their own crypto coin for their models.
S Speaker 221:50Yeah, there's, it's like, crypto is one of those funny things where they're like, I think that, you know, our vision for, like, I come from the AI world. I've been in the air for a decade now. My partner, he's, he's more of savvy when it comes to crypto, but both of us are very, you know, strong believers that you know, the future of AI will be powered by crypto, and then it'll be more and more important for, you know, agents to have bank accounts that will that are, you know, crypto wallets, essentially, right and and so I think that the infrastructure that crypto has created will really power the next wave of AI innovation going forward with it, especially with agents.
Yeah, there's, it's like, crypto is one of those funny things where they're like, I think that, you know, our vision for, like, I come from the AI world. I've been in the air for a decade now. My partner, he's, he's more of savvy when it comes to crypto, but both of us are very, you know, strong believers that you know, the future of AI will be powered by crypto, and then it'll be more and more important for, you know, agents to have bank accounts that will that are, you know, crypto wallets, essentially, right and and so I think that the infrastructure that crypto has created will really power the next wave of AI innovation going forward with it, especially with agents.
Yeah, there's, it's like, crypto is one of those funny things where they're like, I think that, you know, our vision for, like, I come from the AI world. I've been in the air for a decade now. My partner, he's, he's more of savvy when it comes to crypto, but both of us are very, you know, strong believers that you know, the future of AI will be powered by crypto, and then it'll be more and more important for, you know, agents to have bank accounts that will that are, you know, crypto wallets, essentially, right and and so I think that the infrastructure that crypto has created will really power the next wave of AI innovation going forward with it, especially with agents.
Yeah, there's, it's like, crypto is one of those funny things where they're like, I think that, you know, our vision for, like, I come from the AI world. I've been in the air for a decade now. My partner, he's, he's more of savvy when it comes to crypto, but both of us are very, you know, strong believers that you know, the future of AI will be powered by crypto, and then it'll be more and more important for, you know, agents to have bank accounts that will that are, you know, crypto wallets, essentially, right and and so I think that the infrastructure that crypto has created will really power the next wave of AI innovation going forward with it, especially with agents.
S Speaker 122:27That's interesting one. We haven't spent a lot of time sort of unpacking that thesis. I do have come across some of those thought pieces around if you have any notes for me to read, or especially how the team thinks about decentralized plus, sort of intersection of decentralized plus, AI would love to do that as well.
That's interesting one. We haven't spent a lot of time sort of unpacking that thesis. I do have come across some of those thought pieces around if you have any notes for me to read, or especially how the team thinks about decentralized plus, sort of intersection of decentralized plus, AI would love to do that as well.
That's interesting one. We haven't spent a lot of time sort of unpacking that thesis. I do have come across some of those thought pieces around if you have any notes for me to read, or especially how the team thinks about decentralized plus, sort of intersection of decentralized plus, AI would love to do that as well.
That's interesting one. We haven't spent a lot of time sort of unpacking that thesis. I do have come across some of those thought pieces around if you have any notes for me to read, or especially how the team thinks about decentralized plus, sort of intersection of decentralized plus, AI would love to do that as well.
S Speaker 222:47Okay, sounds great. Yeah, I have a few resources that I could send
Okay, sounds great. Yeah, I have a few resources that I could send
Okay, sounds great. Yeah, I have a few resources that I could send
Okay, sounds great. Yeah, I have a few resources that I could send
S Speaker 222:50to you that would be amazing. One, cool, okay, great, thanks.
to you that would be amazing. One, cool, okay, great, thanks.
to you that would be amazing. One, cool, okay, great, thanks.
to you that would be amazing. One, cool, okay, great, thanks.
S Speaker 122:55One, I appreciate the time today. Yes, yes, likewise, and just we didn't get to cover this, but just quickly on on how we operate as a fund check sizes would be anywhere from two to $15 million typically, our sweet spot is, say, riding five to seven. So that's where we come in. Prefer to come in at around, say, series A we have started doing seed recently. We have done, say, three or four seed rounds in the last year and a half. So we are developing our seed muscle. If we get in here, I would say we would do something like a $2 million check, but, but I'll, I'll keep you posted on that.
One, I appreciate the time today. Yes, yes, likewise, and just we didn't get to cover this, but just quickly on on how we operate as a fund check sizes would be anywhere from two to $15 million typically, our sweet spot is, say, riding five to seven. So that's where we come in. Prefer to come in at around, say, series A we have started doing seed recently. We have done, say, three or four seed rounds in the last year and a half. So we are developing our seed muscle. If we get in here, I would say we would do something like a $2 million check, but, but I'll, I'll keep you posted on that.
One, I appreciate the time today. Yes, yes, likewise, and just we didn't get to cover this, but just quickly on on how we operate as a fund check sizes would be anywhere from two to $15 million typically, our sweet spot is, say, riding five to seven. So that's where we come in. Prefer to come in at around, say, series A we have started doing seed recently. We have done, say, three or four seed rounds in the last year and a half. So we are developing our seed muscle. If we get in here, I would say we would do something like a $2 million check, but, but I'll, I'll keep you posted on that.
One, I appreciate the time today. Yes, yes, likewise, and just we didn't get to cover this, but just quickly on on how we operate as a fund check sizes would be anywhere from two to $15 million typically, our sweet spot is, say, riding five to seven. So that's where we come in. Prefer to come in at around, say, series A we have started doing seed recently. We have done, say, three or four seed rounds in the last year and a half. So we are developing our seed muscle. If we get in here, I would say we would do something like a $2 million check, but, but I'll, I'll keep you posted on that.
S Speaker 223:37Okay, great. Sounds good. Thank you. I'll send you the email over right away, and we'll go
Okay, great. Sounds good. Thank you. I'll send you the email over right away, and we'll go
Okay, great. Sounds good. Thank you. I'll send you the email over right away, and we'll go
Okay, great. Sounds good. Thank you. I'll send you the email over right away, and we'll go
S Speaker 123:41from there. Sounds good. Sounds good, I'll come back to you. Great. Thank you. Super nice to meet you. Have a great day. You too. Bye, for now, bye. You.
from there. Sounds good. Sounds good, I'll come back to you. Great. Thank you. Super nice to meet you. Have a great day. You too. Bye, for now, bye. You.
from there. Sounds good. Sounds good, I'll come back to you. Great. Thank you. Super nice to meet you. Have a great day. You too. Bye, for now, bye. You.
from there. Sounds good. Sounds good, I'll come back to you. Great. Thank you. Super nice to meet you. Have a great day. You too. Bye, for now, bye. You.