Meeting: Captions AI - Pitch
Fri, Apr 4
10:30 AM
31 min
Priyesh P
Introduction and Purpose of the Meeting
1:1
URL: https://otter.ai/u/hqoTh8xuLa4REUsQvIrKPVKtWrg
Downloaded: 2025-12-22T12:22:36.985795
Method: text_extraction
============================================================

1:12Hey. Hey, hi Dwight. Hi Nan.
Hey. Hey, hi Dwight. Hi Nan.
Hey. Hey, hi Dwight. Hi Nan.
Hey. Hey, hi Dwight. Hi Nan.
S Speaker 11:17Dwight, so thanks. Thanks a lot for taking the time. I had a great discussion with Sam during GTC, and wanted to follow up on our conversation. I'm Priyesh, a Senior Associate here at Qualcomm ventures, and Nan is a partner at Qualcomm ventures. She's looking at a lot of video AI companies as well. So you wanted to have an introductory conversation today and then maybe follow up with a longer conversation down the
Dwight, so thanks. Thanks a lot for taking the time. I had a great discussion with Sam during GTC, and wanted to follow up on our conversation. I'm Priyesh, a Senior Associate here at Qualcomm ventures, and Nan is a partner at Qualcomm ventures. She's looking at a lot of video AI companies as well. So you wanted to have an introductory conversation today and then maybe follow up with a longer conversation down the
Dwight, so thanks. Thanks a lot for taking the time. I had a great discussion with Sam during GTC, and wanted to follow up on our conversation. I'm Priyesh, a Senior Associate here at Qualcomm ventures, and Nan is a partner at Qualcomm ventures. She's looking at a lot of video AI companies as well. So you wanted to have an introductory conversation today and then maybe follow up with a longer conversation down the
Dwight, so thanks. Thanks a lot for taking the time. I had a great discussion with Sam during GTC, and wanted to follow up on our conversation. I'm Priyesh, a Senior Associate here at Qualcomm ventures, and Nan is a partner at Qualcomm ventures. She's looking at a lot of video AI companies as well. So you wanted to have an introductory conversation today and then maybe follow up with a longer conversation down the
1:37line. Sounds good? Nice to meet you. Nice
line. Sounds good? Nice to meet you. Nice
line. Sounds good? Nice to meet you. Nice
line. Sounds good? Nice to meet you. Nice
S Speaker 21:40to meet you. Yeah. So very grateful to your time. So our team
to meet you. Yeah. So very grateful to your time. So our team
to meet you. Yeah. So very grateful to your time. So our team
to meet you. Yeah. So very grateful to your time. So our team
S Speaker 31:45right now is looking at a lot of video AI startups, and we're trying to but we went on to leading ones. And we've been looking at a generative AI video for, you know, the pure content generation, and also looking at the application like a page in or a cool to for the enterprise application directly. So yeah. So of course, caption system on our kind of the target list, give them all the, you know, all the great product you build, and also the, of course, you have a really strong investor team and the team, but also the funding team, and we'd
right now is looking at a lot of video AI startups, and we're trying to but we went on to leading ones. And we've been looking at a generative AI video for, you know, the pure content generation, and also looking at the application like a page in or a cool to for the enterprise application directly. So yeah. So of course, caption system on our kind of the target list, give them all the, you know, all the great product you build, and also the, of course, you have a really strong investor team and the team, but also the funding team, and we'd
right now is looking at a lot of video AI startups, and we're trying to but we went on to leading ones. And we've been looking at a generative AI video for, you know, the pure content generation, and also looking at the application like a page in or a cool to for the enterprise application directly. So yeah. So of course, caption system on our kind of the target list, give them all the, you know, all the great product you build, and also the, of course, you have a really strong investor team and the team, but also the funding team, and we'd
right now is looking at a lot of video AI startups, and we're trying to but we went on to leading ones. And we've been looking at a generative AI video for, you know, the pure content generation, and also looking at the application like a page in or a cool to for the enterprise application directly. So yeah. So of course, caption system on our kind of the target list, give them all the, you know, all the great product you build, and also the, of course, you have a really strong investor team and the team, but also the funding team, and we'd
2:21love to learn more about about it
love to learn more about about it
love to learn more about about it
love to learn more about about it
S Speaker 42:25no, sounds great. Thank you. And thank you for the kind of context so we're, yeah, I can kind of, like, back up a little bit and give it a little more of that original and then, yeah, go into what it looks like more like today. So my co founders, Grove and I started the company a little over four years ago. But to be honest, like mostly like product market fit, like in market product and true, like fit has only been only about maybe two and a half of those years, the first 18 months we we the first thing we built was captions. We launched it before we even raised our seed round or anything back in January 2021 but we actually worked on a bunch of like social components to it, and a million things. And so we weren't exactly sure if we were just investing in the tool and so. And that was probably that that first year or so, and then that's when we started to really concentrate on captions. And once we did that, that's when things really started to take off. And I couldn't consider that point like something like the summer of 2022 something like that. So it's actually been a very short timeline. Once you kind of take that into context, almost the entire company was hired between 2023 and 2024 basically we were, to give you an idea, I think we were, we are four five employees at in at the end of 2022 so we're today. We're about 75 so we're all of us are here in New York, our office in Union Square, and everyone's in person. So like, culture wise and stuff, everyone's in person. We come in five days a week. We have, like, a pretty tight knit culture as a result, and a little bit of a different one, but whatever, and we, I would say the company is too the way to think about the team is maybe a little over two thirds, so probably something like 70%
no, sounds great. Thank you. And thank you for the kind of context so we're, yeah, I can kind of, like, back up a little bit and give it a little more of that original and then, yeah, go into what it looks like more like today. So my co founders, Grove and I started the company a little over four years ago. But to be honest, like mostly like product market fit, like in market product and true, like fit has only been only about maybe two and a half of those years, the first 18 months we we the first thing we built was captions. We launched it before we even raised our seed round or anything back in January 2021 but we actually worked on a bunch of like social components to it, and a million things. And so we weren't exactly sure if we were just investing in the tool and so. And that was probably that that first year or so, and then that's when we started to really concentrate on captions. And once we did that, that's when things really started to take off. And I couldn't consider that point like something like the summer of 2022 something like that. So it's actually been a very short timeline. Once you kind of take that into context, almost the entire company was hired between 2023 and 2024 basically we were, to give you an idea, I think we were, we are four five employees at in at the end of 2022 so we're today. We're about 75 so we're all of us are here in New York, our office in Union Square, and everyone's in person. So like, culture wise and stuff, everyone's in person. We come in five days a week. We have, like, a pretty tight knit culture as a result, and a little bit of a different one, but whatever, and we, I would say the company is too the way to think about the team is maybe a little over two thirds, so probably something like 70%
no, sounds great. Thank you. And thank you for the kind of context so we're, yeah, I can kind of, like, back up a little bit and give it a little more of that original and then, yeah, go into what it looks like more like today. So my co founders, Grove and I started the company a little over four years ago. But to be honest, like mostly like product market fit, like in market product and true, like fit has only been only about maybe two and a half of those years, the first 18 months we we the first thing we built was captions. We launched it before we even raised our seed round or anything back in January 2021 but we actually worked on a bunch of like social components to it, and a million things. And so we weren't exactly sure if we were just investing in the tool and so. And that was probably that that first year or so, and then that's when we started to really concentrate on captions. And once we did that, that's when things really started to take off. And I couldn't consider that point like something like the summer of 2022 something like that. So it's actually been a very short timeline. Once you kind of take that into context, almost the entire company was hired between 2023 and 2024 basically we were, to give you an idea, I think we were, we are four five employees at in at the end of 2022 so we're today. We're about 75 so we're all of us are here in New York, our office in Union Square, and everyone's in person. So like, culture wise and stuff, everyone's in person. We come in five days a week. We have, like, a pretty tight knit culture as a result, and a little bit of a different one, but whatever, and we, I would say the company is too the way to think about the team is maybe a little over two thirds, so probably something like 70%
no, sounds great. Thank you. And thank you for the kind of context so we're, yeah, I can kind of, like, back up a little bit and give it a little more of that original and then, yeah, go into what it looks like more like today. So my co founders, Grove and I started the company a little over four years ago. But to be honest, like mostly like product market fit, like in market product and true, like fit has only been only about maybe two and a half of those years, the first 18 months we we the first thing we built was captions. We launched it before we even raised our seed round or anything back in January 2021 but we actually worked on a bunch of like social components to it, and a million things. And so we weren't exactly sure if we were just investing in the tool and so. And that was probably that that first year or so, and then that's when we started to really concentrate on captions. And once we did that, that's when things really started to take off. And I couldn't consider that point like something like the summer of 2022 something like that. So it's actually been a very short timeline. Once you kind of take that into context, almost the entire company was hired between 2023 and 2024 basically we were, to give you an idea, I think we were, we are four five employees at in at the end of 2022 so we're today. We're about 75 so we're all of us are here in New York, our office in Union Square, and everyone's in person. So like, culture wise and stuff, everyone's in person. We come in five days a week. We have, like, a pretty tight knit culture as a result, and a little bit of a different one, but whatever, and we, I would say the company is too the way to think about the team is maybe a little over two thirds, so probably something like 70%
4:26is Research and engineering,
is Research and engineering,
is Research and engineering,
is Research and engineering,
S Speaker 44:30and then about 30% is operations in GTM. Basically GTM, historically for us, have been all marketing, kind of our own, demand, Gen, performance marketing, etc. They're kind of a decent mix and stuff through that organic channels and SEO, whatnot. And then, more recently, the last like, call it like two months, we've begun to hire our sales team and start a much more aggressive up market approach, targeting mid market and enterprises. We're continuing the prosumer and SMB, which self serve business. So an easy way to think about the business, in my mind, is think very much like Canva style, plg type company, you have self serve, prosumers, SMBs, and then now we have, we've already brought on a little over 100 enterprise and B to B companies that are either high end, kind of self serve or a little smaller, like mid market. And then we've been working on a number of enterprise contracts, which we've actually closed a couple already, and yeah, and so and I can talk to who we're targeting there, which is a very different. Buyer.
and then about 30% is operations in GTM. Basically GTM, historically for us, have been all marketing, kind of our own, demand, Gen, performance marketing, etc. They're kind of a decent mix and stuff through that organic channels and SEO, whatnot. And then, more recently, the last like, call it like two months, we've begun to hire our sales team and start a much more aggressive up market approach, targeting mid market and enterprises. We're continuing the prosumer and SMB, which self serve business. So an easy way to think about the business, in my mind, is think very much like Canva style, plg type company, you have self serve, prosumers, SMBs, and then now we have, we've already brought on a little over 100 enterprise and B to B companies that are either high end, kind of self serve or a little smaller, like mid market. And then we've been working on a number of enterprise contracts, which we've actually closed a couple already, and yeah, and so and I can talk to who we're targeting there, which is a very different. Buyer.
and then about 30% is operations in GTM. Basically GTM, historically for us, have been all marketing, kind of our own, demand, Gen, performance marketing, etc. They're kind of a decent mix and stuff through that organic channels and SEO, whatnot. And then, more recently, the last like, call it like two months, we've begun to hire our sales team and start a much more aggressive up market approach, targeting mid market and enterprises. We're continuing the prosumer and SMB, which self serve business. So an easy way to think about the business, in my mind, is think very much like Canva style, plg type company, you have self serve, prosumers, SMBs, and then now we have, we've already brought on a little over 100 enterprise and B to B companies that are either high end, kind of self serve or a little smaller, like mid market. And then we've been working on a number of enterprise contracts, which we've actually closed a couple already, and yeah, and so and I can talk to who we're targeting there, which is a very different. Buyer.
and then about 30% is operations in GTM. Basically GTM, historically for us, have been all marketing, kind of our own, demand, Gen, performance marketing, etc. They're kind of a decent mix and stuff through that organic channels and SEO, whatnot. And then, more recently, the last like, call it like two months, we've begun to hire our sales team and start a much more aggressive up market approach, targeting mid market and enterprises. We're continuing the prosumer and SMB, which self serve business. So an easy way to think about the business, in my mind, is think very much like Canva style, plg type company, you have self serve, prosumers, SMBs, and then now we have, we've already brought on a little over 100 enterprise and B to B companies that are either high end, kind of self serve or a little smaller, like mid market. And then we've been working on a number of enterprise contracts, which we've actually closed a couple already, and yeah, and so and I can talk to who we're targeting there, which is a very different. Buyer.
S Speaker 35:53Love to Learn more. Because, from the website of what I saw is, do you feel like yourself is a little bit similar to like Hayden or something, or it's it's a different because,
Love to Learn more. Because, from the website of what I saw is, do you feel like yourself is a little bit similar to like Hayden or something, or it's it's a different because,
Love to Learn more. Because, from the website of what I saw is, do you feel like yourself is a little bit similar to like Hayden or something, or it's it's a different because,
Love to Learn more. Because, from the website of what I saw is, do you feel like yourself is a little bit similar to like Hayden or something, or it's it's a different because,
S Speaker 46:08I would say the thing about, oh, and I can talk about this from the research angle to Synthesia. And hey Gen, I think of Asia and hey Gen, very similarly, they have a very similar technology. The neural rendering model that they have is like a little something between neuro rendering and neuro based model. They so with Hey Gen, they historically targeted learning and development teams that most of their revenue. But honestly, Synthesia has won that market, like pretty extensively, and I think that so as a result, agent, I think, reacted to that by pivoting back to more consumer use cases. And like, they let go of their sales, most of their sales team and and then I actually just talked to Synthesia Victor at sandeshia. He told me that they are now rehiring. And so I actually think they're pivoting back to this, and so back to like, enterprise, but I think it's slightly more like marketing use cases. That's the latest I understand. So I think with that, we don't like they don't come up in deals. Quite frankly, that's because we target marketing teams. We don't target L, D, and translation is a, is a, is a sub bullet of kind of what we do. And then again, I can talk about the technology differences, because there is vastly different one there, but, but, yeah, we don't like, I think, generally speaking, we think of it as competitive, but not, not really in the sense of like, the same customer, per se, like, but that's a little TBD. I think they are interested in marketing use cases now. So that made the change. And then, yeah, so we have a video. We have a lips back to the technology at different education. So very similarly to agents, we also have a lip sync model. We called the product AI creator. Actually been out for years. We released it in 2022 but the first version in late 2022 and that was our first video model that was built from scratch. And we we actually met the like we were going to work back then with SYNC labs folks. We met them before they came to the country and even started the company. They were in India, and my co founders from India, and they had, we had just been connected through things and then, and that's where, yeah, that's where, like, a lot of they were, wrote the paper on wave to live. And that's basically where all these models were birthed from. But honestly, today, it's, like, not really differentiated. In my mind, it's open source. Literally, everyone can train it. Loom is training them. Now, I just spoke to them. Everyone's going to have that model. Like it's available. You can buy it from tapas. You can buy it from how FAU. Like, it's pretty ubiquitous. The lips model, yeah, the lip sync model, exactly, yes. It's just ubiquitous. I mean, I don't do it as differentiated technology anymore, and honestly, I think that's been true probably the last year, like, maybe something like that. And the problem with that, those models, and I'll kind of speak to why we took a very different road is, you know, at the end of the day, like, they're very, very good at translation, but that's because, not because of the technology, but kind of like, somewhat like luck, almost. It's like, it's really good in moments where you're not changing the subject of the speaker, and so you have your hand movements and your body expression and your reactions and all right, which is half of communication, right? Like you and I on this call, like you can mute me and you can probably tell or I'm angry or sad or, you know, like, that's communication, right? And so I think that where those that, and that's where so lip sync models, I think shine. I think they do a great job at that. Where I think they fall down is when you need to say something else. And so I think Synthesia has been able to build a business around that, because they do internal videos, and like, people don't care, like, at the end of the viewer doesn't care. They're probably mandated to watch the video, really not they just don't care. And and with translation, and which has been a lot of an L and D for agents business too, is like that it works well for that. Like, I think it, I think it's actually does very well. And I think you could build a big business around that. I think what we found is that one is we don't really want to be in the translation space. I think we have people use it. And like, when I say the translation space, I mean literally doing the translation. So our video models will do translation, but the actual translation is probably being done by whisper or open AI, something like that in the background. You know, state of the art kind of model there. And the problem is, is that you have to infer dialects. And you know, if you're just talking about like Hindi, you have formality, and if you have German, you have neutral genders, and you don't necessarily have male and female and like, and the video models have nothing to do with that, right? Like they don't. They're not deciding those things. And if you're talking to come in ass, like, They that machine translation is not going to cut it. Basically, they care deeply about that. So you need to build very deep workflows where their local teams can check and recheck. And that has nothing to do with the video model, that is just purely the translation. So I don't think it's a honestly. I think it's almost like a service level business, service business that's very operationally heavy and intense, or it is for either the buyer or the seller. And so I don't know, I don't, I don't love the business. And so we haven't really gone super deep into translation, but I do think it's where lip sync models do very well. And I think for us, so very differently we train. We started in early 2024 we started, kind of embarked on a foundation model approach to this. So the neural render, the lip sync models, all use neural rendering stuff, which are pre date diffusion models. This is like when I was, like 20 or something, you know. And honestly, even way before, like nerf and such, much older technologies, it doesn't really take advantage of any of the transform architecture, diffusion, stable diffusion, any of that stuff that's come out, and then the last call it about 10 years. And I suppose to transform a little younger than that. And so we basically view that that's a huge opportunity to basically go after the foundation model space, or as an approach here, because where those things are different is now you're talking about more companies, more like in the text of video space, like people, ads, Luma runway, you know, etc, etc. And for those, their avatars can't speak. So if you notice, like all the models, they can generate incredible video footage, but it's more like B roll, landscape, that kind of stuff. And when you generate a person, you can't they can't speak until you they have to apply a lip sync model afterwards, which still has the same problems of what I kind of was talking about before. And with our model, what we set out to do is we have a very unique and proprietary data set that we can train on, because we have a consumer business, we take a license to that data, and we have for a long time, we've built up probably so we've hired people from DeepMind and fair, and multiple people have told us that we have the largest talking head data set at this point in the world. There may be something unknown in some AI labs, particularly ones we've spoken Alibaba, but we don't know about bytedance that potentially is bigger, but it is. It is arguably, if not the largest, one of the largest. And what we've done with that is including some other licensed data. Have trained a foundation model on talking to people, and so are very differently than the text video companies are speak, we generate video of fully around the person. Our Mirage is the kind of marketing name of this. We can generate the entire environment around them. So that's a normal kind of foundation model, right? So you can prompt anything, and then we can but interestingly, we can also do audio to video. So we can take an audio file and generate video from it. Normally, that's usually like only generated audio. We can take a real audio and and generate it. You actually don't need to prompt anything. You can actually just hit the audio, which gets a little spicier, but and then, because you don't exactly know what you're gonna get and then, but you can obviously use text prompt as part of that. Then we can also do generative audio too. Of course, if you don't have, if you don't have an audio file, for whatever reason, we can, we use 11 labs or any of the audio models. We use cartesia, play HD. We're kind of Switzerland in terms of the other media generation that are related to the to it. So that's where we are from technology standpoint. I think I mentioned DeepMind, the person running that we hired from DeepMind, who is the inventor of the perceiver architecture. He's the first icon on the perceiver architecture, which is the cousin of a transformer architecture family, and he's leading the AI team here, which is about 10 researchers between research engineering and research science. So that's generally where we're at. I would say the one, the one there on Mirage, is our first GTM. This is all public like our first GTM for that is, is up market if we're selling to performance marketers doing paid at paid social so they're making video ads for, you know, DTC brands, consumer finance, gaming companies, those, those types of things.
I would say the thing about, oh, and I can talk about this from the research angle to Synthesia. And hey Gen, I think of Asia and hey Gen, very similarly, they have a very similar technology. The neural rendering model that they have is like a little something between neuro rendering and neuro based model. They so with Hey Gen, they historically targeted learning and development teams that most of their revenue. But honestly, Synthesia has won that market, like pretty extensively, and I think that so as a result, agent, I think, reacted to that by pivoting back to more consumer use cases. And like, they let go of their sales, most of their sales team and and then I actually just talked to Synthesia Victor at sandeshia. He told me that they are now rehiring. And so I actually think they're pivoting back to this, and so back to like, enterprise, but I think it's slightly more like marketing use cases. That's the latest I understand. So I think with that, we don't like they don't come up in deals. Quite frankly, that's because we target marketing teams. We don't target L, D, and translation is a, is a, is a sub bullet of kind of what we do. And then again, I can talk about the technology differences, because there is vastly different one there, but, but, yeah, we don't like, I think, generally speaking, we think of it as competitive, but not, not really in the sense of like, the same customer, per se, like, but that's a little TBD. I think they are interested in marketing use cases now. So that made the change. And then, yeah, so we have a video. We have a lips back to the technology at different education. So very similarly to agents, we also have a lip sync model. We called the product AI creator. Actually been out for years. We released it in 2022 but the first version in late 2022 and that was our first video model that was built from scratch. And we we actually met the like we were going to work back then with SYNC labs folks. We met them before they came to the country and even started the company. They were in India, and my co founders from India, and they had, we had just been connected through things and then, and that's where, yeah, that's where, like, a lot of they were, wrote the paper on wave to live. And that's basically where all these models were birthed from. But honestly, today, it's, like, not really differentiated. In my mind, it's open source. Literally, everyone can train it. Loom is training them. Now, I just spoke to them. Everyone's going to have that model. Like it's available. You can buy it from tapas. You can buy it from how FAU. Like, it's pretty ubiquitous. The lips model, yeah, the lip sync model, exactly, yes. It's just ubiquitous. I mean, I don't do it as differentiated technology anymore, and honestly, I think that's been true probably the last year, like, maybe something like that. And the problem with that, those models, and I'll kind of speak to why we took a very different road is, you know, at the end of the day, like, they're very, very good at translation, but that's because, not because of the technology, but kind of like, somewhat like luck, almost. It's like, it's really good in moments where you're not changing the subject of the speaker, and so you have your hand movements and your body expression and your reactions and all right, which is half of communication, right? Like you and I on this call, like you can mute me and you can probably tell or I'm angry or sad or, you know, like, that's communication, right? And so I think that where those that, and that's where so lip sync models, I think shine. I think they do a great job at that. Where I think they fall down is when you need to say something else. And so I think Synthesia has been able to build a business around that, because they do internal videos, and like, people don't care, like, at the end of the viewer doesn't care. They're probably mandated to watch the video, really not they just don't care. And and with translation, and which has been a lot of an L and D for agents business too, is like that it works well for that. Like, I think it, I think it's actually does very well. And I think you could build a big business around that. I think what we found is that one is we don't really want to be in the translation space. I think we have people use it. And like, when I say the translation space, I mean literally doing the translation. So our video models will do translation, but the actual translation is probably being done by whisper or open AI, something like that in the background. You know, state of the art kind of model there. And the problem is, is that you have to infer dialects. And you know, if you're just talking about like Hindi, you have formality, and if you have German, you have neutral genders, and you don't necessarily have male and female and like, and the video models have nothing to do with that, right? Like they don't. They're not deciding those things. And if you're talking to come in ass, like, They that machine translation is not going to cut it. Basically, they care deeply about that. So you need to build very deep workflows where their local teams can check and recheck. And that has nothing to do with the video model, that is just purely the translation. So I don't think it's a honestly. I think it's almost like a service level business, service business that's very operationally heavy and intense, or it is for either the buyer or the seller. And so I don't know, I don't, I don't love the business. And so we haven't really gone super deep into translation, but I do think it's where lip sync models do very well. And I think for us, so very differently we train. We started in early 2024 we started, kind of embarked on a foundation model approach to this. So the neural render, the lip sync models, all use neural rendering stuff, which are pre date diffusion models. This is like when I was, like 20 or something, you know. And honestly, even way before, like nerf and such, much older technologies, it doesn't really take advantage of any of the transform architecture, diffusion, stable diffusion, any of that stuff that's come out, and then the last call it about 10 years. And I suppose to transform a little younger than that. And so we basically view that that's a huge opportunity to basically go after the foundation model space, or as an approach here, because where those things are different is now you're talking about more companies, more like in the text of video space, like people, ads, Luma runway, you know, etc, etc. And for those, their avatars can't speak. So if you notice, like all the models, they can generate incredible video footage, but it's more like B roll, landscape, that kind of stuff. And when you generate a person, you can't they can't speak until you they have to apply a lip sync model afterwards, which still has the same problems of what I kind of was talking about before. And with our model, what we set out to do is we have a very unique and proprietary data set that we can train on, because we have a consumer business, we take a license to that data, and we have for a long time, we've built up probably so we've hired people from DeepMind and fair, and multiple people have told us that we have the largest talking head data set at this point in the world. There may be something unknown in some AI labs, particularly ones we've spoken Alibaba, but we don't know about bytedance that potentially is bigger, but it is. It is arguably, if not the largest, one of the largest. And what we've done with that is including some other licensed data. Have trained a foundation model on talking to people, and so are very differently than the text video companies are speak, we generate video of fully around the person. Our Mirage is the kind of marketing name of this. We can generate the entire environment around them. So that's a normal kind of foundation model, right? So you can prompt anything, and then we can but interestingly, we can also do audio to video. So we can take an audio file and generate video from it. Normally, that's usually like only generated audio. We can take a real audio and and generate it. You actually don't need to prompt anything. You can actually just hit the audio, which gets a little spicier, but and then, because you don't exactly know what you're gonna get and then, but you can obviously use text prompt as part of that. Then we can also do generative audio too. Of course, if you don't have, if you don't have an audio file, for whatever reason, we can, we use 11 labs or any of the audio models. We use cartesia, play HD. We're kind of Switzerland in terms of the other media generation that are related to the to it. So that's where we are from technology standpoint. I think I mentioned DeepMind, the person running that we hired from DeepMind, who is the inventor of the perceiver architecture. He's the first icon on the perceiver architecture, which is the cousin of a transformer architecture family, and he's leading the AI team here, which is about 10 researchers between research engineering and research science. So that's generally where we're at. I would say the one, the one there on Mirage, is our first GTM. This is all public like our first GTM for that is, is up market if we're selling to performance marketers doing paid at paid social so they're making video ads for, you know, DTC brands, consumer finance, gaming companies, those, those types of things.
I would say the thing about, oh, and I can talk about this from the research angle to Synthesia. And hey Gen, I think of Asia and hey Gen, very similarly, they have a very similar technology. The neural rendering model that they have is like a little something between neuro rendering and neuro based model. They so with Hey Gen, they historically targeted learning and development teams that most of their revenue. But honestly, Synthesia has won that market, like pretty extensively, and I think that so as a result, agent, I think, reacted to that by pivoting back to more consumer use cases. And like, they let go of their sales, most of their sales team and and then I actually just talked to Synthesia Victor at sandeshia. He told me that they are now rehiring. And so I actually think they're pivoting back to this, and so back to like, enterprise, but I think it's slightly more like marketing use cases. That's the latest I understand. So I think with that, we don't like they don't come up in deals. Quite frankly, that's because we target marketing teams. We don't target L, D, and translation is a, is a, is a sub bullet of kind of what we do. And then again, I can talk about the technology differences, because there is vastly different one there, but, but, yeah, we don't like, I think, generally speaking, we think of it as competitive, but not, not really in the sense of like, the same customer, per se, like, but that's a little TBD. I think they are interested in marketing use cases now. So that made the change. And then, yeah, so we have a video. We have a lips back to the technology at different education. So very similarly to agents, we also have a lip sync model. We called the product AI creator. Actually been out for years. We released it in 2022 but the first version in late 2022 and that was our first video model that was built from scratch. And we we actually met the like we were going to work back then with SYNC labs folks. We met them before they came to the country and even started the company. They were in India, and my co founders from India, and they had, we had just been connected through things and then, and that's where, yeah, that's where, like, a lot of they were, wrote the paper on wave to live. And that's basically where all these models were birthed from. But honestly, today, it's, like, not really differentiated. In my mind, it's open source. Literally, everyone can train it. Loom is training them. Now, I just spoke to them. Everyone's going to have that model. Like it's available. You can buy it from tapas. You can buy it from how FAU. Like, it's pretty ubiquitous. The lips model, yeah, the lip sync model, exactly, yes. It's just ubiquitous. I mean, I don't do it as differentiated technology anymore, and honestly, I think that's been true probably the last year, like, maybe something like that. And the problem with that, those models, and I'll kind of speak to why we took a very different road is, you know, at the end of the day, like, they're very, very good at translation, but that's because, not because of the technology, but kind of like, somewhat like luck, almost. It's like, it's really good in moments where you're not changing the subject of the speaker, and so you have your hand movements and your body expression and your reactions and all right, which is half of communication, right? Like you and I on this call, like you can mute me and you can probably tell or I'm angry or sad or, you know, like, that's communication, right? And so I think that where those that, and that's where so lip sync models, I think shine. I think they do a great job at that. Where I think they fall down is when you need to say something else. And so I think Synthesia has been able to build a business around that, because they do internal videos, and like, people don't care, like, at the end of the viewer doesn't care. They're probably mandated to watch the video, really not they just don't care. And and with translation, and which has been a lot of an L and D for agents business too, is like that it works well for that. Like, I think it, I think it's actually does very well. And I think you could build a big business around that. I think what we found is that one is we don't really want to be in the translation space. I think we have people use it. And like, when I say the translation space, I mean literally doing the translation. So our video models will do translation, but the actual translation is probably being done by whisper or open AI, something like that in the background. You know, state of the art kind of model there. And the problem is, is that you have to infer dialects. And you know, if you're just talking about like Hindi, you have formality, and if you have German, you have neutral genders, and you don't necessarily have male and female and like, and the video models have nothing to do with that, right? Like they don't. They're not deciding those things. And if you're talking to come in ass, like, They that machine translation is not going to cut it. Basically, they care deeply about that. So you need to build very deep workflows where their local teams can check and recheck. And that has nothing to do with the video model, that is just purely the translation. So I don't think it's a honestly. I think it's almost like a service level business, service business that's very operationally heavy and intense, or it is for either the buyer or the seller. And so I don't know, I don't, I don't love the business. And so we haven't really gone super deep into translation, but I do think it's where lip sync models do very well. And I think for us, so very differently we train. We started in early 2024 we started, kind of embarked on a foundation model approach to this. So the neural render, the lip sync models, all use neural rendering stuff, which are pre date diffusion models. This is like when I was, like 20 or something, you know. And honestly, even way before, like nerf and such, much older technologies, it doesn't really take advantage of any of the transform architecture, diffusion, stable diffusion, any of that stuff that's come out, and then the last call it about 10 years. And I suppose to transform a little younger than that. And so we basically view that that's a huge opportunity to basically go after the foundation model space, or as an approach here, because where those things are different is now you're talking about more companies, more like in the text of video space, like people, ads, Luma runway, you know, etc, etc. And for those, their avatars can't speak. So if you notice, like all the models, they can generate incredible video footage, but it's more like B roll, landscape, that kind of stuff. And when you generate a person, you can't they can't speak until you they have to apply a lip sync model afterwards, which still has the same problems of what I kind of was talking about before. And with our model, what we set out to do is we have a very unique and proprietary data set that we can train on, because we have a consumer business, we take a license to that data, and we have for a long time, we've built up probably so we've hired people from DeepMind and fair, and multiple people have told us that we have the largest talking head data set at this point in the world. There may be something unknown in some AI labs, particularly ones we've spoken Alibaba, but we don't know about bytedance that potentially is bigger, but it is. It is arguably, if not the largest, one of the largest. And what we've done with that is including some other licensed data. Have trained a foundation model on talking to people, and so are very differently than the text video companies are speak, we generate video of fully around the person. Our Mirage is the kind of marketing name of this. We can generate the entire environment around them. So that's a normal kind of foundation model, right? So you can prompt anything, and then we can but interestingly, we can also do audio to video. So we can take an audio file and generate video from it. Normally, that's usually like only generated audio. We can take a real audio and and generate it. You actually don't need to prompt anything. You can actually just hit the audio, which gets a little spicier, but and then, because you don't exactly know what you're gonna get and then, but you can obviously use text prompt as part of that. Then we can also do generative audio too. Of course, if you don't have, if you don't have an audio file, for whatever reason, we can, we use 11 labs or any of the audio models. We use cartesia, play HD. We're kind of Switzerland in terms of the other media generation that are related to the to it. So that's where we are from technology standpoint. I think I mentioned DeepMind, the person running that we hired from DeepMind, who is the inventor of the perceiver architecture. He's the first icon on the perceiver architecture, which is the cousin of a transformer architecture family, and he's leading the AI team here, which is about 10 researchers between research engineering and research science. So that's generally where we're at. I would say the one, the one there on Mirage, is our first GTM. This is all public like our first GTM for that is, is up market if we're selling to performance marketers doing paid at paid social so they're making video ads for, you know, DTC brands, consumer finance, gaming companies, those, those types of things.
I would say the thing about, oh, and I can talk about this from the research angle to Synthesia. And hey Gen, I think of Asia and hey Gen, very similarly, they have a very similar technology. The neural rendering model that they have is like a little something between neuro rendering and neuro based model. They so with Hey Gen, they historically targeted learning and development teams that most of their revenue. But honestly, Synthesia has won that market, like pretty extensively, and I think that so as a result, agent, I think, reacted to that by pivoting back to more consumer use cases. And like, they let go of their sales, most of their sales team and and then I actually just talked to Synthesia Victor at sandeshia. He told me that they are now rehiring. And so I actually think they're pivoting back to this, and so back to like, enterprise, but I think it's slightly more like marketing use cases. That's the latest I understand. So I think with that, we don't like they don't come up in deals. Quite frankly, that's because we target marketing teams. We don't target L, D, and translation is a, is a, is a sub bullet of kind of what we do. And then again, I can talk about the technology differences, because there is vastly different one there, but, but, yeah, we don't like, I think, generally speaking, we think of it as competitive, but not, not really in the sense of like, the same customer, per se, like, but that's a little TBD. I think they are interested in marketing use cases now. So that made the change. And then, yeah, so we have a video. We have a lips back to the technology at different education. So very similarly to agents, we also have a lip sync model. We called the product AI creator. Actually been out for years. We released it in 2022 but the first version in late 2022 and that was our first video model that was built from scratch. And we we actually met the like we were going to work back then with SYNC labs folks. We met them before they came to the country and even started the company. They were in India, and my co founders from India, and they had, we had just been connected through things and then, and that's where, yeah, that's where, like, a lot of they were, wrote the paper on wave to live. And that's basically where all these models were birthed from. But honestly, today, it's, like, not really differentiated. In my mind, it's open source. Literally, everyone can train it. Loom is training them. Now, I just spoke to them. Everyone's going to have that model. Like it's available. You can buy it from tapas. You can buy it from how FAU. Like, it's pretty ubiquitous. The lips model, yeah, the lip sync model, exactly, yes. It's just ubiquitous. I mean, I don't do it as differentiated technology anymore, and honestly, I think that's been true probably the last year, like, maybe something like that. And the problem with that, those models, and I'll kind of speak to why we took a very different road is, you know, at the end of the day, like, they're very, very good at translation, but that's because, not because of the technology, but kind of like, somewhat like luck, almost. It's like, it's really good in moments where you're not changing the subject of the speaker, and so you have your hand movements and your body expression and your reactions and all right, which is half of communication, right? Like you and I on this call, like you can mute me and you can probably tell or I'm angry or sad or, you know, like, that's communication, right? And so I think that where those that, and that's where so lip sync models, I think shine. I think they do a great job at that. Where I think they fall down is when you need to say something else. And so I think Synthesia has been able to build a business around that, because they do internal videos, and like, people don't care, like, at the end of the viewer doesn't care. They're probably mandated to watch the video, really not they just don't care. And and with translation, and which has been a lot of an L and D for agents business too, is like that it works well for that. Like, I think it, I think it's actually does very well. And I think you could build a big business around that. I think what we found is that one is we don't really want to be in the translation space. I think we have people use it. And like, when I say the translation space, I mean literally doing the translation. So our video models will do translation, but the actual translation is probably being done by whisper or open AI, something like that in the background. You know, state of the art kind of model there. And the problem is, is that you have to infer dialects. And you know, if you're just talking about like Hindi, you have formality, and if you have German, you have neutral genders, and you don't necessarily have male and female and like, and the video models have nothing to do with that, right? Like they don't. They're not deciding those things. And if you're talking to come in ass, like, They that machine translation is not going to cut it. Basically, they care deeply about that. So you need to build very deep workflows where their local teams can check and recheck. And that has nothing to do with the video model, that is just purely the translation. So I don't think it's a honestly. I think it's almost like a service level business, service business that's very operationally heavy and intense, or it is for either the buyer or the seller. And so I don't know, I don't, I don't love the business. And so we haven't really gone super deep into translation, but I do think it's where lip sync models do very well. And I think for us, so very differently we train. We started in early 2024 we started, kind of embarked on a foundation model approach to this. So the neural render, the lip sync models, all use neural rendering stuff, which are pre date diffusion models. This is like when I was, like 20 or something, you know. And honestly, even way before, like nerf and such, much older technologies, it doesn't really take advantage of any of the transform architecture, diffusion, stable diffusion, any of that stuff that's come out, and then the last call it about 10 years. And I suppose to transform a little younger than that. And so we basically view that that's a huge opportunity to basically go after the foundation model space, or as an approach here, because where those things are different is now you're talking about more companies, more like in the text of video space, like people, ads, Luma runway, you know, etc, etc. And for those, their avatars can't speak. So if you notice, like all the models, they can generate incredible video footage, but it's more like B roll, landscape, that kind of stuff. And when you generate a person, you can't they can't speak until you they have to apply a lip sync model afterwards, which still has the same problems of what I kind of was talking about before. And with our model, what we set out to do is we have a very unique and proprietary data set that we can train on, because we have a consumer business, we take a license to that data, and we have for a long time, we've built up probably so we've hired people from DeepMind and fair, and multiple people have told us that we have the largest talking head data set at this point in the world. There may be something unknown in some AI labs, particularly ones we've spoken Alibaba, but we don't know about bytedance that potentially is bigger, but it is. It is arguably, if not the largest, one of the largest. And what we've done with that is including some other licensed data. Have trained a foundation model on talking to people, and so are very differently than the text video companies are speak, we generate video of fully around the person. Our Mirage is the kind of marketing name of this. We can generate the entire environment around them. So that's a normal kind of foundation model, right? So you can prompt anything, and then we can but interestingly, we can also do audio to video. So we can take an audio file and generate video from it. Normally, that's usually like only generated audio. We can take a real audio and and generate it. You actually don't need to prompt anything. You can actually just hit the audio, which gets a little spicier, but and then, because you don't exactly know what you're gonna get and then, but you can obviously use text prompt as part of that. Then we can also do generative audio too. Of course, if you don't have, if you don't have an audio file, for whatever reason, we can, we use 11 labs or any of the audio models. We use cartesia, play HD. We're kind of Switzerland in terms of the other media generation that are related to the to it. So that's where we are from technology standpoint. I think I mentioned DeepMind, the person running that we hired from DeepMind, who is the inventor of the perceiver architecture. He's the first icon on the perceiver architecture, which is the cousin of a transformer architecture family, and he's leading the AI team here, which is about 10 researchers between research engineering and research science. So that's generally where we're at. I would say the one, the one there on Mirage, is our first GTM. This is all public like our first GTM for that is, is up market if we're selling to performance marketers doing paid at paid social so they're making video ads for, you know, DTC brands, consumer finance, gaming companies, those, those types of things.
15:59Oh, I think you're on mute, sorry.
Oh, I think you're on mute, sorry.
Oh, I think you're on mute, sorry.
Oh, I think you're on mute, sorry.
S Speaker 316:03So in that sense, your customers more on consumer and consumer, right? Like, not really, not necessarily, target
So in that sense, your customers more on consumer and consumer, right? Like, not really, not necessarily, target
So in that sense, your customers more on consumer and consumer, right? Like, not really, not necessarily, target
So in that sense, your customers more on consumer and consumer, right? Like, not really, not necessarily, target
16:11on enterprise customers.
on enterprise customers.
on enterprise customers.
on enterprise customers.
S Speaker 416:14So the first GTM is actually very much targeted up market, so think like mid market and enterprise. That's actually price, Yep,
So the first GTM is actually very much targeted up market, so think like mid market and enterprise. That's actually price, Yep,
So the first GTM is actually very much targeted up market, so think like mid market and enterprise. That's actually price, Yep,
So the first GTM is actually very much targeted up market, so think like mid market and enterprise. That's actually price, Yep,
16:25yeah, we're very, very much targeted towards
yeah, we're very, very much targeted towards
yeah, we're very, very much targeted towards
yeah, we're very, very much targeted towards
S Speaker 316:28earlier that you the data you gather is from consumer. That's correct. How does that coming from like, do you when you in the earlier days, do you
earlier that you the data you gather is from consumer. That's correct. How does that coming from like, do you when you in the earlier days, do you
earlier that you the data you gather is from consumer. That's correct. How does that coming from like, do you when you in the earlier days, do you
earlier that you the data you gather is from consumer. That's correct. How does that coming from like, do you when you in the earlier days, do you
S Speaker 416:38know we still, most of our revenue still today comes from prosumer and the consumer, consumer tiers. We have a free tier. So we go
know we still, most of our revenue still today comes from prosumer and the consumer, consumer tiers. We have a free tier. So we go
know we still, most of our revenue still today comes from prosumer and the consumer, consumer tiers. We have a free tier. So we go
know we still, most of our revenue still today comes from prosumer and the consumer, consumer tiers. We have a free tier. So we go
16:50and the 1000s of dollars,
and the 1000s of dollars,
and the 1000s of dollars,
and the 1000s of dollars,
S Speaker 416:53and the, yeah, the consumer is still available. We were not killing it. We're continuing to grow. It, very healthy. Business growing really fast, like that. I would think of us as like Canva in that regard, that for the first, you know, five plus years, they only sold consumer, they had no teams product, and they layered in an enterprise A, B to B, enterprise business. On top of that, that was like 2019 or something for them, so as much later, but that's the same thing we're doing. We're not we're not getting rid of our lower tiers or our self serve business, but we are now building a sales team and targeting very much, targeting up market. Got
and the, yeah, the consumer is still available. We were not killing it. We're continuing to grow. It, very healthy. Business growing really fast, like that. I would think of us as like Canva in that regard, that for the first, you know, five plus years, they only sold consumer, they had no teams product, and they layered in an enterprise A, B to B, enterprise business. On top of that, that was like 2019 or something for them, so as much later, but that's the same thing we're doing. We're not we're not getting rid of our lower tiers or our self serve business, but we are now building a sales team and targeting very much, targeting up market. Got
and the, yeah, the consumer is still available. We were not killing it. We're continuing to grow. It, very healthy. Business growing really fast, like that. I would think of us as like Canva in that regard, that for the first, you know, five plus years, they only sold consumer, they had no teams product, and they layered in an enterprise A, B to B, enterprise business. On top of that, that was like 2019 or something for them, so as much later, but that's the same thing we're doing. We're not we're not getting rid of our lower tiers or our self serve business, but we are now building a sales team and targeting very much, targeting up market. Got
and the, yeah, the consumer is still available. We were not killing it. We're continuing to grow. It, very healthy. Business growing really fast, like that. I would think of us as like Canva in that regard, that for the first, you know, five plus years, they only sold consumer, they had no teams product, and they layered in an enterprise A, B to B, enterprise business. On top of that, that was like 2019 or something for them, so as much later, but that's the same thing we're doing. We're not we're not getting rid of our lower tiers or our self serve business, but we are now building a sales team and targeting very much, targeting up market. Got
S Speaker 317:29it and for consumer use case you mentioned is on, like, a
it and for consumer use case you mentioned is on, like, a
it and for consumer use case you mentioned is on, like, a
it and for consumer use case you mentioned is on, like, a
17:35podcast and gaming, or
podcast and gaming, or
podcast and gaming, or
podcast and gaming, or
S Speaker 317:37much more broader use cases, and also social media and
much more broader use cases, and also social media and
much more broader use cases, and also social media and
much more broader use cases, and also social media and
S Speaker 417:41the consumer. Yeah, consumer, like, we have a lot of editing products. I've only been really talking about video generation here, just because, based on your question, that the other big differentiation of our platform is we have a full, full fledged video editor. So we have a consumer we compete with CAP cut. That's really we, you know, we have a mobile and desktop based editor. Our real differentiation there is honestly similar, again, bringing it back to the analogy, is we always want to start, start you with something. We don't want you to just come in and be a plain old video editor. There's 2000 other video editors you can go and do that in the way we think about it. Is our primary product there is called AI edit. So we want you to be able to come in with a video you actually you the person. Our target user does not know how to edit video. They're not a video editor or like a pro creator, or anything like that. They probably just talking on camera. They probably posting on LinkedIn at the real consumer level, and they they look to us to edit the video for them. So we do that. Algorithm will do trimming, zooming in, zooming out. You can try this out. It's available, and you can, it'll add a generative imagery, sound effects, music, like all the components that go into a video. And you come you'll come out with a fully edited video that's ready to be posted on LinkedIn or Instagram or something, you know, for and that's, you know, a consumer all the way through, you know, a dentist office or something, some local small business, right? Like they have no resources. They don't know how to edit video, but they're, you know, they know that they're supposed to be posting video to promote their business, right? And so that's a huge part of captions. And honestly, kind of like the magic behind captions. And as you can probably guess, as we are going to roll out our video generation model to consumer as well. And as you can probably guess, like those things are actually getting married and so come out, you can generate video and get a fully edited output. And that's very important to us, because the editing is also a big component of communication at the end of the day and telling the story, no one just wants to like not, nothing wrong with the talking head companies, Avatar companies, but, like, just personally, like, we don't think that the future is like, just person talking on camera, like, that's just not fully usable to social media or marketing use cases. They needed attention edited. You need it's critical. Like, editing side is, honestly, it's probably like 80% of the actual attention to the video, whether you're making an ad or you're just like, again, you're just like, posting social media. So we're very focused on, actually, on both sides of this, not, not just one
the consumer. Yeah, consumer, like, we have a lot of editing products. I've only been really talking about video generation here, just because, based on your question, that the other big differentiation of our platform is we have a full, full fledged video editor. So we have a consumer we compete with CAP cut. That's really we, you know, we have a mobile and desktop based editor. Our real differentiation there is honestly similar, again, bringing it back to the analogy, is we always want to start, start you with something. We don't want you to just come in and be a plain old video editor. There's 2000 other video editors you can go and do that in the way we think about it. Is our primary product there is called AI edit. So we want you to be able to come in with a video you actually you the person. Our target user does not know how to edit video. They're not a video editor or like a pro creator, or anything like that. They probably just talking on camera. They probably posting on LinkedIn at the real consumer level, and they they look to us to edit the video for them. So we do that. Algorithm will do trimming, zooming in, zooming out. You can try this out. It's available, and you can, it'll add a generative imagery, sound effects, music, like all the components that go into a video. And you come you'll come out with a fully edited video that's ready to be posted on LinkedIn or Instagram or something, you know, for and that's, you know, a consumer all the way through, you know, a dentist office or something, some local small business, right? Like they have no resources. They don't know how to edit video, but they're, you know, they know that they're supposed to be posting video to promote their business, right? And so that's a huge part of captions. And honestly, kind of like the magic behind captions. And as you can probably guess, as we are going to roll out our video generation model to consumer as well. And as you can probably guess, like those things are actually getting married and so come out, you can generate video and get a fully edited output. And that's very important to us, because the editing is also a big component of communication at the end of the day and telling the story, no one just wants to like not, nothing wrong with the talking head companies, Avatar companies, but, like, just personally, like, we don't think that the future is like, just person talking on camera, like, that's just not fully usable to social media or marketing use cases. They needed attention edited. You need it's critical. Like, editing side is, honestly, it's probably like 80% of the actual attention to the video, whether you're making an ad or you're just like, again, you're just like, posting social media. So we're very focused on, actually, on both sides of this, not, not just one
the consumer. Yeah, consumer, like, we have a lot of editing products. I've only been really talking about video generation here, just because, based on your question, that the other big differentiation of our platform is we have a full, full fledged video editor. So we have a consumer we compete with CAP cut. That's really we, you know, we have a mobile and desktop based editor. Our real differentiation there is honestly similar, again, bringing it back to the analogy, is we always want to start, start you with something. We don't want you to just come in and be a plain old video editor. There's 2000 other video editors you can go and do that in the way we think about it. Is our primary product there is called AI edit. So we want you to be able to come in with a video you actually you the person. Our target user does not know how to edit video. They're not a video editor or like a pro creator, or anything like that. They probably just talking on camera. They probably posting on LinkedIn at the real consumer level, and they they look to us to edit the video for them. So we do that. Algorithm will do trimming, zooming in, zooming out. You can try this out. It's available, and you can, it'll add a generative imagery, sound effects, music, like all the components that go into a video. And you come you'll come out with a fully edited video that's ready to be posted on LinkedIn or Instagram or something, you know, for and that's, you know, a consumer all the way through, you know, a dentist office or something, some local small business, right? Like they have no resources. They don't know how to edit video, but they're, you know, they know that they're supposed to be posting video to promote their business, right? And so that's a huge part of captions. And honestly, kind of like the magic behind captions. And as you can probably guess, as we are going to roll out our video generation model to consumer as well. And as you can probably guess, like those things are actually getting married and so come out, you can generate video and get a fully edited output. And that's very important to us, because the editing is also a big component of communication at the end of the day and telling the story, no one just wants to like not, nothing wrong with the talking head companies, Avatar companies, but, like, just personally, like, we don't think that the future is like, just person talking on camera, like, that's just not fully usable to social media or marketing use cases. They needed attention edited. You need it's critical. Like, editing side is, honestly, it's probably like 80% of the actual attention to the video, whether you're making an ad or you're just like, again, you're just like, posting social media. So we're very focused on, actually, on both sides of this, not, not just one
the consumer. Yeah, consumer, like, we have a lot of editing products. I've only been really talking about video generation here, just because, based on your question, that the other big differentiation of our platform is we have a full, full fledged video editor. So we have a consumer we compete with CAP cut. That's really we, you know, we have a mobile and desktop based editor. Our real differentiation there is honestly similar, again, bringing it back to the analogy, is we always want to start, start you with something. We don't want you to just come in and be a plain old video editor. There's 2000 other video editors you can go and do that in the way we think about it. Is our primary product there is called AI edit. So we want you to be able to come in with a video you actually you the person. Our target user does not know how to edit video. They're not a video editor or like a pro creator, or anything like that. They probably just talking on camera. They probably posting on LinkedIn at the real consumer level, and they they look to us to edit the video for them. So we do that. Algorithm will do trimming, zooming in, zooming out. You can try this out. It's available, and you can, it'll add a generative imagery, sound effects, music, like all the components that go into a video. And you come you'll come out with a fully edited video that's ready to be posted on LinkedIn or Instagram or something, you know, for and that's, you know, a consumer all the way through, you know, a dentist office or something, some local small business, right? Like they have no resources. They don't know how to edit video, but they're, you know, they know that they're supposed to be posting video to promote their business, right? And so that's a huge part of captions. And honestly, kind of like the magic behind captions. And as you can probably guess, as we are going to roll out our video generation model to consumer as well. And as you can probably guess, like those things are actually getting married and so come out, you can generate video and get a fully edited output. And that's very important to us, because the editing is also a big component of communication at the end of the day and telling the story, no one just wants to like not, nothing wrong with the talking head companies, Avatar companies, but, like, just personally, like, we don't think that the future is like, just person talking on camera, like, that's just not fully usable to social media or marketing use cases. They needed attention edited. You need it's critical. Like, editing side is, honestly, it's probably like 80% of the actual attention to the video, whether you're making an ad or you're just like, again, you're just like, posting social media. So we're very focused on, actually, on both sides of this, not, not just one
S Speaker 320:24or something. Would you need, I guess, a user to have their own script first, right? We
or something. Would you need, I guess, a user to have their own script first, right? We
or something. Would you need, I guess, a user to have their own script first, right? We
or something. Would you need, I guess, a user to have their own script first, right? We
S Speaker 420:31have a script. We have a script writer. You don't need, yeah, you don't need your own. You can just type in, you know, we work with, like, rock pick and stuff. We don't concentrate on the LLM side. But you can, yeah, you can just type in an idea, and we'll come up with the whole script for you. And you can either record yourself or, well, you or you can generate the video. You don't even need to do anything, yeah,
have a script. We have a script writer. You don't need, yeah, you don't need your own. You can just type in, you know, we work with, like, rock pick and stuff. We don't concentrate on the LLM side. But you can, yeah, you can just type in an idea, and we'll come up with the whole script for you. And you can either record yourself or, well, you or you can generate the video. You don't even need to do anything, yeah,
have a script. We have a script writer. You don't need, yeah, you don't need your own. You can just type in, you know, we work with, like, rock pick and stuff. We don't concentrate on the LLM side. But you can, yeah, you can just type in an idea, and we'll come up with the whole script for you. And you can either record yourself or, well, you or you can generate the video. You don't even need to do anything, yeah,
have a script. We have a script writer. You don't need, yeah, you don't need your own. You can just type in, you know, we work with, like, rock pick and stuff. We don't concentrate on the LLM side. But you can, yeah, you can just type in an idea, and we'll come up with the whole script for you. And you can either record yourself or, well, you or you can generate the video. You don't even need to do anything, yeah,
20:52yeah. And you have
S Speaker 320:54the, like, when you say about lips model, but the,
the, like, when you say about lips model, but the,
the, like, when you say about lips model, but the,
the, like, when you say about lips model, but the,
20:59I guess, the Talking Figure, the
I guess, the Talking Figure, the
I guess, the Talking Figure, the
I guess, the Talking Figure, the
21:01model, you can also have that, like
model, you can also have that, like
model, you can also have that, like
model, you can also have that, like
21:03the face and everything
the face and everything
the face and everything
the face and everything
S Speaker 421:04off, yeah? So like the foundation model, the Mirage generates a full person with body language, hand movement, everything. So, I mean, yeah, you can enter off camera. Yeah, you have full control. But
off, yeah? So like the foundation model, the Mirage generates a full person with body language, hand movement, everything. So, I mean, yeah, you can enter off camera. Yeah, you have full control. But
off, yeah? So like the foundation model, the Mirage generates a full person with body language, hand movement, everything. So, I mean, yeah, you can enter off camera. Yeah, you have full control. But
off, yeah? So like the foundation model, the Mirage generates a full person with body language, hand movement, everything. So, I mean, yeah, you can enter off camera. Yeah, you have full control. But
S Speaker 321:19literally, the only, the only video you generate, is the, sorry, the own model that
literally, the only, the only video you generate, is the, sorry, the own model that
literally, the only, the only video you generate, is the, sorry, the own model that
literally, the only, the only video you generate, is the, sorry, the own model that
21:25you bought as a lips model,
you bought as a lips model,
you bought as a lips model,
you bought as a lips model,
21:29right? Sorry, did you say the old model?
right? Sorry, did you say the old model?
right? Sorry, did you say the old model?
right? Sorry, did you say the old model?
S Speaker 321:32I mean, the your, your yourself, own, the model, self, build model.
I mean, the your, your yourself, own, the model, self, build model.
I mean, the your, your yourself, own, the model, self, build model.
I mean, the your, your yourself, own, the model, self, build model.
S Speaker 421:38Oh, we, actually, we developed both. Okay, okay. Do we have two, two models that we developed from scratch, the foundation model that we've been talking about. And then, as well as the lip sync model, we also build that from scratch. Okay,
Oh, we, actually, we developed both. Okay, okay. Do we have two, two models that we developed from scratch, the foundation model that we've been talking about. And then, as well as the lip sync model, we also build that from scratch. Okay,
Oh, we, actually, we developed both. Okay, okay. Do we have two, two models that we developed from scratch, the foundation model that we've been talking about. And then, as well as the lip sync model, we also build that from scratch. Okay,
Oh, we, actually, we developed both. Okay, okay. Do we have two, two models that we developed from scratch, the foundation model that we've been talking about. And then, as well as the lip sync model, we also build that from scratch. Okay,
S Speaker 121:52got it and quite your new model, Mirage. Does that also natively generate voice? Or do you still use an 11 apps
got it and quite your new model, Mirage. Does that also natively generate voice? Or do you still use an 11 apps
got it and quite your new model, Mirage. Does that also natively generate voice? Or do you still use an 11 apps
got it and quite your new model, Mirage. Does that also natively generate voice? Or do you still use an 11 apps
22:01API to generate voice?
API to generate voice?
API to generate voice?
API to generate voice?
S Speaker 422:02Yeah, so we can take in an auto file in which we don't manipulate at all, or we generate from one of the voice providers we actually work with, I guess actually, I did mention
Yeah, so we can take in an auto file in which we don't manipulate at all, or we generate from one of the voice providers we actually work with, I guess actually, I did mention
Yeah, so we can take in an auto file in which we don't manipulate at all, or we generate from one of the voice providers we actually work with, I guess actually, I did mention
Yeah, so we can take in an auto file in which we don't manipulate at all, or we generate from one of the voice providers we actually work with, I guess actually, I did mention
S Speaker 322:13you said cartesia, and also 11 labs, yeah. Okay. So the question is, how do you compete with this so many similar, I guess, players in the sector? Because I recently, I talked to Jenny and video sector, that is, who is a go to market advisor. And there she advised, you know, a bunch of genetic, video, application specific companies. And he she said, right now, it's kind of a competitive and even Haitian is reducing the price in order to maintain the retention. Yeah, like, this is, like, first a bit more competitive, is getting more more and more competitive. I
you said cartesia, and also 11 labs, yeah. Okay. So the question is, how do you compete with this so many similar, I guess, players in the sector? Because I recently, I talked to Jenny and video sector, that is, who is a go to market advisor. And there she advised, you know, a bunch of genetic, video, application specific companies. And he she said, right now, it's kind of a competitive and even Haitian is reducing the price in order to maintain the retention. Yeah, like, this is, like, first a bit more competitive, is getting more more and more competitive. I
you said cartesia, and also 11 labs, yeah. Okay. So the question is, how do you compete with this so many similar, I guess, players in the sector? Because I recently, I talked to Jenny and video sector, that is, who is a go to market advisor. And there she advised, you know, a bunch of genetic, video, application specific companies. And he she said, right now, it's kind of a competitive and even Haitian is reducing the price in order to maintain the retention. Yeah, like, this is, like, first a bit more competitive, is getting more more and more competitive. I
you said cartesia, and also 11 labs, yeah. Okay. So the question is, how do you compete with this so many similar, I guess, players in the sector? Because I recently, I talked to Jenny and video sector, that is, who is a go to market advisor. And there she advised, you know, a bunch of genetic, video, application specific companies. And he she said, right now, it's kind of a competitive and even Haitian is reducing the price in order to maintain the retention. Yeah, like, this is, like, first a bit more competitive, is getting more more and more competitive. I
S Speaker 423:00completely agree. I think the in the that world, like, I think the how we compete is we, we completely stopped working on lip sync. Like, I don't think it's a differentiator in technology. I think that no one offers a foundation model like Mirage today. No one has a model that Avatars can speak. It's literally not available anywhere. So we're right now. We have no competition in that regard. And so for performance marketing teams and selling them like we we don't even run into other people in the in the deal, most of the cell is actually an additional cell because they don't know how to generate video. That's really what we're up against right now with other companies, to be honest, like those like Synthesia, hey Gen and they're like, I was they just haven't focused on marketing use cases. So to be honest, we haven't seen much competition in that, in the space with, I think, in the longer form videos and like, like, in podcasting and like, or L and D, those kinds of things, I think there's a tons of competition, but to be honest, we don't really play in that space. So I said you say
completely agree. I think the in the that world, like, I think the how we compete is we, we completely stopped working on lip sync. Like, I don't think it's a differentiator in technology. I think that no one offers a foundation model like Mirage today. No one has a model that Avatars can speak. It's literally not available anywhere. So we're right now. We have no competition in that regard. And so for performance marketing teams and selling them like we we don't even run into other people in the in the deal, most of the cell is actually an additional cell because they don't know how to generate video. That's really what we're up against right now with other companies, to be honest, like those like Synthesia, hey Gen and they're like, I was they just haven't focused on marketing use cases. So to be honest, we haven't seen much competition in that, in the space with, I think, in the longer form videos and like, like, in podcasting and like, or L and D, those kinds of things, I think there's a tons of competition, but to be honest, we don't really play in that space. So I said you say
completely agree. I think the in the that world, like, I think the how we compete is we, we completely stopped working on lip sync. Like, I don't think it's a differentiator in technology. I think that no one offers a foundation model like Mirage today. No one has a model that Avatars can speak. It's literally not available anywhere. So we're right now. We have no competition in that regard. And so for performance marketing teams and selling them like we we don't even run into other people in the in the deal, most of the cell is actually an additional cell because they don't know how to generate video. That's really what we're up against right now with other companies, to be honest, like those like Synthesia, hey Gen and they're like, I was they just haven't focused on marketing use cases. So to be honest, we haven't seen much competition in that, in the space with, I think, in the longer form videos and like, like, in podcasting and like, or L and D, those kinds of things, I think there's a tons of competition, but to be honest, we don't really play in that space. So I said you say
completely agree. I think the in the that world, like, I think the how we compete is we, we completely stopped working on lip sync. Like, I don't think it's a differentiator in technology. I think that no one offers a foundation model like Mirage today. No one has a model that Avatars can speak. It's literally not available anywhere. So we're right now. We have no competition in that regard. And so for performance marketing teams and selling them like we we don't even run into other people in the in the deal, most of the cell is actually an additional cell because they don't know how to generate video. That's really what we're up against right now with other companies, to be honest, like those like Synthesia, hey Gen and they're like, I was they just haven't focused on marketing use cases. So to be honest, we haven't seen much competition in that, in the space with, I think, in the longer form videos and like, like, in podcasting and like, or L and D, those kinds of things, I think there's a tons of competition, but to be honest, we don't really play in that space. So I said you say
24:12you are the only one can do the talking avatar.
you are the only one can do the talking avatar.
you are the only one can do the talking avatar.
you are the only one can do the talking avatar.
S Speaker 424:18When we will, we say that. I mean, like we only were, the only ones that offer a foundation model
When we will, we say that. I mean, like we only were, the only ones that offer a foundation model
When we will, we say that. I mean, like we only were, the only ones that offer a foundation model
When we will, we say that. I mean, like we only were, the only ones that offer a foundation model
S Speaker 324:22that can make a model for talking like all, like
that can make a model for talking like all, like
that can make a model for talking like all, like
that can make a model for talking like all, like
S Speaker 424:26sandesh agent, all the ones you've mentioned before, too, is they have talking avatars. But you, you can't control, you don't have control over the avatar. You can't tell to you can do things like expressive or something like that. But those are all like, I think that's like, the misnomer in that world is like, that's not generative video. That's not That's not even a generated like, it's not a generalized model. So when you see someone, a video of someone that's been produced on agent, that's a real video that's not generated at all. That person was in the studio and literally was recorded. All that's being applied is the lip model, and at most, there's a face model in which they manipulate what the person is saying. But that comes with all the problems we've talked about with a foundation model, nothing is real like that, that base model. So there's a there's a million things wrong with the our problems of the listing model built business, right, especially when you're doing enterprise, because, like, those act, those are actors who are being contracted, and so they're signing, like Synthesia, signs like two year deals, agents, very similar. They go through casting agencies, right? They sign up and hundreds of people to do this. But like, if any of those people bow out, they can't use the avatar anymore. And so your business and you've been using that avatar, they literally go away. And so, like, there's a million problems with that model. And so, yeah, we didn't want to play playing in that at all. And so that's when we started to really invest in that, in that new
sandesh agent, all the ones you've mentioned before, too, is they have talking avatars. But you, you can't control, you don't have control over the avatar. You can't tell to you can do things like expressive or something like that. But those are all like, I think that's like, the misnomer in that world is like, that's not generative video. That's not That's not even a generated like, it's not a generalized model. So when you see someone, a video of someone that's been produced on agent, that's a real video that's not generated at all. That person was in the studio and literally was recorded. All that's being applied is the lip model, and at most, there's a face model in which they manipulate what the person is saying. But that comes with all the problems we've talked about with a foundation model, nothing is real like that, that base model. So there's a there's a million things wrong with the our problems of the listing model built business, right, especially when you're doing enterprise, because, like, those act, those are actors who are being contracted, and so they're signing, like Synthesia, signs like two year deals, agents, very similar. They go through casting agencies, right? They sign up and hundreds of people to do this. But like, if any of those people bow out, they can't use the avatar anymore. And so your business and you've been using that avatar, they literally go away. And so, like, there's a million problems with that model. And so, yeah, we didn't want to play playing in that at all. And so that's when we started to really invest in that, in that new
sandesh agent, all the ones you've mentioned before, too, is they have talking avatars. But you, you can't control, you don't have control over the avatar. You can't tell to you can do things like expressive or something like that. But those are all like, I think that's like, the misnomer in that world is like, that's not generative video. That's not That's not even a generated like, it's not a generalized model. So when you see someone, a video of someone that's been produced on agent, that's a real video that's not generated at all. That person was in the studio and literally was recorded. All that's being applied is the lip model, and at most, there's a face model in which they manipulate what the person is saying. But that comes with all the problems we've talked about with a foundation model, nothing is real like that, that base model. So there's a there's a million things wrong with the our problems of the listing model built business, right, especially when you're doing enterprise, because, like, those act, those are actors who are being contracted, and so they're signing, like Synthesia, signs like two year deals, agents, very similar. They go through casting agencies, right? They sign up and hundreds of people to do this. But like, if any of those people bow out, they can't use the avatar anymore. And so your business and you've been using that avatar, they literally go away. And so, like, there's a million problems with that model. And so, yeah, we didn't want to play playing in that at all. And so that's when we started to really invest in that, in that new
sandesh agent, all the ones you've mentioned before, too, is they have talking avatars. But you, you can't control, you don't have control over the avatar. You can't tell to you can do things like expressive or something like that. But those are all like, I think that's like, the misnomer in that world is like, that's not generative video. That's not That's not even a generated like, it's not a generalized model. So when you see someone, a video of someone that's been produced on agent, that's a real video that's not generated at all. That person was in the studio and literally was recorded. All that's being applied is the lip model, and at most, there's a face model in which they manipulate what the person is saying. But that comes with all the problems we've talked about with a foundation model, nothing is real like that, that base model. So there's a there's a million things wrong with the our problems of the listing model built business, right, especially when you're doing enterprise, because, like, those act, those are actors who are being contracted, and so they're signing, like Synthesia, signs like two year deals, agents, very similar. They go through casting agencies, right? They sign up and hundreds of people to do this. But like, if any of those people bow out, they can't use the avatar anymore. And so your business and you've been using that avatar, they literally go away. And so, like, there's a million problems with that model. And so, yeah, we didn't want to play playing in that at all. And so that's when we started to really invest in that, in that new
S Speaker 325:58model. Got it, got it, but interesting somehow, I think the agent one you can just do, like a record, maybe a video in studio, and then they can give them a script, and then that talking monitor can continue? Yeah,
model. Got it, got it, but interesting somehow, I think the agent one you can just do, like a record, maybe a video in studio, and then they can give them a script, and then that talking monitor can continue? Yeah,
model. Got it, got it, but interesting somehow, I think the agent one you can just do, like a record, maybe a video in studio, and then they can give them a script, and then that talking monitor can continue? Yeah,
model. Got it, got it, but interesting somehow, I think the agent one you can just do, like a record, maybe a video in studio, and then they can give them a script, and then that talking monitor can continue? Yeah,
26:12I thought that's a use case. Yeah, yeah, yeah.
I thought that's a use case. Yeah, yeah, yeah.
I thought that's a use case. Yeah, yeah, yeah.
I thought that's a use case. Yeah, yeah, yeah.
S Speaker 426:15So you recording like the person can be remembered studio, and then you avatar, a script, and then they calculate. But that comes with all the problems we discussed, right? Which is, you can't actually change the body movement again. You can,
So you recording like the person can be remembered studio, and then you avatar, a script, and then they calculate. But that comes with all the problems we discussed, right? Which is, you can't actually change the body movement again. You can,
So you recording like the person can be remembered studio, and then you avatar, a script, and then they calculate. But that comes with all the problems we discussed, right? Which is, you can't actually change the body movement again. You can,
So you recording like the person can be remembered studio, and then you avatar, a script, and then they calculate. But that comes with all the problems we discussed, right? Which is, you can't actually change the body movement again. You can,
26:27no, you cannot do that.
no, you cannot do that.
no, you cannot do that.
no, you cannot do that.
S Speaker 426:31It all falls. If that just doesn't work in marketing. There's a million other ways that that works in, I think, in L, D and then other spaces. But with you're doing paid advertising or something you're selling to marketers like this just says not it's just not a it's not usable, yeah, okay,
It all falls. If that just doesn't work in marketing. There's a million other ways that that works in, I think, in L, D and then other spaces. But with you're doing paid advertising or something you're selling to marketers like this just says not it's just not a it's not usable, yeah, okay,
It all falls. If that just doesn't work in marketing. There's a million other ways that that works in, I think, in L, D and then other spaces. But with you're doing paid advertising or something you're selling to marketers like this just says not it's just not a it's not usable, yeah, okay,
It all falls. If that just doesn't work in marketing. There's a million other ways that that works in, I think, in L, D and then other spaces. But with you're doing paid advertising or something you're selling to marketers like this just says not it's just not a it's not usable, yeah, okay,
S Speaker 326:47which means that captions, model can have the body movement, but it's limited to the upper body, like for
which means that captions, model can have the body movement, but it's limited to the upper body, like for
which means that captions, model can have the body movement, but it's limited to the upper body, like for
which means that captions, model can have the body movement, but it's limited to the upper body, like for
S Speaker 426:56the entire avatar, yeah, generate their entire body, yep. Okay, we started. We started because it's performance advertising targeted. Most of the videos and stuff you see are like a portrait, but it can generate. It can generate much more. And we have a couple of things coming, with longer generation coming, also releasing, like imagery and clone and image that kind of
the entire avatar, yeah, generate their entire body, yep. Okay, we started. We started because it's performance advertising targeted. Most of the videos and stuff you see are like a portrait, but it can generate. It can generate much more. And we have a couple of things coming, with longer generation coming, also releasing, like imagery and clone and image that kind of
the entire avatar, yeah, generate their entire body, yep. Okay, we started. We started because it's performance advertising targeted. Most of the videos and stuff you see are like a portrait, but it can generate. It can generate much more. And we have a couple of things coming, with longer generation coming, also releasing, like imagery and clone and image that kind of
the entire avatar, yeah, generate their entire body, yep. Okay, we started. We started because it's performance advertising targeted. Most of the videos and stuff you see are like a portrait, but it can generate. It can generate much more. And we have a couple of things coming, with longer generation coming, also releasing, like imagery and clone and image that kind of
27:21stuff. I see in that perspective.
stuff. I see in that perspective.
stuff. I see in that perspective.
stuff. I see in that perspective.
S Speaker 327:23Do you see your go to work here is a little bit different from a gym, because agents also talking in the corporate marketing team? Yeah.
Do you see your go to work here is a little bit different from a gym, because agents also talking in the corporate marketing team? Yeah.
Do you see your go to work here is a little bit different from a gym, because agents also talking in the corporate marketing team? Yeah.
Do you see your go to work here is a little bit different from a gym, because agents also talking in the corporate marketing team? Yeah.
27:30Yeah. I know they, I know they are.
Yeah. I know they, I know they are.
Yeah. I know they, I know they are.
Yeah. I know they, I know they are.
S Speaker 427:35I don't fully I have any extensive use cases there. Most of the business has been translation in L and D, historically. I think they all that's a very recent pivot
I don't fully I have any extensive use cases there. Most of the business has been translation in L and D, historically. I think they all that's a very recent pivot
I don't fully I have any extensive use cases there. Most of the business has been translation in L and D, historically. I think they all that's a very recent pivot
I don't fully I have any extensive use cases there. Most of the business has been translation in L and D, historically. I think they all that's a very recent pivot
S Speaker 327:46I see, I see, and I know, like a cool is also targeting at that sector as well. I don't know if I look at a cool business,
I see, I see, and I know, like a cool is also targeting at that sector as well. I don't know if I look at a cool business,
I see, I see, and I know, like a cool is also targeting at that sector as well. I don't know if I look at a cool business,
I see, I see, and I know, like a cool is also targeting at that sector as well. I don't know if I look at a cool business,
S Speaker 527:56yeah, that, I don't know I see,
yeah, that, I don't know I see,
yeah, that, I don't know I see,
yeah, that, I don't know I see,
S Speaker 328:00I see, okay, but you guys for enterprise is more like, I guess your current enterprise goes to market is for corporate marketing, right? Definitely, marketing, yeah. Oh, got it. And as you said that right now, you still have majority of the portion for consumer and consumer, and then the rest for enterprise, and you just have post market. So you're gonna enterprise
I see, okay, but you guys for enterprise is more like, I guess your current enterprise goes to market is for corporate marketing, right? Definitely, marketing, yeah. Oh, got it. And as you said that right now, you still have majority of the portion for consumer and consumer, and then the rest for enterprise, and you just have post market. So you're gonna enterprise
I see, okay, but you guys for enterprise is more like, I guess your current enterprise goes to market is for corporate marketing, right? Definitely, marketing, yeah. Oh, got it. And as you said that right now, you still have majority of the portion for consumer and consumer, and then the rest for enterprise, and you just have post market. So you're gonna enterprise
I see, okay, but you guys for enterprise is more like, I guess your current enterprise goes to market is for corporate marketing, right? Definitely, marketing, yeah. Oh, got it. And as you said that right now, you still have majority of the portion for consumer and consumer, and then the rest for enterprise, and you just have post market. So you're gonna enterprise
S Speaker 428:20is very new for us, so this only in the last couple of months where we've really started to build the team extensively. The rest of the prosumer business has been around for, you know, three years.
is very new for us, so this only in the last couple of months where we've really started to build the team extensively. The rest of the prosumer business has been around for, you know, three years.
is very new for us, so this only in the last couple of months where we've really started to build the team extensively. The rest of the prosumer business has been around for, you know, three years.
is very new for us, so this only in the last couple of months where we've really started to build the team extensively. The rest of the prosumer business has been around for, you know, three years.
S Speaker 328:30Oh, for three years. Yeah. So what is the ARR for now? So
Oh, for three years. Yeah. So what is the ARR for now? So
Oh, for three years. Yeah. So what is the ARR for now? So
Oh, for three years. Yeah. So what is the ARR for now? So
28:35we're not, we don't say it public,
we're not, we don't say it public,
we're not, we don't say it public,
we're not, we don't say it public,
S Speaker 428:38but think in the, think in the range, we'll put it this way, we are very much targeting nine figure ARR at this point. Okay, got it.
but think in the, think in the range, we'll put it this way, we are very much targeting nine figure ARR at this point. Okay, got it.
but think in the, think in the range, we'll put it this way, we are very much targeting nine figure ARR at this point. Okay, got it.
but think in the, think in the range, we'll put it this way, we are very much targeting nine figure ARR at this point. Okay, got it.
S Speaker 128:50Dwight, your enterprise offering Mirage today. Is that an API offering, do you have a platform built on top of it, and second follow up there would be, do you have, have you built a lot of creative group on the output generation? Is it just a text, audio to video generation? Or can you also control how the video output would be? Yeah,
Dwight, your enterprise offering Mirage today. Is that an API offering, do you have a platform built on top of it, and second follow up there would be, do you have, have you built a lot of creative group on the output generation? Is it just a text, audio to video generation? Or can you also control how the video output would be? Yeah,
Dwight, your enterprise offering Mirage today. Is that an API offering, do you have a platform built on top of it, and second follow up there would be, do you have, have you built a lot of creative group on the output generation? Is it just a text, audio to video generation? Or can you also control how the video output would be? Yeah,
Dwight, your enterprise offering Mirage today. Is that an API offering, do you have a platform built on top of it, and second follow up there would be, do you have, have you built a lot of creative group on the output generation? Is it just a text, audio to video generation? Or can you also control how the video output would be? Yeah,
S Speaker 429:07so you can control the output. But on the API question, we have not put this model behind an API, but that's largely because it's we just, we haven't. We need to build more safety regards around it is a very dangerous model, as you probably can guess, and so we haven't from just mainly for that reason. And the other reason is we want to have direct relationship with the customer right now, and so we don't want to do, we're not embedding in other products yet, like partnerships and stuff. We are talking to a couple customers about you just having direct customers have API access, but not, but not something we're going to do probably too soon. Our old the lip sync model, the AI, edit, feature translation, and those things are available in an API. So we do have people using that programmatically. Okay,
so you can control the output. But on the API question, we have not put this model behind an API, but that's largely because it's we just, we haven't. We need to build more safety regards around it is a very dangerous model, as you probably can guess, and so we haven't from just mainly for that reason. And the other reason is we want to have direct relationship with the customer right now, and so we don't want to do, we're not embedding in other products yet, like partnerships and stuff. We are talking to a couple customers about you just having direct customers have API access, but not, but not something we're going to do probably too soon. Our old the lip sync model, the AI, edit, feature translation, and those things are available in an API. So we do have people using that programmatically. Okay,
so you can control the output. But on the API question, we have not put this model behind an API, but that's largely because it's we just, we haven't. We need to build more safety regards around it is a very dangerous model, as you probably can guess, and so we haven't from just mainly for that reason. And the other reason is we want to have direct relationship with the customer right now, and so we don't want to do, we're not embedding in other products yet, like partnerships and stuff. We are talking to a couple customers about you just having direct customers have API access, but not, but not something we're going to do probably too soon. Our old the lip sync model, the AI, edit, feature translation, and those things are available in an API. So we do have people using that programmatically. Okay,
so you can control the output. But on the API question, we have not put this model behind an API, but that's largely because it's we just, we haven't. We need to build more safety regards around it is a very dangerous model, as you probably can guess, and so we haven't from just mainly for that reason. And the other reason is we want to have direct relationship with the customer right now, and so we don't want to do, we're not embedding in other products yet, like partnerships and stuff. We are talking to a couple customers about you just having direct customers have API access, but not, but not something we're going to do probably too soon. Our old the lip sync model, the AI, edit, feature translation, and those things are available in an API. So we do have people using that programmatically. Okay,
S Speaker 330:01Yeah, all right. So curious about your right now, almost 10 o'clock, so are you going to do another round of fundraising? Or,
Yeah, all right. So curious about your right now, almost 10 o'clock, so are you going to do another round of fundraising? Or,
Yeah, all right. So curious about your right now, almost 10 o'clock, so are you going to do another round of fundraising? Or,
Yeah, all right. So curious about your right now, almost 10 o'clock, so are you going to do another round of fundraising? Or,
S Speaker 430:10yeah, I think we, we want to build up, like, we don't really need cash right now from a like, head count or marketing costs, like the kind of two other areas, but we do want to build up our infrastructure, and we are thinking about taking on more capital to do that so we don't have, I don't have a definitive date, but we are thinking about,
yeah, I think we, we want to build up, like, we don't really need cash right now from a like, head count or marketing costs, like the kind of two other areas, but we do want to build up our infrastructure, and we are thinking about taking on more capital to do that so we don't have, I don't have a definitive date, but we are thinking about,
yeah, I think we, we want to build up, like, we don't really need cash right now from a like, head count or marketing costs, like the kind of two other areas, but we do want to build up our infrastructure, and we are thinking about taking on more capital to do that so we don't have, I don't have a definitive date, but we are thinking about,
yeah, I think we, we want to build up, like, we don't really need cash right now from a like, head count or marketing costs, like the kind of two other areas, but we do want to build up our infrastructure, and we are thinking about taking on more capital to do that so we don't have, I don't have a definitive date, but we are thinking about,
30:36like early summer, like end of q2 timeframe, somewhere in the
like early summer, like end of q2 timeframe, somewhere in the
like early summer, like end of q2 timeframe, somewhere in the
like early summer, like end of q2 timeframe, somewhere in the
30:39start racing, okay?
30:43And I apologize I am gonna have to run.
And I apologize I am gonna have to run.
And I apologize I am gonna have to run.
And I apologize I am gonna have to run.
S Speaker 530:47Yeah, yeah, all right. So I really appreciate that. I really appreciate the
Yeah, yeah, all right. So I really appreciate that. I really appreciate the
Yeah, yeah, all right. So I really appreciate that. I really appreciate the
Yeah, yeah, all right. So I really appreciate that. I really appreciate the
30:54time. Yeah, we I'd love to
time. Yeah, we I'd love to
time. Yeah, we I'd love to
time. Yeah, we I'd love to
S Speaker 430:56have follow up. If you guys are kind of actively looking at the space and stuff,
have follow up. If you guys are kind of actively looking at the space and stuff,
have follow up. If you guys are kind of actively looking at the space and stuff,
have follow up. If you guys are kind of actively looking at the space and stuff,
31:21Yes, sounds good. All right. Okay, thanks.
Yes, sounds good. All right. Okay, thanks.
Yes, sounds good. All right. Okay, thanks.
Yes, sounds good. All right. Okay, thanks.