Meeting: Fiddler Pitch
Mon, Sep 29
9:32 AM
35 min
Priyesh P
Introduction and Overview of Qualcomm Ventures
0:
URL: https://otter.ai/u/Rdv8cixbjtsHmryaaI4rZxpZ2to
Downloaded: 2025-12-21T20:18:19.868771
Method: text_extraction
============================================================

S Speaker 10:12So, you know, I know this close game, though, and I was in a good
So, you know, I know this close game, though, and I was in a good
So, you know, I know this close game, though, and I was in a good
So, you know, I know this close game, though, and I was in a good
S Speaker 10:31I may have spoken to you for like, many years ago.
I may have spoken to you for like, many years ago.
I may have spoken to you for like, many years ago.
I may have spoken to you for like, many years ago.
S Speaker 20:33So that was also pre, I think maybe even
So that was also pre, I think maybe even
So that was also pre, I think maybe even
So that was also pre, I think maybe even
0:38before MLM, right. Yeah. Pre Gen AI, the
before MLM, right. Yeah. Pre Gen AI, the
before MLM, right. Yeah. Pre Gen AI, the
before MLM, right. Yeah. Pre Gen AI, the
S Speaker 13:56Nice to meet you, Priyesh and Deepak, thanks for the great intro. We can go next. Krishna gaude, the founder, CEO of Fiddler. We started the companies seven years ago in the space of ml observability. And then now, as the you know, sort of the industry is moving towards Gen AI and agents. We have refocused the company on Gen AI and agent, and sort of like becoming the control blame for AI and pass it down to Fahad for equipment.
Nice to meet you, Priyesh and Deepak, thanks for the great intro. We can go next. Krishna gaude, the founder, CEO of Fiddler. We started the companies seven years ago in the space of ml observability. And then now, as the you know, sort of the industry is moving towards Gen AI and agents. We have refocused the company on Gen AI and agent, and sort of like becoming the control blame for AI and pass it down to Fahad for equipment.
Nice to meet you, Priyesh and Deepak, thanks for the great intro. We can go next. Krishna gaude, the founder, CEO of Fiddler. We started the companies seven years ago in the space of ml observability. And then now, as the you know, sort of the industry is moving towards Gen AI and agents. We have refocused the company on Gen AI and agent, and sort of like becoming the control blame for AI and pass it down to Fahad for equipment.
Nice to meet you, Priyesh and Deepak, thanks for the great intro. We can go next. Krishna gaude, the founder, CEO of Fiddler. We started the companies seven years ago in the space of ml observability. And then now, as the you know, sort of the industry is moving towards Gen AI and agents. We have refocused the company on Gen AI and agent, and sort of like becoming the control blame for AI and pass it down to Fahad for equipment.
S Speaker 34:22Fahad for Yeah, hi Deepak and Priyesh. My name is Fahad priyeshky. I am the president of go to market here at fiddler, I've been on board for about three months or so now. And for that, I was with another startup called tigera for about seven years, and then was at identity and access management company before that, called Centrify, which got acquired by Thelma Bravo for 600 million. And I've been in security and infrastructure my entire career.
Fahad for Yeah, hi Deepak and Priyesh. My name is Fahad priyeshky. I am the president of go to market here at fiddler, I've been on board for about three months or so now. And for that, I was with another startup called tigera for about seven years, and then was at identity and access management company before that, called Centrify, which got acquired by Thelma Bravo for 600 million. And I've been in security and infrastructure my entire career.
Fahad for Yeah, hi Deepak and Priyesh. My name is Fahad priyeshky. I am the president of go to market here at fiddler, I've been on board for about three months or so now. And for that, I was with another startup called tigera for about seven years, and then was at identity and access management company before that, called Centrify, which got acquired by Thelma Bravo for 600 million. And I've been in security and infrastructure my entire career.
Fahad for Yeah, hi Deepak and Priyesh. My name is Fahad priyeshky. I am the president of go to market here at fiddler, I've been on board for about three months or so now. And for that, I was with another startup called tigera for about seven years, and then was at identity and access management company before that, called Centrify, which got acquired by Thelma Bravo for 600 million. And I've been in security and infrastructure my entire career.
S Speaker 27:16One question, Krishna, and maybe we can keep this, you know, open ended. Uh, so one, one of the use cases or value props that AIG might have had, right? So they might have, did they start with ml first? Or were they building applications? And they were like, you know, this is like a black box, or it's, it's hard to get a pulse on how good we are building, or what you're building, and we don't have the confidence to deploy it. Let me find a solution, and Fiddler's out there, or they were already doing ml. And then they were like, let me tag on, you know, LLM and compound systems, eig happened.
One question, Krishna, and maybe we can keep this, you know, open ended. Uh, so one, one of the use cases or value props that AIG might have had, right? So they might have, did they start with ml first? Or were they building applications? And they were like, you know, this is like a black box, or it's, it's hard to get a pulse on how good we are building, or what you're building, and we don't have the confidence to deploy it. Let me find a solution, and Fiddler's out there, or they were already doing ml. And then they were like, let me tag on, you know, LLM and compound systems, eig happened.
One question, Krishna, and maybe we can keep this, you know, open ended. Uh, so one, one of the use cases or value props that AIG might have had, right? So they might have, did they start with ml first? Or were they building applications? And they were like, you know, this is like a black box, or it's, it's hard to get a pulse on how good we are building, or what you're building, and we don't have the confidence to deploy it. Let me find a solution, and Fiddler's out there, or they were already doing ml. And then they were like, let me tag on, you know, LLM and compound systems, eig happened.
One question, Krishna, and maybe we can keep this, you know, open ended. Uh, so one, one of the use cases or value props that AIG might have had, right? So they might have, did they start with ml first? Or were they building applications? And they were like, you know, this is like a black box, or it's, it's hard to get a pulse on how good we are building, or what you're building, and we don't have the confidence to deploy it. Let me find a solution, and Fiddler's out there, or they were already doing ml. And then they were like, let me tag on, you know, LLM and compound systems, eig happened.
S Speaker 211:12within Qualcomm, right? This resonates because we are building multiple AI systems for multiple different use cases, from automating some document generation to, you know, something as complicated as log analysis, right? Which is hard for llms to do. So we've seen this pain point. I think what I'm trying to also understand is, how deep do you guys go in terms of evaluating models and the performance of AI systems? Because it's a lot of different components that drive to an accurate system. And when I say accurate, relatively accurate, there's nothing like deterministic, as it said. And there's a lot of this, you know, test testing, and lot of like, you know, training the model and, you know, guardrails and policy enforcement, right? But that's, that's post deployment, right? But before deployment, you have to have, you know, understanding of the nuts and bolts of the model, data component. So where, where are you strong and where do you spend more time, and where the product resonates a lot, because there's a lot of pieces here. Yeah, absolutely.
within Qualcomm, right? This resonates because we are building multiple AI systems for multiple different use cases, from automating some document generation to, you know, something as complicated as log analysis, right? Which is hard for llms to do. So we've seen this pain point. I think what I'm trying to also understand is, how deep do you guys go in terms of evaluating models and the performance of AI systems? Because it's a lot of different components that drive to an accurate system. And when I say accurate, relatively accurate, there's nothing like deterministic, as it said. And there's a lot of this, you know, test testing, and lot of like, you know, training the model and, you know, guardrails and policy enforcement, right? But that's, that's post deployment, right? But before deployment, you have to have, you know, understanding of the nuts and bolts of the model, data component. So where, where are you strong and where do you spend more time, and where the product resonates a lot, because there's a lot of pieces here. Yeah, absolutely.
within Qualcomm, right? This resonates because we are building multiple AI systems for multiple different use cases, from automating some document generation to, you know, something as complicated as log analysis, right? Which is hard for llms to do. So we've seen this pain point. I think what I'm trying to also understand is, how deep do you guys go in terms of evaluating models and the performance of AI systems? Because it's a lot of different components that drive to an accurate system. And when I say accurate, relatively accurate, there's nothing like deterministic, as it said. And there's a lot of this, you know, test testing, and lot of like, you know, training the model and, you know, guardrails and policy enforcement, right? But that's, that's post deployment, right? But before deployment, you have to have, you know, understanding of the nuts and bolts of the model, data component. So where, where are you strong and where do you spend more time, and where the product resonates a lot, because there's a lot of pieces here. Yeah, absolutely.
within Qualcomm, right? This resonates because we are building multiple AI systems for multiple different use cases, from automating some document generation to, you know, something as complicated as log analysis, right? Which is hard for llms to do. So we've seen this pain point. I think what I'm trying to also understand is, how deep do you guys go in terms of evaluating models and the performance of AI systems? Because it's a lot of different components that drive to an accurate system. And when I say accurate, relatively accurate, there's nothing like deterministic, as it said. And there's a lot of this, you know, test testing, and lot of like, you know, training the model and, you know, guardrails and policy enforcement, right? But that's, that's post deployment, right? But before deployment, you have to have, you know, understanding of the nuts and bolts of the model, data component. So where, where are you strong and where do you spend more time, and where the product resonates a lot, because there's a lot of pieces here. Yeah, absolutely.
S Speaker 215:58right? So that we are saying when they get into evals, right? So, observability
right? So that we are saying when they get into evals, right? So, observability
right? So that we are saying when they get into evals, right? So, observability
right? So that we are saying when they get into evals, right? So, observability
S Speaker 116:02player, yeah, pre production, because, you know, LLM
player, yeah, pre production, because, you know, LLM
player, yeah, pre production, because, you know, LLM
player, yeah, pre production, because, you know, LLM
S Speaker 216:06as a judge, will only come when you have to evaluate something correct,
as a judge, will only come when you have to evaluate something correct,
as a judge, will only come when you have to evaluate something correct,
as a judge, will only come when you have to evaluate something correct,
S Speaker 116:10and you have to evaluate in both online and offline, right. Even production, observability, you need that online evaluation, right? You're going to know if something is hallucinatory in provide, right? Or is it unsafe input, or has a security attack? You know that that you need an online evaluation, right?
and you have to evaluate in both online and offline, right. Even production, observability, you need that online evaluation, right? You're going to know if something is hallucinatory in provide, right? Or is it unsafe input, or has a security attack? You know that that you need an online evaluation, right?
and you have to evaluate in both online and offline, right. Even production, observability, you need that online evaluation, right? You're going to know if something is hallucinatory in provide, right? Or is it unsafe input, or has a security attack? You know that that you need an online evaluation, right?
and you have to evaluate in both online and offline, right. Even production, observability, you need that online evaluation, right? You're going to know if something is hallucinatory in provide, right? Or is it unsafe input, or has a security attack? You know that that you need an online evaluation, right?
S Speaker 216:29But, I mean, I'm trying to understand like that. A lot of it is also driven by, like, an SME, who's also saying, this is accepted, this is not like, let's say, you know, plot, code or cursor, right? I actually, when I say no, this not accepted. It's kind of changed that. So how are you like? There's, it's not as automated as maybe I'm I'm understanding.
But, I mean, I'm trying to understand like that. A lot of it is also driven by, like, an SME, who's also saying, this is accepted, this is not like, let's say, you know, plot, code or cursor, right? I actually, when I say no, this not accepted. It's kind of changed that. So how are you like? There's, it's not as automated as maybe I'm I'm understanding.
But, I mean, I'm trying to understand like that. A lot of it is also driven by, like, an SME, who's also saying, this is accepted, this is not like, let's say, you know, plot, code or cursor, right? I actually, when I say no, this not accepted. It's kind of changed that. So how are you like? There's, it's not as automated as maybe I'm I'm understanding.
But, I mean, I'm trying to understand like that. A lot of it is also driven by, like, an SME, who's also saying, this is accepted, this is not like, let's say, you know, plot, code or cursor, right? I actually, when I say no, this not accepted. It's kind of changed that. So how are you like? There's, it's not as automated as maybe I'm I'm understanding.
S Speaker 218:17I see, I see, I see. So this is, of course, you know, a second order, derivative observability. What is the first order? What's the foundation layer, right? So when you start with, you know, either pre production or post, what data points are you capturing?
I see, I see, I see. So this is, of course, you know, a second order, derivative observability. What is the first order? What's the foundation layer, right? So when you start with, you know, either pre production or post, what data points are you capturing?
I see, I see, I see. So this is, of course, you know, a second order, derivative observability. What is the first order? What's the foundation layer, right? So when you start with, you know, either pre production or post, what data points are you capturing?
I see, I see, I see. So this is, of course, you know, a second order, derivative observability. What is the first order? What's the foundation layer, right? So when you start with, you know, either pre production or post, what data points are you capturing?
S Speaker 118:31So we are capturing, again, we are capturing the inputs and outputs of these agentic systems, or outputs like prompt responses and and then we are then computing the faithfulness as toxicity, so they're out of the box metrics. There's no cold start problem here, because we have the inbuilt models. So you have, like, the metrics available. And then you can then bring your humans, your subject matter experts, to annotate things. You can create custom KPIs. You can say, okay, let's say I'm trying to build an arithmetic, you know, assistant. I can have my own custom KPIs that okay, this is my test for additions. This is my test for subtractions. I can create my own custom got it, and I can define it either in native Python, other ways that you can do it. One thing
So we are capturing, again, we are capturing the inputs and outputs of these agentic systems, or outputs like prompt responses and and then we are then computing the faithfulness as toxicity, so they're out of the box metrics. There's no cold start problem here, because we have the inbuilt models. So you have, like, the metrics available. And then you can then bring your humans, your subject matter experts, to annotate things. You can create custom KPIs. You can say, okay, let's say I'm trying to build an arithmetic, you know, assistant. I can have my own custom KPIs that okay, this is my test for additions. This is my test for subtractions. I can create my own custom got it, and I can define it either in native Python, other ways that you can do it. One thing
So we are capturing, again, we are capturing the inputs and outputs of these agentic systems, or outputs like prompt responses and and then we are then computing the faithfulness as toxicity, so they're out of the box metrics. There's no cold start problem here, because we have the inbuilt models. So you have, like, the metrics available. And then you can then bring your humans, your subject matter experts, to annotate things. You can create custom KPIs. You can say, okay, let's say I'm trying to build an arithmetic, you know, assistant. I can have my own custom KPIs that okay, this is my test for additions. This is my test for subtractions. I can create my own custom got it, and I can define it either in native Python, other ways that you can do it. One thing
So we are capturing, again, we are capturing the inputs and outputs of these agentic systems, or outputs like prompt responses and and then we are then computing the faithfulness as toxicity, so they're out of the box metrics. There's no cold start problem here, because we have the inbuilt models. So you have, like, the metrics available. And then you can then bring your humans, your subject matter experts, to annotate things. You can create custom KPIs. You can say, okay, let's say I'm trying to build an arithmetic, you know, assistant. I can have my own custom KPIs that okay, this is my test for additions. This is my test for subtractions. I can create my own custom got it, and I can define it either in native Python, other ways that you can do it. One thing
S Speaker 219:18I was thinking about was a can you alter weights of the model, for example, right? Can you actually go in deeper into the model and optimize for Hey, these are the trends we are seeing. I get the input output. But I was trying to understand, how much depth do you have in model monitoring and model improvement?
I was thinking about was a can you alter weights of the model, for example, right? Can you actually go in deeper into the model and optimize for Hey, these are the trends we are seeing. I get the input output. But I was trying to understand, how much depth do you have in model monitoring and model improvement?
I was thinking about was a can you alter weights of the model, for example, right? Can you actually go in deeper into the model and optimize for Hey, these are the trends we are seeing. I get the input output. But I was trying to understand, how much depth do you have in model monitoring and model improvement?
I was thinking about was a can you alter weights of the model, for example, right? Can you actually go in deeper into the model and optimize for Hey, these are the trends we are seeing. I get the input output. But I was trying to understand, how much depth do you have in model monitoring and model improvement?
S Speaker 119:35Yeah. So the way we we do it is we cannot, right? So what we can do is we can sort of control the model behavior. So if, like, the model is hallucinating, or if there's a tool call that is failing, we can retry. So the way that this is the user workflow, right? So going back to that customer support, Zendesk example, yeah, an agent, you know, gets a customer inquiry, retrieves to get history from Zendesk, you know, calls Titan or whatever, gets an action, and we are instrumenting in each of the nodes of the agentic workflow. And if an agent prepares a message that, hey, refund will be issued Friday, Fiddler can evaluate it against the policy. So this is where policy enforcement can happen. Yeah, they said we cannot promise a refund without it. And so therefore it can basically fall back to a guardrail, saying that, hey, your request is being reviewed, or whatever, right, right? Also login incident. And then if the tool call is having this you know problems, right? Like, if you're calling the API with wrong parameters, right, you know, or you're not authenticated to call this API, you can basically get, you know, evaluated like from a tool, tool access perspective. All of these data is centrally stored for audit and replay purposes. So this is the workflow understood here. It doesn't matter what the model is under the scenes, it's agnostic to the model.
Yeah. So the way we we do it is we cannot, right? So what we can do is we can sort of control the model behavior. So if, like, the model is hallucinating, or if there's a tool call that is failing, we can retry. So the way that this is the user workflow, right? So going back to that customer support, Zendesk example, yeah, an agent, you know, gets a customer inquiry, retrieves to get history from Zendesk, you know, calls Titan or whatever, gets an action, and we are instrumenting in each of the nodes of the agentic workflow. And if an agent prepares a message that, hey, refund will be issued Friday, Fiddler can evaluate it against the policy. So this is where policy enforcement can happen. Yeah, they said we cannot promise a refund without it. And so therefore it can basically fall back to a guardrail, saying that, hey, your request is being reviewed, or whatever, right, right? Also login incident. And then if the tool call is having this you know problems, right? Like, if you're calling the API with wrong parameters, right, you know, or you're not authenticated to call this API, you can basically get, you know, evaluated like from a tool, tool access perspective. All of these data is centrally stored for audit and replay purposes. So this is the workflow understood here. It doesn't matter what the model is under the scenes, it's agnostic to the model.
Yeah. So the way we we do it is we cannot, right? So what we can do is we can sort of control the model behavior. So if, like, the model is hallucinating, or if there's a tool call that is failing, we can retry. So the way that this is the user workflow, right? So going back to that customer support, Zendesk example, yeah, an agent, you know, gets a customer inquiry, retrieves to get history from Zendesk, you know, calls Titan or whatever, gets an action, and we are instrumenting in each of the nodes of the agentic workflow. And if an agent prepares a message that, hey, refund will be issued Friday, Fiddler can evaluate it against the policy. So this is where policy enforcement can happen. Yeah, they said we cannot promise a refund without it. And so therefore it can basically fall back to a guardrail, saying that, hey, your request is being reviewed, or whatever, right, right? Also login incident. And then if the tool call is having this you know problems, right? Like, if you're calling the API with wrong parameters, right, you know, or you're not authenticated to call this API, you can basically get, you know, evaluated like from a tool, tool access perspective. All of these data is centrally stored for audit and replay purposes. So this is the workflow understood here. It doesn't matter what the model is under the scenes, it's agnostic to the model.
Yeah. So the way we we do it is we cannot, right? So what we can do is we can sort of control the model behavior. So if, like, the model is hallucinating, or if there's a tool call that is failing, we can retry. So the way that this is the user workflow, right? So going back to that customer support, Zendesk example, yeah, an agent, you know, gets a customer inquiry, retrieves to get history from Zendesk, you know, calls Titan or whatever, gets an action, and we are instrumenting in each of the nodes of the agentic workflow. And if an agent prepares a message that, hey, refund will be issued Friday, Fiddler can evaluate it against the policy. So this is where policy enforcement can happen. Yeah, they said we cannot promise a refund without it. And so therefore it can basically fall back to a guardrail, saying that, hey, your request is being reviewed, or whatever, right, right? Also login incident. And then if the tool call is having this you know problems, right? Like, if you're calling the API with wrong parameters, right, you know, or you're not authenticated to call this API, you can basically get, you know, evaluated like from a tool, tool access perspective. All of these data is centrally stored for audit and replay purposes. So this is the workflow understood here. It doesn't matter what the model is under the scenes, it's agnostic to the model.
S Speaker 220:53I got it. I got it. That's why, because I read ML model monitoring on the websites, it got me a little confused, because a lot of this is workflow monitoring, because what you do is you basically evaluate input output and make sure they're working fine, and it's picking out. So it's different from what my understanding was coming in. I still thought of you guys as ml observability tool from the agency, and that's what I was trying to fill
I got it. I got it. That's why, because I read ML model monitoring on the websites, it got me a little confused, because a lot of this is workflow monitoring, because what you do is you basically evaluate input output and make sure they're working fine, and it's picking out. So it's different from what my understanding was coming in. I still thought of you guys as ml observability tool from the agency, and that's what I was trying to fill
I got it. I got it. That's why, because I read ML model monitoring on the websites, it got me a little confused, because a lot of this is workflow monitoring, because what you do is you basically evaluate input output and make sure they're working fine, and it's picking out. So it's different from what my understanding was coming in. I still thought of you guys as ml observability tool from the agency, and that's what I was trying to fill
I got it. I got it. That's why, because I read ML model monitoring on the websites, it got me a little confused, because a lot of this is workflow monitoring, because what you do is you basically evaluate input output and make sure they're working fine, and it's picking out. So it's different from what my understanding was coming in. I still thought of you guys as ml observability tool from the agency, and that's what I was trying to fill
S Speaker 121:20in the gaps. Even in ML observability, we would not modify the model weights. We would basically be more looking at the inputs and outputs. We would look at the drift in inputs, output changes, yes, yes, yes. How do the inputs and correlate with outputs? And try to, like, do it that way. So, and otherwise, it's not scalable, because you would not be able to fit it to different models.
in the gaps. Even in ML observability, we would not modify the model weights. We would basically be more looking at the inputs and outputs. We would look at the drift in inputs, output changes, yes, yes, yes. How do the inputs and correlate with outputs? And try to, like, do it that way. So, and otherwise, it's not scalable, because you would not be able to fit it to different models.
in the gaps. Even in ML observability, we would not modify the model weights. We would basically be more looking at the inputs and outputs. We would look at the drift in inputs, output changes, yes, yes, yes. How do the inputs and correlate with outputs? And try to, like, do it that way. So, and otherwise, it's not scalable, because you would not be able to fit it to different models.
in the gaps. Even in ML observability, we would not modify the model weights. We would basically be more looking at the inputs and outputs. We would look at the drift in inputs, output changes, yes, yes, yes. How do the inputs and correlate with outputs? And try to, like, do it that way. So, and otherwise, it's not scalable, because you would not be able to fit it to different models.
S Speaker 221:39That's exactly, that's exactly what I was and maybe a couple of questions. One, what enables you to look at input output? So is it there are an API driven piece? Do you have to make sure integrations happen?
That's exactly, that's exactly what I was and maybe a couple of questions. One, what enables you to look at input output? So is it there are an API driven piece? Do you have to make sure integrations happen?
That's exactly, that's exactly what I was and maybe a couple of questions. One, what enables you to look at input output? So is it there are an API driven piece? Do you have to make sure integrations happen?
That's exactly, that's exactly what I was and maybe a couple of questions. One, what enables you to look at input output? So is it there are an API driven piece? Do you have to make sure integrations happen?
S Speaker 121:52So we have integrations with most of the agentic frameworks. We have SDK level integrations with a one line of code within line graph, line, chain. You can send like logs to fiddler. We have integrations with like cloud providers, like, you know, if you're hosting models on bedrock or Gemini, we can have logs come to Fiddler from in a batch oriented manner. We also can, you know, sort of instrument tool calls, if the tools themselves are behaving as agents to like, we can get, like, their audit logs, like, through MCP, like, we've recently done some work with glean and Agent force and other third party agents where we can, we are able to get the logs from them right, essentially, either through SDK that runs inside the code, or through some sort of a connector that gets the logs, you know, from the system. That's how it that's how we get it. And once we are able to get that log, then we can illuminate the dashboards, we can set up all the metrics and essentially make sure that we can run all the guardrails and whatnot, right? So that's kind of the vision, from that observability to this control plane, is we become that in that runtime path as much as possible. You know, we can, we can do that live, controlling of the agent. We can monitor the tool calls, monitor the model calls, and and make sure that we are evaluating budgets, token budgets, and, you know, authentication aspects are created. And all of this is in one place for you like, in terms to look at what's going on with your AI system.
So we have integrations with most of the agentic frameworks. We have SDK level integrations with a one line of code within line graph, line, chain. You can send like logs to fiddler. We have integrations with like cloud providers, like, you know, if you're hosting models on bedrock or Gemini, we can have logs come to Fiddler from in a batch oriented manner. We also can, you know, sort of instrument tool calls, if the tools themselves are behaving as agents to like, we can get, like, their audit logs, like, through MCP, like, we've recently done some work with glean and Agent force and other third party agents where we can, we are able to get the logs from them right, essentially, either through SDK that runs inside the code, or through some sort of a connector that gets the logs, you know, from the system. That's how it that's how we get it. And once we are able to get that log, then we can illuminate the dashboards, we can set up all the metrics and essentially make sure that we can run all the guardrails and whatnot, right? So that's kind of the vision, from that observability to this control plane, is we become that in that runtime path as much as possible. You know, we can, we can do that live, controlling of the agent. We can monitor the tool calls, monitor the model calls, and and make sure that we are evaluating budgets, token budgets, and, you know, authentication aspects are created. And all of this is in one place for you like, in terms to look at what's going on with your AI system.
So we have integrations with most of the agentic frameworks. We have SDK level integrations with a one line of code within line graph, line, chain. You can send like logs to fiddler. We have integrations with like cloud providers, like, you know, if you're hosting models on bedrock or Gemini, we can have logs come to Fiddler from in a batch oriented manner. We also can, you know, sort of instrument tool calls, if the tools themselves are behaving as agents to like, we can get, like, their audit logs, like, through MCP, like, we've recently done some work with glean and Agent force and other third party agents where we can, we are able to get the logs from them right, essentially, either through SDK that runs inside the code, or through some sort of a connector that gets the logs, you know, from the system. That's how it that's how we get it. And once we are able to get that log, then we can illuminate the dashboards, we can set up all the metrics and essentially make sure that we can run all the guardrails and whatnot, right? So that's kind of the vision, from that observability to this control plane, is we become that in that runtime path as much as possible. You know, we can, we can do that live, controlling of the agent. We can monitor the tool calls, monitor the model calls, and and make sure that we are evaluating budgets, token budgets, and, you know, authentication aspects are created. And all of this is in one place for you like, in terms to look at what's going on with your AI system.
So we have integrations with most of the agentic frameworks. We have SDK level integrations with a one line of code within line graph, line, chain. You can send like logs to fiddler. We have integrations with like cloud providers, like, you know, if you're hosting models on bedrock or Gemini, we can have logs come to Fiddler from in a batch oriented manner. We also can, you know, sort of instrument tool calls, if the tools themselves are behaving as agents to like, we can get, like, their audit logs, like, through MCP, like, we've recently done some work with glean and Agent force and other third party agents where we can, we are able to get the logs from them right, essentially, either through SDK that runs inside the code, or through some sort of a connector that gets the logs, you know, from the system. That's how it that's how we get it. And once we are able to get that log, then we can illuminate the dashboards, we can set up all the metrics and essentially make sure that we can run all the guardrails and whatnot, right? So that's kind of the vision, from that observability to this control plane, is we become that in that runtime path as much as possible. You know, we can, we can do that live, controlling of the agent. We can monitor the tool calls, monitor the model calls, and and make sure that we are evaluating budgets, token budgets, and, you know, authentication aspects are created. And all of this is in one place for you like, in terms to look at what's going on with your AI system.
S Speaker 223:20Got it. Got it. And maybe last question before maybe I'll let Priyesh Also ask if he has any questions. By the way, do you guys have time? We can go beyond time. Yeah. So one question I had was within an enterprise today, right? Let's take the global Yeah. Are you seeing, you know, real AI projects being deployed, and maybe Farhad you can address some part of it, right? You know, my understanding is there is a lot of pressure from this suite, and there's a lot of show and tell, which is, you know, there's a lot of noise that, hey, we are doing something, you're patching some system together. And, you know, jobs are at stake or whatever. You know, goals are KPIs are at stake. But are real systems being deployed? And I know agents, majority of them, are failing today with enterprise for different reasons. It has nothing to do with, you know, the teams tablet or anything. How much of the global 1000 or 2000 Ian would be really doing serious AI
Got it. Got it. And maybe last question before maybe I'll let Priyesh Also ask if he has any questions. By the way, do you guys have time? We can go beyond time. Yeah. So one question I had was within an enterprise today, right? Let's take the global Yeah. Are you seeing, you know, real AI projects being deployed, and maybe Farhad you can address some part of it, right? You know, my understanding is there is a lot of pressure from this suite, and there's a lot of show and tell, which is, you know, there's a lot of noise that, hey, we are doing something, you're patching some system together. And, you know, jobs are at stake or whatever. You know, goals are KPIs are at stake. But are real systems being deployed? And I know agents, majority of them, are failing today with enterprise for different reasons. It has nothing to do with, you know, the teams tablet or anything. How much of the global 1000 or 2000 Ian would be really doing serious AI
Got it. Got it. And maybe last question before maybe I'll let Priyesh Also ask if he has any questions. By the way, do you guys have time? We can go beyond time. Yeah. So one question I had was within an enterprise today, right? Let's take the global Yeah. Are you seeing, you know, real AI projects being deployed, and maybe Farhad you can address some part of it, right? You know, my understanding is there is a lot of pressure from this suite, and there's a lot of show and tell, which is, you know, there's a lot of noise that, hey, we are doing something, you're patching some system together. And, you know, jobs are at stake or whatever. You know, goals are KPIs are at stake. But are real systems being deployed? And I know agents, majority of them, are failing today with enterprise for different reasons. It has nothing to do with, you know, the teams tablet or anything. How much of the global 1000 or 2000 Ian would be really doing serious AI
Got it. Got it. And maybe last question before maybe I'll let Priyesh Also ask if he has any questions. By the way, do you guys have time? We can go beyond time. Yeah. So one question I had was within an enterprise today, right? Let's take the global Yeah. Are you seeing, you know, real AI projects being deployed, and maybe Farhad you can address some part of it, right? You know, my understanding is there is a lot of pressure from this suite, and there's a lot of show and tell, which is, you know, there's a lot of noise that, hey, we are doing something, you're patching some system together. And, you know, jobs are at stake or whatever. You know, goals are KPIs are at stake. But are real systems being deployed? And I know agents, majority of them, are failing today with enterprise for different reasons. It has nothing to do with, you know, the teams tablet or anything. How much of the global 1000 or 2000 Ian would be really doing serious AI
S Speaker 324:23work today? Yeah, yeah. No, that's a great question, right? And the first part of that, what you were saying, you know, you kind of took the words out of my mouth, I think, because what we are seeing in the Fortune 500 especially the regulated side, is that there is a lot of board pressure. There's a lot of pressure coming from the CEO, from the board, and this is just not a CIO level initiative. So what we are seeing in, for example, at AIG, you know, they have a mandate that every single AIG employee has to use AI in some capacity for productivity improvements. Otherwise they, you know, they might get fired, right? Yeah, quite literally. And so what, what, you know, folks like AIG allied bank fidelity, you know, all these large fortune 500 companies, they, they do have projects, right? A lot of these projects, yes, are sort of these back office type of projects that they have, but a lot of them also have initial projects in production with, you know, customer facing apps, right, right? For example, Fidelity has a wealth management app, and AIG has a underwriting app that they have, right? But what is happening is, because you know, every you know, especially on the regulated side, security is essential. Compliance is essential. They you know, if anything goes wrong, their entire brand could get marginalized, right? And so this is where they bring us in. Is before they do anything, they need a solution like ours to ensure that right there is deep observability, and they can monitor hallucinations and drift and all you know, all the different metrics, and put guardrails in place to ensure that nothing nothing bad happens, right? So we are seeing projects, obviously, still early days in most cases. But that is also be the big opportunity, because this is not a if it will happen, it's when it will happen, right? And the progress that these companies are making across that paradigm. So every single monocleation
work today? Yeah, yeah. No, that's a great question, right? And the first part of that, what you were saying, you know, you kind of took the words out of my mouth, I think, because what we are seeing in the Fortune 500 especially the regulated side, is that there is a lot of board pressure. There's a lot of pressure coming from the CEO, from the board, and this is just not a CIO level initiative. So what we are seeing in, for example, at AIG, you know, they have a mandate that every single AIG employee has to use AI in some capacity for productivity improvements. Otherwise they, you know, they might get fired, right? Yeah, quite literally. And so what, what, you know, folks like AIG allied bank fidelity, you know, all these large fortune 500 companies, they, they do have projects, right? A lot of these projects, yes, are sort of these back office type of projects that they have, but a lot of them also have initial projects in production with, you know, customer facing apps, right, right? For example, Fidelity has a wealth management app, and AIG has a underwriting app that they have, right? But what is happening is, because you know, every you know, especially on the regulated side, security is essential. Compliance is essential. They you know, if anything goes wrong, their entire brand could get marginalized, right? And so this is where they bring us in. Is before they do anything, they need a solution like ours to ensure that right there is deep observability, and they can monitor hallucinations and drift and all you know, all the different metrics, and put guardrails in place to ensure that nothing nothing bad happens, right? So we are seeing projects, obviously, still early days in most cases. But that is also be the big opportunity, because this is not a if it will happen, it's when it will happen, right? And the progress that these companies are making across that paradigm. So every single monocleation
work today? Yeah, yeah. No, that's a great question, right? And the first part of that, what you were saying, you know, you kind of took the words out of my mouth, I think, because what we are seeing in the Fortune 500 especially the regulated side, is that there is a lot of board pressure. There's a lot of pressure coming from the CEO, from the board, and this is just not a CIO level initiative. So what we are seeing in, for example, at AIG, you know, they have a mandate that every single AIG employee has to use AI in some capacity for productivity improvements. Otherwise they, you know, they might get fired, right? Yeah, quite literally. And so what, what, you know, folks like AIG allied bank fidelity, you know, all these large fortune 500 companies, they, they do have projects, right? A lot of these projects, yes, are sort of these back office type of projects that they have, but a lot of them also have initial projects in production with, you know, customer facing apps, right, right? For example, Fidelity has a wealth management app, and AIG has a underwriting app that they have, right? But what is happening is, because you know, every you know, especially on the regulated side, security is essential. Compliance is essential. They you know, if anything goes wrong, their entire brand could get marginalized, right? And so this is where they bring us in. Is before they do anything, they need a solution like ours to ensure that right there is deep observability, and they can monitor hallucinations and drift and all you know, all the different metrics, and put guardrails in place to ensure that nothing nothing bad happens, right? So we are seeing projects, obviously, still early days in most cases. But that is also be the big opportunity, because this is not a if it will happen, it's when it will happen, right? And the progress that these companies are making across that paradigm. So every single monocleation
work today? Yeah, yeah. No, that's a great question, right? And the first part of that, what you were saying, you know, you kind of took the words out of my mouth, I think, because what we are seeing in the Fortune 500 especially the regulated side, is that there is a lot of board pressure. There's a lot of pressure coming from the CEO, from the board, and this is just not a CIO level initiative. So what we are seeing in, for example, at AIG, you know, they have a mandate that every single AIG employee has to use AI in some capacity for productivity improvements. Otherwise they, you know, they might get fired, right? Yeah, quite literally. And so what, what, you know, folks like AIG allied bank fidelity, you know, all these large fortune 500 companies, they, they do have projects, right? A lot of these projects, yes, are sort of these back office type of projects that they have, but a lot of them also have initial projects in production with, you know, customer facing apps, right, right? For example, Fidelity has a wealth management app, and AIG has a underwriting app that they have, right? But what is happening is, because you know, every you know, especially on the regulated side, security is essential. Compliance is essential. They you know, if anything goes wrong, their entire brand could get marginalized, right? And so this is where they bring us in. Is before they do anything, they need a solution like ours to ensure that right there is deep observability, and they can monitor hallucinations and drift and all you know, all the different metrics, and put guardrails in place to ensure that nothing nothing bad happens, right? So we are seeing projects, obviously, still early days in most cases. But that is also be the big opportunity, because this is not a if it will happen, it's when it will happen, right? And the progress that these companies are making across that paradigm. So every single monocleation
S Speaker 226:35I have for that is these are different buyers within the same company, somebody who's building for a customer facing application with an AIG, which is underwriting, is different from who's telling employees that hey, or you need to use AI, and you might be automating some of the workflows right to the the head, head of that project, versus the head of the one who owns underwriting. And I'm trying to say, do you run into that, that piece where, so we sell into the AI infrastructure buyer, right? So ultimately, all of this comes down to the center of excellence team, you know, within these companies, and they are trying to set up the AI infrastructure, and that's, and that's it. That's the point I was pointing along, which is not every organization in the Fortune 1000, 1000 has that AI excellence team well established, because there's a lot of noise, and we are seeing this. I'm just saying it from within. What I'm seeing at Qualcomm, there's no single core, centralized AI buyer, and that's fine. I'm not saying that's a problem. It's just where the market evolution is, where the market maturity is. It's not gotten to a centralized org level, at least within the top 1000 enterprises. So it might be like you might have to go
I have for that is these are different buyers within the same company, somebody who's building for a customer facing application with an AIG, which is underwriting, is different from who's telling employees that hey, or you need to use AI, and you might be automating some of the workflows right to the the head, head of that project, versus the head of the one who owns underwriting. And I'm trying to say, do you run into that, that piece where, so we sell into the AI infrastructure buyer, right? So ultimately, all of this comes down to the center of excellence team, you know, within these companies, and they are trying to set up the AI infrastructure, and that's, and that's it. That's the point I was pointing along, which is not every organization in the Fortune 1000, 1000 has that AI excellence team well established, because there's a lot of noise, and we are seeing this. I'm just saying it from within. What I'm seeing at Qualcomm, there's no single core, centralized AI buyer, and that's fine. I'm not saying that's a problem. It's just where the market evolution is, where the market maturity is. It's not gotten to a centralized org level, at least within the top 1000 enterprises. So it might be like you might have to go
I have for that is these are different buyers within the same company, somebody who's building for a customer facing application with an AIG, which is underwriting, is different from who's telling employees that hey, or you need to use AI, and you might be automating some of the workflows right to the the head, head of that project, versus the head of the one who owns underwriting. And I'm trying to say, do you run into that, that piece where, so we sell into the AI infrastructure buyer, right? So ultimately, all of this comes down to the center of excellence team, you know, within these companies, and they are trying to set up the AI infrastructure, and that's, and that's it. That's the point I was pointing along, which is not every organization in the Fortune 1000, 1000 has that AI excellence team well established, because there's a lot of noise, and we are seeing this. I'm just saying it from within. What I'm seeing at Qualcomm, there's no single core, centralized AI buyer, and that's fine. I'm not saying that's a problem. It's just where the market evolution is, where the market maturity is. It's not gotten to a centralized org level, at least within the top 1000 enterprises. So it might be like you might have to go
I have for that is these are different buyers within the same company, somebody who's building for a customer facing application with an AIG, which is underwriting, is different from who's telling employees that hey, or you need to use AI, and you might be automating some of the workflows right to the the head, head of that project, versus the head of the one who owns underwriting. And I'm trying to say, do you run into that, that piece where, so we sell into the AI infrastructure buyer, right? So ultimately, all of this comes down to the center of excellence team, you know, within these companies, and they are trying to set up the AI infrastructure, and that's, and that's it. That's the point I was pointing along, which is not every organization in the Fortune 1000, 1000 has that AI excellence team well established, because there's a lot of noise, and we are seeing this. I'm just saying it from within. What I'm seeing at Qualcomm, there's no single core, centralized AI buyer, and that's fine. I'm not saying that's a problem. It's just where the market evolution is, where the market maturity is. It's not gotten to a centralized org level, at least within the top 1000 enterprises. So it might be like you might have to go
S Speaker 329:00is, I mean, right now, concurrently out of this right hand side, you know, this is a small sample of what we have in the pipeline, but we have seven concurrent POCs going on. I just got off a forecast review call this morning, and we have, we're right in the middle of these POVs like Qualtrics and Lennar and Ericsson ADP, and, you know, a synchrony VMP bus is starting up, right? And so we have seven, eight of these concurrently happening, and every single one of them, they're agentic first. Agentic first, right?
is, I mean, right now, concurrently out of this right hand side, you know, this is a small sample of what we have in the pipeline, but we have seven concurrent POCs going on. I just got off a forecast review call this morning, and we have, we're right in the middle of these POVs like Qualtrics and Lennar and Ericsson ADP, and, you know, a synchrony VMP bus is starting up, right? And so we have seven, eight of these concurrently happening, and every single one of them, they're agentic first. Agentic first, right?
is, I mean, right now, concurrently out of this right hand side, you know, this is a small sample of what we have in the pipeline, but we have seven concurrent POCs going on. I just got off a forecast review call this morning, and we have, we're right in the middle of these POVs like Qualtrics and Lennar and Ericsson ADP, and, you know, a synchrony VMP bus is starting up, right? And so we have seven, eight of these concurrently happening, and every single one of them, they're agentic first. Agentic first, right?
is, I mean, right now, concurrently out of this right hand side, you know, this is a small sample of what we have in the pipeline, but we have seven concurrent POCs going on. I just got off a forecast review call this morning, and we have, we're right in the middle of these POVs like Qualtrics and Lennar and Ericsson ADP, and, you know, a synchrony VMP bus is starting up, right? And so we have seven, eight of these concurrently happening, and every single one of them, they're agentic first. Agentic first, right?
29:38So that's great about,
So that's great about,
So that's great about,
So that's great about,
S Speaker 329:39yeah, so what that, what that is telling you that's a very, very strong signal where the market is going, right? I mean, they, I mean, they're taking a lot of their time going deep on these POVs, and they wouldn't be doing that, at least, obviously, if they weren't serious about Of
yeah, so what that, what that is telling you that's a very, very strong signal where the market is going, right? I mean, they, I mean, they're taking a lot of their time going deep on these POVs, and they wouldn't be doing that, at least, obviously, if they weren't serious about Of
yeah, so what that, what that is telling you that's a very, very strong signal where the market is going, right? I mean, they, I mean, they're taking a lot of their time going deep on these POVs, and they wouldn't be doing that, at least, obviously, if they weren't serious about Of
yeah, so what that, what that is telling you that's a very, very strong signal where the market is going, right? I mean, they, I mean, they're taking a lot of their time going deep on these POVs, and they wouldn't be doing that, at least, obviously, if they weren't serious about Of
S Speaker 229:56course, of course. No, I Well, when I said not serious in the sense that it hasn't matured to that level. Of course, everybody wants to get there. I didn't have to question the potential of the space. I think that's a given. I'm just saying in terms of timing, where we are in terms of that tipping point. One question on my end, who do you see in these Poes most often, and where do you guys win?
course, of course. No, I Well, when I said not serious in the sense that it hasn't matured to that level. Of course, everybody wants to get there. I didn't have to question the potential of the space. I think that's a given. I'm just saying in terms of timing, where we are in terms of that tipping point. One question on my end, who do you see in these Poes most often, and where do you guys win?
course, of course. No, I Well, when I said not serious in the sense that it hasn't matured to that level. Of course, everybody wants to get there. I didn't have to question the potential of the space. I think that's a given. I'm just saying in terms of timing, where we are in terms of that tipping point. One question on my end, who do you see in these Poes most often, and where do you guys win?
course, of course. No, I Well, when I said not serious in the sense that it hasn't matured to that level. Of course, everybody wants to get there. I didn't have to question the potential of the space. I think that's a given. I'm just saying in terms of timing, where we are in terms of that tipping point. One question on my end, who do you see in these Poes most often, and where do you guys win?
S Speaker 231:59There's a lot to unpack here. Krishna, I know, and maybe we can do a follow up call. But this is super interesting and great to see that, you know, even established vendors are falling behind, right? I see Datadog and, like, observability has been a large market with multiple
There's a lot to unpack here. Krishna, I know, and maybe we can do a follow up call. But this is super interesting and great to see that, you know, even established vendors are falling behind, right? I see Datadog and, like, observability has been a large market with multiple
There's a lot to unpack here. Krishna, I know, and maybe we can do a follow up call. But this is super interesting and great to see that, you know, even established vendors are falling behind, right? I see Datadog and, like, observability has been a large market with multiple
There's a lot to unpack here. Krishna, I know, and maybe we can do a follow up call. But this is super interesting and great to see that, you know, even established vendors are falling behind, right? I see Datadog and, like, observability has been a large market with multiple
S Speaker 132:17vendors, right? And, yeah, so this architecture is very different, right? Like, yeah. Basically we have, we combined with, like, all the log receivers we have given our own columnar database and all the, you know, inferencing system for all the evaluator models and all of
vendors, right? And, yeah, so this architecture is very different, right? Like, yeah. Basically we have, we combined with, like, all the log receivers we have given our own columnar database and all the, you know, inferencing system for all the evaluator models and all of
vendors, right? And, yeah, so this architecture is very different, right? Like, yeah. Basically we have, we combined with, like, all the log receivers we have given our own columnar database and all the, you know, inferencing system for all the evaluator models and all of
vendors, right? And, yeah, so this architecture is very different, right? Like, yeah. Basically we have, we combined with, like, all the log receivers we have given our own columnar database and all the, you know, inferencing system for all the evaluator models and all of
S Speaker 232:32you know, one area was looking at data dogs, the agent observability, they give an execution flow right, real time, like, which tools it's calling, where did it fail?
you know, one area was looking at data dogs, the agent observability, they give an execution flow right, real time, like, which tools it's calling, where did it fail?
you know, one area was looking at data dogs, the agent observability, they give an execution flow right, real time, like, which tools it's calling, where did it fail?
you know, one area was looking at data dogs, the agent observability, they give an execution flow right, real time, like, which tools it's calling, where did it fail?
S Speaker 132:40Right? Like, we do the same thing. In fact, we do like a full blown agent in graph execution and metrics.
Right? Like, we do the same thing. In fact, we do like a full blown agent in graph execution and metrics.
Right? Like, we do the same thing. In fact, we do like a full blown agent in graph execution and metrics.
Right? Like, we do the same thing. In fact, we do like a full blown agent in graph execution and metrics.
S Speaker 232:46That's why I was saying we need a, we need a deeper dive into, like, we can go on and on, yeah,
That's why I was saying we need a, we need a deeper dive into, like, we can go on and on, yeah,
That's why I was saying we need a, we need a deeper dive into, like, we can go on and on, yeah,
That's why I was saying we need a, we need a deeper dive into, like, we can go on and on, yeah,
S Speaker 132:52happy to bring a product deep dive here. More interested in the product discussion we can do like a, more like a product demo and
happy to bring a product deep dive here. More interested in the product discussion we can do like a, more like a product demo and
happy to bring a product deep dive here. More interested in the product discussion we can do like a, more like a product demo and
happy to bring a product deep dive here. More interested in the product discussion we can do like a, more like a product demo and
S Speaker 133:19are raising 35 to 50 million, you know, basically right now. And we've got a term sheet, you know, last week for a 35 million round. And we are now fielding a couple of other offers as we speak, and it will mature this week, and we hope to be able to sign the lead term sheet end of the week and then put together a syndicate around these, you
are raising 35 to 50 million, you know, basically right now. And we've got a term sheet, you know, last week for a 35 million round. And we are now fielding a couple of other offers as we speak, and it will mature this week, and we hope to be able to sign the lead term sheet end of the week and then put together a syndicate around these, you
are raising 35 to 50 million, you know, basically right now. And we've got a term sheet, you know, last week for a 35 million round. And we are now fielding a couple of other offers as we speak, and it will mature this week, and we hope to be able to sign the lead term sheet end of the week and then put together a syndicate around these, you
are raising 35 to 50 million, you know, basically right now. And we've got a term sheet, you know, last week for a 35 million round. And we are now fielding a couple of other offers as we speak, and it will mature this week, and we hope to be able to sign the lead term sheet end of the week and then put together a syndicate around these, you
S Speaker 233:40know, around these, and how much was the valuation, or what range are you expecting? What is the
know, around these, and how much was the valuation, or what range are you expecting? What is the
know, around these, and how much was the valuation, or what range are you expecting? What is the
know, around these, and how much was the valuation, or what range are you expecting? What is the
33:44last post? So the last post was one 40 million.
last post? So the last post was one 40 million.
last post? So the last post was one 40 million.
last post? So the last post was one 40 million.
S Speaker 233:47Okay, got it. And where would this land? Any initial
Okay, got it. And where would this land? Any initial
Okay, got it. And where would this land? Any initial
Okay, got it. And where would this land? Any initial
S Speaker 133:51So, yeah, I'll, it'll be too early to say, once I have done sign terms, term sheet, I'll be able to tell you maybe next week. You know, we can, we can give you a firm answer, yeah.
So, yeah, I'll, it'll be too early to say, once I have done sign terms, term sheet, I'll be able to tell you maybe next week. You know, we can, we can give you a firm answer, yeah.
So, yeah, I'll, it'll be too early to say, once I have done sign terms, term sheet, I'll be able to tell you maybe next week. You know, we can, we can give you a firm answer, yeah.
So, yeah, I'll, it'll be too early to say, once I have done sign terms, term sheet, I'll be able to tell you maybe next week. You know, we can, we can give you a firm answer, yeah.
S Speaker 234:00I mean, I'm also happy to meet in person and do, like, yeah, proper team dive. Like, maybe block to couple of hours, and then, you know, I can bring some of my team members. And this is exciting. It could be. And have you talked to Qualcomm as a customer?
I mean, I'm also happy to meet in person and do, like, yeah, proper team dive. Like, maybe block to couple of hours, and then, you know, I can bring some of my team members. And this is exciting. It could be. And have you talked to Qualcomm as a customer?
I mean, I'm also happy to meet in person and do, like, yeah, proper team dive. Like, maybe block to couple of hours, and then, you know, I can bring some of my team members. And this is exciting. It could be. And have you talked to Qualcomm as a customer?
I mean, I'm also happy to meet in person and do, like, yeah, proper team dive. Like, maybe block to couple of hours, and then, you know, I can bring some of my team members. And this is exciting. It could be. And have you talked to Qualcomm as a customer?
S Speaker 134:13Or, if not, you'll come to, you know, we are talking to other telecom, you know, and so other, other type of Qualcomm. I mean,
Or, if not, you'll come to, you know, we are talking to other telecom, you know, and so other, other type of Qualcomm. I mean,
Or, if not, you'll come to, you know, we are talking to other telecom, you know, and so other, other type of Qualcomm. I mean,
Or, if not, you'll come to, you know, we are talking to other telecom, you know, and so other, other type of Qualcomm. I mean,
34:41Yeah, some data room or some materials
Yeah, some data room or some materials
Yeah, some data room or some materials
Yeah, some data room or some materials
S Speaker 334:45that you can Yeah, I can send that over to you. Alright, Father,
that you can Yeah, I can send that over to you. Alright, Father,
that you can Yeah, I can send that over to you. Alright, Father,
that you can Yeah, I can send that over to you. Alright, Father,
35:33Hey, hi, Deepa, Good morning.
Hey, hi, Deepa, Good morning.
Hey, hi, Deepa, Good morning.
Hey, hi, Deepa, Good morning.
S Speaker 235:35How's it going? Good morning. It's going good. Good call. Yeah, actually, uh.
How's it going? Good morning. It's going good. Good call. Yeah, actually, uh.
How's it going? Good morning. It's going good. Good call. Yeah, actually, uh.
How's it going? Good morning. It's going good. Good call. Yeah, actually, uh.