Meeting: Panel - Scale@
Wed, Jun 25
1:36 PM
35 min
Priyesh P
Panelist Introductions and Backgrounds
0:00
Disc
URL: https://otter.ai/u/EhWWjdIl-h9oBOEL3ZrmIZLp4H0
Downloaded: 2025-12-21T21:51:02.444092
Method: text_extraction
============================================================

S Speaker 10:00On Gen AI startups. So I'm Joe spiesak. I'll be the moderator for the panel. I lead product and meta for our pytorch efforts, and have been past, worked on our large language models called llama. And this is kind of a conference after my own heart. So I was actually luckily able to pick some of the best panelists, probably some of the most impactful startups in the world, and you're gonna get to learn from them and kind of get to know them, and we're gonna have a little bit of fun. So let's welcome the other panelists.
On Gen AI startups. So I'm Joe spiesak. I'll be the moderator for the panel. I lead product and meta for our pytorch efforts, and have been past, worked on our large language models called llama. And this is kind of a conference after my own heart. So I was actually luckily able to pick some of the best panelists, probably some of the most impactful startups in the world, and you're gonna get to learn from them and kind of get to know them, and we're gonna have a little bit of fun. So let's welcome the other panelists.
On Gen AI startups. So I'm Joe spiesak. I'll be the moderator for the panel. I lead product and meta for our pytorch efforts, and have been past, worked on our large language models called llama. And this is kind of a conference after my own heart. So I was actually luckily able to pick some of the best panelists, probably some of the most impactful startups in the world, and you're gonna get to learn from them and kind of get to know them, and we're gonna have a little bit of fun. So let's welcome the other panelists.
On Gen AI startups. So I'm Joe spiesak. I'll be the moderator for the panel. I lead product and meta for our pytorch efforts, and have been past, worked on our large language models called llama. And this is kind of a conference after my own heart. So I was actually luckily able to pick some of the best panelists, probably some of the most impactful startups in the world, and you're gonna get to learn from them and kind of get to know them, and we're gonna have a little bit of fun. So let's welcome the other panelists.
S Speaker 20:28Please welcome to the stage. Horace, he drew Batra, Sal Candido, Eugen pote and Karina Hong. All
Please welcome to the stage. Horace, he drew Batra, Sal Candido, Eugen pote and Karina Hong. All
Please welcome to the stage. Horace, he drew Batra, Sal Candido, Eugen pote and Karina Hong. All
Please welcome to the stage. Horace, he drew Batra, Sal Candido, Eugen pote and Karina Hong. All
S Speaker 10:47right, so we do jumping jacks, right? Okay. All right, cool. All right. Well, let's start with some intros and get kind of acquainted with who's up here on the other panel. So starting with Horace, my old colleague, pie George. So, so three things about Boris. One is a core contributor, still pytorch, although it says Ed meta here, that's strange, but actually now at thinking of machines, he's also an ML systems researcher with papers at OOPSLA Eichler, and kind of, like the who's who of conferences, including Europe's. He's also a competitive coder and open source contributor. And yeah, kind of, I guess you reached the ICPC World Finance. That's pretty cool. Awesome. Welcome Horace, all right. And then I have Dhruv. So Drew also a former researcher in fair. So kind of embodied AI pioneer and tech leader formerly led Fair's embodied AI work, including habitat. He's also an award winning educator, academic out of Georgia Tech. Actually, we're wife is also a professor there. So power couple, for sure,
right, so we do jumping jacks, right? Okay. All right, cool. All right. Well, let's start with some intros and get kind of acquainted with who's up here on the other panel. So starting with Horace, my old colleague, pie George. So, so three things about Boris. One is a core contributor, still pytorch, although it says Ed meta here, that's strange, but actually now at thinking of machines, he's also an ML systems researcher with papers at OOPSLA Eichler, and kind of, like the who's who of conferences, including Europe's. He's also a competitive coder and open source contributor. And yeah, kind of, I guess you reached the ICPC World Finance. That's pretty cool. Awesome. Welcome Horace, all right. And then I have Dhruv. So Drew also a former researcher in fair. So kind of embodied AI pioneer and tech leader formerly led Fair's embodied AI work, including habitat. He's also an award winning educator, academic out of Georgia Tech. Actually, we're wife is also a professor there. So power couple, for sure,
right, so we do jumping jacks, right? Okay. All right, cool. All right. Well, let's start with some intros and get kind of acquainted with who's up here on the other panel. So starting with Horace, my old colleague, pie George. So, so three things about Boris. One is a core contributor, still pytorch, although it says Ed meta here, that's strange, but actually now at thinking of machines, he's also an ML systems researcher with papers at OOPSLA Eichler, and kind of, like the who's who of conferences, including Europe's. He's also a competitive coder and open source contributor. And yeah, kind of, I guess you reached the ICPC World Finance. That's pretty cool. Awesome. Welcome Horace, all right. And then I have Dhruv. So Drew also a former researcher in fair. So kind of embodied AI pioneer and tech leader formerly led Fair's embodied AI work, including habitat. He's also an award winning educator, academic out of Georgia Tech. Actually, we're wife is also a professor there. So power couple, for sure,
right, so we do jumping jacks, right? Okay. All right, cool. All right. Well, let's start with some intros and get kind of acquainted with who's up here on the other panel. So starting with Horace, my old colleague, pie George. So, so three things about Boris. One is a core contributor, still pytorch, although it says Ed meta here, that's strange, but actually now at thinking of machines, he's also an ML systems researcher with papers at OOPSLA Eichler, and kind of, like the who's who of conferences, including Europe's. He's also a competitive coder and open source contributor. And yeah, kind of, I guess you reached the ICPC World Finance. That's pretty cool. Awesome. Welcome Horace, all right. And then I have Dhruv. So Drew also a former researcher in fair. So kind of embodied AI pioneer and tech leader formerly led Fair's embodied AI work, including habitat. He's also an award winning educator, academic out of Georgia Tech. Actually, we're wife is also a professor there. So power couple, for sure,
S Speaker 12:07And then obviously, a huge advocate of multimodal AI over the years, and that makes sense. And then To his left is Sal pindito. So another ex colleague of mine, co founder and CTO of evolutionary scale, AI, focused on AI and biology, one of my favorite areas that I think AI could have a really positive impact in. Let's see. He was formerly at Google X, working on the loon project. The CTO of that his research multi scale, geospatial and sustainable AI with lots of papers, including a seminal paper on scale, mass auto encoder at ICCD 2023 and actually also works on and biological sequence modeling, as I mentioned. So awesome. And then his left is Yugen Kotey, member of technical staff at perplexity, former worked on lava post training, and so he's a ml engineer formerly Google also at perplexity, worked on open source, including he worked on GPD two. That's cool. Okay. Not bad, too. You worked on style transfer and a bunch of other stuff. And you also worked on adaptive neural architecture search in the past. Awesome. Welcome again. And last, but certainly not least, Karina Hogg, founder and CEO of Axiom for Stanford PhD, a Rhodes Scholar.
And then obviously, a huge advocate of multimodal AI over the years, and that makes sense. And then To his left is Sal pindito. So another ex colleague of mine, co founder and CTO of evolutionary scale, AI, focused on AI and biology, one of my favorite areas that I think AI could have a really positive impact in. Let's see. He was formerly at Google X, working on the loon project. The CTO of that his research multi scale, geospatial and sustainable AI with lots of papers, including a seminal paper on scale, mass auto encoder at ICCD 2023 and actually also works on and biological sequence modeling, as I mentioned. So awesome. And then his left is Yugen Kotey, member of technical staff at perplexity, former worked on lava post training, and so he's a ml engineer formerly Google also at perplexity, worked on open source, including he worked on GPD two. That's cool. Okay. Not bad, too. You worked on style transfer and a bunch of other stuff. And you also worked on adaptive neural architecture search in the past. Awesome. Welcome again. And last, but certainly not least, Karina Hogg, founder and CEO of Axiom for Stanford PhD, a Rhodes Scholar.
And then obviously, a huge advocate of multimodal AI over the years, and that makes sense. And then To his left is Sal pindito. So another ex colleague of mine, co founder and CTO of evolutionary scale, AI, focused on AI and biology, one of my favorite areas that I think AI could have a really positive impact in. Let's see. He was formerly at Google X, working on the loon project. The CTO of that his research multi scale, geospatial and sustainable AI with lots of papers, including a seminal paper on scale, mass auto encoder at ICCD 2023 and actually also works on and biological sequence modeling, as I mentioned. So awesome. And then his left is Yugen Kotey, member of technical staff at perplexity, former worked on lava post training, and so he's a ml engineer formerly Google also at perplexity, worked on open source, including he worked on GPD two. That's cool. Okay. Not bad, too. You worked on style transfer and a bunch of other stuff. And you also worked on adaptive neural architecture search in the past. Awesome. Welcome again. And last, but certainly not least, Karina Hogg, founder and CEO of Axiom for Stanford PhD, a Rhodes Scholar.
And then obviously, a huge advocate of multimodal AI over the years, and that makes sense. And then To his left is Sal pindito. So another ex colleague of mine, co founder and CTO of evolutionary scale, AI, focused on AI and biology, one of my favorite areas that I think AI could have a really positive impact in. Let's see. He was formerly at Google X, working on the loon project. The CTO of that his research multi scale, geospatial and sustainable AI with lots of papers, including a seminal paper on scale, mass auto encoder at ICCD 2023 and actually also works on and biological sequence modeling, as I mentioned. So awesome. And then his left is Yugen Kotey, member of technical staff at perplexity, former worked on lava post training, and so he's a ml engineer formerly Google also at perplexity, worked on open source, including he worked on GPD two. That's cool. Okay. Not bad, too. You worked on style transfer and a bunch of other stuff. And you also worked on adaptive neural architecture search in the past. Awesome. Welcome again. And last, but certainly not least, Karina Hogg, founder and CEO of Axiom for Stanford PhD, a Rhodes Scholar.
3:41Let's see award winning mathematical researcher.
Let's see award winning mathematical researcher.
Let's see award winning mathematical researcher.
Let's see award winning mathematical researcher.
3:46What else could you do?
What else could you do?
What else could you do?
What else could you do?
3:50I know, right? I got a crazy camera
I know, right? I got a crazy camera
I know, right? I got a crazy camera
I know, right? I got a crazy camera
S Speaker 13:56here. Let's see and a community builder and student leader, wow. So you actually led MIT's undergrad and undergrad math Association, recipient of mathematics community building awards, NSF awards, wow, OK, so crazy. So from MIT to Stanford, PhD founder, raised money building a team in a company. Amazing. Thank you. Welcome, awesome, actually. So fun. Fact, all those BIOS were actually generated by chat GPT. So I'm kind of curious, actually. Like, did chat GPT hallucinate at all? It was, it was accurate. Was there anything actually you? Again, I know it said that you're still at meta, which is, like, maybe wishful thinking for me, but like, I don't know.
here. Let's see and a community builder and student leader, wow. So you actually led MIT's undergrad and undergrad math Association, recipient of mathematics community building awards, NSF awards, wow, OK, so crazy. So from MIT to Stanford, PhD founder, raised money building a team in a company. Amazing. Thank you. Welcome, awesome, actually. So fun. Fact, all those BIOS were actually generated by chat GPT. So I'm kind of curious, actually. Like, did chat GPT hallucinate at all? It was, it was accurate. Was there anything actually you? Again, I know it said that you're still at meta, which is, like, maybe wishful thinking for me, but like, I don't know.
here. Let's see and a community builder and student leader, wow. So you actually led MIT's undergrad and undergrad math Association, recipient of mathematics community building awards, NSF awards, wow, OK, so crazy. So from MIT to Stanford, PhD founder, raised money building a team in a company. Amazing. Thank you. Welcome, awesome, actually. So fun. Fact, all those BIOS were actually generated by chat GPT. So I'm kind of curious, actually. Like, did chat GPT hallucinate at all? It was, it was accurate. Was there anything actually you? Again, I know it said that you're still at meta, which is, like, maybe wishful thinking for me, but like, I don't know.
here. Let's see and a community builder and student leader, wow. So you actually led MIT's undergrad and undergrad math Association, recipient of mathematics community building awards, NSF awards, wow, OK, so crazy. So from MIT to Stanford, PhD founder, raised money building a team in a company. Amazing. Thank you. Welcome, awesome, actually. So fun. Fact, all those BIOS were actually generated by chat GPT. So I'm kind of curious, actually. Like, did chat GPT hallucinate at all? It was, it was accurate. Was there anything actually you? Again, I know it said that you're still at meta, which is, like, maybe wishful thinking for me, but like, I don't know.
S Speaker 34:41Yeah, I have big news. I'm at a perplexity now, yeah, but yeah. Besides that, everything else sounded pretty reasonable, okay, I think also, like I said, I was in deep mine, which I was at, like, Google research, but I guess, small, small, mixing, cool, yeah, any other hallucinations? Everything out there is for a few of us, right? Yeah, which is wrong, right?
Yeah, I have big news. I'm at a perplexity now, yeah, but yeah. Besides that, everything else sounded pretty reasonable, okay, I think also, like I said, I was in deep mine, which I was at, like, Google research, but I guess, small, small, mixing, cool, yeah, any other hallucinations? Everything out there is for a few of us, right? Yeah, which is wrong, right?
Yeah, I have big news. I'm at a perplexity now, yeah, but yeah. Besides that, everything else sounded pretty reasonable, okay, I think also, like I said, I was in deep mine, which I was at, like, Google research, but I guess, small, small, mixing, cool, yeah, any other hallucinations? Everything out there is for a few of us, right? Yeah, which is wrong, right?
Yeah, I have big news. I'm at a perplexity now, yeah, but yeah. Besides that, everything else sounded pretty reasonable, okay, I think also, like I said, I was in deep mine, which I was at, like, Google research, but I guess, small, small, mixing, cool, yeah, any other hallucinations? Everything out there is for a few of us, right? Yeah, which is wrong, right?
S Speaker 45:06Most like yours. Did you feed it out bios, or is this from this
Most like yours. Did you feed it out bios, or is this from this
Most like yours. Did you feed it out bios, or is this from this
Most like yours. Did you feed it out bios, or is this from this
S Speaker 15:10is so it's probably got some knowledge cut off, and I'm sure it did some like, you know, indexing and that. But I fed it, I literally asked it your name and, like, Give me three lines about you all, and it sent pretty good information. I did play with the Tory API. By the way, that's actually getting better. So shameless plug for for juice startup.
is so it's probably got some knowledge cut off, and I'm sure it did some like, you know, indexing and that. But I fed it, I literally asked it your name and, like, Give me three lines about you all, and it sent pretty good information. I did play with the Tory API. By the way, that's actually getting better. So shameless plug for for juice startup.
is so it's probably got some knowledge cut off, and I'm sure it did some like, you know, indexing and that. But I fed it, I literally asked it your name and, like, Give me three lines about you all, and it sent pretty good information. I did play with the Tory API. By the way, that's actually getting better. So shameless plug for for juice startup.
is so it's probably got some knowledge cut off, and I'm sure it did some like, you know, indexing and that. But I fed it, I literally asked it your name and, like, Give me three lines about you all, and it sent pretty good information. I did play with the Tory API. By the way, that's actually getting better. So shameless plug for for juice startup.
S Speaker 55:31Like, I got it right, but it was, like, interesting choice on, like, what it like picked out, like, the three things like, yeah, probably, like, this guy gets a metric for vision transformer, yeah. Yeah, I did do
Like, I got it right, but it was, like, interesting choice on, like, what it like picked out, like, the three things like, yeah, probably, like, this guy gets a metric for vision transformer, yeah. Yeah, I did do
Like, I got it right, but it was, like, interesting choice on, like, what it like picked out, like, the three things like, yeah, probably, like, this guy gets a metric for vision transformer, yeah. Yeah, I did do
Like, I got it right, but it was, like, interesting choice on, like, what it like picked out, like, the three things like, yeah, probably, like, this guy gets a metric for vision transformer, yeah. Yeah, I did do
S Speaker 65:44it was part of that. So, yeah, I do really, like, the facial statements were right, but like, the emphasis were, like, a little bit strange, like, classroom project, and went back to, like, My undergrad was probably being able, like, high school, you know, club,
it was part of that. So, yeah, I do really, like, the facial statements were right, but like, the emphasis were, like, a little bit strange, like, classroom project, and went back to, like, My undergrad was probably being able, like, high school, you know, club,
it was part of that. So, yeah, I do really, like, the facial statements were right, but like, the emphasis were, like, a little bit strange, like, classroom project, and went back to, like, My undergrad was probably being able, like, high school, you know, club,
it was part of that. So, yeah, I do really, like, the facial statements were right, but like, the emphasis were, like, a little bit strange, like, classroom project, and went back to, like, My undergrad was probably being able, like, high school, you know, club,
S Speaker 15:56interesting. Okay, AI still, it's still a work in progress. We're not there yet. So amazing. Okay, all right, so let's kick off some actual questions here. I guess the first one is for Boris, and then I want others, obviously, to jump in. So I mean, you and I have been building open source tools for a long time. We're gonna pivot together. I was like, literally surfing through like GitHub the other day, looking at awesome lists on agentic frameworks, I saw one of them had 93 frameworks. Like, a bunch of them were, like, vertically centric, like, build an AI agent for, like, legal or, I don't know whatever it was, but like, 93 like, What the hell is happening here? Like, you know, we've seen you and I have seen, like, the framework wars happen with pytorch and TensorFlow and all that. Like, is there, like, what is the best one you've tried, and what do you think the winner is? Or do you think, like, we're in for this, like, crazy entropy for a couple of years, and then stuff sells out. Where do you see this going?
interesting. Okay, AI still, it's still a work in progress. We're not there yet. So amazing. Okay, all right, so let's kick off some actual questions here. I guess the first one is for Boris, and then I want others, obviously, to jump in. So I mean, you and I have been building open source tools for a long time. We're gonna pivot together. I was like, literally surfing through like GitHub the other day, looking at awesome lists on agentic frameworks, I saw one of them had 93 frameworks. Like, a bunch of them were, like, vertically centric, like, build an AI agent for, like, legal or, I don't know whatever it was, but like, 93 like, What the hell is happening here? Like, you know, we've seen you and I have seen, like, the framework wars happen with pytorch and TensorFlow and all that. Like, is there, like, what is the best one you've tried, and what do you think the winner is? Or do you think, like, we're in for this, like, crazy entropy for a couple of years, and then stuff sells out. Where do you see this going?
interesting. Okay, AI still, it's still a work in progress. We're not there yet. So amazing. Okay, all right, so let's kick off some actual questions here. I guess the first one is for Boris, and then I want others, obviously, to jump in. So I mean, you and I have been building open source tools for a long time. We're gonna pivot together. I was like, literally surfing through like GitHub the other day, looking at awesome lists on agentic frameworks, I saw one of them had 93 frameworks. Like, a bunch of them were, like, vertically centric, like, build an AI agent for, like, legal or, I don't know whatever it was, but like, 93 like, What the hell is happening here? Like, you know, we've seen you and I have seen, like, the framework wars happen with pytorch and TensorFlow and all that. Like, is there, like, what is the best one you've tried, and what do you think the winner is? Or do you think, like, we're in for this, like, crazy entropy for a couple of years, and then stuff sells out. Where do you see this going?
interesting. Okay, AI still, it's still a work in progress. We're not there yet. So amazing. Okay, all right, so let's kick off some actual questions here. I guess the first one is for Boris, and then I want others, obviously, to jump in. So I mean, you and I have been building open source tools for a long time. We're gonna pivot together. I was like, literally surfing through like GitHub the other day, looking at awesome lists on agentic frameworks, I saw one of them had 93 frameworks. Like, a bunch of them were, like, vertically centric, like, build an AI agent for, like, legal or, I don't know whatever it was, but like, 93 like, What the hell is happening here? Like, you know, we've seen you and I have seen, like, the framework wars happen with pytorch and TensorFlow and all that. Like, is there, like, what is the best one you've tried, and what do you think the winner is? Or do you think, like, we're in for this, like, crazy entropy for a couple of years, and then stuff sells out. Where do you see this going?
S Speaker 66:53Yeah, for sure. I think definitely, like, when you kind of see these, like, framework fights happen, definitely at the beginning there's always, like a Cambrian explosion, especially when, like, the barrier to entry is so low, like it is here, like, we're like, you know, fundamentally, Agent framework is, like a string templating library of some kind. And, like, anybody can write a string template in library. And so, like, yeah, I definitely think you're gonna see, like, a huge explosion, like, soon, like, like, right now, and then eventually it'll start consolidating. But I think to me, like, the more interesting question about like, the agent frameworks is kind of not even about like, which one will win. It's kind of more about like, how, like, what is even the place for agent frameworks in the long run? Because I think one of the things you kind of see right with like, the labs and stuff like this is like things like deep research, they're kind of very much focused on, like, end to end. RL trained like, like, agentic tasks, and it's kind of not super clear, like, how these, like, plug into, like, the existing agentic frameworks, like, as in, like, our training framework, kind of like going down the wrong path, and they should be focused instead on, like, building RL environments. Like, definitely, like, even if you're doing RL, there's, like, a lot of nuances and like, prompting and, like, you know, this kind of stuff. But I think it's not super clear to me, like, where the battery ends for like post training and where it should start for agentic frameworks. And I think my personal suspicion is kind of that we're gonna start seeing like, more and more stuff get, like, swallowed up a little bit by like post training, as opposed to like purely being done in the agentic Yeah.
Yeah, for sure. I think definitely, like, when you kind of see these, like, framework fights happen, definitely at the beginning there's always, like a Cambrian explosion, especially when, like, the barrier to entry is so low, like it is here, like, we're like, you know, fundamentally, Agent framework is, like a string templating library of some kind. And, like, anybody can write a string template in library. And so, like, yeah, I definitely think you're gonna see, like, a huge explosion, like, soon, like, like, right now, and then eventually it'll start consolidating. But I think to me, like, the more interesting question about like, the agent frameworks is kind of not even about like, which one will win. It's kind of more about like, how, like, what is even the place for agent frameworks in the long run? Because I think one of the things you kind of see right with like, the labs and stuff like this is like things like deep research, they're kind of very much focused on, like, end to end. RL trained like, like, agentic tasks, and it's kind of not super clear, like, how these, like, plug into, like, the existing agentic frameworks, like, as in, like, our training framework, kind of like going down the wrong path, and they should be focused instead on, like, building RL environments. Like, definitely, like, even if you're doing RL, there's, like, a lot of nuances and like, prompting and, like, you know, this kind of stuff. But I think it's not super clear to me, like, where the battery ends for like post training and where it should start for agentic frameworks. And I think my personal suspicion is kind of that we're gonna start seeing like, more and more stuff get, like, swallowed up a little bit by like post training, as opposed to like purely being done in the agentic Yeah.
Yeah, for sure. I think definitely, like, when you kind of see these, like, framework fights happen, definitely at the beginning there's always, like a Cambrian explosion, especially when, like, the barrier to entry is so low, like it is here, like, we're like, you know, fundamentally, Agent framework is, like a string templating library of some kind. And, like, anybody can write a string template in library. And so, like, yeah, I definitely think you're gonna see, like, a huge explosion, like, soon, like, like, right now, and then eventually it'll start consolidating. But I think to me, like, the more interesting question about like, the agent frameworks is kind of not even about like, which one will win. It's kind of more about like, how, like, what is even the place for agent frameworks in the long run? Because I think one of the things you kind of see right with like, the labs and stuff like this is like things like deep research, they're kind of very much focused on, like, end to end. RL trained like, like, agentic tasks, and it's kind of not super clear, like, how these, like, plug into, like, the existing agentic frameworks, like, as in, like, our training framework, kind of like going down the wrong path, and they should be focused instead on, like, building RL environments. Like, definitely, like, even if you're doing RL, there's, like, a lot of nuances and like, prompting and, like, you know, this kind of stuff. But I think it's not super clear to me, like, where the battery ends for like post training and where it should start for agentic frameworks. And I think my personal suspicion is kind of that we're gonna start seeing like, more and more stuff get, like, swallowed up a little bit by like post training, as opposed to like purely being done in the agentic Yeah.
Yeah, for sure. I think definitely, like, when you kind of see these, like, framework fights happen, definitely at the beginning there's always, like a Cambrian explosion, especially when, like, the barrier to entry is so low, like it is here, like, we're like, you know, fundamentally, Agent framework is, like a string templating library of some kind. And, like, anybody can write a string template in library. And so, like, yeah, I definitely think you're gonna see, like, a huge explosion, like, soon, like, like, right now, and then eventually it'll start consolidating. But I think to me, like, the more interesting question about like, the agent frameworks is kind of not even about like, which one will win. It's kind of more about like, how, like, what is even the place for agent frameworks in the long run? Because I think one of the things you kind of see right with like, the labs and stuff like this is like things like deep research, they're kind of very much focused on, like, end to end. RL trained like, like, agentic tasks, and it's kind of not super clear, like, how these, like, plug into, like, the existing agentic frameworks, like, as in, like, our training framework, kind of like going down the wrong path, and they should be focused instead on, like, building RL environments. Like, definitely, like, even if you're doing RL, there's, like, a lot of nuances and like, prompting and, like, you know, this kind of stuff. But I think it's not super clear to me, like, where the battery ends for like post training and where it should start for agentic frameworks. And I think my personal suspicion is kind of that we're gonna start seeing like, more and more stuff get, like, swallowed up a little bit by like post training, as opposed to like purely being done in the agentic Yeah.
S Speaker 18:21That's kind of yeah, if you were to, like, look, level, you'll just see, like, so many, like, four letter word like descriptions of like chain and, you know, just like this. It means basically, like the, you know, Dag control flow, like, give me components and orchestrate them in Python and all that. And people are like, definitely, like, looking for something better. But, yeah, my feeling is definitely that the models, once they start reasoning much better, like you'll less need an actual framework. At that point, you'll need stuff, but you'll but the model itself, in theory, should like be able to reason about which tools to use, like which files to touch, if it's doing a coding task, whatever. So, yeah, anyone else like have a feeling on that or a framework they love or hate
That's kind of yeah, if you were to, like, look, level, you'll just see, like, so many, like, four letter word like descriptions of like chain and, you know, just like this. It means basically, like the, you know, Dag control flow, like, give me components and orchestrate them in Python and all that. And people are like, definitely, like, looking for something better. But, yeah, my feeling is definitely that the models, once they start reasoning much better, like you'll less need an actual framework. At that point, you'll need stuff, but you'll but the model itself, in theory, should like be able to reason about which tools to use, like which files to touch, if it's doing a coding task, whatever. So, yeah, anyone else like have a feeling on that or a framework they love or hate
That's kind of yeah, if you were to, like, look, level, you'll just see, like, so many, like, four letter word like descriptions of like chain and, you know, just like this. It means basically, like the, you know, Dag control flow, like, give me components and orchestrate them in Python and all that. And people are like, definitely, like, looking for something better. But, yeah, my feeling is definitely that the models, once they start reasoning much better, like you'll less need an actual framework. At that point, you'll need stuff, but you'll but the model itself, in theory, should like be able to reason about which tools to use, like which files to touch, if it's doing a coding task, whatever. So, yeah, anyone else like have a feeling on that or a framework they love or hate
That's kind of yeah, if you were to, like, look, level, you'll just see, like, so many, like, four letter word like descriptions of like chain and, you know, just like this. It means basically, like the, you know, Dag control flow, like, give me components and orchestrate them in Python and all that. And people are like, definitely, like, looking for something better. But, yeah, my feeling is definitely that the models, once they start reasoning much better, like you'll less need an actual framework. At that point, you'll need stuff, but you'll but the model itself, in theory, should like be able to reason about which tools to use, like which files to touch, if it's doing a coding task, whatever. So, yeah, anyone else like have a feeling on that or a framework they love or hate
9:02perfectly. Hate, I
9:04don't know. I feel like one of the big problems is that the like,
don't know. I feel like one of the big problems is that the like,
don't know. I feel like one of the big problems is that the like,
don't know. I feel like one of the big problems is that the like,
S Speaker 59:10the the best thing kind of to do, like, not even like a framework, but like, the best thing that you want to do if you're just doing like an agent based system is different than what you want to do if you're generating data for post training, and then, you know, trying to fix the model through that. And ideally we would want to be in a situation where you can start from an agentic flow, and then you can improve the model doing that. But I just don't see, like there's a really good solution yet to doing that whole piece. So hopefully soon it will save us all a lot of COVID, right? So all
the the best thing kind of to do, like, not even like a framework, but like, the best thing that you want to do if you're just doing like an agent based system is different than what you want to do if you're generating data for post training, and then, you know, trying to fix the model through that. And ideally we would want to be in a situation where you can start from an agentic flow, and then you can improve the model doing that. But I just don't see, like there's a really good solution yet to doing that whole piece. So hopefully soon it will save us all a lot of COVID, right? So all
the the best thing kind of to do, like, not even like a framework, but like, the best thing that you want to do if you're just doing like an agent based system is different than what you want to do if you're generating data for post training, and then, you know, trying to fix the model through that. And ideally we would want to be in a situation where you can start from an agentic flow, and then you can improve the model doing that. But I just don't see, like there's a really good solution yet to doing that whole piece. So hopefully soon it will save us all a lot of COVID, right? So all
the the best thing kind of to do, like, not even like a framework, but like, the best thing that you want to do if you're just doing like an agent based system is different than what you want to do if you're generating data for post training, and then, you know, trying to fix the model through that. And ideally we would want to be in a situation where you can start from an agentic flow, and then you can improve the model doing that. But I just don't see, like there's a really good solution yet to doing that whole piece. So hopefully soon it will save us all a lot of COVID, right? So all
S Speaker 19:45these kind of, like, have hopefully, like, planned obsolescence, like, they're all just gonna die in like two years, and like, RL will take over.
these kind of, like, have hopefully, like, planned obsolescence, like, they're all just gonna die in like two years, and like, RL will take over.
these kind of, like, have hopefully, like, planned obsolescence, like, they're all just gonna die in like two years, and like, RL will take over.
these kind of, like, have hopefully, like, planned obsolescence, like, they're all just gonna die in like two years, and like, RL will take over.
S Speaker 49:53I think we do this sympathize a bit with the framework makers, yeah. The reason why so many frameworks aren't useful is because the ground is shifting underneath them. When you make a framework, you kind of playing this balance of having some opinions. Otherwise you're not providing any value, but providing enough modularity that people can build what they want to build, and you folks in Python to get those decisions right. The history of pytorch is that it came from a pull right, like researchers were already hacking away watch, and you wanted to improve that process while not limiting anybody's scope. Today, you built an agentic framework. And if you misjudge where the capabilities are going to be, you end up putting walls that are just getting in the way of developers trying to get things done, and it's much easier for them to just go directly to the model and your framework doesn't provide anything. Yeah, I
I think we do this sympathize a bit with the framework makers, yeah. The reason why so many frameworks aren't useful is because the ground is shifting underneath them. When you make a framework, you kind of playing this balance of having some opinions. Otherwise you're not providing any value, but providing enough modularity that people can build what they want to build, and you folks in Python to get those decisions right. The history of pytorch is that it came from a pull right, like researchers were already hacking away watch, and you wanted to improve that process while not limiting anybody's scope. Today, you built an agentic framework. And if you misjudge where the capabilities are going to be, you end up putting walls that are just getting in the way of developers trying to get things done, and it's much easier for them to just go directly to the model and your framework doesn't provide anything. Yeah, I
I think we do this sympathize a bit with the framework makers, yeah. The reason why so many frameworks aren't useful is because the ground is shifting underneath them. When you make a framework, you kind of playing this balance of having some opinions. Otherwise you're not providing any value, but providing enough modularity that people can build what they want to build, and you folks in Python to get those decisions right. The history of pytorch is that it came from a pull right, like researchers were already hacking away watch, and you wanted to improve that process while not limiting anybody's scope. Today, you built an agentic framework. And if you misjudge where the capabilities are going to be, you end up putting walls that are just getting in the way of developers trying to get things done, and it's much easier for them to just go directly to the model and your framework doesn't provide anything. Yeah, I
I think we do this sympathize a bit with the framework makers, yeah. The reason why so many frameworks aren't useful is because the ground is shifting underneath them. When you make a framework, you kind of playing this balance of having some opinions. Otherwise you're not providing any value, but providing enough modularity that people can build what they want to build, and you folks in Python to get those decisions right. The history of pytorch is that it came from a pull right, like researchers were already hacking away watch, and you wanted to improve that process while not limiting anybody's scope. Today, you built an agentic framework. And if you misjudge where the capabilities are going to be, you end up putting walls that are just getting in the way of developers trying to get things done, and it's much easier for them to just go directly to the model and your framework doesn't provide anything. Yeah, I
S Speaker 110:46just chat with beyond story code earlier today, we did, like this podcast, and we were talking about abstractions, and I think, like, that's like, the you just nailed it. I mean, I think like, if you pick the wrong abstraction, then you're it's a dead end right here. You can't either hit like the right target user, or, like, people want to click down to a lower level or and so like, I think, like, we are starting, we are kind of building, hopefully, like a like a level where we understand the patterns or things start to emerge, and then you kind of build, build from there. But like, I feel like there's plenty of frameworks that have kind of started all the way at the top, and they're like, they're losing right? They're just like, because all of a sudden I want to have more flexibility, and I don't have that. And that's like, the framework. That's the gaming frameworks, right?
just chat with beyond story code earlier today, we did, like this podcast, and we were talking about abstractions, and I think, like, that's like, the you just nailed it. I mean, I think like, if you pick the wrong abstraction, then you're it's a dead end right here. You can't either hit like the right target user, or, like, people want to click down to a lower level or and so like, I think, like, we are starting, we are kind of building, hopefully, like a like a level where we understand the patterns or things start to emerge, and then you kind of build, build from there. But like, I feel like there's plenty of frameworks that have kind of started all the way at the top, and they're like, they're losing right? They're just like, because all of a sudden I want to have more flexibility, and I don't have that. And that's like, the framework. That's the gaming frameworks, right?
just chat with beyond story code earlier today, we did, like this podcast, and we were talking about abstractions, and I think, like, that's like, the you just nailed it. I mean, I think like, if you pick the wrong abstraction, then you're it's a dead end right here. You can't either hit like the right target user, or, like, people want to click down to a lower level or and so like, I think, like, we are starting, we are kind of building, hopefully, like a like a level where we understand the patterns or things start to emerge, and then you kind of build, build from there. But like, I feel like there's plenty of frameworks that have kind of started all the way at the top, and they're like, they're losing right? They're just like, because all of a sudden I want to have more flexibility, and I don't have that. And that's like, the framework. That's the gaming frameworks, right?
just chat with beyond story code earlier today, we did, like this podcast, and we were talking about abstractions, and I think, like, that's like, the you just nailed it. I mean, I think like, if you pick the wrong abstraction, then you're it's a dead end right here. You can't either hit like the right target user, or, like, people want to click down to a lower level or and so like, I think, like, we are starting, we are kind of building, hopefully, like a like a level where we understand the patterns or things start to emerge, and then you kind of build, build from there. But like, I feel like there's plenty of frameworks that have kind of started all the way at the top, and they're like, they're losing right? They're just like, because all of a sudden I want to have more flexibility, and I don't have that. And that's like, the framework. That's the gaming frameworks, right?
S Speaker 611:27Yeah, I'll hear about another argument, which is like, I think one way to think about is like, what is the point of like, agentic framework in the first place? It's like, the reason for a genic framework exists is that the base model cannot immediately, like, do it by itself, or like it's like, not capable of, like, you know, doing all the actions, or like, you know, like doing its memory, or like handling all the things by itself. And so in some sense, like, where you're asking for majoring framework is you're trying to, like, get the model to do something that it does not like, immediately, like, naturally, do, do by itself. And if you want to do this, I think in many cases, like I said, like RL seemed like a like, a more principled way to, like, elicit, like, a different behavior, as opposed to kind of purely something that's like,
Yeah, I'll hear about another argument, which is like, I think one way to think about is like, what is the point of like, agentic framework in the first place? It's like, the reason for a genic framework exists is that the base model cannot immediately, like, do it by itself, or like it's like, not capable of, like, you know, doing all the actions, or like, you know, like doing its memory, or like handling all the things by itself. And so in some sense, like, where you're asking for majoring framework is you're trying to, like, get the model to do something that it does not like, immediately, like, naturally, do, do by itself. And if you want to do this, I think in many cases, like I said, like RL seemed like a like, a more principled way to, like, elicit, like, a different behavior, as opposed to kind of purely something that's like,
Yeah, I'll hear about another argument, which is like, I think one way to think about is like, what is the point of like, agentic framework in the first place? It's like, the reason for a genic framework exists is that the base model cannot immediately, like, do it by itself, or like it's like, not capable of, like, you know, doing all the actions, or like, you know, like doing its memory, or like handling all the things by itself. And so in some sense, like, where you're asking for majoring framework is you're trying to, like, get the model to do something that it does not like, immediately, like, naturally, do, do by itself. And if you want to do this, I think in many cases, like I said, like RL seemed like a like, a more principled way to, like, elicit, like, a different behavior, as opposed to kind of purely something that's like,
Yeah, I'll hear about another argument, which is like, I think one way to think about is like, what is the point of like, agentic framework in the first place? It's like, the reason for a genic framework exists is that the base model cannot immediately, like, do it by itself, or like it's like, not capable of, like, you know, doing all the actions, or like, you know, like doing its memory, or like handling all the things by itself. And so in some sense, like, where you're asking for majoring framework is you're trying to, like, get the model to do something that it does not like, immediately, like, naturally, do, do by itself. And if you want to do this, I think in many cases, like I said, like RL seemed like a like, a more principled way to, like, elicit, like, a different behavior, as opposed to kind of purely something that's like,
S Speaker 112:13Okay, so I keep hearing RL, RL, everyone is, of course, talking about RL these days. Like, I remember I was sitting in a room with Gary Marcus, like, probably 11 years ago, and he was pitching us like he had geometric intelligence. I think Uber ended up buying them. And he had this crazy cast. He had zoom in, right? Yeah, like, Jeff. So he had this, like, amazing set researchers. And they had, they were just formed the company, and also they wanted to sell, right? And half of them, half the people, were, like, half time at the company, and they showed us a demo. It was like, you know, I think it was like, pong or something, right? And it showed, like, the really smooth actions, like, and then they compared it to, like, DQ, N, and we're like, okay, that's kind of cool. But that was, like, the that was like, What was exciting at that time about RL? We kind of walked away, like, laughing, like, this, great researchers, what did they just pitch ups, and they wanted, by the way, you know, like, eight figures per employee to buy that, right? Something crazy at the time, like,
Okay, so I keep hearing RL, RL, everyone is, of course, talking about RL these days. Like, I remember I was sitting in a room with Gary Marcus, like, probably 11 years ago, and he was pitching us like he had geometric intelligence. I think Uber ended up buying them. And he had this crazy cast. He had zoom in, right? Yeah, like, Jeff. So he had this, like, amazing set researchers. And they had, they were just formed the company, and also they wanted to sell, right? And half of them, half the people, were, like, half time at the company, and they showed us a demo. It was like, you know, I think it was like, pong or something, right? And it showed, like, the really smooth actions, like, and then they compared it to, like, DQ, N, and we're like, okay, that's kind of cool. But that was, like, the that was like, What was exciting at that time about RL? We kind of walked away, like, laughing, like, this, great researchers, what did they just pitch ups, and they wanted, by the way, you know, like, eight figures per employee to buy that, right? Something crazy at the time, like,
Okay, so I keep hearing RL, RL, everyone is, of course, talking about RL these days. Like, I remember I was sitting in a room with Gary Marcus, like, probably 11 years ago, and he was pitching us like he had geometric intelligence. I think Uber ended up buying them. And he had this crazy cast. He had zoom in, right? Yeah, like, Jeff. So he had this, like, amazing set researchers. And they had, they were just formed the company, and also they wanted to sell, right? And half of them, half the people, were, like, half time at the company, and they showed us a demo. It was like, you know, I think it was like, pong or something, right? And it showed, like, the really smooth actions, like, and then they compared it to, like, DQ, N, and we're like, okay, that's kind of cool. But that was, like, the that was like, What was exciting at that time about RL? We kind of walked away, like, laughing, like, this, great researchers, what did they just pitch ups, and they wanted, by the way, you know, like, eight figures per employee to buy that, right? Something crazy at the time, like,
Okay, so I keep hearing RL, RL, everyone is, of course, talking about RL these days. Like, I remember I was sitting in a room with Gary Marcus, like, probably 11 years ago, and he was pitching us like he had geometric intelligence. I think Uber ended up buying them. And he had this crazy cast. He had zoom in, right? Yeah, like, Jeff. So he had this, like, amazing set researchers. And they had, they were just formed the company, and also they wanted to sell, right? And half of them, half the people, were, like, half time at the company, and they showed us a demo. It was like, you know, I think it was like, pong or something, right? And it showed, like, the really smooth actions, like, and then they compared it to, like, DQ, N, and we're like, okay, that's kind of cool. But that was, like, the that was like, What was exciting at that time about RL? We kind of walked away, like, laughing, like, this, great researchers, what did they just pitch ups, and they wanted, by the way, you know, like, eight figures per employee to buy that, right? Something crazy at the time, like,
13:07that seems low. Now, I know I had no
that seems low. Now, I know I had no
that seems low. Now, I know I had no
that seems low. Now, I know I had no
13:11idea I need one of those 900 offers.
idea I need one of those 900 offers.
idea I need one of those 900 offers.
idea I need one of those 900 offers.
S Speaker 113:14Yeah, let's put that aside for a second. But like, Drew Tell me. Like, is RL for real this time. I know we had RHF, and RHF is largely to kind of fake RL, but is, are we? Is this really the future, or is there going to be like something beyond RL, or
Yeah, let's put that aside for a second. But like, Drew Tell me. Like, is RL for real this time. I know we had RHF, and RHF is largely to kind of fake RL, but is, are we? Is this really the future, or is there going to be like something beyond RL, or
Yeah, let's put that aside for a second. But like, Drew Tell me. Like, is RL for real this time. I know we had RHF, and RHF is largely to kind of fake RL, but is, are we? Is this really the future, or is there going to be like something beyond RL, or
Yeah, let's put that aside for a second. But like, Drew Tell me. Like, is RL for real this time. I know we had RHF, and RHF is largely to kind of fake RL, but is, are we? Is this really the future, or is there going to be like something beyond RL, or
S Speaker 118:11curious to see Karina stake here, because, like for mathematics, feels like an area where you can have verifiable work, right? There's a ground truth to how you actually solve, or formally solve a theorem, for example. Or, you know, back when we were building the theorem, we had the improvement team, which is now the other Mistral team, largely out of meta. We we have environments like lean, for example. And, you know, you had, we were doing MCTs, and it's kind of a brute force approach, right? It's like LM plus MCTs, Monte Carlo, tree search. But how do you see, like, mathematics? Is it?
curious to see Karina stake here, because, like for mathematics, feels like an area where you can have verifiable work, right? There's a ground truth to how you actually solve, or formally solve a theorem, for example. Or, you know, back when we were building the theorem, we had the improvement team, which is now the other Mistral team, largely out of meta. We we have environments like lean, for example. And, you know, you had, we were doing MCTs, and it's kind of a brute force approach, right? It's like LM plus MCTs, Monte Carlo, tree search. But how do you see, like, mathematics? Is it?
curious to see Karina stake here, because, like for mathematics, feels like an area where you can have verifiable work, right? There's a ground truth to how you actually solve, or formally solve a theorem, for example. Or, you know, back when we were building the theorem, we had the improvement team, which is now the other Mistral team, largely out of meta. We we have environments like lean, for example. And, you know, you had, we were doing MCTs, and it's kind of a brute force approach, right? It's like LM plus MCTs, Monte Carlo, tree search. But how do you see, like, mathematics? Is it?
curious to see Karina stake here, because, like for mathematics, feels like an area where you can have verifiable work, right? There's a ground truth to how you actually solve, or formally solve a theorem, for example. Or, you know, back when we were building the theorem, we had the improvement team, which is now the other Mistral team, largely out of meta. We we have environments like lean, for example. And, you know, you had, we were doing MCTs, and it's kind of a brute force approach, right? It's like LM plus MCTs, Monte Carlo, tree search. But how do you see, like, mathematics? Is it?
S Speaker 718:43Yeah, 100% I think I'm very excited about the future of, you know, applying RL in areas where there's really good grounding and verifier, just like math and code. I think it's very intriguing to think of, because math, when we kind of think about and we see the math data that's being fed into the large language model. It's doing formal reasoning in natural language, but like, if you use like programming language like Link four, then you can suddenly attempt to turn the entire mathematical literature into GitHub, actionable code, and to apply all the amazing techniques and progress in the coding space to mathematics. If we think about kind of math and how it's different from, say, Chess and Go, it is a very big action space. Like at each step, you can take many possible moves. And if you just, like, kind of, you know, apply the sort of techniques that we have seen that made, like, amazing gains in coding to math. But you have a data scarcity problem. So, you know, if you scrape all the link code on the internet, there's only about, like, I think, like a couple million lines of code which is, which is not, not enough, and how to kind of convert these natural language math data into being code. I think that remains a stubborn technical challenge that we are super excited to try to solve it as
Yeah, 100% I think I'm very excited about the future of, you know, applying RL in areas where there's really good grounding and verifier, just like math and code. I think it's very intriguing to think of, because math, when we kind of think about and we see the math data that's being fed into the large language model. It's doing formal reasoning in natural language, but like, if you use like programming language like Link four, then you can suddenly attempt to turn the entire mathematical literature into GitHub, actionable code, and to apply all the amazing techniques and progress in the coding space to mathematics. If we think about kind of math and how it's different from, say, Chess and Go, it is a very big action space. Like at each step, you can take many possible moves. And if you just, like, kind of, you know, apply the sort of techniques that we have seen that made, like, amazing gains in coding to math. But you have a data scarcity problem. So, you know, if you scrape all the link code on the internet, there's only about, like, I think, like a couple million lines of code which is, which is not, not enough, and how to kind of convert these natural language math data into being code. I think that remains a stubborn technical challenge that we are super excited to try to solve it as
Yeah, 100% I think I'm very excited about the future of, you know, applying RL in areas where there's really good grounding and verifier, just like math and code. I think it's very intriguing to think of, because math, when we kind of think about and we see the math data that's being fed into the large language model. It's doing formal reasoning in natural language, but like, if you use like programming language like Link four, then you can suddenly attempt to turn the entire mathematical literature into GitHub, actionable code, and to apply all the amazing techniques and progress in the coding space to mathematics. If we think about kind of math and how it's different from, say, Chess and Go, it is a very big action space. Like at each step, you can take many possible moves. And if you just, like, kind of, you know, apply the sort of techniques that we have seen that made, like, amazing gains in coding to math. But you have a data scarcity problem. So, you know, if you scrape all the link code on the internet, there's only about, like, I think, like a couple million lines of code which is, which is not, not enough, and how to kind of convert these natural language math data into being code. I think that remains a stubborn technical challenge that we are super excited to try to solve it as
Yeah, 100% I think I'm very excited about the future of, you know, applying RL in areas where there's really good grounding and verifier, just like math and code. I think it's very intriguing to think of, because math, when we kind of think about and we see the math data that's being fed into the large language model. It's doing formal reasoning in natural language, but like, if you use like programming language like Link four, then you can suddenly attempt to turn the entire mathematical literature into GitHub, actionable code, and to apply all the amazing techniques and progress in the coding space to mathematics. If we think about kind of math and how it's different from, say, Chess and Go, it is a very big action space. Like at each step, you can take many possible moves. And if you just, like, kind of, you know, apply the sort of techniques that we have seen that made, like, amazing gains in coding to math. But you have a data scarcity problem. So, you know, if you scrape all the link code on the internet, there's only about, like, I think, like a couple million lines of code which is, which is not, not enough, and how to kind of convert these natural language math data into being code. I think that remains a stubborn technical challenge that we are super excited to try to solve it as
S Speaker 120:07cool. Where do you see them? So if you, if you're able to, we still haven't solved, like, Brainer hypothesis through AI that was like, if you recall, we were putting our North Star goals together, back when we were in fair together. Yeah, that was, like an aspirational goal. I don't think we solved that yet. No one solved it, AI, I guess, what? How would you see like? What would a breakthrough look like? I guess in your space
cool. Where do you see them? So if you, if you're able to, we still haven't solved, like, Brainer hypothesis through AI that was like, if you recall, we were putting our North Star goals together, back when we were in fair together. Yeah, that was, like an aspirational goal. I don't think we solved that yet. No one solved it, AI, I guess, what? How would you see like? What would a breakthrough look like? I guess in your space
cool. Where do you see them? So if you, if you're able to, we still haven't solved, like, Brainer hypothesis through AI that was like, if you recall, we were putting our North Star goals together, back when we were in fair together. Yeah, that was, like an aspirational goal. I don't think we solved that yet. No one solved it, AI, I guess, what? How would you see like? What would a breakthrough look like? I guess in your space
cool. Where do you see them? So if you, if you're able to, we still haven't solved, like, Brainer hypothesis through AI that was like, if you recall, we were putting our North Star goals together, back when we were in fair together. Yeah, that was, like an aspirational goal. I don't think we solved that yet. No one solved it, AI, I guess, what? How would you see like? What would a breakthrough look like? I guess in your space
S Speaker 720:29100% I think there are really, like four stages, as we see it. One is kind of problem solving. We have seen a lot of mechanical benchmarks that are numerical, answer graded. So if I get the answer right, then I score. If I don't, then, you know, it's zero. But the next stage will be theorem proving, where I want to make sure that the intermediate steps leading to the final solutions are correct and rigorous mathematical reason, we're not there yet. Large language models currently are generating a lot of proofs based on sort of pattern matching on what's, you know, fed into the training data, we kind of have not seen it being able to reason with that. We haven't been able to see it doing grounded reasoning without without hallucination on the intermediate steps. And then that's the second stage, which I call the rigorous stage. And there's a big leap to the third stage, which I would call conjecturing, which is having large language models propose interesting mathematical problems to try to solve. And these conjectures are, you know, it's so different from what it has seen before. And the fourth stage is kind of intuitions and theory building, connecting these dots, this new conjectures problems and the solutions into theory, into mathematical theories and frameworks, and put them into context, like gross and Dick has this idea of, if you put mathematical concepts in the right framework, then the solution might emerge like a land mass being swallowed by the rising sea. And that's a very beautiful mathematical idea and a very powerful one. But we, have not seen, you know, the AI mathematician, being able to do that, and that's what we're really excited to be able to work, very cool,
100% I think there are really, like four stages, as we see it. One is kind of problem solving. We have seen a lot of mechanical benchmarks that are numerical, answer graded. So if I get the answer right, then I score. If I don't, then, you know, it's zero. But the next stage will be theorem proving, where I want to make sure that the intermediate steps leading to the final solutions are correct and rigorous mathematical reason, we're not there yet. Large language models currently are generating a lot of proofs based on sort of pattern matching on what's, you know, fed into the training data, we kind of have not seen it being able to reason with that. We haven't been able to see it doing grounded reasoning without without hallucination on the intermediate steps. And then that's the second stage, which I call the rigorous stage. And there's a big leap to the third stage, which I would call conjecturing, which is having large language models propose interesting mathematical problems to try to solve. And these conjectures are, you know, it's so different from what it has seen before. And the fourth stage is kind of intuitions and theory building, connecting these dots, this new conjectures problems and the solutions into theory, into mathematical theories and frameworks, and put them into context, like gross and Dick has this idea of, if you put mathematical concepts in the right framework, then the solution might emerge like a land mass being swallowed by the rising sea. And that's a very beautiful mathematical idea and a very powerful one. But we, have not seen, you know, the AI mathematician, being able to do that, and that's what we're really excited to be able to work, very cool,
100% I think there are really, like four stages, as we see it. One is kind of problem solving. We have seen a lot of mechanical benchmarks that are numerical, answer graded. So if I get the answer right, then I score. If I don't, then, you know, it's zero. But the next stage will be theorem proving, where I want to make sure that the intermediate steps leading to the final solutions are correct and rigorous mathematical reason, we're not there yet. Large language models currently are generating a lot of proofs based on sort of pattern matching on what's, you know, fed into the training data, we kind of have not seen it being able to reason with that. We haven't been able to see it doing grounded reasoning without without hallucination on the intermediate steps. And then that's the second stage, which I call the rigorous stage. And there's a big leap to the third stage, which I would call conjecturing, which is having large language models propose interesting mathematical problems to try to solve. And these conjectures are, you know, it's so different from what it has seen before. And the fourth stage is kind of intuitions and theory building, connecting these dots, this new conjectures problems and the solutions into theory, into mathematical theories and frameworks, and put them into context, like gross and Dick has this idea of, if you put mathematical concepts in the right framework, then the solution might emerge like a land mass being swallowed by the rising sea. And that's a very beautiful mathematical idea and a very powerful one. But we, have not seen, you know, the AI mathematician, being able to do that, and that's what we're really excited to be able to work, very cool,
100% I think there are really, like four stages, as we see it. One is kind of problem solving. We have seen a lot of mechanical benchmarks that are numerical, answer graded. So if I get the answer right, then I score. If I don't, then, you know, it's zero. But the next stage will be theorem proving, where I want to make sure that the intermediate steps leading to the final solutions are correct and rigorous mathematical reason, we're not there yet. Large language models currently are generating a lot of proofs based on sort of pattern matching on what's, you know, fed into the training data, we kind of have not seen it being able to reason with that. We haven't been able to see it doing grounded reasoning without without hallucination on the intermediate steps. And then that's the second stage, which I call the rigorous stage. And there's a big leap to the third stage, which I would call conjecturing, which is having large language models propose interesting mathematical problems to try to solve. And these conjectures are, you know, it's so different from what it has seen before. And the fourth stage is kind of intuitions and theory building, connecting these dots, this new conjectures problems and the solutions into theory, into mathematical theories and frameworks, and put them into context, like gross and Dick has this idea of, if you put mathematical concepts in the right framework, then the solution might emerge like a land mass being swallowed by the rising sea. And that's a very beautiful mathematical idea and a very powerful one. But we, have not seen, you know, the AI mathematician, being able to do that, and that's what we're really excited to be able to work, very cool,
S Speaker 122:12I guess, tangential to math in the biology space, because I have a biologist and a nouveau biologist, very new bio very new biologist. I play
I guess, tangential to math in the biology space, because I have a biologist and a nouveau biologist, very new bio very new biologist. I play
I guess, tangential to math in the biology space, because I have a biologist and a nouveau biologist, very new bio very new biologist. I play
I guess, tangential to math in the biology space, because I have a biologist and a nouveau biologist, very new bio very new biologist. I play
22:22biologist on Apples, I guess
biologist on Apples, I guess
biologist on Apples, I guess
biologist on Apples, I guess
S Speaker 122:26so. As you know, Sal is a former, you know, CTO of the loon project, which was what balloons flying in or in the atmosphere, providing Internet access, correct, right? Okay, so he works on crazy, crazy stuff using
so. As you know, Sal is a former, you know, CTO of the loon project, which was what balloons flying in or in the atmosphere, providing Internet access, correct, right? Okay, so he works on crazy, crazy stuff using
so. As you know, Sal is a former, you know, CTO of the loon project, which was what balloons flying in or in the atmosphere, providing Internet access, correct, right? Okay, so he works on crazy, crazy stuff using
so. As you know, Sal is a former, you know, CTO of the loon project, which was what balloons flying in or in the atmosphere, providing Internet access, correct, right? Okay, so he works on crazy, crazy stuff using
22:39reinforcement learning. Of course. Using, of course, it's how you
reinforcement learning. Of course. Using, of course, it's how you
reinforcement learning. Of course. Using, of course, it's how you
reinforcement learning. Of course. Using, of course, it's how you
22:44get funding, right? Use, RL, RL,
get funding, right? Use, RL, RL,
get funding, right? Use, RL, RL,
get funding, right? Use, RL, RL,
S Speaker 122:48I guess. How has your role changed? I mean, when we were back in fair and we were training, training large language models, or, you know, they were, quote, unquote, large language models, they were actually pretty small at the time, how is this kind of agentic rule change, biology, changed how you guys approach, like, has it changed, or you still just kind of like doing, is your roadmap at all changed? Has it allowed you to raise more money, calling it agentic biology startup?
I guess. How has your role changed? I mean, when we were back in fair and we were training, training large language models, or, you know, they were, quote, unquote, large language models, they were actually pretty small at the time, how is this kind of agentic rule change, biology, changed how you guys approach, like, has it changed, or you still just kind of like doing, is your roadmap at all changed? Has it allowed you to raise more money, calling it agentic biology startup?
I guess. How has your role changed? I mean, when we were back in fair and we were training, training large language models, or, you know, they were, quote, unquote, large language models, they were actually pretty small at the time, how is this kind of agentic rule change, biology, changed how you guys approach, like, has it changed, or you still just kind of like doing, is your roadmap at all changed? Has it allowed you to raise more money, calling it agentic biology startup?
I guess. How has your role changed? I mean, when we were back in fair and we were training, training large language models, or, you know, they were, quote, unquote, large language models, they were actually pretty small at the time, how is this kind of agentic rule change, biology, changed how you guys approach, like, has it changed, or you still just kind of like doing, is your roadmap at all changed? Has it allowed you to raise more money, calling it agentic biology startup?
S Speaker 523:15Well, I mean, so the world has definitely changed, but I think it's just, it's a big progression for, like, AI for math and science, right? Like, from where it was. So I think we've seen, like, I mean, very large sums of money go into, you know, some of the big pre training labs, right? Like, in other companies. And I think that, like, for AI for science, it's just a little bit behind on that same curve, but it feels like it's very much on the same trajectory. So, you know, you think about maybe, like five years ago, or something like that. You know, we all of a sudden, like large language models happened, and we said we should train these things to write code. And like, people started working on that, and now they're pretty good at writing code, right. Like, and there's a huge amount of value coming out of that, right? Like, both in terms of, like, economic value, but also just like, our time, right? Like, you go and you are able to build things so much faster. And I think we're seeing the same thing, you know, at evolutionary scale, with the kind of the biology that we're working on. So I'm not a biologist. I work with biologists, but I have no idea what's going on with some of these things, especially when I started, but I was able to do some, like, actually, the reason I got into evolutionary scale was I started working on protein design just because I was interested. And I was working with some colleagues on that. And, you know, I was playing around with the models, and I was doing things which I thought, Oh, this is cool. This is interesting. This is a nice little weekend project. And then I talked to people who I knew, who are experts in the field, and they were, like, you did what? And like, I'm kind of an unremarkable person, so you kind of know, it's like the models which are getting a lot better, and we just see that happening, right? Like, so how do I end up working on, you know, going from something which is pre training, large language models on biological data to now working on, like, making drugs, right? Like, it's because, like, that curve of capabilities, like, all the models are climbing up it. And so I do think that, like, you know, it's a little bit behind, but we're going to start seeing people have great we are seeing people have great world results with these things, and it's going to come more and more to the focus of people's attention, then I think more investment will come in. I should say that, like, our investors put a lot of money into our company. So I think there's like, I think there's a lot of people who are who are thinking this way, there is the smart money doing it right now. Yeah,
Well, I mean, so the world has definitely changed, but I think it's just, it's a big progression for, like, AI for math and science, right? Like, from where it was. So I think we've seen, like, I mean, very large sums of money go into, you know, some of the big pre training labs, right? Like, in other companies. And I think that, like, for AI for science, it's just a little bit behind on that same curve, but it feels like it's very much on the same trajectory. So, you know, you think about maybe, like five years ago, or something like that. You know, we all of a sudden, like large language models happened, and we said we should train these things to write code. And like, people started working on that, and now they're pretty good at writing code, right. Like, and there's a huge amount of value coming out of that, right? Like, both in terms of, like, economic value, but also just like, our time, right? Like, you go and you are able to build things so much faster. And I think we're seeing the same thing, you know, at evolutionary scale, with the kind of the biology that we're working on. So I'm not a biologist. I work with biologists, but I have no idea what's going on with some of these things, especially when I started, but I was able to do some, like, actually, the reason I got into evolutionary scale was I started working on protein design just because I was interested. And I was working with some colleagues on that. And, you know, I was playing around with the models, and I was doing things which I thought, Oh, this is cool. This is interesting. This is a nice little weekend project. And then I talked to people who I knew, who are experts in the field, and they were, like, you did what? And like, I'm kind of an unremarkable person, so you kind of know, it's like the models which are getting a lot better, and we just see that happening, right? Like, so how do I end up working on, you know, going from something which is pre training, large language models on biological data to now working on, like, making drugs, right? Like, it's because, like, that curve of capabilities, like, all the models are climbing up it. And so I do think that, like, you know, it's a little bit behind, but we're going to start seeing people have great we are seeing people have great world results with these things, and it's going to come more and more to the focus of people's attention, then I think more investment will come in. I should say that, like, our investors put a lot of money into our company. So I think there's like, I think there's a lot of people who are who are thinking this way, there is the smart money doing it right now. Yeah,
Well, I mean, so the world has definitely changed, but I think it's just, it's a big progression for, like, AI for math and science, right? Like, from where it was. So I think we've seen, like, I mean, very large sums of money go into, you know, some of the big pre training labs, right? Like, in other companies. And I think that, like, for AI for science, it's just a little bit behind on that same curve, but it feels like it's very much on the same trajectory. So, you know, you think about maybe, like five years ago, or something like that. You know, we all of a sudden, like large language models happened, and we said we should train these things to write code. And like, people started working on that, and now they're pretty good at writing code, right. Like, and there's a huge amount of value coming out of that, right? Like, both in terms of, like, economic value, but also just like, our time, right? Like, you go and you are able to build things so much faster. And I think we're seeing the same thing, you know, at evolutionary scale, with the kind of the biology that we're working on. So I'm not a biologist. I work with biologists, but I have no idea what's going on with some of these things, especially when I started, but I was able to do some, like, actually, the reason I got into evolutionary scale was I started working on protein design just because I was interested. And I was working with some colleagues on that. And, you know, I was playing around with the models, and I was doing things which I thought, Oh, this is cool. This is interesting. This is a nice little weekend project. And then I talked to people who I knew, who are experts in the field, and they were, like, you did what? And like, I'm kind of an unremarkable person, so you kind of know, it's like the models which are getting a lot better, and we just see that happening, right? Like, so how do I end up working on, you know, going from something which is pre training, large language models on biological data to now working on, like, making drugs, right? Like, it's because, like, that curve of capabilities, like, all the models are climbing up it. And so I do think that, like, you know, it's a little bit behind, but we're going to start seeing people have great we are seeing people have great world results with these things, and it's going to come more and more to the focus of people's attention, then I think more investment will come in. I should say that, like, our investors put a lot of money into our company. So I think there's like, I think there's a lot of people who are who are thinking this way, there is the smart money doing it right now. Yeah,
Well, I mean, so the world has definitely changed, but I think it's just, it's a big progression for, like, AI for math and science, right? Like, from where it was. So I think we've seen, like, I mean, very large sums of money go into, you know, some of the big pre training labs, right? Like, in other companies. And I think that, like, for AI for science, it's just a little bit behind on that same curve, but it feels like it's very much on the same trajectory. So, you know, you think about maybe, like five years ago, or something like that. You know, we all of a sudden, like large language models happened, and we said we should train these things to write code. And like, people started working on that, and now they're pretty good at writing code, right. Like, and there's a huge amount of value coming out of that, right? Like, both in terms of, like, economic value, but also just like, our time, right? Like, you go and you are able to build things so much faster. And I think we're seeing the same thing, you know, at evolutionary scale, with the kind of the biology that we're working on. So I'm not a biologist. I work with biologists, but I have no idea what's going on with some of these things, especially when I started, but I was able to do some, like, actually, the reason I got into evolutionary scale was I started working on protein design just because I was interested. And I was working with some colleagues on that. And, you know, I was playing around with the models, and I was doing things which I thought, Oh, this is cool. This is interesting. This is a nice little weekend project. And then I talked to people who I knew, who are experts in the field, and they were, like, you did what? And like, I'm kind of an unremarkable person, so you kind of know, it's like the models which are getting a lot better, and we just see that happening, right? Like, so how do I end up working on, you know, going from something which is pre training, large language models on biological data to now working on, like, making drugs, right? Like, it's because, like, that curve of capabilities, like, all the models are climbing up it. And so I do think that, like, you know, it's a little bit behind, but we're going to start seeing people have great we are seeing people have great world results with these things, and it's going to come more and more to the focus of people's attention, then I think more investment will come in. I should say that, like, our investors put a lot of money into our company. So I think there's like, I think there's a lot of people who are who are thinking this way, there is the smart money doing it right now. Yeah,
S Speaker 726:03I'm curious, because, like, I feel like it's so cool that you guys are doing I would never say that it's scientific super intelligence. How do you think about like, working toward pretty kind of, like, general capabilities versus specialized, like problem? Because we in math, we have trade off between, like, you know, good general performance on benchmarks that are getting harder and harder. I mean, they are currently at what graduate or research level. But also, there's a totally different question to, you know, like, work toward a specialized problem. Like, for example, you see the million in price problems, is that the same sort of trade off you see in AI for science, like, you know, is model being able to do many, many things.
I'm curious, because, like, I feel like it's so cool that you guys are doing I would never say that it's scientific super intelligence. How do you think about like, working toward pretty kind of, like, general capabilities versus specialized, like problem? Because we in math, we have trade off between, like, you know, good general performance on benchmarks that are getting harder and harder. I mean, they are currently at what graduate or research level. But also, there's a totally different question to, you know, like, work toward a specialized problem. Like, for example, you see the million in price problems, is that the same sort of trade off you see in AI for science, like, you know, is model being able to do many, many things.
I'm curious, because, like, I feel like it's so cool that you guys are doing I would never say that it's scientific super intelligence. How do you think about like, working toward pretty kind of, like, general capabilities versus specialized, like problem? Because we in math, we have trade off between, like, you know, good general performance on benchmarks that are getting harder and harder. I mean, they are currently at what graduate or research level. But also, there's a totally different question to, you know, like, work toward a specialized problem. Like, for example, you see the million in price problems, is that the same sort of trade off you see in AI for science, like, you know, is model being able to do many, many things.
I'm curious, because, like, I feel like it's so cool that you guys are doing I would never say that it's scientific super intelligence. How do you think about like, working toward pretty kind of, like, general capabilities versus specialized, like problem? Because we in math, we have trade off between, like, you know, good general performance on benchmarks that are getting harder and harder. I mean, they are currently at what graduate or research level. But also, there's a totally different question to, you know, like, work toward a specialized problem. Like, for example, you see the million in price problems, is that the same sort of trade off you see in AI for science, like, you know, is model being able to do many, many things.
26:45So I hear Donald Trump somewhere in
So I hear Donald Trump somewhere in
So I hear Donald Trump somewhere in
So I hear Donald Trump somewhere in
26:51the audience. I really don't want to hear that voice right now.
the audience. I really don't want to hear that voice right now.
the audience. I really don't want to hear that voice right now.
the audience. I really don't want to hear that voice right now.
27:00So yeah, I think that's a great question. So we,
So yeah, I think that's a great question. So we,
So yeah, I think that's a great question. So we,
So yeah, I think that's a great question. So we,
S Speaker 527:04you know, the way we kind of see it is, there's, there's, I think there's two critical importance, like, there's two critical things to building, like a virtual scientist, right? Or even going beyond, like, what a scientist could be. So there are simulators, and like a lot of the models that we built, like our traditional models, like ESM models, are essentially simulators of biology. They're not like molecular dynamics or something like that, but it's a simulator built of an information bottleneck, right, like, and it simulates a protein or a genome. The other thing that's happening, right? Like many in the field are building. It is reasoning models, right? Like and when you fuse these two things together, you allow the reasoning model to reason, not only in the space of words or whatever space it's you know, choosing to reason it, but also in the latent space of something that you know sees the world through, like, amino acids or something like that. I think that's a really interesting fusion. So we work at it from those angles, to bring those pieces together.
you know, the way we kind of see it is, there's, there's, I think there's two critical importance, like, there's two critical things to building, like a virtual scientist, right? Or even going beyond, like, what a scientist could be. So there are simulators, and like a lot of the models that we built, like our traditional models, like ESM models, are essentially simulators of biology. They're not like molecular dynamics or something like that, but it's a simulator built of an information bottleneck, right, like, and it simulates a protein or a genome. The other thing that's happening, right? Like many in the field are building. It is reasoning models, right? Like and when you fuse these two things together, you allow the reasoning model to reason, not only in the space of words or whatever space it's you know, choosing to reason it, but also in the latent space of something that you know sees the world through, like, amino acids or something like that. I think that's a really interesting fusion. So we work at it from those angles, to bring those pieces together.
you know, the way we kind of see it is, there's, there's, I think there's two critical importance, like, there's two critical things to building, like a virtual scientist, right? Or even going beyond, like, what a scientist could be. So there are simulators, and like a lot of the models that we built, like our traditional models, like ESM models, are essentially simulators of biology. They're not like molecular dynamics or something like that, but it's a simulator built of an information bottleneck, right, like, and it simulates a protein or a genome. The other thing that's happening, right? Like many in the field are building. It is reasoning models, right? Like and when you fuse these two things together, you allow the reasoning model to reason, not only in the space of words or whatever space it's you know, choosing to reason it, but also in the latent space of something that you know sees the world through, like, amino acids or something like that. I think that's a really interesting fusion. So we work at it from those angles, to bring those pieces together.
you know, the way we kind of see it is, there's, there's, I think there's two critical importance, like, there's two critical things to building, like a virtual scientist, right? Or even going beyond, like, what a scientist could be. So there are simulators, and like a lot of the models that we built, like our traditional models, like ESM models, are essentially simulators of biology. They're not like molecular dynamics or something like that, but it's a simulator built of an information bottleneck, right, like, and it simulates a protein or a genome. The other thing that's happening, right? Like many in the field are building. It is reasoning models, right? Like and when you fuse these two things together, you allow the reasoning model to reason, not only in the space of words or whatever space it's you know, choosing to reason it, but also in the latent space of something that you know sees the world through, like, amino acids or something like that. I think that's a really interesting fusion. So we work at it from those angles, to bring those pieces together.
S Speaker 128:07It does feel, I mean, generally, like the world is kind of going in a certain direction. I mean, the sciences will kind of like fast slash will follow, depending on who you ask. I'm kind of curious, though, for you again, though, like on this, you've worked at, like, a lot of different layers of the stack. And, like we talked about agent tech frameworks, there's obviously that we go all the way down to the metal, where we're generating kernels, you know, for models like, what, like, where do you see the bottleneck in things and where, like, where's the most innovation needed, I guess, in the stack?
It does feel, I mean, generally, like the world is kind of going in a certain direction. I mean, the sciences will kind of like fast slash will follow, depending on who you ask. I'm kind of curious, though, for you again, though, like on this, you've worked at, like, a lot of different layers of the stack. And, like we talked about agent tech frameworks, there's obviously that we go all the way down to the metal, where we're generating kernels, you know, for models like, what, like, where do you see the bottleneck in things and where, like, where's the most innovation needed, I guess, in the stack?
It does feel, I mean, generally, like the world is kind of going in a certain direction. I mean, the sciences will kind of like fast slash will follow, depending on who you ask. I'm kind of curious, though, for you again, though, like on this, you've worked at, like, a lot of different layers of the stack. And, like we talked about agent tech frameworks, there's obviously that we go all the way down to the metal, where we're generating kernels, you know, for models like, what, like, where do you see the bottleneck in things and where, like, where's the most innovation needed, I guess, in the stack?
It does feel, I mean, generally, like the world is kind of going in a certain direction. I mean, the sciences will kind of like fast slash will follow, depending on who you ask. I'm kind of curious, though, for you again, though, like on this, you've worked at, like, a lot of different layers of the stack. And, like we talked about agent tech frameworks, there's obviously that we go all the way down to the metal, where we're generating kernels, you know, for models like, what, like, where do you see the bottleneck in things and where, like, where's the most innovation needed, I guess, in the stack?
S Speaker 328:37Yeah, I would say, still, it's like trying to take these models and training them on like, hundreds of 1000s of GPUs we've kind of seen over the past few years. This trend, even in open source, towards like, bigger models, as we like, try to close the gap with API models like llama, for example, llama three, I was like 70 B for the media model. Now for like, 400 B, same trend is happening for quiet same trend is happening for deep seek. And there are like, libraries out there that kind of try to let you do this, like try to let you scale models, but sort of the kind of use cases they solve are very disparate. Like, there's some libraries that you know are nice to use, but maybe don't support all parallelisms, but you can't scale past a certain limit. There's other ones that maybe support everything but don't support the architecture and, like, the sort of detection mechanisms you care about. There's other ones that are, like, way too complicated to use. You can't really iterate. Like, it's hard to understand. And now with, with RL, kind of like, as Drew was talking about, like, we see that in our kind of workflows as well, like, we're moving towards this world where we don't just have single chat models. You have these, like, agentic models that use tools. This adds, like, a new layer of complexity. Like, how do you do generations? Well, like, how do you transfer, like, your huge model weights from your training GPUs, your inference GPUs efficiently and so on. And in that area, especially, there's like nothing, sort of, well, I shouldn't say nothing, but they're like very few sort of tools to use in open source. And at least for us, like, because of all these issues, we've kind of had to build everything from scratch, and it's not a place that we necessarily want to be in, like, we're pretty small post training team. We'd love to just run experiments, but yeah, there just isn't anything like too great out there. I think there's potentially an opportunity to kind of improve things.
Yeah, I would say, still, it's like trying to take these models and training them on like, hundreds of 1000s of GPUs we've kind of seen over the past few years. This trend, even in open source, towards like, bigger models, as we like, try to close the gap with API models like llama, for example, llama three, I was like 70 B for the media model. Now for like, 400 B, same trend is happening for quiet same trend is happening for deep seek. And there are like, libraries out there that kind of try to let you do this, like try to let you scale models, but sort of the kind of use cases they solve are very disparate. Like, there's some libraries that you know are nice to use, but maybe don't support all parallelisms, but you can't scale past a certain limit. There's other ones that maybe support everything but don't support the architecture and, like, the sort of detection mechanisms you care about. There's other ones that are, like, way too complicated to use. You can't really iterate. Like, it's hard to understand. And now with, with RL, kind of like, as Drew was talking about, like, we see that in our kind of workflows as well, like, we're moving towards this world where we don't just have single chat models. You have these, like, agentic models that use tools. This adds, like, a new layer of complexity. Like, how do you do generations? Well, like, how do you transfer, like, your huge model weights from your training GPUs, your inference GPUs efficiently and so on. And in that area, especially, there's like nothing, sort of, well, I shouldn't say nothing, but they're like very few sort of tools to use in open source. And at least for us, like, because of all these issues, we've kind of had to build everything from scratch, and it's not a place that we necessarily want to be in, like, we're pretty small post training team. We'd love to just run experiments, but yeah, there just isn't anything like too great out there. I think there's potentially an opportunity to kind of improve things.
Yeah, I would say, still, it's like trying to take these models and training them on like, hundreds of 1000s of GPUs we've kind of seen over the past few years. This trend, even in open source, towards like, bigger models, as we like, try to close the gap with API models like llama, for example, llama three, I was like 70 B for the media model. Now for like, 400 B, same trend is happening for quiet same trend is happening for deep seek. And there are like, libraries out there that kind of try to let you do this, like try to let you scale models, but sort of the kind of use cases they solve are very disparate. Like, there's some libraries that you know are nice to use, but maybe don't support all parallelisms, but you can't scale past a certain limit. There's other ones that maybe support everything but don't support the architecture and, like, the sort of detection mechanisms you care about. There's other ones that are, like, way too complicated to use. You can't really iterate. Like, it's hard to understand. And now with, with RL, kind of like, as Drew was talking about, like, we see that in our kind of workflows as well, like, we're moving towards this world where we don't just have single chat models. You have these, like, agentic models that use tools. This adds, like, a new layer of complexity. Like, how do you do generations? Well, like, how do you transfer, like, your huge model weights from your training GPUs, your inference GPUs efficiently and so on. And in that area, especially, there's like nothing, sort of, well, I shouldn't say nothing, but they're like very few sort of tools to use in open source. And at least for us, like, because of all these issues, we've kind of had to build everything from scratch, and it's not a place that we necessarily want to be in, like, we're pretty small post training team. We'd love to just run experiments, but yeah, there just isn't anything like too great out there. I think there's potentially an opportunity to kind of improve things.
Yeah, I would say, still, it's like trying to take these models and training them on like, hundreds of 1000s of GPUs we've kind of seen over the past few years. This trend, even in open source, towards like, bigger models, as we like, try to close the gap with API models like llama, for example, llama three, I was like 70 B for the media model. Now for like, 400 B, same trend is happening for quiet same trend is happening for deep seek. And there are like, libraries out there that kind of try to let you do this, like try to let you scale models, but sort of the kind of use cases they solve are very disparate. Like, there's some libraries that you know are nice to use, but maybe don't support all parallelisms, but you can't scale past a certain limit. There's other ones that maybe support everything but don't support the architecture and, like, the sort of detection mechanisms you care about. There's other ones that are, like, way too complicated to use. You can't really iterate. Like, it's hard to understand. And now with, with RL, kind of like, as Drew was talking about, like, we see that in our kind of workflows as well, like, we're moving towards this world where we don't just have single chat models. You have these, like, agentic models that use tools. This adds, like, a new layer of complexity. Like, how do you do generations? Well, like, how do you transfer, like, your huge model weights from your training GPUs, your inference GPUs efficiently and so on. And in that area, especially, there's like nothing, sort of, well, I shouldn't say nothing, but they're like very few sort of tools to use in open source. And at least for us, like, because of all these issues, we've kind of had to build everything from scratch, and it's not a place that we necessarily want to be in, like, we're pretty small post training team. We'd love to just run experiments, but yeah, there just isn't anything like too great out there. I think there's potentially an opportunity to kind of improve things.
S Speaker 130:37That's a perfect segue to my last question for all of you, lightning ground, if you could have anything from pytorch Or I guess if you want lava, that's fine too. But if you could have anything from pytorch, what would you want? Like your deepest
That's a perfect segue to my last question for all of you, lightning ground, if you could have anything from pytorch Or I guess if you want lava, that's fine too. But if you could have anything from pytorch, what would you want? Like your deepest
That's a perfect segue to my last question for all of you, lightning ground, if you could have anything from pytorch Or I guess if you want lava, that's fine too. But if you could have anything from pytorch, what would you want? Like your deepest
That's a perfect segue to my last question for all of you, lightning ground, if you could have anything from pytorch Or I guess if you want lava, that's fine too. But if you could have anything from pytorch, what would you want? Like your deepest
30:53wish, whatever you can have, maybe I'll start with forest over
wish, whatever you can have, maybe I'll start with forest over
wish, whatever you can have, maybe I'll start with forest over
wish, whatever you can have, maybe I'll start with forest over
30:57here. I think my requests are like pretty different, sure. Okay.
here. I think my requests are like pretty different, sure. Okay.
here. I think my requests are like pretty different, sure. Okay.
here. I think my requests are like pretty different, sure. Okay.
S Speaker 631:03My request for mostly, I like the lowest level, so like, things like kernel authoring, things like d tensor, things like symmetric memory, like, it's yeah, like, I think when you're kind of at like a more like frontier type lab, it is kind of like you end up building, like, a lot of things yourself. Or it like very capable of building a lot of things yourself. And so a lot of the times, the things that are useful are like the fundamental building blocks that are like reusable and are like solid that you can reuse. And so those are kind of things
My request for mostly, I like the lowest level, so like, things like kernel authoring, things like d tensor, things like symmetric memory, like, it's yeah, like, I think when you're kind of at like a more like frontier type lab, it is kind of like you end up building, like, a lot of things yourself. Or it like very capable of building a lot of things yourself. And so a lot of the times, the things that are useful are like the fundamental building blocks that are like reusable and are like solid that you can reuse. And so those are kind of things
My request for mostly, I like the lowest level, so like, things like kernel authoring, things like d tensor, things like symmetric memory, like, it's yeah, like, I think when you're kind of at like a more like frontier type lab, it is kind of like you end up building, like, a lot of things yourself. Or it like very capable of building a lot of things yourself. And so a lot of the times, the things that are useful are like the fundamental building blocks that are like reusable and are like solid that you can reuse. And so those are kind of things
My request for mostly, I like the lowest level, so like, things like kernel authoring, things like d tensor, things like symmetric memory, like, it's yeah, like, I think when you're kind of at like a more like frontier type lab, it is kind of like you end up building, like, a lot of things yourself. Or it like very capable of building a lot of things yourself. And so a lot of the times, the things that are useful are like the fundamental building blocks that are like reusable and are like solid that you can reuse. And so those are kind of things
31:34we'll keep working with you guys on. That for sure.
we'll keep working with you guys on. That for sure.
we'll keep working with you guys on. That for sure.
we'll keep working with you guys on. That for sure.
S Speaker 431:38Here's what I want. Then I'll backtrack into what pytorch can give me, every digital surface that a human interacts with, their
Here's what I want. Then I'll backtrack into what pytorch can give me, every digital surface that a human interacts with, their
Here's what I want. Then I'll backtrack into what pytorch can give me, every digital surface that a human interacts with, their
Here's what I want. Then I'll backtrack into what pytorch can give me, every digital surface that a human interacts with, their
S Speaker 831:47phone, their app, their browser, their laptop, their email, their calendar,
phone, their app, their browser, their laptop, their email, their calendar,
phone, their app, their browser, their laptop, their email, their calendar,
phone, their app, their browser, their laptop, their email, their calendar,
S Speaker 431:52everything is a tool call for an AI system. It's just an external memory mechanism. It's a way of communicating. It's worth dealing with the world, and I want every digital surface to just be I got a road map, a tool call away in high throughput, high bandwidth, so
everything is a tool call for an AI system. It's just an external memory mechanism. It's a way of communicating. It's worth dealing with the world, and I want every digital surface to just be I got a road map, a tool call away in high throughput, high bandwidth, so
everything is a tool call for an AI system. It's just an external memory mechanism. It's a way of communicating. It's worth dealing with the world, and I want every digital surface to just be I got a road map, a tool call away in high throughput, high bandwidth, so
everything is a tool call for an AI system. It's just an external memory mechanism. It's a way of communicating. It's worth dealing with the world, and I want every digital surface to just be I got a road map, a tool call away in high throughput, high bandwidth, so
S Speaker 132:10essentially a library of incredibly high performance environments. I mean, believe it or not, actually I'm already kind of building something like that. So OK, so we'll talk later. OK, awesome. Sal, what can I do for you, sir?
essentially a library of incredibly high performance environments. I mean, believe it or not, actually I'm already kind of building something like that. So OK, so we'll talk later. OK, awesome. Sal, what can I do for you, sir?
essentially a library of incredibly high performance environments. I mean, believe it or not, actually I'm already kind of building something like that. So OK, so we'll talk later. OK, awesome. Sal, what can I do for you, sir?
essentially a library of incredibly high performance environments. I mean, believe it or not, actually I'm already kind of building something like that. So OK, so we'll talk later. OK, awesome. Sal, what can I do for you, sir?
S Speaker 532:24We also want, we also want to scale models. So actually, I knew you were asked this question, so I asked my engineering team on Slack, yeah, and their answer is same as mine, and similar to ours, is like, we better start in check, boarding the enhancer, all the, okay, all the level stuff. I mean, when we train out models, we kind of have the pytorch layer, and then you have a layer of other libraries, and then we write our own stuff, either on the top or bottom or whatever. And the more of that stuff, we can kind of get rid of them just have, like, pytorch and what we want to customize, the better for us. Okay, you
We also want, we also want to scale models. So actually, I knew you were asked this question, so I asked my engineering team on Slack, yeah, and their answer is same as mine, and similar to ours, is like, we better start in check, boarding the enhancer, all the, okay, all the level stuff. I mean, when we train out models, we kind of have the pytorch layer, and then you have a layer of other libraries, and then we write our own stuff, either on the top or bottom or whatever. And the more of that stuff, we can kind of get rid of them just have, like, pytorch and what we want to customize, the better for us. Okay, you
We also want, we also want to scale models. So actually, I knew you were asked this question, so I asked my engineering team on Slack, yeah, and their answer is same as mine, and similar to ours, is like, we better start in check, boarding the enhancer, all the, okay, all the level stuff. I mean, when we train out models, we kind of have the pytorch layer, and then you have a layer of other libraries, and then we write our own stuff, either on the top or bottom or whatever. And the more of that stuff, we can kind of get rid of them just have, like, pytorch and what we want to customize, the better for us. Okay, you
We also want, we also want to scale models. So actually, I knew you were asked this question, so I asked my engineering team on Slack, yeah, and their answer is same as mine, and similar to ours, is like, we better start in check, boarding the enhancer, all the, okay, all the level stuff. I mean, when we train out models, we kind of have the pytorch layer, and then you have a layer of other libraries, and then we write our own stuff, either on the top or bottom or whatever. And the more of that stuff, we can kind of get rid of them just have, like, pytorch and what we want to customize, the better for us. Okay, you
S Speaker 332:59good for us pretty similar, honestly. Like, we want to be able to scale models. And I know pytorch has been doing a lot of, like, awesome work here around different scalability methods, and, like, we're pretty excited about that. I guess only more on the model side, like, we'd love a better tool calling model, at least, like on the chat side, there are open source models out there that kind of close the gap with API models. But for tool calling, like, yeah, the client deep seek, like, they're okay, but they're not, like, as as good. And like, having somebody better there would be amazing for us. Cool,
good for us pretty similar, honestly. Like, we want to be able to scale models. And I know pytorch has been doing a lot of, like, awesome work here around different scalability methods, and, like, we're pretty excited about that. I guess only more on the model side, like, we'd love a better tool calling model, at least, like on the chat side, there are open source models out there that kind of close the gap with API models. But for tool calling, like, yeah, the client deep seek, like, they're okay, but they're not, like, as as good. And like, having somebody better there would be amazing for us. Cool,
good for us pretty similar, honestly. Like, we want to be able to scale models. And I know pytorch has been doing a lot of, like, awesome work here around different scalability methods, and, like, we're pretty excited about that. I guess only more on the model side, like, we'd love a better tool calling model, at least, like on the chat side, there are open source models out there that kind of close the gap with API models. But for tool calling, like, yeah, the client deep seek, like, they're okay, but they're not, like, as as good. And like, having somebody better there would be amazing for us. Cool,
good for us pretty similar, honestly. Like, we want to be able to scale models. And I know pytorch has been doing a lot of, like, awesome work here around different scalability methods, and, like, we're pretty excited about that. I guess only more on the model side, like, we'd love a better tool calling model, at least, like on the chat side, there are open source models out there that kind of close the gap with API models. But for tool calling, like, yeah, the client deep seek, like, they're okay, but they're not, like, as as good. And like, having somebody better there would be amazing for us. Cool,
S Speaker 733:34yeah. We would love to see just like, you know, like, plug and play, like, to do our skill like, like, like, torch tune. I know that, like, there are some like, really amazing recent updates. It's getting kind of more and more keeping up with the state of the art, like algorithms. It's rolling out like, you know, like, I It's quite interesting. Now we think about grpo. It actually came from deep sea math. It's like, you know, like, these algorithms would like to kind of be able to have them available
yeah. We would love to see just like, you know, like, plug and play, like, to do our skill like, like, like, torch tune. I know that, like, there are some like, really amazing recent updates. It's getting kind of more and more keeping up with the state of the art, like algorithms. It's rolling out like, you know, like, I It's quite interesting. Now we think about grpo. It actually came from deep sea math. It's like, you know, like, these algorithms would like to kind of be able to have them available
yeah. We would love to see just like, you know, like, plug and play, like, to do our skill like, like, like, torch tune. I know that, like, there are some like, really amazing recent updates. It's getting kind of more and more keeping up with the state of the art, like algorithms. It's rolling out like, you know, like, I It's quite interesting. Now we think about grpo. It actually came from deep sea math. It's like, you know, like, these algorithms would like to kind of be able to have them available
yeah. We would love to see just like, you know, like, plug and play, like, to do our skill like, like, like, torch tune. I know that, like, there are some like, really amazing recent updates. It's getting kind of more and more keeping up with the state of the art, like algorithms. It's rolling out like, you know, like, I It's quite interesting. Now we think about grpo. It actually came from deep sea math. It's like, you know, like, these algorithms would like to kind of be able to have them available
S Speaker 134:03Yeah, and if you caught, it was very subtle in a part of this keynote this morning, with a little butterflies in there. So if you kind of like, look for monarch and pytorch, you might see forest knows. Maybe not everyone else knows, but, but, yeah, you can imagine, kind of like a god mode being able to scale. I think you guys would be right up his alley. Because, like, I think we're just, like, fundamentally running out of weight, just like, aggregate enough power and GPUs in one place. And so if you can do this, like, across data centers, and actually, like, literally, scale up to like, that level, which I think is like insane, it could be inference, could be post training. Could be, you know, obviously pre training, but make it feel like you're kind of writing code locally, and feel like natural, like leaders sitting around your back, like, you know, development locally, like, that's, that's kind of like our objective. So awesome. I think we're out of time. How about a round of applause for our panel?
Yeah, and if you caught, it was very subtle in a part of this keynote this morning, with a little butterflies in there. So if you kind of like, look for monarch and pytorch, you might see forest knows. Maybe not everyone else knows, but, but, yeah, you can imagine, kind of like a god mode being able to scale. I think you guys would be right up his alley. Because, like, I think we're just, like, fundamentally running out of weight, just like, aggregate enough power and GPUs in one place. And so if you can do this, like, across data centers, and actually, like, literally, scale up to like, that level, which I think is like insane, it could be inference, could be post training. Could be, you know, obviously pre training, but make it feel like you're kind of writing code locally, and feel like natural, like leaders sitting around your back, like, you know, development locally, like, that's, that's kind of like our objective. So awesome. I think we're out of time. How about a round of applause for our panel?
Yeah, and if you caught, it was very subtle in a part of this keynote this morning, with a little butterflies in there. So if you kind of like, look for monarch and pytorch, you might see forest knows. Maybe not everyone else knows, but, but, yeah, you can imagine, kind of like a god mode being able to scale. I think you guys would be right up his alley. Because, like, I think we're just, like, fundamentally running out of weight, just like, aggregate enough power and GPUs in one place. And so if you can do this, like, across data centers, and actually, like, literally, scale up to like, that level, which I think is like insane, it could be inference, could be post training. Could be, you know, obviously pre training, but make it feel like you're kind of writing code locally, and feel like natural, like leaders sitting around your back, like, you know, development locally, like, that's, that's kind of like our objective. So awesome. I think we're out of time. How about a round of applause for our panel?
Yeah, and if you caught, it was very subtle in a part of this keynote this morning, with a little butterflies in there. So if you kind of like, look for monarch and pytorch, you might see forest knows. Maybe not everyone else knows, but, but, yeah, you can imagine, kind of like a god mode being able to scale. I think you guys would be right up his alley. Because, like, I think we're just, like, fundamentally running out of weight, just like, aggregate enough power and GPUs in one place. And so if you can do this, like, across data centers, and actually, like, literally, scale up to like, that level, which I think is like insane, it could be inference, could be post training. Could be, you know, obviously pre training, but make it feel like you're kind of writing code locally, and feel like natural, like leaders sitting around your back, like, you know, development locally, like, that's, that's kind of like our objective. So awesome. I think we're out of time. How about a round of applause for our panel?
S Speaker 135:00Yeah, I appreciate all the answers. Really diverse panel and so many different perspectives. Thank you again. I appreciate it. Cheers, applause.
Yeah, I appreciate all the answers. Really diverse panel and so many different perspectives. Thank you again. I appreciate it. Cheers, applause.
Yeah, I appreciate all the answers. Really diverse panel and so many different perspectives. Thank you again. I appreciate it. Cheers, applause.
Yeah, I appreciate all the answers. Really diverse panel and so many different perspectives. Thank you again. I appreciate it. Cheers, applause.