Meeting: PoetiQ
Wed, Jul 30
2:59 PM
52 min
Priyesh P
Introductions and Initial Conversations
0:18
Qualcomm Ve
URL: https://otter.ai/u/vbMJXhikQNT_R-xPYlnxZw8F_s0
Downloaded: 2025-12-21T21:30:35.209034
Method: text_extraction
============================================================

0:18Hey, hi, shameet. Hey, how's it going?
Hey, hi, shameet. Hey, how's it going?
Hey, hi, shameet. Hey, how's it going?
Hey, hi, shameet. Hey, how's it going?
S Speaker 10:20Hi, SHAMET, pleasure meeting you. I'm Priyesh. I am an investor in Tushar team. Oh, got it good to meet you too. How's it going? Absolutely, it's very good. I was just looking through the website. Seems like very interesting stuff. Would love to learn more. Yeah,
Hi, SHAMET, pleasure meeting you. I'm Priyesh. I am an investor in Tushar team. Oh, got it good to meet you too. How's it going? Absolutely, it's very good. I was just looking through the website. Seems like very interesting stuff. Would love to learn more. Yeah,
Hi, SHAMET, pleasure meeting you. I'm Priyesh. I am an investor in Tushar team. Oh, got it good to meet you too. How's it going? Absolutely, it's very good. I was just looking through the website. Seems like very interesting stuff. Would love to learn more. Yeah,
Hi, SHAMET, pleasure meeting you. I'm Priyesh. I am an investor in Tushar team. Oh, got it good to meet you too. How's it going? Absolutely, it's very good. I was just looking through the website. Seems like very interesting stuff. Would love to learn more. Yeah,
0:34Okay, sounds good. Yeah.
Okay, sounds good. Yeah.
Okay, sounds good. Yeah.
Okay, sounds good. Yeah.
0:40Tushar should be joining any moment.
Tushar should be joining any moment.
Tushar should be joining any moment.
Tushar should be joining any moment.
S Speaker 10:43We can wait for him, but you're based in San Diego, is it?
We can wait for him, but you're based in San Diego, is it?
We can wait for him, but you're based in San Diego, is it?
We can wait for him, but you're based in San Diego, is it?
S Speaker 20:47Yeah. So I'm in San Diego, and one of the founders is in the Bay Area, and the other one's in Florida. So we're pretty distributed, right, right?
Yeah. So I'm in San Diego, and one of the founders is in the Bay Area, and the other one's in Florida. So we're pretty distributed, right, right?
Yeah. So I'm in San Diego, and one of the founders is in the Bay Area, and the other one's in Florida. So we're pretty distributed, right, right?
Yeah. So I'm in San Diego, and one of the founders is in the Bay Area, and the other one's in Florida. So we're pretty distributed, right, right?
S Speaker 31:01Yeah. Know, hey there. How's it going good
Yeah. Know, hey there. How's it going good
Yeah. Know, hey there. How's it going good
Yeah. Know, hey there. How's it going good
1:06to connect? Yeah, you too.
to connect? Yeah, you too.
to connect? Yeah, you too.
to connect? Yeah, you too.
1:11I guess you guys were talking where you based?
I guess you guys were talking where you based?
I guess you guys were talking where you based?
I guess you guys were talking where you based?
S Speaker 21:13Yeah, oh yeah, I'm in Sandy. I'm in San Diego. The rest of the team is geographically dispersed, but our other founders in in the Bay Area, our next four hires are all in the Bay Area also. So they're all there, yeah, okay.
Yeah, oh yeah, I'm in Sandy. I'm in San Diego. The rest of the team is geographically dispersed, but our other founders in in the Bay Area, our next four hires are all in the Bay Area also. So they're all there, yeah, okay.
Yeah, oh yeah, I'm in Sandy. I'm in San Diego. The rest of the team is geographically dispersed, but our other founders in in the Bay Area, our next four hires are all in the Bay Area also. So they're all there, yeah, okay.
Yeah, oh yeah, I'm in Sandy. I'm in San Diego. The rest of the team is geographically dispersed, but our other founders in in the Bay Area, our next four hires are all in the Bay Area also. So they're all there, yeah, okay.
1:29I used to live in San Diego. Spent
I used to live in San Diego. Spent
I used to live in San Diego. Spent
I used to live in San Diego. Spent
S Speaker 21:3514 years in San Diego. Oh, yeah, no. I was living in DC, and I came out here for vacation. Like, yeah, I don't want to leave. I mean, where in San Diego are you in La Jolla? Yeah, that's nice, yeah, yeah, I came and honestly, like, I didn't realize how hoity toity this place really was right, like, I'm not into the whole hoity toity nest, but I just saw it. I was like, beautiful move here. And then only, only after I moved here, did I realize, Wow, this place is a little different than
14 years in San Diego. Oh, yeah, no. I was living in DC, and I came out here for vacation. Like, yeah, I don't want to leave. I mean, where in San Diego are you in La Jolla? Yeah, that's nice, yeah, yeah, I came and honestly, like, I didn't realize how hoity toity this place really was right, like, I'm not into the whole hoity toity nest, but I just saw it. I was like, beautiful move here. And then only, only after I moved here, did I realize, Wow, this place is a little different than
14 years in San Diego. Oh, yeah, no. I was living in DC, and I came out here for vacation. Like, yeah, I don't want to leave. I mean, where in San Diego are you in La Jolla? Yeah, that's nice, yeah, yeah, I came and honestly, like, I didn't realize how hoity toity this place really was right, like, I'm not into the whole hoity toity nest, but I just saw it. I was like, beautiful move here. And then only, only after I moved here, did I realize, Wow, this place is a little different than
14 years in San Diego. Oh, yeah, no. I was living in DC, and I came out here for vacation. Like, yeah, I don't want to leave. I mean, where in San Diego are you in La Jolla? Yeah, that's nice, yeah, yeah, I came and honestly, like, I didn't realize how hoity toity this place really was right, like, I'm not into the whole hoity toity nest, but I just saw it. I was like, beautiful move here. And then only, only after I moved here, did I realize, Wow, this place is a little different than
S Speaker 32:14I expected. It's a nice neighborhood, and everything is beautiful. Also it's, yeah, it's also the beaches, yeah, San Diego is nice, yeah, yeah.
I expected. It's a nice neighborhood, and everything is beautiful. Also it's, yeah, it's also the beaches, yeah, San Diego is nice, yeah, yeah.
I expected. It's a nice neighborhood, and everything is beautiful. Also it's, yeah, it's also the beaches, yeah, San Diego is nice, yeah, yeah.
I expected. It's a nice neighborhood, and everything is beautiful. Also it's, yeah, it's also the beaches, yeah, San Diego is nice, yeah, yeah.
2:30So hey, she mean, why don't we do this?
So hey, she mean, why don't we do this?
So hey, she mean, why don't we do this?
So hey, she mean, why don't we do this?
S Speaker 32:34Let's do that. I'll quickly introduce myself, and love to learn more about you and
Let's do that. I'll quickly introduce myself, and love to learn more about you and
Let's do that. I'll quickly introduce myself, and love to learn more about you and
Let's do that. I'll quickly introduce myself, and love to learn more about you and
S Speaker 32:46I've been with Qualcomm most of my career. That's how I started as an Internet in Qualcomm and San Diego and and so my background is in so I started as software engineer, like actually started an embedded and then I was a software engineer that learned hardware the hard way, and then I rent products. So I used to be a product manager for a bunch of different our software products. Then I became a product manager for our hardware products. Then I ran a few businesses and and then eight years at Qualcomm ventures. So now I manage North America investments team at Qualcomm ventures and
I've been with Qualcomm most of my career. That's how I started as an Internet in Qualcomm and San Diego and and so my background is in so I started as software engineer, like actually started an embedded and then I was a software engineer that learned hardware the hard way, and then I rent products. So I used to be a product manager for a bunch of different our software products. Then I became a product manager for our hardware products. Then I ran a few businesses and and then eight years at Qualcomm ventures. So now I manage North America investments team at Qualcomm ventures and
I've been with Qualcomm most of my career. That's how I started as an Internet in Qualcomm and San Diego and and so my background is in so I started as software engineer, like actually started an embedded and then I was a software engineer that learned hardware the hard way, and then I rent products. So I used to be a product manager for a bunch of different our software products. Then I became a product manager for our hardware products. Then I ran a few businesses and and then eight years at Qualcomm ventures. So now I manage North America investments team at Qualcomm ventures and
I've been with Qualcomm most of my career. That's how I started as an Internet in Qualcomm and San Diego and and so my background is in so I started as software engineer, like actually started an embedded and then I was a software engineer that learned hardware the hard way, and then I rent products. So I used to be a product manager for a bunch of different our software products. Then I became a product manager for our hardware products. Then I ran a few businesses and and then eight years at Qualcomm ventures. So now I manage North America investments team at Qualcomm ventures and
3:35Qualcomm ventures,
S Speaker 33:38we invest in areas that are strategic. The Qualcomm, so off the balance sheet, one $50 million every year around there. And so we invest globally. But I'd say 60 to 70% of the dollars are invested in the US and we. We can lead. We can lead, we can follow. Our check size are up to $10 million initial check size,
we invest in areas that are strategic. The Qualcomm, so off the balance sheet, one $50 million every year around there. And so we invest globally. But I'd say 60 to 70% of the dollars are invested in the US and we. We can lead. We can lead, we can follow. Our check size are up to $10 million initial check size,
we invest in areas that are strategic. The Qualcomm, so off the balance sheet, one $50 million every year around there. And so we invest globally. But I'd say 60 to 70% of the dollars are invested in the US and we. We can lead. We can lead, we can follow. Our check size are up to $10 million initial check size,
we invest in areas that are strategic. The Qualcomm, so off the balance sheet, one $50 million every year around there. And so we invest globally. But I'd say 60 to 70% of the dollars are invested in the US and we. We can lead. We can lead, we can follow. Our check size are up to $10 million initial check size,
4:09our investment thesis is mostly
our investment thesis is mostly
our investment thesis is mostly
our investment thesis is mostly
S Speaker 34:12it's broad enough because we like to invest in companies, applications, infrastructure, or that essentially pushes the boundaries of compute and connectivity. We build pretty solid compute and connectivity products, and then we want the worldly to use it more. So that's the broad thesis. And then in that process, of course, many companies become partners for Qualcomm. Many companies become go to market channel partners, or in some cases, welcome becomes their customer income. Some cases they became, become our customer. So it's, that's how it typically adds out, but it's not, we don't necessarily need, like a partnership to make an investment in a company. Yeah. I mean, and then, as Sam mentioned great things about you and what you guys are building. So he gave me like a brief overview, but I wouldn't have to learn more. So we've been investing in heavily in AI. We're investors and companies like
it's broad enough because we like to invest in companies, applications, infrastructure, or that essentially pushes the boundaries of compute and connectivity. We build pretty solid compute and connectivity products, and then we want the worldly to use it more. So that's the broad thesis. And then in that process, of course, many companies become partners for Qualcomm. Many companies become go to market channel partners, or in some cases, welcome becomes their customer income. Some cases they became, become our customer. So it's, that's how it typically adds out, but it's not, we don't necessarily need, like a partnership to make an investment in a company. Yeah. I mean, and then, as Sam mentioned great things about you and what you guys are building. So he gave me like a brief overview, but I wouldn't have to learn more. So we've been investing in heavily in AI. We're investors and companies like
it's broad enough because we like to invest in companies, applications, infrastructure, or that essentially pushes the boundaries of compute and connectivity. We build pretty solid compute and connectivity products, and then we want the worldly to use it more. So that's the broad thesis. And then in that process, of course, many companies become partners for Qualcomm. Many companies become go to market channel partners, or in some cases, welcome becomes their customer income. Some cases they became, become our customer. So it's, that's how it typically adds out, but it's not, we don't necessarily need, like a partnership to make an investment in a company. Yeah. I mean, and then, as Sam mentioned great things about you and what you guys are building. So he gave me like a brief overview, but I wouldn't have to learn more. So we've been investing in heavily in AI. We're investors and companies like
it's broad enough because we like to invest in companies, applications, infrastructure, or that essentially pushes the boundaries of compute and connectivity. We build pretty solid compute and connectivity products, and then we want the worldly to use it more. So that's the broad thesis. And then in that process, of course, many companies become partners for Qualcomm. Many companies become go to market channel partners, or in some cases, welcome becomes their customer income. Some cases they became, become our customer. So it's, that's how it typically adds out, but it's not, we don't necessarily need, like a partnership to make an investment in a company. Yeah. I mean, and then, as Sam mentioned great things about you and what you guys are building. So he gave me like a brief overview, but I wouldn't have to learn more. So we've been investing in heavily in AI. We're investors and companies like
5:18scale, hugging face and tropic and
scale, hugging face and tropic and
scale, hugging face and tropic and
scale, hugging face and tropic and
S Speaker 35:22we and lot of other companies that got acquired, like Octa ml, Omni ml and and then, more recently, we've done a few seed investments in companies like context and poky. And so these are more applications infrastructure. So most of these investments are either infrastructure, developer platforms or applications. So that's yeah,
we and lot of other companies that got acquired, like Octa ml, Omni ml and and then, more recently, we've done a few seed investments in companies like context and poky. And so these are more applications infrastructure. So most of these investments are either infrastructure, developer platforms or applications. So that's yeah,
we and lot of other companies that got acquired, like Octa ml, Omni ml and and then, more recently, we've done a few seed investments in companies like context and poky. And so these are more applications infrastructure. So most of these investments are either infrastructure, developer platforms or applications. So that's yeah,
we and lot of other companies that got acquired, like Octa ml, Omni ml and and then, more recently, we've done a few seed investments in companies like context and poky. And so these are more applications infrastructure. So most of these investments are either infrastructure, developer platforms or applications. So that's yeah,
5:55and with most of these, we end up working.
and with most of these, we end up working.
and with most of these, we end up working.
and with most of these, we end up working.
S Speaker 35:59So it's been, it's been great. I Priyesh, you want to give a quick introduction.
So it's been, it's been great. I Priyesh, you want to give a quick introduction.
So it's been, it's been great. I Priyesh, you want to give a quick introduction.
So it's been, it's been great. I Priyesh, you want to give a quick introduction.
S Speaker 16:03Absolutely, we were talking about it. I am an engineer by background. Myself. Spent early days working with a lot of startups, and then have been in venture for about three years. Spent my time with Menlo Ventures and Morpheus for some time. I'm currently a senior associate with Qualcomm.
Absolutely, we were talking about it. I am an engineer by background. Myself. Spent early days working with a lot of startups, and then have been in venture for about three years. Spent my time with Menlo Ventures and Morpheus for some time. I'm currently a senior associate with Qualcomm.
Absolutely, we were talking about it. I am an engineer by background. Myself. Spent early days working with a lot of startups, and then have been in venture for about three years. Spent my time with Menlo Ventures and Morpheus for some time. I'm currently a senior associate with Qualcomm.
Absolutely, we were talking about it. I am an engineer by background. Myself. Spent early days working with a lot of startups, and then have been in venture for about three years. Spent my time with Menlo Ventures and Morpheus for some time. I'm currently a senior associate with Qualcomm.
S Speaker 26:21Yeah. All right, happy to tell you a little bit about myself. Just, you know, I believe that Qualcomm actually invested in my last startup. Oh, my last startup that did, like quite a while ago now, was called jamdat mobile.
Yeah. All right, happy to tell you a little bit about myself. Just, you know, I believe that Qualcomm actually invested in my last startup. Oh, my last startup that did, like quite a while ago now, was called jamdat mobile.
Yeah. All right, happy to tell you a little bit about myself. Just, you know, I believe that Qualcomm actually invested in my last startup. Oh, my last startup that did, like quite a while ago now, was called jamdat mobile.
Yeah. All right, happy to tell you a little bit about myself. Just, you know, I believe that Qualcomm actually invested in my last startup. Oh, my last startup that did, like quite a while ago now, was called jamdat mobile.
S Speaker 36:38I think so this was before me, but I've seen jam dad on our
I think so this was before me, but I've seen jam dad on our
I think so this was before me, but I've seen jam dad on our
I think so this was before me, but I've seen jam dad on our
S Speaker 39:36that's awesome. That's great journey.
that's awesome. That's great journey.
that's awesome. That's great journey.
that's awesome. That's great journey.
S Speaker 29:39Yeah, no, it's been, it's been it's been fun, it's been good. And then I finally decided, you know, I was gonna retire again. But then this idea, we worked on some really hard problems, and we have a novel approach to I'll describe it to you. That's what I'll talk to you today about. I was like, Okay, I have to do this one last thing, and this could be the biggest thing ever, right? So, so we left, a group of three of us left, and that's how we started. Started this journey. It's
Yeah, no, it's been, it's been it's been fun, it's been good. And then I finally decided, you know, I was gonna retire again. But then this idea, we worked on some really hard problems, and we have a novel approach to I'll describe it to you. That's what I'll talk to you today about. I was like, Okay, I have to do this one last thing, and this could be the biggest thing ever, right? So, so we left, a group of three of us left, and that's how we started. Started this journey. It's
Yeah, no, it's been, it's been it's been fun, it's been good. And then I finally decided, you know, I was gonna retire again. But then this idea, we worked on some really hard problems, and we have a novel approach to I'll describe it to you. That's what I'll talk to you today about. I was like, Okay, I have to do this one last thing, and this could be the biggest thing ever, right? So, so we left, a group of three of us left, and that's how we started. Started this journey. It's
Yeah, no, it's been, it's been it's been fun, it's been good. And then I finally decided, you know, I was gonna retire again. But then this idea, we worked on some really hard problems, and we have a novel approach to I'll describe it to you. That's what I'll talk to you today about. I was like, Okay, I have to do this one last thing, and this could be the biggest thing ever, right? So, so we left, a group of three of us left, and that's how we started. Started this journey. It's
S Speaker 310:04great, yeah, three of you left from where? Which is the other two were,
great, yeah, three of you left from where? Which is the other two were,
great, yeah, three of you left from where? Which is the other two were,
great, yeah, three of you left from where? Which is the other two were,
S Speaker 210:12yep. So we're, we are. Three of us left. We're from Google Deep Mind, right? So this is, this is you guys can see this? Yes, yeah, OK, right. So it's Ian Fisher myself and somebody named David Marwood. We're all long time Googlers. We have about 51 years experience in Google, Deep Mind as it is between us, we have one IPO and two acquisitions, and we have all developed mass market products. So hundreds of millions of users have used our products that we ourselves developed this company was, you know, we started pitching it, I guess, five, five months ago now, we generally just closed our funding about a month ago. Yeah, we raised, we raised 15 million for our seed. Since then, we have hired four more people, and these are all people we've worked with. These are all kind of the hotshots through Google, DeepMind folks. And now we have a total of five total from DeepMind, one from Amazon, right? So it's very it's a complete tech team. That's what we do right now. That's all we need. So this is the team we have right now, the team of seven. We have two more scheduled to be hired, but we'll see when they're in the pipeline. So we'll see how that goes. All right, so, right. So let me actually tell you kind of how we started on this journey. This journey started. We were working on Gemini. Of course, we're at Google and what else. That's the only thing we're allowed to work on, basically, Gemini and those models, right? And what we noticed is that models can't solve, not only really hard problems, but they can't solve even really using problems which you think they should be able to solve. So let's talk about some natural problems to solve. Let's like, you could ask it to plan a trip for you. So the thing is, like, you know, we've all done chatgpt To get more and more complex problems, but let's say, from end to end, I want you to plan a trip for me. Like, I want you to plan a trip for me to five different cities, staying in five star or four star hotels. I want to eat at Japanese restaurants, and I want the whole thing to be budget limited, right? Ask it to solve this problem, and you'll find that majority of the time it's going to fail. And it's going to fail not not only because it's loosen its answers, like I think we have that that under control. What's really happening is it can't propagate these constraints. So like, if it finds a great place for you to stay, it'll be over budget. If it finds if it finds a good restaurant, it won't be the right one with an in budget. So keeping together all these constraints is actually a really hard problem that even the reasoning models can't solve. I'm working the very latest reasoning model, but this is just simple trip planning, right? Seems like it should be able to be solved, but it's worse than that. Like if I ask you to schedule a meeting with three different people, like, if all three of us wanted to have a meeting in San Francisco at some point, and give you the calendars and say, find an open time, including travel time, so we can meet somewhere, it'll fail most of the time. These are surprising results, considering how good it is at coding in math, but what I'll show you is that even the stuff that is really awesome at which is math, it'll fail to do seventh grade math, even though it's a linear level math. But if you ask the basic stuff like seventh grade math, these things often fail. And I'll show you how we can resolve this problem really, really cheaply, and whatever we consider are kind of magic.
yep. So we're, we are. Three of us left. We're from Google Deep Mind, right? So this is, this is you guys can see this? Yes, yeah, OK, right. So it's Ian Fisher myself and somebody named David Marwood. We're all long time Googlers. We have about 51 years experience in Google, Deep Mind as it is between us, we have one IPO and two acquisitions, and we have all developed mass market products. So hundreds of millions of users have used our products that we ourselves developed this company was, you know, we started pitching it, I guess, five, five months ago now, we generally just closed our funding about a month ago. Yeah, we raised, we raised 15 million for our seed. Since then, we have hired four more people, and these are all people we've worked with. These are all kind of the hotshots through Google, DeepMind folks. And now we have a total of five total from DeepMind, one from Amazon, right? So it's very it's a complete tech team. That's what we do right now. That's all we need. So this is the team we have right now, the team of seven. We have two more scheduled to be hired, but we'll see when they're in the pipeline. So we'll see how that goes. All right, so, right. So let me actually tell you kind of how we started on this journey. This journey started. We were working on Gemini. Of course, we're at Google and what else. That's the only thing we're allowed to work on, basically, Gemini and those models, right? And what we noticed is that models can't solve, not only really hard problems, but they can't solve even really using problems which you think they should be able to solve. So let's talk about some natural problems to solve. Let's like, you could ask it to plan a trip for you. So the thing is, like, you know, we've all done chatgpt To get more and more complex problems, but let's say, from end to end, I want you to plan a trip for me. Like, I want you to plan a trip for me to five different cities, staying in five star or four star hotels. I want to eat at Japanese restaurants, and I want the whole thing to be budget limited, right? Ask it to solve this problem, and you'll find that majority of the time it's going to fail. And it's going to fail not not only because it's loosen its answers, like I think we have that that under control. What's really happening is it can't propagate these constraints. So like, if it finds a great place for you to stay, it'll be over budget. If it finds if it finds a good restaurant, it won't be the right one with an in budget. So keeping together all these constraints is actually a really hard problem that even the reasoning models can't solve. I'm working the very latest reasoning model, but this is just simple trip planning, right? Seems like it should be able to be solved, but it's worse than that. Like if I ask you to schedule a meeting with three different people, like, if all three of us wanted to have a meeting in San Francisco at some point, and give you the calendars and say, find an open time, including travel time, so we can meet somewhere, it'll fail most of the time. These are surprising results, considering how good it is at coding in math, but what I'll show you is that even the stuff that is really awesome at which is math, it'll fail to do seventh grade math, even though it's a linear level math. But if you ask the basic stuff like seventh grade math, these things often fail. And I'll show you how we can resolve this problem really, really cheaply, and whatever we consider are kind of magic.
yep. So we're, we are. Three of us left. We're from Google Deep Mind, right? So this is, this is you guys can see this? Yes, yeah, OK, right. So it's Ian Fisher myself and somebody named David Marwood. We're all long time Googlers. We have about 51 years experience in Google, Deep Mind as it is between us, we have one IPO and two acquisitions, and we have all developed mass market products. So hundreds of millions of users have used our products that we ourselves developed this company was, you know, we started pitching it, I guess, five, five months ago now, we generally just closed our funding about a month ago. Yeah, we raised, we raised 15 million for our seed. Since then, we have hired four more people, and these are all people we've worked with. These are all kind of the hotshots through Google, DeepMind folks. And now we have a total of five total from DeepMind, one from Amazon, right? So it's very it's a complete tech team. That's what we do right now. That's all we need. So this is the team we have right now, the team of seven. We have two more scheduled to be hired, but we'll see when they're in the pipeline. So we'll see how that goes. All right, so, right. So let me actually tell you kind of how we started on this journey. This journey started. We were working on Gemini. Of course, we're at Google and what else. That's the only thing we're allowed to work on, basically, Gemini and those models, right? And what we noticed is that models can't solve, not only really hard problems, but they can't solve even really using problems which you think they should be able to solve. So let's talk about some natural problems to solve. Let's like, you could ask it to plan a trip for you. So the thing is, like, you know, we've all done chatgpt To get more and more complex problems, but let's say, from end to end, I want you to plan a trip for me. Like, I want you to plan a trip for me to five different cities, staying in five star or four star hotels. I want to eat at Japanese restaurants, and I want the whole thing to be budget limited, right? Ask it to solve this problem, and you'll find that majority of the time it's going to fail. And it's going to fail not not only because it's loosen its answers, like I think we have that that under control. What's really happening is it can't propagate these constraints. So like, if it finds a great place for you to stay, it'll be over budget. If it finds if it finds a good restaurant, it won't be the right one with an in budget. So keeping together all these constraints is actually a really hard problem that even the reasoning models can't solve. I'm working the very latest reasoning model, but this is just simple trip planning, right? Seems like it should be able to be solved, but it's worse than that. Like if I ask you to schedule a meeting with three different people, like, if all three of us wanted to have a meeting in San Francisco at some point, and give you the calendars and say, find an open time, including travel time, so we can meet somewhere, it'll fail most of the time. These are surprising results, considering how good it is at coding in math, but what I'll show you is that even the stuff that is really awesome at which is math, it'll fail to do seventh grade math, even though it's a linear level math. But if you ask the basic stuff like seventh grade math, these things often fail. And I'll show you how we can resolve this problem really, really cheaply, and whatever we consider are kind of magic.
yep. So we're, we are. Three of us left. We're from Google Deep Mind, right? So this is, this is you guys can see this? Yes, yeah, OK, right. So it's Ian Fisher myself and somebody named David Marwood. We're all long time Googlers. We have about 51 years experience in Google, Deep Mind as it is between us, we have one IPO and two acquisitions, and we have all developed mass market products. So hundreds of millions of users have used our products that we ourselves developed this company was, you know, we started pitching it, I guess, five, five months ago now, we generally just closed our funding about a month ago. Yeah, we raised, we raised 15 million for our seed. Since then, we have hired four more people, and these are all people we've worked with. These are all kind of the hotshots through Google, DeepMind folks. And now we have a total of five total from DeepMind, one from Amazon, right? So it's very it's a complete tech team. That's what we do right now. That's all we need. So this is the team we have right now, the team of seven. We have two more scheduled to be hired, but we'll see when they're in the pipeline. So we'll see how that goes. All right, so, right. So let me actually tell you kind of how we started on this journey. This journey started. We were working on Gemini. Of course, we're at Google and what else. That's the only thing we're allowed to work on, basically, Gemini and those models, right? And what we noticed is that models can't solve, not only really hard problems, but they can't solve even really using problems which you think they should be able to solve. So let's talk about some natural problems to solve. Let's like, you could ask it to plan a trip for you. So the thing is, like, you know, we've all done chatgpt To get more and more complex problems, but let's say, from end to end, I want you to plan a trip for me. Like, I want you to plan a trip for me to five different cities, staying in five star or four star hotels. I want to eat at Japanese restaurants, and I want the whole thing to be budget limited, right? Ask it to solve this problem, and you'll find that majority of the time it's going to fail. And it's going to fail not not only because it's loosen its answers, like I think we have that that under control. What's really happening is it can't propagate these constraints. So like, if it finds a great place for you to stay, it'll be over budget. If it finds if it finds a good restaurant, it won't be the right one with an in budget. So keeping together all these constraints is actually a really hard problem that even the reasoning models can't solve. I'm working the very latest reasoning model, but this is just simple trip planning, right? Seems like it should be able to be solved, but it's worse than that. Like if I ask you to schedule a meeting with three different people, like, if all three of us wanted to have a meeting in San Francisco at some point, and give you the calendars and say, find an open time, including travel time, so we can meet somewhere, it'll fail most of the time. These are surprising results, considering how good it is at coding in math, but what I'll show you is that even the stuff that is really awesome at which is math, it'll fail to do seventh grade math, even though it's a linear level math. But if you ask the basic stuff like seventh grade math, these things often fail. And I'll show you how we can resolve this problem really, really cheaply, and whatever we consider are kind of magic.
13:51So that's kind of where we are.
So that's kind of where we are.
So that's kind of where we are.
So that's kind of where we are.
13:52Why did it fail?
S Speaker 213:55It's the reasoning part, right? So you know these Okay, so I'll skip to perhaps the most controversial statement I'm going to make. The most controversial statement I'm going to make is, what are llms? Llms are basically very large databases. That's it, right? Let's, let's, let's not kid ourselves, right? They're huge databases which you can query in really awesome ways. You can query this database with natural language, and you can get an answer out, and it's encapsulated a lot of human knowledge. But in the end, they're just great databases, and if you try to put reasoning on top of them, that becomes really expensive. It's basically the way, the way, I like to say it is you're trying to put logic or a function inside an Oracle database. You can do it, but it's really hard. Rather, wouldn't you rather just have a programming language to write on the outside and just query the database to get the bits of information you need and not worry about writing it inside an Oracle database? That's the equivalent of what we're doing now. Like, I think all this work that's going into putting reasoning inside the models,
It's the reasoning part, right? So you know these Okay, so I'll skip to perhaps the most controversial statement I'm going to make. The most controversial statement I'm going to make is, what are llms? Llms are basically very large databases. That's it, right? Let's, let's, let's not kid ourselves, right? They're huge databases which you can query in really awesome ways. You can query this database with natural language, and you can get an answer out, and it's encapsulated a lot of human knowledge. But in the end, they're just great databases, and if you try to put reasoning on top of them, that becomes really expensive. It's basically the way, the way, I like to say it is you're trying to put logic or a function inside an Oracle database. You can do it, but it's really hard. Rather, wouldn't you rather just have a programming language to write on the outside and just query the database to get the bits of information you need and not worry about writing it inside an Oracle database? That's the equivalent of what we're doing now. Like, I think all this work that's going into putting reasoning inside the models,
It's the reasoning part, right? So you know these Okay, so I'll skip to perhaps the most controversial statement I'm going to make. The most controversial statement I'm going to make is, what are llms? Llms are basically very large databases. That's it, right? Let's, let's, let's not kid ourselves, right? They're huge databases which you can query in really awesome ways. You can query this database with natural language, and you can get an answer out, and it's encapsulated a lot of human knowledge. But in the end, they're just great databases, and if you try to put reasoning on top of them, that becomes really expensive. It's basically the way, the way, I like to say it is you're trying to put logic or a function inside an Oracle database. You can do it, but it's really hard. Rather, wouldn't you rather just have a programming language to write on the outside and just query the database to get the bits of information you need and not worry about writing it inside an Oracle database? That's the equivalent of what we're doing now. Like, I think all this work that's going into putting reasoning inside the models,
It's the reasoning part, right? So you know these Okay, so I'll skip to perhaps the most controversial statement I'm going to make. The most controversial statement I'm going to make is, what are llms? Llms are basically very large databases. That's it, right? Let's, let's, let's not kid ourselves, right? They're huge databases which you can query in really awesome ways. You can query this database with natural language, and you can get an answer out, and it's encapsulated a lot of human knowledge. But in the end, they're just great databases, and if you try to put reasoning on top of them, that becomes really expensive. It's basically the way, the way, I like to say it is you're trying to put logic or a function inside an Oracle database. You can do it, but it's really hard. Rather, wouldn't you rather just have a programming language to write on the outside and just query the database to get the bits of information you need and not worry about writing it inside an Oracle database? That's the equivalent of what we're doing now. Like, I think all this work that's going into putting reasoning inside the models,
15:07it's going to be very limited,
it's going to be very limited,
it's going to be very limited,
it's going to be very limited,
S Speaker 323:02Yeah, that makes so so
Yeah, that makes so so
Yeah, that makes so so
Yeah, that makes so so
23:07for most of like, for example,
for most of like, for example,
for most of like, for example,
for most of like, for example,
23:11you know, like open AI agents,
you know, like open AI agents,
you know, like open AI agents,
you know, like open AI agents,
23:14are they also using a similar approach? Or,
are they also using a similar approach? Or,
are they also using a similar approach? Or,
are they also using a similar approach? Or,
S Speaker 323:19because I see, I don't know what's happening, because I see active learning is in the sense that they're taking my query and they're converting it to code, yep, and then I don't know, of course, they make it very opaque as to what else they're doing, but, but are they also using a similar approach now? Okay, so, right, so
because I see, I don't know what's happening, because I see active learning is in the sense that they're taking my query and they're converting it to code, yep, and then I don't know, of course, they make it very opaque as to what else they're doing, but, but are they also using a similar approach now? Okay, so, right, so
because I see, I don't know what's happening, because I see active learning is in the sense that they're taking my query and they're converting it to code, yep, and then I don't know, of course, they make it very opaque as to what else they're doing, but, but are they also using a similar approach now? Okay, so, right, so
because I see, I don't know what's happening, because I see active learning is in the sense that they're taking my query and they're converting it to code, yep, and then I don't know, of course, they make it very opaque as to what else they're doing, but, but are they also using a similar approach now? Okay, so, right, so
23:39I mean, there. So
28:39which SSI reference over there.
which SSI reference over there.
which SSI reference over there.
which SSI reference over there.
28:41Of course, you have to have an SSI reference.
Of course, you have to have an SSI reference.
Of course, you have to have an SSI reference.
Of course, you have to have an SSI reference.
S Speaker 228:45I mean, I mean, there, you know, before Mira,
I mean, I mean, there, you know, before Mira,
I mean, I mean, there, you know, before Mira,
I mean, I mean, there, you know, before Mira,
28:49there was another slide before this.
there was another slide before this.
there was another slide before this.
there was another slide before this.
S Speaker 328:53Oh, we had a vote, right? Oh, the scorecard, yeah. We talked about, well, you don't know. Do you know what SSI is doing. Yeah, a little bit I mean we have we have people who
Oh, we had a vote, right? Oh, the scorecard, yeah. We talked about, well, you don't know. Do you know what SSI is doing. Yeah, a little bit I mean we have we have people who
Oh, we had a vote, right? Oh, the scorecard, yeah. We talked about, well, you don't know. Do you know what SSI is doing. Yeah, a little bit I mean we have we have people who
Oh, we had a vote, right? Oh, the scorecard, yeah. We talked about, well, you don't know. Do you know what SSI is doing. Yeah, a little bit I mean we have we have people who
S Speaker 229:07recruited there and such. So yeah, a little bit I mean, we could tell you that in broad in broad strokes right.
recruited there and such. So yeah, a little bit I mean, we could tell you that in broad in broad strokes right.
recruited there and such. So yeah, a little bit I mean, we could tell you that in broad in broad strokes right.
recruited there and such. So yeah, a little bit I mean, we could tell you that in broad in broad strokes right.
29:51In the back. It's, it's,
In the back. It's, it's,
In the back. It's, it's,
In the back. It's, it's,
S Speaker 229:57it's a bet that he made, right? I get it. It's fine, yeah. Who am I to say? Elon, very successful. So, yeah, yeah. Okay. So the way we're working with companies this is this, I won't spend too much time. I just want to quickly tell you that we basically look at you have these, these companies are trying to solve real tasks. And they basically have tried to use all these models out there so that our, our, our, like, ideal customer profile is someone who's been sitting there using CLI, like anthropic CLI in particular, they've been using it for a while. They're just really frustrated. So they have all their data collected. They come to us and say, Look, I can't get this to work. Here's my data. Please. Go figure out something. So we view ourselves as a middle layer between their problems that they want to solve, which they spread as just a few 100 to 1000 samples of data. We interact with any of the four big, major Chad models out there, and we translate their problems into things that they can solve, into things these things can solve by querying one or more of both models. And so in the end, what we'll get back to them. So our pricing, the way we're imagining it's going to go right now, is the pricing is going to be we'll solve this for you. It's so cheap for us to solve this, like every example of showing you, I told you, is under $100 I'm going to show you some more results that actually, when you tune to a new into a new domain, it cost us around $7 it's so cheap for us to train these models, to train our own agents, or model, whatever you want to call them, that would do that for you for free and only charge you when you use it. So for example, for this bio company, will train our model for free, and then once you start using it and you send us new new proteins, you want us to categorize that from the first charging, right? It's just cheap for us to do this
it's a bet that he made, right? I get it. It's fine, yeah. Who am I to say? Elon, very successful. So, yeah, yeah. Okay. So the way we're working with companies this is this, I won't spend too much time. I just want to quickly tell you that we basically look at you have these, these companies are trying to solve real tasks. And they basically have tried to use all these models out there so that our, our, our, like, ideal customer profile is someone who's been sitting there using CLI, like anthropic CLI in particular, they've been using it for a while. They're just really frustrated. So they have all their data collected. They come to us and say, Look, I can't get this to work. Here's my data. Please. Go figure out something. So we view ourselves as a middle layer between their problems that they want to solve, which they spread as just a few 100 to 1000 samples of data. We interact with any of the four big, major Chad models out there, and we translate their problems into things that they can solve, into things these things can solve by querying one or more of both models. And so in the end, what we'll get back to them. So our pricing, the way we're imagining it's going to go right now, is the pricing is going to be we'll solve this for you. It's so cheap for us to solve this, like every example of showing you, I told you, is under $100 I'm going to show you some more results that actually, when you tune to a new into a new domain, it cost us around $7 it's so cheap for us to train these models, to train our own agents, or model, whatever you want to call them, that would do that for you for free and only charge you when you use it. So for example, for this bio company, will train our model for free, and then once you start using it and you send us new new proteins, you want us to categorize that from the first charging, right? It's just cheap for us to do this
it's a bet that he made, right? I get it. It's fine, yeah. Who am I to say? Elon, very successful. So, yeah, yeah. Okay. So the way we're working with companies this is this, I won't spend too much time. I just want to quickly tell you that we basically look at you have these, these companies are trying to solve real tasks. And they basically have tried to use all these models out there so that our, our, our, like, ideal customer profile is someone who's been sitting there using CLI, like anthropic CLI in particular, they've been using it for a while. They're just really frustrated. So they have all their data collected. They come to us and say, Look, I can't get this to work. Here's my data. Please. Go figure out something. So we view ourselves as a middle layer between their problems that they want to solve, which they spread as just a few 100 to 1000 samples of data. We interact with any of the four big, major Chad models out there, and we translate their problems into things that they can solve, into things these things can solve by querying one or more of both models. And so in the end, what we'll get back to them. So our pricing, the way we're imagining it's going to go right now, is the pricing is going to be we'll solve this for you. It's so cheap for us to solve this, like every example of showing you, I told you, is under $100 I'm going to show you some more results that actually, when you tune to a new into a new domain, it cost us around $7 it's so cheap for us to train these models, to train our own agents, or model, whatever you want to call them, that would do that for you for free and only charge you when you use it. So for example, for this bio company, will train our model for free, and then once you start using it and you send us new new proteins, you want us to categorize that from the first charging, right? It's just cheap for us to do this
it's a bet that he made, right? I get it. It's fine, yeah. Who am I to say? Elon, very successful. So, yeah, yeah. Okay. So the way we're working with companies this is this, I won't spend too much time. I just want to quickly tell you that we basically look at you have these, these companies are trying to solve real tasks. And they basically have tried to use all these models out there so that our, our, our, like, ideal customer profile is someone who's been sitting there using CLI, like anthropic CLI in particular, they've been using it for a while. They're just really frustrated. So they have all their data collected. They come to us and say, Look, I can't get this to work. Here's my data. Please. Go figure out something. So we view ourselves as a middle layer between their problems that they want to solve, which they spread as just a few 100 to 1000 samples of data. We interact with any of the four big, major Chad models out there, and we translate their problems into things that they can solve, into things these things can solve by querying one or more of both models. And so in the end, what we'll get back to them. So our pricing, the way we're imagining it's going to go right now, is the pricing is going to be we'll solve this for you. It's so cheap for us to solve this, like every example of showing you, I told you, is under $100 I'm going to show you some more results that actually, when you tune to a new into a new domain, it cost us around $7 it's so cheap for us to train these models, to train our own agents, or model, whatever you want to call them, that would do that for you for free and only charge you when you use it. So for example, for this bio company, will train our model for free, and then once you start using it and you send us new new proteins, you want us to categorize that from the first charging, right? It's just cheap for us to do this
S Speaker 331:57so and then. But are you training these for any vertical, like, give to us whatever you got, or are you
so and then. But are you training these for any vertical, like, give to us whatever you got, or are you
so and then. But are you training these for any vertical, like, give to us whatever you got, or are you
so and then. But are you training these for any vertical, like, give to us whatever you got, or are you
S Speaker 232:10right? So let me, I don't want to overstate this is you're actually talking to someone who's been funded for almost three and a half weeks. So I'm talking about what we want to do and versus what we are doing. So right now, right we have one customer in the coding domain, which we're working with what we want to do. So let me actually quickly tell you what we want to do. So this is so what we what we fully imagine, and what is going to set us apart from anybody else out there, including all the big players, is how we are going to build our moat, right? So what is our moat? Exactly? So our moat, and the reason we got so excited about this is that we actually think that what we've come up with is is a path, maybe not the only path, but a path to AGI. And that's what intuitive in a very academic fashion, but we're going to get there in a very, in a very business oriented fashion. So, so here's what we're going to do whenever somebody comes to us with with a vertical right. So that I mentioned, I mentioned the medical side, the chemistry side, whatever. There's also programming side, and there's also the, you know, another one, another one, we're working with a legal side. When they come to us with a task, what we'll basically do is try to solve their task, of course. So let me just quickly walk you through how we act, a little bit more detail, how we solve this task. So imagine someone came to us with a math task that we wanted to solve, right? So this is a very easy because it's really easy surprising. So let's say that someone wants to solve seventh grade math problems. You're like, oh, this is already solved, right? This should not be a problem. Well, it turns out that even if we gave it to open AI code three mini reasoning model, this is a reasoning model. It can solve Olympiad level math, but if we give it seventh grade math, it actually starts failing, and the reason is because it's trained with reinforcement learning. So it's trained to do really hard math problems, but if you give it a long seventh grade math problem, it'll actually fail and get it wrong, which is bizarre and interesting, but it's just for that, just interesting fact. So how do we solve this problem? Imagine someone came, okay, here's 100 problems I want you to solve. So this is potential customer. What we're going to do is, basically, we're going to, instead of trying to solve that problem directly, we're going to have a two phase procedure. And this is, this is the core tool our model. So we're going to create what we call a professor. This professor is going to spawn off dozens of teachers which basically teach the LLM, or basically tell the LLM to write code for how to solve this problem. So there are a bunch of teachers all out there trying to write code for the active learning method I just described. They're all writing code, etc, etc, and the professor is just watching them. And these teachers are all different because they're initialized slightly differently. They try different learning strategies. They could say, one could be like, Oh, pay attention to order precedence. What's like, oh, you know, overall, double and triple check each sub module, whatever. It doesn't matter what they come up with. But they're all different learning strategies. This professor, all it's doing is watching how this how these teachers work, and the ones that do better, we can solve more problems. It pays more attention to modifies them, keeps them around. Ones that do poorly, just throws them away, right? So, very evolutionary, Darwinistic type thing. So like, what we're we're just so somebody. So this is person came to us. We let our system run for two hours. Okay, just automatically running for two hours doing this procedure. Sorry. We beat open IO three mini in two and a half hours on solving seventh grade math problems. The total cost to us to run this was $4.50 because all we're doing is we're just querying the model, right? You're saying, oh, you know, we can send 1000s of queries to the model, as long as they are just prompts and getting the answer back. It's so cheap, right? So we just queried the model 1000s of times, saying, improve this teacher. Improve this teacher. Let our system learn how to improve this teacher asked another question. Improve this teacher again, etc, etc. Two and a half hours, we let this model run, and we beat OpenAI with too many on then on this thing that may be, I matter of fact, like we use the same thing our prompts that we came up with, or code and prompts and data we came up with also be for humidity. But in the meantime, we also be jumping. We also need all the models out there, one
right? So let me, I don't want to overstate this is you're actually talking to someone who's been funded for almost three and a half weeks. So I'm talking about what we want to do and versus what we are doing. So right now, right we have one customer in the coding domain, which we're working with what we want to do. So let me actually quickly tell you what we want to do. So this is so what we what we fully imagine, and what is going to set us apart from anybody else out there, including all the big players, is how we are going to build our moat, right? So what is our moat? Exactly? So our moat, and the reason we got so excited about this is that we actually think that what we've come up with is is a path, maybe not the only path, but a path to AGI. And that's what intuitive in a very academic fashion, but we're going to get there in a very, in a very business oriented fashion. So, so here's what we're going to do whenever somebody comes to us with with a vertical right. So that I mentioned, I mentioned the medical side, the chemistry side, whatever. There's also programming side, and there's also the, you know, another one, another one, we're working with a legal side. When they come to us with a task, what we'll basically do is try to solve their task, of course. So let me just quickly walk you through how we act, a little bit more detail, how we solve this task. So imagine someone came to us with a math task that we wanted to solve, right? So this is a very easy because it's really easy surprising. So let's say that someone wants to solve seventh grade math problems. You're like, oh, this is already solved, right? This should not be a problem. Well, it turns out that even if we gave it to open AI code three mini reasoning model, this is a reasoning model. It can solve Olympiad level math, but if we give it seventh grade math, it actually starts failing, and the reason is because it's trained with reinforcement learning. So it's trained to do really hard math problems, but if you give it a long seventh grade math problem, it'll actually fail and get it wrong, which is bizarre and interesting, but it's just for that, just interesting fact. So how do we solve this problem? Imagine someone came, okay, here's 100 problems I want you to solve. So this is potential customer. What we're going to do is, basically, we're going to, instead of trying to solve that problem directly, we're going to have a two phase procedure. And this is, this is the core tool our model. So we're going to create what we call a professor. This professor is going to spawn off dozens of teachers which basically teach the LLM, or basically tell the LLM to write code for how to solve this problem. So there are a bunch of teachers all out there trying to write code for the active learning method I just described. They're all writing code, etc, etc, and the professor is just watching them. And these teachers are all different because they're initialized slightly differently. They try different learning strategies. They could say, one could be like, Oh, pay attention to order precedence. What's like, oh, you know, overall, double and triple check each sub module, whatever. It doesn't matter what they come up with. But they're all different learning strategies. This professor, all it's doing is watching how this how these teachers work, and the ones that do better, we can solve more problems. It pays more attention to modifies them, keeps them around. Ones that do poorly, just throws them away, right? So, very evolutionary, Darwinistic type thing. So like, what we're we're just so somebody. So this is person came to us. We let our system run for two hours. Okay, just automatically running for two hours doing this procedure. Sorry. We beat open IO three mini in two and a half hours on solving seventh grade math problems. The total cost to us to run this was $4.50 because all we're doing is we're just querying the model, right? You're saying, oh, you know, we can send 1000s of queries to the model, as long as they are just prompts and getting the answer back. It's so cheap, right? So we just queried the model 1000s of times, saying, improve this teacher. Improve this teacher. Let our system learn how to improve this teacher asked another question. Improve this teacher again, etc, etc. Two and a half hours, we let this model run, and we beat OpenAI with too many on then on this thing that may be, I matter of fact, like we use the same thing our prompts that we came up with, or code and prompts and data we came up with also be for humidity. But in the meantime, we also be jumping. We also need all the models out there, one
right? So let me, I don't want to overstate this is you're actually talking to someone who's been funded for almost three and a half weeks. So I'm talking about what we want to do and versus what we are doing. So right now, right we have one customer in the coding domain, which we're working with what we want to do. So let me actually quickly tell you what we want to do. So this is so what we what we fully imagine, and what is going to set us apart from anybody else out there, including all the big players, is how we are going to build our moat, right? So what is our moat? Exactly? So our moat, and the reason we got so excited about this is that we actually think that what we've come up with is is a path, maybe not the only path, but a path to AGI. And that's what intuitive in a very academic fashion, but we're going to get there in a very, in a very business oriented fashion. So, so here's what we're going to do whenever somebody comes to us with with a vertical right. So that I mentioned, I mentioned the medical side, the chemistry side, whatever. There's also programming side, and there's also the, you know, another one, another one, we're working with a legal side. When they come to us with a task, what we'll basically do is try to solve their task, of course. So let me just quickly walk you through how we act, a little bit more detail, how we solve this task. So imagine someone came to us with a math task that we wanted to solve, right? So this is a very easy because it's really easy surprising. So let's say that someone wants to solve seventh grade math problems. You're like, oh, this is already solved, right? This should not be a problem. Well, it turns out that even if we gave it to open AI code three mini reasoning model, this is a reasoning model. It can solve Olympiad level math, but if we give it seventh grade math, it actually starts failing, and the reason is because it's trained with reinforcement learning. So it's trained to do really hard math problems, but if you give it a long seventh grade math problem, it'll actually fail and get it wrong, which is bizarre and interesting, but it's just for that, just interesting fact. So how do we solve this problem? Imagine someone came, okay, here's 100 problems I want you to solve. So this is potential customer. What we're going to do is, basically, we're going to, instead of trying to solve that problem directly, we're going to have a two phase procedure. And this is, this is the core tool our model. So we're going to create what we call a professor. This professor is going to spawn off dozens of teachers which basically teach the LLM, or basically tell the LLM to write code for how to solve this problem. So there are a bunch of teachers all out there trying to write code for the active learning method I just described. They're all writing code, etc, etc, and the professor is just watching them. And these teachers are all different because they're initialized slightly differently. They try different learning strategies. They could say, one could be like, Oh, pay attention to order precedence. What's like, oh, you know, overall, double and triple check each sub module, whatever. It doesn't matter what they come up with. But they're all different learning strategies. This professor, all it's doing is watching how this how these teachers work, and the ones that do better, we can solve more problems. It pays more attention to modifies them, keeps them around. Ones that do poorly, just throws them away, right? So, very evolutionary, Darwinistic type thing. So like, what we're we're just so somebody. So this is person came to us. We let our system run for two hours. Okay, just automatically running for two hours doing this procedure. Sorry. We beat open IO three mini in two and a half hours on solving seventh grade math problems. The total cost to us to run this was $4.50 because all we're doing is we're just querying the model, right? You're saying, oh, you know, we can send 1000s of queries to the model, as long as they are just prompts and getting the answer back. It's so cheap, right? So we just queried the model 1000s of times, saying, improve this teacher. Improve this teacher. Let our system learn how to improve this teacher asked another question. Improve this teacher again, etc, etc. Two and a half hours, we let this model run, and we beat OpenAI with too many on then on this thing that may be, I matter of fact, like we use the same thing our prompts that we came up with, or code and prompts and data we came up with also be for humidity. But in the meantime, we also be jumping. We also need all the models out there, one
right? So let me, I don't want to overstate this is you're actually talking to someone who's been funded for almost three and a half weeks. So I'm talking about what we want to do and versus what we are doing. So right now, right we have one customer in the coding domain, which we're working with what we want to do. So let me actually quickly tell you what we want to do. So this is so what we what we fully imagine, and what is going to set us apart from anybody else out there, including all the big players, is how we are going to build our moat, right? So what is our moat? Exactly? So our moat, and the reason we got so excited about this is that we actually think that what we've come up with is is a path, maybe not the only path, but a path to AGI. And that's what intuitive in a very academic fashion, but we're going to get there in a very, in a very business oriented fashion. So, so here's what we're going to do whenever somebody comes to us with with a vertical right. So that I mentioned, I mentioned the medical side, the chemistry side, whatever. There's also programming side, and there's also the, you know, another one, another one, we're working with a legal side. When they come to us with a task, what we'll basically do is try to solve their task, of course. So let me just quickly walk you through how we act, a little bit more detail, how we solve this task. So imagine someone came to us with a math task that we wanted to solve, right? So this is a very easy because it's really easy surprising. So let's say that someone wants to solve seventh grade math problems. You're like, oh, this is already solved, right? This should not be a problem. Well, it turns out that even if we gave it to open AI code three mini reasoning model, this is a reasoning model. It can solve Olympiad level math, but if we give it seventh grade math, it actually starts failing, and the reason is because it's trained with reinforcement learning. So it's trained to do really hard math problems, but if you give it a long seventh grade math problem, it'll actually fail and get it wrong, which is bizarre and interesting, but it's just for that, just interesting fact. So how do we solve this problem? Imagine someone came, okay, here's 100 problems I want you to solve. So this is potential customer. What we're going to do is, basically, we're going to, instead of trying to solve that problem directly, we're going to have a two phase procedure. And this is, this is the core tool our model. So we're going to create what we call a professor. This professor is going to spawn off dozens of teachers which basically teach the LLM, or basically tell the LLM to write code for how to solve this problem. So there are a bunch of teachers all out there trying to write code for the active learning method I just described. They're all writing code, etc, etc, and the professor is just watching them. And these teachers are all different because they're initialized slightly differently. They try different learning strategies. They could say, one could be like, Oh, pay attention to order precedence. What's like, oh, you know, overall, double and triple check each sub module, whatever. It doesn't matter what they come up with. But they're all different learning strategies. This professor, all it's doing is watching how this how these teachers work, and the ones that do better, we can solve more problems. It pays more attention to modifies them, keeps them around. Ones that do poorly, just throws them away, right? So, very evolutionary, Darwinistic type thing. So like, what we're we're just so somebody. So this is person came to us. We let our system run for two hours. Okay, just automatically running for two hours doing this procedure. Sorry. We beat open IO three mini in two and a half hours on solving seventh grade math problems. The total cost to us to run this was $4.50 because all we're doing is we're just querying the model, right? You're saying, oh, you know, we can send 1000s of queries to the model, as long as they are just prompts and getting the answer back. It's so cheap, right? So we just queried the model 1000s of times, saying, improve this teacher. Improve this teacher. Let our system learn how to improve this teacher asked another question. Improve this teacher again, etc, etc. Two and a half hours, we let this model run, and we beat OpenAI with too many on then on this thing that may be, I matter of fact, like we use the same thing our prompts that we came up with, or code and prompts and data we came up with also be for humidity. But in the meantime, we also be jumping. We also need all the models out there, one
S Speaker 336:39So then, when you do this, is there like in terms of latency, in terms of latency of solving the problem? Does it take longer than I mean by definition, it should, right?
So then, when you do this, is there like in terms of latency, in terms of latency of solving the problem? Does it take longer than I mean by definition, it should, right?
So then, when you do this, is there like in terms of latency, in terms of latency of solving the problem? Does it take longer than I mean by definition, it should, right?
So then, when you do this, is there like in terms of latency, in terms of latency of solving the problem? Does it take longer than I mean by definition, it should, right?
S Speaker 236:52Well, it depends. So in this case, what? So obviously, we spent two and a half hours trying to solve this problem, right? So imagine, come to us, we go up our training procedure. In this case, it found out it didn't even need to write code to solve this problem. In this case, it was like, forget all the code I wrote. Just change the prompt to add these extra lines in it right to say what you should pay attention to and your rules of precedent. So it just added all that to the problem, and that was it. And then I said, now you can solve these things, right? So it gave kind of hints inside the prompt, but didn't even need to use the code. So in this case, it was, it was it was a, it was a degenerate case, but exactly we wanted to see. So now, anytime, anytime with that supposed customer came to us, we just use this prompt with an agent we have running our servers geared to them, so they basically would get, like, your company name slash wedding slash your company name, upload your thing, and we would run that prompt to solve this problem. Target a lot, right? And in that case, it was just a simple, a simple prompt change, right? So that actually was a case that was a prompt authorization. But what I want to get to is really cool. So what's really neat is remember, as all this time creating a professor agent that's watching these teachers learn. Well, what is this professor? This professor is now are involved, because this professor now has watched these teachers solve seventh grade math problems. So then we change the game. Okay? Now a new customer comes to us and wants to solve 10th grade math problems, much harder problems. They involve recursion. They have like function calling there. It's much, much harder, but it knew where to start. So now we started with the with the professor that had solved seventh grade problems. And we gave these problems again, OpenAI and Gemini, and in two and a half minutes, this professor was able to create a teacher that can beat all the state of the art models and solve these 10th grade math problems better than any state of the art models. And it did it in two and a half minutes, which is all of 50 cents worth of competition.
Well, it depends. So in this case, what? So obviously, we spent two and a half hours trying to solve this problem, right? So imagine, come to us, we go up our training procedure. In this case, it found out it didn't even need to write code to solve this problem. In this case, it was like, forget all the code I wrote. Just change the prompt to add these extra lines in it right to say what you should pay attention to and your rules of precedent. So it just added all that to the problem, and that was it. And then I said, now you can solve these things, right? So it gave kind of hints inside the prompt, but didn't even need to use the code. So in this case, it was, it was it was a, it was a degenerate case, but exactly we wanted to see. So now, anytime, anytime with that supposed customer came to us, we just use this prompt with an agent we have running our servers geared to them, so they basically would get, like, your company name slash wedding slash your company name, upload your thing, and we would run that prompt to solve this problem. Target a lot, right? And in that case, it was just a simple, a simple prompt change, right? So that actually was a case that was a prompt authorization. But what I want to get to is really cool. So what's really neat is remember, as all this time creating a professor agent that's watching these teachers learn. Well, what is this professor? This professor is now are involved, because this professor now has watched these teachers solve seventh grade math problems. So then we change the game. Okay? Now a new customer comes to us and wants to solve 10th grade math problems, much harder problems. They involve recursion. They have like function calling there. It's much, much harder, but it knew where to start. So now we started with the with the professor that had solved seventh grade problems. And we gave these problems again, OpenAI and Gemini, and in two and a half minutes, this professor was able to create a teacher that can beat all the state of the art models and solve these 10th grade math problems better than any state of the art models. And it did it in two and a half minutes, which is all of 50 cents worth of competition.
Well, it depends. So in this case, what? So obviously, we spent two and a half hours trying to solve this problem, right? So imagine, come to us, we go up our training procedure. In this case, it found out it didn't even need to write code to solve this problem. In this case, it was like, forget all the code I wrote. Just change the prompt to add these extra lines in it right to say what you should pay attention to and your rules of precedent. So it just added all that to the problem, and that was it. And then I said, now you can solve these things, right? So it gave kind of hints inside the prompt, but didn't even need to use the code. So in this case, it was, it was it was a, it was a degenerate case, but exactly we wanted to see. So now, anytime, anytime with that supposed customer came to us, we just use this prompt with an agent we have running our servers geared to them, so they basically would get, like, your company name slash wedding slash your company name, upload your thing, and we would run that prompt to solve this problem. Target a lot, right? And in that case, it was just a simple, a simple prompt change, right? So that actually was a case that was a prompt authorization. But what I want to get to is really cool. So what's really neat is remember, as all this time creating a professor agent that's watching these teachers learn. Well, what is this professor? This professor is now are involved, because this professor now has watched these teachers solve seventh grade math problems. So then we change the game. Okay? Now a new customer comes to us and wants to solve 10th grade math problems, much harder problems. They involve recursion. They have like function calling there. It's much, much harder, but it knew where to start. So now we started with the with the professor that had solved seventh grade problems. And we gave these problems again, OpenAI and Gemini, and in two and a half minutes, this professor was able to create a teacher that can beat all the state of the art models and solve these 10th grade math problems better than any state of the art models. And it did it in two and a half minutes, which is all of 50 cents worth of competition.
Well, it depends. So in this case, what? So obviously, we spent two and a half hours trying to solve this problem, right? So imagine, come to us, we go up our training procedure. In this case, it found out it didn't even need to write code to solve this problem. In this case, it was like, forget all the code I wrote. Just change the prompt to add these extra lines in it right to say what you should pay attention to and your rules of precedent. So it just added all that to the problem, and that was it. And then I said, now you can solve these things, right? So it gave kind of hints inside the prompt, but didn't even need to use the code. So in this case, it was, it was it was a, it was a degenerate case, but exactly we wanted to see. So now, anytime, anytime with that supposed customer came to us, we just use this prompt with an agent we have running our servers geared to them, so they basically would get, like, your company name slash wedding slash your company name, upload your thing, and we would run that prompt to solve this problem. Target a lot, right? And in that case, it was just a simple, a simple prompt change, right? So that actually was a case that was a prompt authorization. But what I want to get to is really cool. So what's really neat is remember, as all this time creating a professor agent that's watching these teachers learn. Well, what is this professor? This professor is now are involved, because this professor now has watched these teachers solve seventh grade math problems. So then we change the game. Okay? Now a new customer comes to us and wants to solve 10th grade math problems, much harder problems. They involve recursion. They have like function calling there. It's much, much harder, but it knew where to start. So now we started with the with the professor that had solved seventh grade problems. And we gave these problems again, OpenAI and Gemini, and in two and a half minutes, this professor was able to create a teacher that can beat all the state of the art models and solve these 10th grade math problems better than any state of the art models. And it did it in two and a half minutes, which is all of 50 cents worth of competition.
S Speaker 338:55And what about like you're comparing it with all three mini, what about like, if you were to compare with all three Pro? I mean, so the only
And what about like you're comparing it with all three mini, what about like, if you were to compare with all three Pro? I mean, so the only
And what about like you're comparing it with all three mini, what about like, if you were to compare with all three Pro? I mean, so the only
And what about like you're comparing it with all three mini, what about like, if you were to compare with all three Pro? I mean, so the only
S Speaker 239:05reason we did that it wasn't that public, wasn't, wasn't available yet, but, but we this is compared against all the latest Gemini. I mean, so since then, I'll be honest, we haven't gone back and looked at this, because the results were so, so much better, right? I presume they're all going to catch up, right? But the interesting thing to us is, is that we now have a professor that is very good at math, high school math, right? So in the same sense, when we solve the task of this bio company, and next time somebody comes to us with a protein problem or anything related, we'll say, start with that professor. So that's the whole wall I'm trying to get to. I hope these models are
reason we did that it wasn't that public, wasn't, wasn't available yet, but, but we this is compared against all the latest Gemini. I mean, so since then, I'll be honest, we haven't gone back and looked at this, because the results were so, so much better, right? I presume they're all going to catch up, right? But the interesting thing to us is, is that we now have a professor that is very good at math, high school math, right? So in the same sense, when we solve the task of this bio company, and next time somebody comes to us with a protein problem or anything related, we'll say, start with that professor. So that's the whole wall I'm trying to get to. I hope these models are
reason we did that it wasn't that public, wasn't, wasn't available yet, but, but we this is compared against all the latest Gemini. I mean, so since then, I'll be honest, we haven't gone back and looked at this, because the results were so, so much better, right? I presume they're all going to catch up, right? But the interesting thing to us is, is that we now have a professor that is very good at math, high school math, right? So in the same sense, when we solve the task of this bio company, and next time somebody comes to us with a protein problem or anything related, we'll say, start with that professor. So that's the whole wall I'm trying to get to. I hope these models are
reason we did that it wasn't that public, wasn't, wasn't available yet, but, but we this is compared against all the latest Gemini. I mean, so since then, I'll be honest, we haven't gone back and looked at this, because the results were so, so much better, right? I presume they're all going to catch up, right? But the interesting thing to us is, is that we now have a professor that is very good at math, high school math, right? So in the same sense, when we solve the task of this bio company, and next time somebody comes to us with a protein problem or anything related, we'll say, start with that professor. So that's the whole wall I'm trying to get to. I hope these models are
S Speaker 339:53gonna be and then would you publish benchmarks? Because you know that, like, if you can do out benchmark these
gonna be and then would you publish benchmarks? Because you know that, like, if you can do out benchmark these
gonna be and then would you publish benchmarks? Because you know that, like, if you can do out benchmark these
gonna be and then would you publish benchmarks? Because you know that, like, if you can do out benchmark these
S Speaker 240:05Indeed, indeed, we will publish them. But not we didn't do it yet. So we were actually going out for a much bigger raise. But then we started getting crazy good results on coding benchmarks, which everyone cared about. And so we decided to actually do a
Indeed, indeed, we will publish them. But not we didn't do it yet. So we were actually going out for a much bigger raise. But then we started getting crazy good results on coding benchmarks, which everyone cared about. And so we decided to actually do a
Indeed, indeed, we will publish them. But not we didn't do it yet. So we were actually going out for a much bigger raise. But then we started getting crazy good results on coding benchmarks, which everyone cared about. And so we decided to actually do a
Indeed, indeed, we will publish them. But not we didn't do it yet. So we were actually going out for a much bigger raise. But then we started getting crazy good results on coding benchmarks, which everyone cared about. And so we decided to actually do a
40:23smaller swing benchmark, or
smaller swing benchmark, or
smaller swing benchmark, or
smaller swing benchmark, or
S Speaker 240:25swing we've actually tried swing has a bunch of problems that are not related to reasoning at all, right? And the problem with Swing is all the models have trained on so you guys will get this basically, imagine I use one instead of a for or it will count it wrong, right? It's not exactly that if you use one function calls worse than other function calls that has the same functionality, it counts it wrong. So any so there is not a reasoning issue, it's have, have these models been trained on suite like data? So that one is completely unfair. So what we're actually trying to do is look at pure reasoning coding benchmark. So there are two that are out there. Two is more than two, but there's two that we looked at which are just pure reasoning. That's like, How deep can you reasoning to solve these harder coding benchmarks? We got great results on them, and which are those? We're not We're not releasing that yet. I apologize. Oh, the benchmarks, the names of the benchmarks, competing for them. Yeah. So the reason we're not releasing it yet is we, we plan to go out for a second, raise, very, very, you know, within the six time frame. So we will release it right around that time, right? Yeah, yeah. But these are, these are standard benchmarks. We beat them across all the leaderboards. These are in the last time they were operate updated was like beginning of 1025, so, you know, things change fast. I will give you that. But at least our models, which we train for, you know, literally, as I said, on the order of under $100 we're able to beat state of the art benchmarks, state of the art models across all both these benchmarks and all of them. So we were really jazzed about those results, right? So we plan to release these results as we get closer to their next funding graph.
swing we've actually tried swing has a bunch of problems that are not related to reasoning at all, right? And the problem with Swing is all the models have trained on so you guys will get this basically, imagine I use one instead of a for or it will count it wrong, right? It's not exactly that if you use one function calls worse than other function calls that has the same functionality, it counts it wrong. So any so there is not a reasoning issue, it's have, have these models been trained on suite like data? So that one is completely unfair. So what we're actually trying to do is look at pure reasoning coding benchmark. So there are two that are out there. Two is more than two, but there's two that we looked at which are just pure reasoning. That's like, How deep can you reasoning to solve these harder coding benchmarks? We got great results on them, and which are those? We're not We're not releasing that yet. I apologize. Oh, the benchmarks, the names of the benchmarks, competing for them. Yeah. So the reason we're not releasing it yet is we, we plan to go out for a second, raise, very, very, you know, within the six time frame. So we will release it right around that time, right? Yeah, yeah. But these are, these are standard benchmarks. We beat them across all the leaderboards. These are in the last time they were operate updated was like beginning of 1025, so, you know, things change fast. I will give you that. But at least our models, which we train for, you know, literally, as I said, on the order of under $100 we're able to beat state of the art benchmarks, state of the art models across all both these benchmarks and all of them. So we were really jazzed about those results, right? So we plan to release these results as we get closer to their next funding graph.
swing we've actually tried swing has a bunch of problems that are not related to reasoning at all, right? And the problem with Swing is all the models have trained on so you guys will get this basically, imagine I use one instead of a for or it will count it wrong, right? It's not exactly that if you use one function calls worse than other function calls that has the same functionality, it counts it wrong. So any so there is not a reasoning issue, it's have, have these models been trained on suite like data? So that one is completely unfair. So what we're actually trying to do is look at pure reasoning coding benchmark. So there are two that are out there. Two is more than two, but there's two that we looked at which are just pure reasoning. That's like, How deep can you reasoning to solve these harder coding benchmarks? We got great results on them, and which are those? We're not We're not releasing that yet. I apologize. Oh, the benchmarks, the names of the benchmarks, competing for them. Yeah. So the reason we're not releasing it yet is we, we plan to go out for a second, raise, very, very, you know, within the six time frame. So we will release it right around that time, right? Yeah, yeah. But these are, these are standard benchmarks. We beat them across all the leaderboards. These are in the last time they were operate updated was like beginning of 1025, so, you know, things change fast. I will give you that. But at least our models, which we train for, you know, literally, as I said, on the order of under $100 we're able to beat state of the art benchmarks, state of the art models across all both these benchmarks and all of them. So we were really jazzed about those results, right? So we plan to release these results as we get closer to their next funding graph.
swing we've actually tried swing has a bunch of problems that are not related to reasoning at all, right? And the problem with Swing is all the models have trained on so you guys will get this basically, imagine I use one instead of a for or it will count it wrong, right? It's not exactly that if you use one function calls worse than other function calls that has the same functionality, it counts it wrong. So any so there is not a reasoning issue, it's have, have these models been trained on suite like data? So that one is completely unfair. So what we're actually trying to do is look at pure reasoning coding benchmark. So there are two that are out there. Two is more than two, but there's two that we looked at which are just pure reasoning. That's like, How deep can you reasoning to solve these harder coding benchmarks? We got great results on them, and which are those? We're not We're not releasing that yet. I apologize. Oh, the benchmarks, the names of the benchmarks, competing for them. Yeah. So the reason we're not releasing it yet is we, we plan to go out for a second, raise, very, very, you know, within the six time frame. So we will release it right around that time, right? Yeah, yeah. But these are, these are standard benchmarks. We beat them across all the leaderboards. These are in the last time they were operate updated was like beginning of 1025, so, you know, things change fast. I will give you that. But at least our models, which we train for, you know, literally, as I said, on the order of under $100 we're able to beat state of the art benchmarks, state of the art models across all both these benchmarks and all of them. So we were really jazzed about those results, right? So we plan to release these results as we get closer to their next funding graph.
S Speaker 342:23Are you publishing? Is your team publishing research papers? No,
Are you publishing? Is your team publishing research papers? No,
Are you publishing? Is your team publishing research papers? No,
Are you publishing? Is your team publishing research papers? No,
S Speaker 242:28zero, right? That's that's not our goal, right? Like we've done, everybody here has done kind of the academic publishing route at Google. And a condition for joining on was, this is completely
zero, right? That's that's not our goal, right? Like we've done, everybody here has done kind of the academic publishing route at Google. And a condition for joining on was, this is completely
zero, right? That's that's not our goal, right? Like we've done, everybody here has done kind of the academic publishing route at Google. And a condition for joining on was, this is completely
zero, right? That's that's not our goal, right? Like we've done, everybody here has done kind of the academic publishing route at Google. And a condition for joining on was, this is completely
S Speaker 342:39commercial. Okay, so then, like, what's your go to market now,
commercial. Okay, so then, like, what's your go to market now,
commercial. Okay, so then, like, what's your go to market now,
commercial. Okay, so then, like, what's your go to market now,
S Speaker 343:33yeah. Oh, how are they like, are they still when they raised the timing, then they've gone quiet for Yeah,
yeah. Oh, how are they like, are they still when they raised the timing, then they've gone quiet for Yeah,
yeah. Oh, how are they like, are they still when they raised the timing, then they've gone quiet for Yeah,
yeah. Oh, how are they like, are they still when they raised the timing, then they've gone quiet for Yeah,
S Speaker 243:41they're executing along, right? I mean, they raised like, 200 million or something, yeah,
they're executing along, right? I mean, they raised like, 200 million or something, yeah,
they're executing along, right? I mean, they raised like, 200 million or something, yeah,
they're executing along, right? I mean, they raised like, 200 million or something, yeah,
S Speaker 343:45they were like, yeah. They were also building a coding,
they were like, yeah. They were also building a coding,
they were like, yeah. They were also building a coding,
they were like, yeah. They were also building a coding,
S Speaker 345:28guess what, one to two months you start taking them on, or, I mean, I
guess what, one to two months you start taking them on, or, I mean, I
guess what, one to two months you start taking them on, or, I mean, I
guess what, one to two months you start taking them on, or, I mean, I
S Speaker 245:34don't want to lie, because I'm so much of a geek. Like, I'm not a business. I want to lie, right? Like, I'm hoping in two months we get to the point where we can take them on, but our system is complex, right? Like, we are building, like, if we plan on building this, well, it's complex, so maybe, like, two and a half to three months, yeah, but like, like I said, like, you do plan to go out for funding a six to nine, so we'll have to have customers by then. So, you know,
don't want to lie, because I'm so much of a geek. Like, I'm not a business. I want to lie, right? Like, I'm hoping in two months we get to the point where we can take them on, but our system is complex, right? Like, we are building, like, if we plan on building this, well, it's complex, so maybe, like, two and a half to three months, yeah, but like, like I said, like, you do plan to go out for funding a six to nine, so we'll have to have customers by then. So, you know,
don't want to lie, because I'm so much of a geek. Like, I'm not a business. I want to lie, right? Like, I'm hoping in two months we get to the point where we can take them on, but our system is complex, right? Like, we are building, like, if we plan on building this, well, it's complex, so maybe, like, two and a half to three months, yeah, but like, like I said, like, you do plan to go out for funding a six to nine, so we'll have to have customers by then. So, you know,
don't want to lie, because I'm so much of a geek. Like, I'm not a business. I want to lie, right? Like, I'm hoping in two months we get to the point where we can take them on, but our system is complex, right? Like, we are building, like, if we plan on building this, well, it's complex, so maybe, like, two and a half to three months, yeah, but like, like I said, like, you do plan to go out for funding a six to nine, so we'll have to have customers by then. So, you know,
46:02and then, so the last round,
and then, so the last round,
and then, so the last round,
and then, so the last round,
S Speaker 246:03what was the valuation? Okay, so we did, we did a 4043, so it wound up around in the end we we raised point eight. So it was like, 50 to 55.8 was our fourth values.
what was the valuation? Okay, so we did, we did a 4043, so it wound up around in the end we we raised point eight. So it was like, 50 to 55.8 was our fourth values.
what was the valuation? Okay, so we did, we did a 4043, so it wound up around in the end we we raised point eight. So it was like, 50 to 55.8 was our fourth values.
what was the valuation? Okay, so we did, we did a 4043, so it wound up around in the end we we raised point eight. So it was like, 50 to 55.8 was our fourth values.
S Speaker 346:20That's and it's definitely very, you know, like, I know you're not you let me know. I know there's several groups at Qualcomm who would be interested in working with you. Like, especially our mobile group is building these, you know, the idea is to build a hybrid mobile cloud agent, tick platform, okay, and then so they're working, there's like, heavy r, d work that's going on around there. And then some of the problems that they're trying to solve are very similar to the proper statements that you are talking about in consumer and that if you have a better way to solve that, then I think these guys would be all ears. Let
That's and it's definitely very, you know, like, I know you're not you let me know. I know there's several groups at Qualcomm who would be interested in working with you. Like, especially our mobile group is building these, you know, the idea is to build a hybrid mobile cloud agent, tick platform, okay, and then so they're working, there's like, heavy r, d work that's going on around there. And then some of the problems that they're trying to solve are very similar to the proper statements that you are talking about in consumer and that if you have a better way to solve that, then I think these guys would be all ears. Let
That's and it's definitely very, you know, like, I know you're not you let me know. I know there's several groups at Qualcomm who would be interested in working with you. Like, especially our mobile group is building these, you know, the idea is to build a hybrid mobile cloud agent, tick platform, okay, and then so they're working, there's like, heavy r, d work that's going on around there. And then some of the problems that they're trying to solve are very similar to the proper statements that you are talking about in consumer and that if you have a better way to solve that, then I think these guys would be all ears. Let
That's and it's definitely very, you know, like, I know you're not you let me know. I know there's several groups at Qualcomm who would be interested in working with you. Like, especially our mobile group is building these, you know, the idea is to build a hybrid mobile cloud agent, tick platform, okay, and then so they're working, there's like, heavy r, d work that's going on around there. And then some of the problems that they're trying to solve are very similar to the proper statements that you are talking about in consumer and that if you have a better way to solve that, then I think these guys would be all ears. Let
S Speaker 247:04me just tell you one, a couple of funny stories, right? So we talked like Novus artists, right? And Nova Sardis is basically they're building their own LLM in house, right? And the thing is, we build your own LLM in house, you know, you're gonna spend so much money doing that, but then what you don't realize is that the LLM will work like a database and not like a reasoning model. So you're not gonna be able to solve problems very well, right? You're just gonna be able to look up information really well. So then they're gonna have to invest in a huge AI team to build out their reasoning.
me just tell you one, a couple of funny stories, right? So we talked like Novus artists, right? And Nova Sardis is basically they're building their own LLM in house, right? And the thing is, we build your own LLM in house, you know, you're gonna spend so much money doing that, but then what you don't realize is that the LLM will work like a database and not like a reasoning model. So you're not gonna be able to solve problems very well, right? You're just gonna be able to look up information really well. So then they're gonna have to invest in a huge AI team to build out their reasoning.
me just tell you one, a couple of funny stories, right? So we talked like Novus artists, right? And Nova Sardis is basically they're building their own LLM in house, right? And the thing is, we build your own LLM in house, you know, you're gonna spend so much money doing that, but then what you don't realize is that the LLM will work like a database and not like a reasoning model. So you're not gonna be able to solve problems very well, right? You're just gonna be able to look up information really well. So then they're gonna have to invest in a huge AI team to build out their reasoning.
me just tell you one, a couple of funny stories, right? So we talked like Novus artists, right? And Nova Sardis is basically they're building their own LLM in house, right? And the thing is, we build your own LLM in house, you know, you're gonna spend so much money doing that, but then what you don't realize is that the LLM will work like a database and not like a reasoning model. So you're not gonna be able to solve problems very well, right? You're just gonna be able to look up information really well. So then they're gonna have to invest in a huge AI team to build out their reasoning.
S Speaker 347:32We are not training our model. We are building the orchestration layer so so we essentially the idea is to still use models that are not built by us, okay, but essentially the idea is to be able to figure out what to run, where, and what to call, to use, and all of those things, right? Exactly what we're working on, that's exactly what we're doing, yeah? So that's where I thought, like, it'll be like a pretty good I mean, you tell me when you think we would be ready for that conversation. If you think, like next week, I can set it up for next week?
We are not training our model. We are building the orchestration layer so so we essentially the idea is to still use models that are not built by us, okay, but essentially the idea is to be able to figure out what to run, where, and what to call, to use, and all of those things, right? Exactly what we're working on, that's exactly what we're doing, yeah? So that's where I thought, like, it'll be like a pretty good I mean, you tell me when you think we would be ready for that conversation. If you think, like next week, I can set it up for next week?
We are not training our model. We are building the orchestration layer so so we essentially the idea is to still use models that are not built by us, okay, but essentially the idea is to be able to figure out what to run, where, and what to call, to use, and all of those things, right? Exactly what we're working on, that's exactly what we're doing, yeah? So that's where I thought, like, it'll be like a pretty good I mean, you tell me when you think we would be ready for that conversation. If you think, like next week, I can set it up for next week?
We are not training our model. We are building the orchestration layer so so we essentially the idea is to still use models that are not built by us, okay, but essentially the idea is to be able to figure out what to run, where, and what to call, to use, and all of those things, right? Exactly what we're working on, that's exactly what we're doing, yeah? So that's where I thought, like, it'll be like a pretty good I mean, you tell me when you think we would be ready for that conversation. If you think, like next week, I can set it up for next week?
S Speaker 248:12Yeah? I mean, I would love to have that conversation, but realistically, we're not able to do anything, at least for, you know, probably, probably start talking in a month or so, right?
Yeah? I mean, I would love to have that conversation, but realistically, we're not able to do anything, at least for, you know, probably, probably start talking in a month or so, right?
Yeah? I mean, I would love to have that conversation, but realistically, we're not able to do anything, at least for, you know, probably, probably start talking in a month or so, right?
Yeah? I mean, I would love to have that conversation, but realistically, we're not able to do anything, at least for, you know, probably, probably start talking in a month or so, right?
S Speaker 348:21Yeah, let's, let's do that when you're we, you can give us, give our engineers something, right? You don't want
Yeah, let's, let's do that when you're we, you can give us, give our engineers something, right? You don't want
Yeah, let's, let's do that when you're we, you can give us, give our engineers something, right? You don't want
Yeah, let's, let's do that when you're we, you can give us, give our engineers something, right? You don't want
48:28to lose team after the
to lose team after the
to lose team after the
to lose team after the
48:31personal design.
48:34So this would be very, very interesting.
So this would be very, very interesting.
So this would be very, very interesting.
So this would be very, very interesting.
S Speaker 348:38Let's This is, yeah, thank I enjoyed learning about this. I'm going to read up more about active learning.
Let's This is, yeah, thank I enjoyed learning about this. I'm going to read up more about active learning.
Let's This is, yeah, thank I enjoyed learning about this. I'm going to read up more about active learning.
Let's This is, yeah, thank I enjoyed learning about this. I'm going to read up more about active learning.
48:48do you have any resources for me to
do you have any resources for me to
do you have any resources for me to
do you have any resources for me to
S Speaker 248:50read up? Yeah, I'll send you. I'll send you a bunch of stuff. Yeah, so and then if you can send me this deck, that'll be great. It'll be slightly redacted, but you'll have most of it. And I do want to mention, right? I don't know where you guys are, what you guys were thinking, so I just will also mention this to you. We've done a 15 million round and talked about, we're also, there's been a lot of interest for people who didn't get to invest in that to do from, from strategics, right? Who want to come in? So we're thinking about doing a small follow on to that seed also. So that's something to keep in mind as well.
read up? Yeah, I'll send you. I'll send you a bunch of stuff. Yeah, so and then if you can send me this deck, that'll be great. It'll be slightly redacted, but you'll have most of it. And I do want to mention, right? I don't know where you guys are, what you guys were thinking, so I just will also mention this to you. We've done a 15 million round and talked about, we're also, there's been a lot of interest for people who didn't get to invest in that to do from, from strategics, right? Who want to come in? So we're thinking about doing a small follow on to that seed also. So that's something to keep in mind as well.
read up? Yeah, I'll send you. I'll send you a bunch of stuff. Yeah, so and then if you can send me this deck, that'll be great. It'll be slightly redacted, but you'll have most of it. And I do want to mention, right? I don't know where you guys are, what you guys were thinking, so I just will also mention this to you. We've done a 15 million round and talked about, we're also, there's been a lot of interest for people who didn't get to invest in that to do from, from strategics, right? Who want to come in? So we're thinking about doing a small follow on to that seed also. So that's something to keep in mind as well.
read up? Yeah, I'll send you. I'll send you a bunch of stuff. Yeah, so and then if you can send me this deck, that'll be great. It'll be slightly redacted, but you'll have most of it. And I do want to mention, right? I don't know where you guys are, what you guys were thinking, so I just will also mention this to you. We've done a 15 million round and talked about, we're also, there's been a lot of interest for people who didn't get to invest in that to do from, from strategics, right? Who want to come in? So we're thinking about doing a small follow on to that seed also. So that's something to keep in mind as well.
S Speaker 349:23So you're extending it by five. Yeah, we're sending
So you're extending it by five. Yeah, we're sending
So you're extending it by five. Yeah, we're sending
So you're extending it by five. Yeah, we're sending
S Speaker 249:27this five all, all of our leads have all asked to come in again, to give some also, and then we have some other outside interest as well. So, you know, if interest, like, it's not something that that we know
this five all, all of our leads have all asked to come in again, to give some also, and then we have some other outside interest as well. So, you know, if interest, like, it's not something that that we know
this five all, all of our leads have all asked to come in again, to give some also, and then we have some other outside interest as well. So, you know, if interest, like, it's not something that that we know
this five all, all of our leads have all asked to come in again, to give some also, and then we have some other outside interest as well. So, you know, if interest, like, it's not something that that we know
S Speaker 349:41so that, I mean, it's still, it's just essentially extending the seat right
so that, I mean, it's still, it's just essentially extending the seat right
so that, I mean, it's still, it's just essentially extending the seat right
so that, I mean, it's still, it's just essentially extending the seat right
49:50then, let me see Jimmy. Then,
then, let me see Jimmy. Then,
then, let me see Jimmy. Then,
then, let me see Jimmy. Then,
S Speaker 349:56then I think I would like to get, like, our, you know, have that conversation, maybe even earlier, to just yours, yeah, like one of the research engineers, the one of the systems guys working on it, He's based in San Diego and everything. So, Oh, perfect, yeah, and so that Okay, so let me follow up, if you can send me some material, yes, then let me follow up on this as well. Yeah, yeah, thank you. Of course, very good, yeah.
then I think I would like to get, like, our, you know, have that conversation, maybe even earlier, to just yours, yeah, like one of the research engineers, the one of the systems guys working on it, He's based in San Diego and everything. So, Oh, perfect, yeah, and so that Okay, so let me follow up, if you can send me some material, yes, then let me follow up on this as well. Yeah, yeah, thank you. Of course, very good, yeah.
then I think I would like to get, like, our, you know, have that conversation, maybe even earlier, to just yours, yeah, like one of the research engineers, the one of the systems guys working on it, He's based in San Diego and everything. So, Oh, perfect, yeah, and so that Okay, so let me follow up, if you can send me some material, yes, then let me follow up on this as well. Yeah, yeah, thank you. Of course, very good, yeah.
then I think I would like to get, like, our, you know, have that conversation, maybe even earlier, to just yours, yeah, like one of the research engineers, the one of the systems guys working on it, He's based in San Diego and everything. So, Oh, perfect, yeah, and so that Okay, so let me follow up, if you can send me some material, yes, then let me follow up on this as well. Yeah, yeah, thank you. Of course, very good, yeah.
50:36Nice power speakers in the
Nice power speakers in the
Nice power speakers in the
Nice power speakers in the
S Speaker 250:39back. Hold out. No one has them anymore, yeah?
back. Hold out. No one has them anymore, yeah?
back. Hold out. No one has them anymore, yeah?
back. Hold out. No one has them anymore, yeah?
S Speaker 350:43Well, you know, I used to have them in San Diego, and when we moved from San Diego to here, I sold mine because, because because I was like, okay, they were taking up too much space. And my wife like, yeah, you know what? Like, they sound the best. So they really do No.
Well, you know, I used to have them in San Diego, and when we moved from San Diego to here, I sold mine because, because because I was like, okay, they were taking up too much space. And my wife like, yeah, you know what? Like, they sound the best. So they really do No.
Well, you know, I used to have them in San Diego, and when we moved from San Diego to here, I sold mine because, because because I was like, okay, they were taking up too much space. And my wife like, yeah, you know what? Like, they sound the best. So they really do No.
Well, you know, I used to have them in San Diego, and when we moved from San Diego to here, I sold mine because, because because I was like, okay, they were taking up too much space. And my wife like, yeah, you know what? Like, they sound the best. So they really do No.
S Speaker 251:05So those are my surround ones. I actually have two others in front of me also, right? So, yeah,
So those are my surround ones. I actually have two others in front of me also, right? So, yeah,
So those are my surround ones. I actually have two others in front of me also, right? So, yeah,
So those are my surround ones. I actually have two others in front of me also, right? So, yeah,
51:12like, what? What's the these?
like, what? What's the these?
like, what? What's the these?
like, what? What's the these?
S Speaker 251:14No one's back. There are folk and you and I know there's those ones I got while I was still not as good situation I'm in now. So I'm still, I'm looking around for a new set, right? Yeah.
No one's back. There are folk and you and I know there's those ones I got while I was still not as good situation I'm in now. So I'm still, I'm looking around for a new set, right? Yeah.
No one's back. There are folk and you and I know there's those ones I got while I was still not as good situation I'm in now. So I'm still, I'm looking around for a new set, right? Yeah.
No one's back. There are folk and you and I know there's those ones I got while I was still not as good situation I'm in now. So I'm still, I'm looking around for a new set, right? Yeah.
51:26Are you an audio file?
Are you an audio file?
Are you an audio file?
Are you an audio file?
S Speaker 251:30No, I mean, I mean, because audio file has so much weight when you carry that, get me wrong, I appreciate good speakers, but there's my friends who are real audio files like that is a whole same
No, I mean, I mean, because audio file has so much weight when you carry that, get me wrong, I appreciate good speakers, but there's my friends who are real audio files like that is a whole same
No, I mean, I mean, because audio file has so much weight when you carry that, get me wrong, I appreciate good speakers, but there's my friends who are real audio files like that is a whole same
No, I mean, I mean, because audio file has so much weight when you carry that, get me wrong, I appreciate good speakers, but there's my friends who are real audio files like that is a whole same
S Speaker 351:44way. So that's why I don't call myself an audiophile. Dangerous.
way. So that's why I don't call myself an audiophile. Dangerous.
way. So that's why I don't call myself an audiophile. Dangerous.
way. So that's why I don't call myself an audiophile. Dangerous.
51:49It's a dangerous Minecraft, right there.
It's a dangerous Minecraft, right there.
It's a dangerous Minecraft, right there.
It's a dangerous Minecraft, right there.
S Speaker 351:54right, pleasure talking to you and then you've been touching the material, that'd be great. Thank you. Thank you guys.
right, pleasure talking to you and then you've been touching the material, that'd be great. Thank you. Thank you guys.
right, pleasure talking to you and then you've been touching the material, that'd be great. Thank you. Thank you guys.
right, pleasure talking to you and then you've been touching the material, that'd be great. Thank you. Thank you guys.