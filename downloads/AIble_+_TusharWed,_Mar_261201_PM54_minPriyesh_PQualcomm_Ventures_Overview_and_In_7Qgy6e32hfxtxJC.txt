Meeting: AIble + Tushar
Wed, Mar 26
12:01 PM
54 min
Priyesh P
Qualcomm Ventures Overview and Investment Strat
URL: https://otter.ai/u/7Qgy6e32hfxtxJC3pT2LV2oA2OA
Downloaded: 2025-12-22T12:29:28.275627
Method: text_extraction
============================================================

0:06I'll introduce myself. I'm based in the Bay, responsible for
I'll introduce myself. I'm based in the Bay, responsible for
I'll introduce myself. I'm based in the Bay, responsible for
I'll introduce myself. I'm based in the Bay, responsible for
S Speaker 10:17us investments and Qualcomm ventures. My background is in engineering product and Qualcomm ventures as a firm has been around for about 25 years. So we invest in areas that are relevant for Qualcomm. So we do, and right now, you know that the definition of that is, of course, a lot of stuff that we're doing around AI, all the way from mobile to cloud to IoT to networking to on prem clouds and data centers. Now we have a data center business unit as well, and several other areas. So we ship our platforms and ships across all of these areas. And so we invest in companies that are that are building across all of these areas. So we try to work with companies that could either be partners of Qualcomm. In some cases, they become customers of Qualcomm. In some cases, Qualcomm is a customer, or in many cases, their channel partners. So we deploy about one $50 million every year, so it's and then our initial check size is anywhere from two to 10 million
us investments and Qualcomm ventures. My background is in engineering product and Qualcomm ventures as a firm has been around for about 25 years. So we invest in areas that are relevant for Qualcomm. So we do, and right now, you know that the definition of that is, of course, a lot of stuff that we're doing around AI, all the way from mobile to cloud to IoT to networking to on prem clouds and data centers. Now we have a data center business unit as well, and several other areas. So we ship our platforms and ships across all of these areas. And so we invest in companies that are that are building across all of these areas. So we try to work with companies that could either be partners of Qualcomm. In some cases, they become customers of Qualcomm. In some cases, Qualcomm is a customer, or in many cases, their channel partners. So we deploy about one $50 million every year, so it's and then our initial check size is anywhere from two to 10 million
us investments and Qualcomm ventures. My background is in engineering product and Qualcomm ventures as a firm has been around for about 25 years. So we invest in areas that are relevant for Qualcomm. So we do, and right now, you know that the definition of that is, of course, a lot of stuff that we're doing around AI, all the way from mobile to cloud to IoT to networking to on prem clouds and data centers. Now we have a data center business unit as well, and several other areas. So we ship our platforms and ships across all of these areas. And so we invest in companies that are that are building across all of these areas. So we try to work with companies that could either be partners of Qualcomm. In some cases, they become customers of Qualcomm. In some cases, Qualcomm is a customer, or in many cases, their channel partners. So we deploy about one $50 million every year, so it's and then our initial check size is anywhere from two to 10 million
us investments and Qualcomm ventures. My background is in engineering product and Qualcomm ventures as a firm has been around for about 25 years. So we invest in areas that are relevant for Qualcomm. So we do, and right now, you know that the definition of that is, of course, a lot of stuff that we're doing around AI, all the way from mobile to cloud to IoT to networking to on prem clouds and data centers. Now we have a data center business unit as well, and several other areas. So we ship our platforms and ships across all of these areas. And so we invest in companies that are that are building across all of these areas. So we try to work with companies that could either be partners of Qualcomm. In some cases, they become customers of Qualcomm. In some cases, Qualcomm is a customer, or in many cases, their channel partners. So we deploy about one $50 million every year, so it's and then our initial check size is anywhere from two to 10 million
S Speaker 11:45like series agnostic, series ABC is our sweet spot, and we we can lead or follow, but mostly we participate with other institutional investors. We have both financial and strategic mandate. So we've got the same financial discipline for investing as an institutional investor, but that's the
like series agnostic, series ABC is our sweet spot, and we we can lead or follow, but mostly we participate with other institutional investors. We have both financial and strategic mandate. So we've got the same financial discipline for investing as an institutional investor, but that's the
like series agnostic, series ABC is our sweet spot, and we we can lead or follow, but mostly we participate with other institutional investors. We have both financial and strategic mandate. So we've got the same financial discipline for investing as an institutional investor, but that's the
like series agnostic, series ABC is our sweet spot, and we we can lead or follow, but mostly we participate with other institutional investors. We have both financial and strategic mandate. So we've got the same financial discipline for investing as an institutional investor, but that's the
2:12mandate for us. So
S Speaker 12:44some with it's not, you don't have to be a Qualcomm partner today to for us to have an investment discussion, and then also, even for us to invest, which is the idea is to to see if you can be a partner. And then, if that's the case, then we can invest in startups.
some with it's not, you don't have to be a Qualcomm partner today to for us to have an investment discussion, and then also, even for us to invest, which is the idea is to to see if you can be a partner. And then, if that's the case, then we can invest in startups.
some with it's not, you don't have to be a Qualcomm partner today to for us to have an investment discussion, and then also, even for us to invest, which is the idea is to to see if you can be a partner. And then, if that's the case, then we can invest in startups.
some with it's not, you don't have to be a Qualcomm partner today to for us to have an investment discussion, and then also, even for us to invest, which is the idea is to to see if you can be a partner. And then, if that's the case, then we can invest in startups.
S Speaker 23:05Now, have you guys invested along with Nvidia before? Like or is
Now, have you guys invested along with Nvidia before? Like or is
Now, have you guys invested along with Nvidia before? Like or is
Now, have you guys invested along with Nvidia before? Like or is
S Speaker 13:13We've invested in several companies along with Nvidia, so that's not a concern.
We've invested in several companies along with Nvidia, so that's not a concern.
We've invested in several companies along with Nvidia, so that's not a concern.
We've invested in several companies along with Nvidia, so that's not a concern.
3:21Why are they investors?
Why are they investors?
Why are they investors?
Why are they investors?
S Speaker 23:23No, they're not investors. We actually never took institutional money. This is going to be our first institutional round. I'll tell you about that in a second, because we were trying to do something very different, which didn't become obvious until a few months back. So the funny thing is, like, this was my pattern with beyond code to the previous company has started, which became Salesforce, science and discovery, that people said it was crazy, and then it became obvious. And I was like, you know, that's a good, good thing. He's like, for years, I kept hearing, this is this is crazy. This is not the way to do it. Do it this way. Do it that way. And then all of a sudden, there were Sunday, oh, this is the obvious way to do things. I'm like, right? That means we're doing something,
No, they're not investors. We actually never took institutional money. This is going to be our first institutional round. I'll tell you about that in a second, because we were trying to do something very different, which didn't become obvious until a few months back. So the funny thing is, like, this was my pattern with beyond code to the previous company has started, which became Salesforce, science and discovery, that people said it was crazy, and then it became obvious. And I was like, you know, that's a good, good thing. He's like, for years, I kept hearing, this is this is crazy. This is not the way to do it. Do it this way. Do it that way. And then all of a sudden, there were Sunday, oh, this is the obvious way to do things. I'm like, right? That means we're doing something,
No, they're not investors. We actually never took institutional money. This is going to be our first institutional round. I'll tell you about that in a second, because we were trying to do something very different, which didn't become obvious until a few months back. So the funny thing is, like, this was my pattern with beyond code to the previous company has started, which became Salesforce, science and discovery, that people said it was crazy, and then it became obvious. And I was like, you know, that's a good, good thing. He's like, for years, I kept hearing, this is this is crazy. This is not the way to do it. Do it this way. Do it that way. And then all of a sudden, there were Sunday, oh, this is the obvious way to do things. I'm like, right? That means we're doing something,
No, they're not investors. We actually never took institutional money. This is going to be our first institutional round. I'll tell you about that in a second, because we were trying to do something very different, which didn't become obvious until a few months back. So the funny thing is, like, this was my pattern with beyond code to the previous company has started, which became Salesforce, science and discovery, that people said it was crazy, and then it became obvious. And I was like, you know, that's a good, good thing. He's like, for years, I kept hearing, this is this is crazy. This is not the way to do it. Do it this way. Do it that way. And then all of a sudden, there were Sunday, oh, this is the obvious way to do things. I'm like, right? That means we're doing something,
S Speaker 14:06right? Yeah. So that's good. And then, so it's I see your background was you were you? Then you were part of Salesforce as well after that. So
right? Yeah. So that's good. And then, so it's I see your background was you were you? Then you were part of Salesforce as well after that. So
right? Yeah. So that's good. And then, so it's I see your background was you were you? Then you were part of Salesforce as well after that. So
right? Yeah. So that's good. And then, so it's I see your background was you were you? Then you were part of Salesforce as well after that. So
S Speaker 24:16I ran beyondcourse, a Salesforce company for 18 months, yeah. And then after fully incorporating it in, I left, and then I started able. So actually, in the middle, I wrote a book called AI is a waste of money. It took six months and a sabbatical to write up. Took 1000 AI projects I had done, and I kind of laid down why they failed. And not all 1000 failed, but the ones that failed, why they failed? And quite a bit of them had failed, and most of it is because of disconnect between business users and data scientists. And my entire premise was, you'll see this thing, I am able this entire premise is, if the business users can't make AI for themselves, you're always going to have that friction, always going to have that disconnect. The moment it has to be it led, the moment it has to be data scientist LED. You're never going to have the 10 agents per employee that Jensen is talking about. It just can't happen. You can't the accentures of the world. Can't physically make enough of the agency, right? So that's the premise we came from. And the other parts of my background of course ended up teaching the first AI course in the MBA program at Harvard. The book basically became the core of that course. And prior to that, I had come to the US to study AI at Stanford, because in those days, even at IIT Kanpur, you could not study AI. It was really sad, but you just could not the bachelor's level. So so ended up doing computer science, economics and dance at Stanford, and working with people like Ed Fauci bomb and others who had kind of failed in AI in the last generation. So one of the good things that I went through is I still met the generation that had disastrously failed with AI. So I think I come at AI with a little bit more humility, as opposed to, oh my god, it just works. So you actually have to understand enterprise adoption. You actually have to understand a lot of things. You can't just say, hey, my demo is cool and it will work, right? So that's that's the background. Jonathan, my co founder, is on the call, but he is almost certainly working while doing this. But Jonathan, if you want to introduce yourself,
I ran beyondcourse, a Salesforce company for 18 months, yeah. And then after fully incorporating it in, I left, and then I started able. So actually, in the middle, I wrote a book called AI is a waste of money. It took six months and a sabbatical to write up. Took 1000 AI projects I had done, and I kind of laid down why they failed. And not all 1000 failed, but the ones that failed, why they failed? And quite a bit of them had failed, and most of it is because of disconnect between business users and data scientists. And my entire premise was, you'll see this thing, I am able this entire premise is, if the business users can't make AI for themselves, you're always going to have that friction, always going to have that disconnect. The moment it has to be it led, the moment it has to be data scientist LED. You're never going to have the 10 agents per employee that Jensen is talking about. It just can't happen. You can't the accentures of the world. Can't physically make enough of the agency, right? So that's the premise we came from. And the other parts of my background of course ended up teaching the first AI course in the MBA program at Harvard. The book basically became the core of that course. And prior to that, I had come to the US to study AI at Stanford, because in those days, even at IIT Kanpur, you could not study AI. It was really sad, but you just could not the bachelor's level. So so ended up doing computer science, economics and dance at Stanford, and working with people like Ed Fauci bomb and others who had kind of failed in AI in the last generation. So one of the good things that I went through is I still met the generation that had disastrously failed with AI. So I think I come at AI with a little bit more humility, as opposed to, oh my god, it just works. So you actually have to understand enterprise adoption. You actually have to understand a lot of things. You can't just say, hey, my demo is cool and it will work, right? So that's that's the background. Jonathan, my co founder, is on the call, but he is almost certainly working while doing this. But Jonathan, if you want to introduce yourself,
I ran beyondcourse, a Salesforce company for 18 months, yeah. And then after fully incorporating it in, I left, and then I started able. So actually, in the middle, I wrote a book called AI is a waste of money. It took six months and a sabbatical to write up. Took 1000 AI projects I had done, and I kind of laid down why they failed. And not all 1000 failed, but the ones that failed, why they failed? And quite a bit of them had failed, and most of it is because of disconnect between business users and data scientists. And my entire premise was, you'll see this thing, I am able this entire premise is, if the business users can't make AI for themselves, you're always going to have that friction, always going to have that disconnect. The moment it has to be it led, the moment it has to be data scientist LED. You're never going to have the 10 agents per employee that Jensen is talking about. It just can't happen. You can't the accentures of the world. Can't physically make enough of the agency, right? So that's the premise we came from. And the other parts of my background of course ended up teaching the first AI course in the MBA program at Harvard. The book basically became the core of that course. And prior to that, I had come to the US to study AI at Stanford, because in those days, even at IIT Kanpur, you could not study AI. It was really sad, but you just could not the bachelor's level. So so ended up doing computer science, economics and dance at Stanford, and working with people like Ed Fauci bomb and others who had kind of failed in AI in the last generation. So one of the good things that I went through is I still met the generation that had disastrously failed with AI. So I think I come at AI with a little bit more humility, as opposed to, oh my god, it just works. So you actually have to understand enterprise adoption. You actually have to understand a lot of things. You can't just say, hey, my demo is cool and it will work, right? So that's that's the background. Jonathan, my co founder, is on the call, but he is almost certainly working while doing this. But Jonathan, if you want to introduce yourself,
I ran beyondcourse, a Salesforce company for 18 months, yeah. And then after fully incorporating it in, I left, and then I started able. So actually, in the middle, I wrote a book called AI is a waste of money. It took six months and a sabbatical to write up. Took 1000 AI projects I had done, and I kind of laid down why they failed. And not all 1000 failed, but the ones that failed, why they failed? And quite a bit of them had failed, and most of it is because of disconnect between business users and data scientists. And my entire premise was, you'll see this thing, I am able this entire premise is, if the business users can't make AI for themselves, you're always going to have that friction, always going to have that disconnect. The moment it has to be it led, the moment it has to be data scientist LED. You're never going to have the 10 agents per employee that Jensen is talking about. It just can't happen. You can't the accentures of the world. Can't physically make enough of the agency, right? So that's the premise we came from. And the other parts of my background of course ended up teaching the first AI course in the MBA program at Harvard. The book basically became the core of that course. And prior to that, I had come to the US to study AI at Stanford, because in those days, even at IIT Kanpur, you could not study AI. It was really sad, but you just could not the bachelor's level. So so ended up doing computer science, economics and dance at Stanford, and working with people like Ed Fauci bomb and others who had kind of failed in AI in the last generation. So one of the good things that I went through is I still met the generation that had disastrously failed with AI. So I think I come at AI with a little bit more humility, as opposed to, oh my god, it just works. So you actually have to understand enterprise adoption. You actually have to understand a lot of things. You can't just say, hey, my demo is cool and it will work, right? So that's that's the background. Jonathan, my co founder, is on the call, but he is almost certainly working while doing this. But Jonathan, if you want to introduce yourself,
S Speaker 27:36Dylan, you want to go on, let's come back to Priyesh. After that was Priyesh, I realized, though Dylan and I met you. Jonathan actually did it, so we should give you a chance to introduce yourself, but I'll go first and we can go to Priyesh.
Dylan, you want to go on, let's come back to Priyesh. After that was Priyesh, I realized, though Dylan and I met you. Jonathan actually did it, so we should give you a chance to introduce yourself, but I'll go first and we can go to Priyesh.
Dylan, you want to go on, let's come back to Priyesh. After that was Priyesh, I realized, though Dylan and I met you. Jonathan actually did it, so we should give you a chance to introduce yourself, but I'll go first and we can go to Priyesh.
Dylan, you want to go on, let's come back to Priyesh. After that was Priyesh, I realized, though Dylan and I met you. Jonathan actually did it, so we should give you a chance to introduce yourself, but I'll go first and we can go to Priyesh.
S Speaker 47:47Absolutely. Hi everyone. Dylan Steele, Business Development able. I've been working with Jonathan origin for about six years, third AI startup. Prior to that, I worked at Intel Capital with Carlos Cochran and Priyesh. I think I mentioned this. And so I think Tushar, he probably worked with Carlos for a number of years. So I worked with him when my boss, oh, okay, yeah. So I worked with Carlos when he managed icap Latin America out of Sao Paulo. So I used to go down and look at deals with him in the in the Brazil area. So could speak with
Absolutely. Hi everyone. Dylan Steele, Business Development able. I've been working with Jonathan origin for about six years, third AI startup. Prior to that, I worked at Intel Capital with Carlos Cochran and Priyesh. I think I mentioned this. And so I think Tushar, he probably worked with Carlos for a number of years. So I worked with him when my boss, oh, okay, yeah. So I worked with Carlos when he managed icap Latin America out of Sao Paulo. So I used to go down and look at deals with him in the in the Brazil area. So could speak with
Absolutely. Hi everyone. Dylan Steele, Business Development able. I've been working with Jonathan origin for about six years, third AI startup. Prior to that, I worked at Intel Capital with Carlos Cochran and Priyesh. I think I mentioned this. And so I think Tushar, he probably worked with Carlos for a number of years. So I worked with him when my boss, oh, okay, yeah. So I worked with Carlos when he managed icap Latin America out of Sao Paulo. So I used to go down and look at deals with him in the in the Brazil area. So could speak with
Absolutely. Hi everyone. Dylan Steele, Business Development able. I've been working with Jonathan origin for about six years, third AI startup. Prior to that, I worked at Intel Capital with Carlos Cochran and Priyesh. I think I mentioned this. And so I think Tushar, he probably worked with Carlos for a number of years. So I worked with him when my boss, oh, okay, yeah. So I worked with Carlos when he managed icap Latin America out of Sao Paulo. So I used to go down and look at deals with him in the in the Brazil area. So could speak with
8:20worked for a number of years before he just left
worked for a number of years before he just left
worked for a number of years before he just left
worked for a number of years before he just left
8:23us. I saw the update. I saw the update right
us. I saw the update. I saw the update right
us. I saw the update. I saw the update right
us. I saw the update. I saw the update right
S Speaker 58:31and I'll give my intro very short. So originally, an engineer from IIT mantras, worked with a lot of startups in India, moved to the US, worked in venture, worked at Menlo Ventures and Morpheus in LA and then with cargo ventures as a senior associate now for a year.
and I'll give my intro very short. So originally, an engineer from IIT mantras, worked with a lot of startups in India, moved to the US, worked in venture, worked at Menlo Ventures and Morpheus in LA and then with cargo ventures as a senior associate now for a year.
and I'll give my intro very short. So originally, an engineer from IIT mantras, worked with a lot of startups in India, moved to the US, worked in venture, worked at Menlo Ventures and Morpheus in LA and then with cargo ventures as a senior associate now for a year.
and I'll give my intro very short. So originally, an engineer from IIT mantras, worked with a lot of startups in India, moved to the US, worked in venture, worked at Menlo Ventures and Morpheus in LA and then with cargo ventures as a senior associate now for a year.
S Speaker 28:49And Tushar, I would say you could tell your colleague that if they want to have an impact on climate, getting 30x better productivity out of GPUs is a good way to do that. That's what we are focused on as well. Because one of our concerns is that if people kind of don't think about how to optimize AI and all the way down to the hardware level, we're going to have a catastrophic situation. It's just going to be a problem. So with your permission, I will show you some slide decks and stuff, and they're not fully polished. They're still being worked on, but we just survived GTC. And now this week, we are getting all our slides ready, and you guys are the second. Yeah, they'll be the second. Vc is getting to see this. The first one was Nvidia, because obviously we had to show it to the first they have been good partners. They've been very good partners. And by the way, that's one thing. If they have objection to you, I'm just being very upfront with you up front, because they're important for our strategy. That's why I asked you the question of, are you guys friendly with them on that perspective?
And Tushar, I would say you could tell your colleague that if they want to have an impact on climate, getting 30x better productivity out of GPUs is a good way to do that. That's what we are focused on as well. Because one of our concerns is that if people kind of don't think about how to optimize AI and all the way down to the hardware level, we're going to have a catastrophic situation. It's just going to be a problem. So with your permission, I will show you some slide decks and stuff, and they're not fully polished. They're still being worked on, but we just survived GTC. And now this week, we are getting all our slides ready, and you guys are the second. Yeah, they'll be the second. Vc is getting to see this. The first one was Nvidia, because obviously we had to show it to the first they have been good partners. They've been very good partners. And by the way, that's one thing. If they have objection to you, I'm just being very upfront with you up front, because they're important for our strategy. That's why I asked you the question of, are you guys friendly with them on that perspective?
And Tushar, I would say you could tell your colleague that if they want to have an impact on climate, getting 30x better productivity out of GPUs is a good way to do that. That's what we are focused on as well. Because one of our concerns is that if people kind of don't think about how to optimize AI and all the way down to the hardware level, we're going to have a catastrophic situation. It's just going to be a problem. So with your permission, I will show you some slide decks and stuff, and they're not fully polished. They're still being worked on, but we just survived GTC. And now this week, we are getting all our slides ready, and you guys are the second. Yeah, they'll be the second. Vc is getting to see this. The first one was Nvidia, because obviously we had to show it to the first they have been good partners. They've been very good partners. And by the way, that's one thing. If they have objection to you, I'm just being very upfront with you up front, because they're important for our strategy. That's why I asked you the question of, are you guys friendly with them on that perspective?
And Tushar, I would say you could tell your colleague that if they want to have an impact on climate, getting 30x better productivity out of GPUs is a good way to do that. That's what we are focused on as well. Because one of our concerns is that if people kind of don't think about how to optimize AI and all the way down to the hardware level, we're going to have a catastrophic situation. It's just going to be a problem. So with your permission, I will show you some slide decks and stuff, and they're not fully polished. They're still being worked on, but we just survived GTC. And now this week, we are getting all our slides ready, and you guys are the second. Yeah, they'll be the second. Vc is getting to see this. The first one was Nvidia, because obviously we had to show it to the first they have been good partners. They've been very good partners. And by the way, that's one thing. If they have objection to you, I'm just being very upfront with you up front, because they're important for our strategy. That's why I asked you the question of, are you guys friendly with them on that perspective?
S Speaker 19:55I mean, we don't. I don't think they would say otherwise, either. So
I mean, we don't. I don't think they would say otherwise, either. So
I mean, we don't. I don't think they would say otherwise, either. So
I mean, we don't. I don't think they would say otherwise, either. So
S Speaker 210:01perfect. So what we are focused on is, how do we have these 1000s of AI agents specialized to the customer's business, to their processes, and how do we actually make it possible for any business user to do it right? And actually, when able first came out, we had started this tradition. Every year we have taken a bunch of high school kids, history majors and MBAs, and given them about 90 minutes of training on able and put them up against expert data scientists. And all four years, the high school kids have beaten the expert data scientists, and that has been our key focuses. Like anyone should be able to do this right? And if you look at what is needed to do these kind of massive scale of AI agents, you first need to have a business user first creation, and you know, you need to be tailored to specific use cases, data structures, user preferences. And this is where the CO pilots of the world are struggling. They do very well when you're doing email summarization or doing something very generic, the moment you get specific to a use case and you deal with like random terminologies in Verizon or like words like jitter that the language model has no way of understanding or the data structures that's where they feel, then it need to be able to do it with only requiring business user network The moment you require data science expertise of any sort, you're dead. You cannot deliver it at scale. They just aren't enough data scientists, right? And the translation back and forth, the waiting for the data scientist, kills the whole project. And we actually had the Chief Digital Officer of CVS Health talking about how this kind of the rapid prototyping approach of able where they could just get things done and get feedback from business stakeholders very, very quickly. That is a fundamental shift to the approach to creating agents. Because the moment they would throw it over the wall from the business user to the data scientist, and imagine they do it in two weeks. Let's just give them a crazy example, and they do it in two weeks. Normally it takes six months. Even then, after you've done two iterations over a month, the business user has lost interest, and that was one of his key points. I don't know whether we send send you guys the video from Gartner conference. Did we?
perfect. So what we are focused on is, how do we have these 1000s of AI agents specialized to the customer's business, to their processes, and how do we actually make it possible for any business user to do it right? And actually, when able first came out, we had started this tradition. Every year we have taken a bunch of high school kids, history majors and MBAs, and given them about 90 minutes of training on able and put them up against expert data scientists. And all four years, the high school kids have beaten the expert data scientists, and that has been our key focuses. Like anyone should be able to do this right? And if you look at what is needed to do these kind of massive scale of AI agents, you first need to have a business user first creation, and you know, you need to be tailored to specific use cases, data structures, user preferences. And this is where the CO pilots of the world are struggling. They do very well when you're doing email summarization or doing something very generic, the moment you get specific to a use case and you deal with like random terminologies in Verizon or like words like jitter that the language model has no way of understanding or the data structures that's where they feel, then it need to be able to do it with only requiring business user network The moment you require data science expertise of any sort, you're dead. You cannot deliver it at scale. They just aren't enough data scientists, right? And the translation back and forth, the waiting for the data scientist, kills the whole project. And we actually had the Chief Digital Officer of CVS Health talking about how this kind of the rapid prototyping approach of able where they could just get things done and get feedback from business stakeholders very, very quickly. That is a fundamental shift to the approach to creating agents. Because the moment they would throw it over the wall from the business user to the data scientist, and imagine they do it in two weeks. Let's just give them a crazy example, and they do it in two weeks. Normally it takes six months. Even then, after you've done two iterations over a month, the business user has lost interest, and that was one of his key points. I don't know whether we send send you guys the video from Gartner conference. Did we?
perfect. So what we are focused on is, how do we have these 1000s of AI agents specialized to the customer's business, to their processes, and how do we actually make it possible for any business user to do it right? And actually, when able first came out, we had started this tradition. Every year we have taken a bunch of high school kids, history majors and MBAs, and given them about 90 minutes of training on able and put them up against expert data scientists. And all four years, the high school kids have beaten the expert data scientists, and that has been our key focuses. Like anyone should be able to do this right? And if you look at what is needed to do these kind of massive scale of AI agents, you first need to have a business user first creation, and you know, you need to be tailored to specific use cases, data structures, user preferences. And this is where the CO pilots of the world are struggling. They do very well when you're doing email summarization or doing something very generic, the moment you get specific to a use case and you deal with like random terminologies in Verizon or like words like jitter that the language model has no way of understanding or the data structures that's where they feel, then it need to be able to do it with only requiring business user network The moment you require data science expertise of any sort, you're dead. You cannot deliver it at scale. They just aren't enough data scientists, right? And the translation back and forth, the waiting for the data scientist, kills the whole project. And we actually had the Chief Digital Officer of CVS Health talking about how this kind of the rapid prototyping approach of able where they could just get things done and get feedback from business stakeholders very, very quickly. That is a fundamental shift to the approach to creating agents. Because the moment they would throw it over the wall from the business user to the data scientist, and imagine they do it in two weeks. Let's just give them a crazy example, and they do it in two weeks. Normally it takes six months. Even then, after you've done two iterations over a month, the business user has lost interest, and that was one of his key points. I don't know whether we send send you guys the video from Gartner conference. Did we?
perfect. So what we are focused on is, how do we have these 1000s of AI agents specialized to the customer's business, to their processes, and how do we actually make it possible for any business user to do it right? And actually, when able first came out, we had started this tradition. Every year we have taken a bunch of high school kids, history majors and MBAs, and given them about 90 minutes of training on able and put them up against expert data scientists. And all four years, the high school kids have beaten the expert data scientists, and that has been our key focuses. Like anyone should be able to do this right? And if you look at what is needed to do these kind of massive scale of AI agents, you first need to have a business user first creation, and you know, you need to be tailored to specific use cases, data structures, user preferences. And this is where the CO pilots of the world are struggling. They do very well when you're doing email summarization or doing something very generic, the moment you get specific to a use case and you deal with like random terminologies in Verizon or like words like jitter that the language model has no way of understanding or the data structures that's where they feel, then it need to be able to do it with only requiring business user network The moment you require data science expertise of any sort, you're dead. You cannot deliver it at scale. They just aren't enough data scientists, right? And the translation back and forth, the waiting for the data scientist, kills the whole project. And we actually had the Chief Digital Officer of CVS Health talking about how this kind of the rapid prototyping approach of able where they could just get things done and get feedback from business stakeholders very, very quickly. That is a fundamental shift to the approach to creating agents. Because the moment they would throw it over the wall from the business user to the data scientist, and imagine they do it in two weeks. Let's just give them a crazy example, and they do it in two weeks. Normally it takes six months. Even then, after you've done two iterations over a month, the business user has lost interest, and that was one of his key points. I don't know whether we send send you guys the video from Gartner conference. Did we?
12:14I don't think so much. No, okay,
I don't think so much. No, okay,
I don't think so much. No, okay,
I don't think so much. No, okay,
S Speaker 212:16I'll follow up after this and get you guys the video, it's actually kind of cool to watch that we actually had the CIO of the CEO of muraskar, talking about how we did a project in two and a half hours, that they had hired a consulting company for six months, for $2 million and not achieved. You know, the last part of this is this rapid prototyping. It's romantic. It has to be fast, because business users will not stay on a project for nine months just to play around with it. This is not it. They need it needs to be something they can do by while doing their day jobs. And there has to immediate payback. If there's no immediate payback, it better financial or our work impact, they're not going to do it.
I'll follow up after this and get you guys the video, it's actually kind of cool to watch that we actually had the CIO of the CEO of muraskar, talking about how we did a project in two and a half hours, that they had hired a consulting company for six months, for $2 million and not achieved. You know, the last part of this is this rapid prototyping. It's romantic. It has to be fast, because business users will not stay on a project for nine months just to play around with it. This is not it. They need it needs to be something they can do by while doing their day jobs. And there has to immediate payback. If there's no immediate payback, it better financial or our work impact, they're not going to do it.
I'll follow up after this and get you guys the video, it's actually kind of cool to watch that we actually had the CIO of the CEO of muraskar, talking about how we did a project in two and a half hours, that they had hired a consulting company for six months, for $2 million and not achieved. You know, the last part of this is this rapid prototyping. It's romantic. It has to be fast, because business users will not stay on a project for nine months just to play around with it. This is not it. They need it needs to be something they can do by while doing their day jobs. And there has to immediate payback. If there's no immediate payback, it better financial or our work impact, they're not going to do it.
I'll follow up after this and get you guys the video, it's actually kind of cool to watch that we actually had the CIO of the CEO of muraskar, talking about how we did a project in two and a half hours, that they had hired a consulting company for six months, for $2 million and not achieved. You know, the last part of this is this rapid prototyping. It's romantic. It has to be fast, because business users will not stay on a project for nine months just to play around with it. This is not it. They need it needs to be something they can do by while doing their day jobs. And there has to immediate payback. If there's no immediate payback, it better financial or our work impact, they're not going to do it.
S Speaker 213:17show you a slide where we have public case studies on them. We always stay confidential. That one he has not, he did not tell people what the specific fix was. He just will get you the video. So I have to be careful to only say what I've approved to say, but it was on the regulatory side for one of his agencies, right? But
show you a slide where we have public case studies on them. We always stay confidential. That one he has not, he did not tell people what the specific fix was. He just will get you the video. So I have to be careful to only say what I've approved to say, but it was on the regulatory side for one of his agencies, right? But
show you a slide where we have public case studies on them. We always stay confidential. That one he has not, he did not tell people what the specific fix was. He just will get you the video. So I have to be careful to only say what I've approved to say, but it was on the regulatory side for one of his agencies, right? But
show you a slide where we have public case studies on them. We always stay confidential. That one he has not, he did not tell people what the specific fix was. He just will get you the video. So I have to be careful to only say what I've approved to say, but it was on the regulatory side for one of his agencies, right? But
S Speaker 113:39case studies in a second doesn't matter. I mean, I guess any other case study, which is what was, what is the problem specifically, and then what is a bunch of them in two slides? Yeah, okay, got it.
case studies in a second doesn't matter. I mean, I guess any other case study, which is what was, what is the problem specifically, and then what is a bunch of them in two slides? Yeah, okay, got it.
case studies in a second doesn't matter. I mean, I guess any other case study, which is what was, what is the problem specifically, and then what is a bunch of them in two slides? Yeah, okay, got it.
case studies in a second doesn't matter. I mean, I guess any other case study, which is what was, what is the problem specifically, and then what is a bunch of them in two slides? Yeah, okay, got it.
S Speaker 116:46it, what we have is not I mean, our architectures are more NPU based versus GPU based. So what we have is equivalent tops, but, but it's a different architecture, so, so I wonder. I don't know. I mean, I won't be I we have, we have other people, yeah, but it'll
it, what we have is not I mean, our architectures are more NPU based versus GPU based. So what we have is equivalent tops, but, but it's a different architecture, so, so I wonder. I don't know. I mean, I won't be I we have, we have other people, yeah, but it'll
it, what we have is not I mean, our architectures are more NPU based versus GPU based. So what we have is equivalent tops, but, but it's a different architecture, so, so I wonder. I don't know. I mean, I won't be I we have, we have other people, yeah, but it'll
it, what we have is not I mean, our architectures are more NPU based versus GPU based. So what we have is equivalent tops, but, but it's a different architecture, so, so I wonder. I don't know. I mean, I won't be I we have, we have other people, yeah, but it'll
S Speaker 217:13be fun to collaborate on that. What we have found is we can often when we work closely with one of our technical partners, and we have done the same thing with Intel previously. We work very closely with our our partners, and we actually often find them surprising things in their processes, because we push them in ways that others are not pushing it, and then we find stuff with them. And so this could be a fun part of the collaboration. It at least has been with both Intel and with Nvidia, right? The second part was at Gartner. Again, we'll get you the video. And the last part was snowflake just presented three case studies at Mobile World Congress. And one of the key things there was this finding unknown unknowns in the data. So if you think about something like snowflake, which has their own AI system, their own NLQ system, they are highlighting us, because what happens enable is we'll go in and ask millions of questions of your data completely automatically, but at very low cost, and then highlight the key insights. So everyone else in analytics is taking the power of AI and putting it behind the human ability to ask a question, but a human asks. There are 2030, questions and stops, but there are millions of questions you could ask and in any market shifts, like whenever you have dynamic conditions, like a Trump presidency, this happened to us in his earlier like when the COVID thing had hit, because market conditions kept shifting, automated systems that find what has changed is much more effective than static systems. So that's why the likes of snowflake are highlighting us. Google BigQuery has highlighted us at the example.
be fun to collaborate on that. What we have found is we can often when we work closely with one of our technical partners, and we have done the same thing with Intel previously. We work very closely with our our partners, and we actually often find them surprising things in their processes, because we push them in ways that others are not pushing it, and then we find stuff with them. And so this could be a fun part of the collaboration. It at least has been with both Intel and with Nvidia, right? The second part was at Gartner. Again, we'll get you the video. And the last part was snowflake just presented three case studies at Mobile World Congress. And one of the key things there was this finding unknown unknowns in the data. So if you think about something like snowflake, which has their own AI system, their own NLQ system, they are highlighting us, because what happens enable is we'll go in and ask millions of questions of your data completely automatically, but at very low cost, and then highlight the key insights. So everyone else in analytics is taking the power of AI and putting it behind the human ability to ask a question, but a human asks. There are 2030, questions and stops, but there are millions of questions you could ask and in any market shifts, like whenever you have dynamic conditions, like a Trump presidency, this happened to us in his earlier like when the COVID thing had hit, because market conditions kept shifting, automated systems that find what has changed is much more effective than static systems. So that's why the likes of snowflake are highlighting us. Google BigQuery has highlighted us at the example.
be fun to collaborate on that. What we have found is we can often when we work closely with one of our technical partners, and we have done the same thing with Intel previously. We work very closely with our our partners, and we actually often find them surprising things in their processes, because we push them in ways that others are not pushing it, and then we find stuff with them. And so this could be a fun part of the collaboration. It at least has been with both Intel and with Nvidia, right? The second part was at Gartner. Again, we'll get you the video. And the last part was snowflake just presented three case studies at Mobile World Congress. And one of the key things there was this finding unknown unknowns in the data. So if you think about something like snowflake, which has their own AI system, their own NLQ system, they are highlighting us, because what happens enable is we'll go in and ask millions of questions of your data completely automatically, but at very low cost, and then highlight the key insights. So everyone else in analytics is taking the power of AI and putting it behind the human ability to ask a question, but a human asks. There are 2030, questions and stops, but there are millions of questions you could ask and in any market shifts, like whenever you have dynamic conditions, like a Trump presidency, this happened to us in his earlier like when the COVID thing had hit, because market conditions kept shifting, automated systems that find what has changed is much more effective than static systems. So that's why the likes of snowflake are highlighting us. Google BigQuery has highlighted us at the example.
be fun to collaborate on that. What we have found is we can often when we work closely with one of our technical partners, and we have done the same thing with Intel previously. We work very closely with our our partners, and we actually often find them surprising things in their processes, because we push them in ways that others are not pushing it, and then we find stuff with them. And so this could be a fun part of the collaboration. It at least has been with both Intel and with Nvidia, right? The second part was at Gartner. Again, we'll get you the video. And the last part was snowflake just presented three case studies at Mobile World Congress. And one of the key things there was this finding unknown unknowns in the data. So if you think about something like snowflake, which has their own AI system, their own NLQ system, they are highlighting us, because what happens enable is we'll go in and ask millions of questions of your data completely automatically, but at very low cost, and then highlight the key insights. So everyone else in analytics is taking the power of AI and putting it behind the human ability to ask a question, but a human asks. There are 2030, questions and stops, but there are millions of questions you could ask and in any market shifts, like whenever you have dynamic conditions, like a Trump presidency, this happened to us in his earlier like when the COVID thing had hit, because market conditions kept shifting, automated systems that find what has changed is much more effective than static systems. So that's why the likes of snowflake are highlighting us. Google BigQuery has highlighted us at the example.
S Speaker 118:53And then, how do you work with these companies? Which is, which is somebody like snowflake, takes you deploy with snowflake, and then they take you to customers. So
And then, how do you work with these companies? Which is, which is somebody like snowflake, takes you deploy with snowflake, and then they take you to customers. So
And then, how do you work with these companies? Which is, which is somebody like snowflake, takes you deploy with snowflake, and then they take you to customers. So
And then, how do you work with these companies? Which is, which is somebody like snowflake, takes you deploy with snowflake, and then they take you to customers. So
S Speaker 219:05snowflake partnership is very new, but Google, we are available on Google paper, we are available on Microsoft paper, we are available on AWS paper, and
snowflake partnership is very new, but Google, we are available on Google paper, we are available on Microsoft paper, we are available on AWS paper, and
snowflake partnership is very new, but Google, we are available on Google paper, we are available on Microsoft paper, we are available on AWS paper, and
snowflake partnership is very new, but Google, we are available on Google paper, we are available on Microsoft paper, we are available on AWS paper, and
19:15then those in AWS bedrock in
then those in AWS bedrock in
then those in AWS bedrock in
then those in AWS bedrock in
S Speaker 219:19on their marketplaces, right? We actually don't use bedrock because we are way more efficient than bedrock. We could. We can use bedrock If a customer asks for it, but we are way more efficient than bedrock,
on their marketplaces, right? We actually don't use bedrock because we are way more efficient than bedrock. We could. We can use bedrock If a customer asks for it, but we are way more efficient than bedrock,
on their marketplaces, right? We actually don't use bedrock because we are way more efficient than bedrock. We could. We can use bedrock If a customer asks for it, but we are way more efficient than bedrock,
on their marketplaces, right? We actually don't use bedrock because we are way more efficient than bedrock. We could. We can use bedrock If a customer asks for it, but we are way more efficient than bedrock,
19:31okay, right? Similarly,
okay, right? Similarly,
okay, right? Similarly,
okay, right? Similarly,
S Speaker 219:33with Google, we don't use vertex AI. We started with vertex AI. We have the ability to support it, but you have way more efficient than vertex AI. So we kept those capabilities in there, because once in a blue moon, a customer will say, No, I want to run it fully native on vertex AI. I want to run it fully native on bedrock. We still have that capability because, you know, we want to support whatever customer wants. And snowflake, we are going on their paper. We are going through the process able does leave all the data where it is. So we don't move data out of snowflake, we don't move data out of BigQuery. And that makes it much more attractive for these partners, because when a customer is using us, they're putting workloads into the underlying data platform.
with Google, we don't use vertex AI. We started with vertex AI. We have the ability to support it, but you have way more efficient than vertex AI. So we kept those capabilities in there, because once in a blue moon, a customer will say, No, I want to run it fully native on vertex AI. I want to run it fully native on bedrock. We still have that capability because, you know, we want to support whatever customer wants. And snowflake, we are going on their paper. We are going through the process able does leave all the data where it is. So we don't move data out of snowflake, we don't move data out of BigQuery. And that makes it much more attractive for these partners, because when a customer is using us, they're putting workloads into the underlying data platform.
with Google, we don't use vertex AI. We started with vertex AI. We have the ability to support it, but you have way more efficient than vertex AI. So we kept those capabilities in there, because once in a blue moon, a customer will say, No, I want to run it fully native on vertex AI. I want to run it fully native on bedrock. We still have that capability because, you know, we want to support whatever customer wants. And snowflake, we are going on their paper. We are going through the process able does leave all the data where it is. So we don't move data out of snowflake, we don't move data out of BigQuery. And that makes it much more attractive for these partners, because when a customer is using us, they're putting workloads into the underlying data platform.
with Google, we don't use vertex AI. We started with vertex AI. We have the ability to support it, but you have way more efficient than vertex AI. So we kept those capabilities in there, because once in a blue moon, a customer will say, No, I want to run it fully native on vertex AI. I want to run it fully native on bedrock. We still have that capability because, you know, we want to support whatever customer wants. And snowflake, we are going on their paper. We are going through the process able does leave all the data where it is. So we don't move data out of snowflake, we don't move data out of BigQuery. And that makes it much more attractive for these partners, because when a customer is using us, they're putting workloads into the underlying data platform.
S Speaker 120:21Harry Ji again, so, which is, so, which is, maybe we jump to that part, which is, what is the problem statement that the customers come to you with? And then, how do you solve that problem for them? And then how do you deploy it for them? Which is, I think those pieces are currently I'm trying to figure out, okay, how do those pieces align?
Harry Ji again, so, which is, so, which is, maybe we jump to that part, which is, what is the problem statement that the customers come to you with? And then, how do you solve that problem for them? And then how do you deploy it for them? Which is, I think those pieces are currently I'm trying to figure out, okay, how do those pieces align?
Harry Ji again, so, which is, so, which is, maybe we jump to that part, which is, what is the problem statement that the customers come to you with? And then, how do you solve that problem for them? And then how do you deploy it for them? Which is, I think those pieces are currently I'm trying to figure out, okay, how do those pieces align?
Harry Ji again, so, which is, so, which is, maybe we jump to that part, which is, what is the problem statement that the customers come to you with? And then, how do you solve that problem for them? And then how do you deploy it for them? Which is, I think those pieces are currently I'm trying to figure out, okay, how do those pieces align?
S Speaker 220:43Absolutely, walk you through it. So our basic premise is that you're going to have lot of different agents, right? You're going to have agents that are analytics oriented, you're going to have agents that are document oriented, you have agents that are process oriented. So we build this thing to support a lot of different use cases, like for this one, this was a, you know, figuring out underlying patterns and customer reviews that the organization had not unlocked before, right? So, having automatically the system go in and get every week's information, every day's information by persona, finding the underlying patterns to them, right?
Absolutely, walk you through it. So our basic premise is that you're going to have lot of different agents, right? You're going to have agents that are analytics oriented, you're going to have agents that are document oriented, you have agents that are process oriented. So we build this thing to support a lot of different use cases, like for this one, this was a, you know, figuring out underlying patterns and customer reviews that the organization had not unlocked before, right? So, having automatically the system go in and get every week's information, every day's information by persona, finding the underlying patterns to them, right?
Absolutely, walk you through it. So our basic premise is that you're going to have lot of different agents, right? You're going to have agents that are analytics oriented, you're going to have agents that are document oriented, you have agents that are process oriented. So we build this thing to support a lot of different use cases, like for this one, this was a, you know, figuring out underlying patterns and customer reviews that the organization had not unlocked before, right? So, having automatically the system go in and get every week's information, every day's information by persona, finding the underlying patterns to them, right?
Absolutely, walk you through it. So our basic premise is that you're going to have lot of different agents, right? You're going to have agents that are analytics oriented, you're going to have agents that are document oriented, you have agents that are process oriented. So we build this thing to support a lot of different use cases, like for this one, this was a, you know, figuring out underlying patterns and customer reviews that the organization had not unlocked before, right? So, having automatically the system go in and get every week's information, every day's information by persona, finding the underlying patterns to them, right?
S Speaker 121:22Which? Is which one? The first one. So healthcare retailer, what was the like? What was the problem statement there,
Which? Is which one? The first one. So healthcare retailer, what was the like? What was the problem statement there,
Which? Is which one? The first one. So healthcare retailer, what was the like? What was the problem statement there,
Which? Is which one? The first one. So healthcare retailer, what was the like? What was the problem statement there,
S Speaker 124:21these customers, which is sort of models that you use. These are models that you developed, or these are models that you use from open source, or the likes of open air anthropic, and then you deploy them in
these customers, which is sort of models that you use. These are models that you developed, or these are models that you use from open source, or the likes of open air anthropic, and then you deploy them in
these customers, which is sort of models that you use. These are models that you developed, or these are models that you use from open source, or the likes of open air anthropic, and then you deploy them in
these customers, which is sort of models that you use. These are models that you developed, or these are models that you use from open source, or the likes of open air anthropic, and then you deploy them in
24:37a cheaper fashion, so
a cheaper fashion, so
a cheaper fashion, so
a cheaper fashion, so
S Speaker 224:39we support whatever models they want us to use. So we will even support things like Gemini and OpenAI if the customer wants to use it, right. But what we announced at GTC, and we are shifting most of our customers to this model, is that when we run these special, fine tuned models with your reasoning models, and I'm going to come to it in a couple of slides again, maybe I need to pick up my pace, like all the questions you're asking. I literally was like coming in a few minutes. But what we are shifting people over to what we are calling these able intern models, which are based on Lama, which are based on other open source things, nemotron and others. And the key thing we did is we put a lot of effort into have the reasoning steps be designed for business users. We make it possible to provide feedback on the reasoning steps, and that actually makes for much more efficient post training. So think of it as like today. You take a bunch of smart kids and lock them up in a room, and you give them test questions, and you're like, Hey, you got this question right. You got this question wrong, but you're not really telling them why they got it right, why they got it wrong, because you also don't know how they got to that answer right. Enable you are saying, hey, on step three, you've got the definition of gross margin wrong. So it becomes much more efficient feedback, and for the users, it's a much better experience. So that's where we are shifting to the ABLE internal models as our preferred way to work with customers, but they're just new. We just did our first wall to wall, this shift from 500k to 1.5 mil. That happened two weeks back. So right after Gartner, and it was at Gartner that we closed that deal, and that is going to be all able to intern models. What they're doing is 1.5 million for non production use wallet to wall, and then we'll make 100k per agent they take into production. So we are expecting to get to about a ten million in ARR there over the next 18 months. And I'll walk you through the details right. The first 20 agents have already been identified. We are literally four, four days a week. They are spending an hour building agents on able and the goal they have set for themselves is they want to build 1000 agent prototypes in 2025, and hopefully take 100 into production. If they take 100 into production, we'll do quite well, okay?
we support whatever models they want us to use. So we will even support things like Gemini and OpenAI if the customer wants to use it, right. But what we announced at GTC, and we are shifting most of our customers to this model, is that when we run these special, fine tuned models with your reasoning models, and I'm going to come to it in a couple of slides again, maybe I need to pick up my pace, like all the questions you're asking. I literally was like coming in a few minutes. But what we are shifting people over to what we are calling these able intern models, which are based on Lama, which are based on other open source things, nemotron and others. And the key thing we did is we put a lot of effort into have the reasoning steps be designed for business users. We make it possible to provide feedback on the reasoning steps, and that actually makes for much more efficient post training. So think of it as like today. You take a bunch of smart kids and lock them up in a room, and you give them test questions, and you're like, Hey, you got this question right. You got this question wrong, but you're not really telling them why they got it right, why they got it wrong, because you also don't know how they got to that answer right. Enable you are saying, hey, on step three, you've got the definition of gross margin wrong. So it becomes much more efficient feedback, and for the users, it's a much better experience. So that's where we are shifting to the ABLE internal models as our preferred way to work with customers, but they're just new. We just did our first wall to wall, this shift from 500k to 1.5 mil. That happened two weeks back. So right after Gartner, and it was at Gartner that we closed that deal, and that is going to be all able to intern models. What they're doing is 1.5 million for non production use wallet to wall, and then we'll make 100k per agent they take into production. So we are expecting to get to about a ten million in ARR there over the next 18 months. And I'll walk you through the details right. The first 20 agents have already been identified. We are literally four, four days a week. They are spending an hour building agents on able and the goal they have set for themselves is they want to build 1000 agent prototypes in 2025, and hopefully take 100 into production. If they take 100 into production, we'll do quite well, okay?
we support whatever models they want us to use. So we will even support things like Gemini and OpenAI if the customer wants to use it, right. But what we announced at GTC, and we are shifting most of our customers to this model, is that when we run these special, fine tuned models with your reasoning models, and I'm going to come to it in a couple of slides again, maybe I need to pick up my pace, like all the questions you're asking. I literally was like coming in a few minutes. But what we are shifting people over to what we are calling these able intern models, which are based on Lama, which are based on other open source things, nemotron and others. And the key thing we did is we put a lot of effort into have the reasoning steps be designed for business users. We make it possible to provide feedback on the reasoning steps, and that actually makes for much more efficient post training. So think of it as like today. You take a bunch of smart kids and lock them up in a room, and you give them test questions, and you're like, Hey, you got this question right. You got this question wrong, but you're not really telling them why they got it right, why they got it wrong, because you also don't know how they got to that answer right. Enable you are saying, hey, on step three, you've got the definition of gross margin wrong. So it becomes much more efficient feedback, and for the users, it's a much better experience. So that's where we are shifting to the ABLE internal models as our preferred way to work with customers, but they're just new. We just did our first wall to wall, this shift from 500k to 1.5 mil. That happened two weeks back. So right after Gartner, and it was at Gartner that we closed that deal, and that is going to be all able to intern models. What they're doing is 1.5 million for non production use wallet to wall, and then we'll make 100k per agent they take into production. So we are expecting to get to about a ten million in ARR there over the next 18 months. And I'll walk you through the details right. The first 20 agents have already been identified. We are literally four, four days a week. They are spending an hour building agents on able and the goal they have set for themselves is they want to build 1000 agent prototypes in 2025, and hopefully take 100 into production. If they take 100 into production, we'll do quite well, okay?
we support whatever models they want us to use. So we will even support things like Gemini and OpenAI if the customer wants to use it, right. But what we announced at GTC, and we are shifting most of our customers to this model, is that when we run these special, fine tuned models with your reasoning models, and I'm going to come to it in a couple of slides again, maybe I need to pick up my pace, like all the questions you're asking. I literally was like coming in a few minutes. But what we are shifting people over to what we are calling these able intern models, which are based on Lama, which are based on other open source things, nemotron and others. And the key thing we did is we put a lot of effort into have the reasoning steps be designed for business users. We make it possible to provide feedback on the reasoning steps, and that actually makes for much more efficient post training. So think of it as like today. You take a bunch of smart kids and lock them up in a room, and you give them test questions, and you're like, Hey, you got this question right. You got this question wrong, but you're not really telling them why they got it right, why they got it wrong, because you also don't know how they got to that answer right. Enable you are saying, hey, on step three, you've got the definition of gross margin wrong. So it becomes much more efficient feedback, and for the users, it's a much better experience. So that's where we are shifting to the ABLE internal models as our preferred way to work with customers, but they're just new. We just did our first wall to wall, this shift from 500k to 1.5 mil. That happened two weeks back. So right after Gartner, and it was at Gartner that we closed that deal, and that is going to be all able to intern models. What they're doing is 1.5 million for non production use wallet to wall, and then we'll make 100k per agent they take into production. So we are expecting to get to about a ten million in ARR there over the next 18 months. And I'll walk you through the details right. The first 20 agents have already been identified. We are literally four, four days a week. They are spending an hour building agents on able and the goal they have set for themselves is they want to build 1000 agent prototypes in 2025, and hopefully take 100 into production. If they take 100 into production, we'll do quite well, okay?
27:04And this customer,
S Speaker 127:08they So currently there are they deployed? How many agents in production? These
they So currently there are they deployed? How many agents in production? These
they So currently there are they deployed? How many agents in production? These
they So currently there are they deployed? How many agents in production? These
S Speaker 227:14guys have deployed three agents in production, right? Got it so that was the first one. This was the first agent. We sold it to them at 250 K, yeah. Then we sold them two more agents for 250 K more that was in their contract. So the way we structured them is we land with the use cases, so that, rather than trying to tell them about our platform and this, that and the other, we just say, Okay, you we have a use case, we'll, we'll sell you that use case, right? And it's a recurring revenue, so it's like that amount every year, right? And you have the option to buy up to a few more, and then very quickly they say, Okay, why don't you give me the platform? Because you're doing these so fast. My guys can do it themselves. Yeah, so. But what we found was, if you go and try to sell them a platform first, it is lot more confusing. It's easier to land a couple of use cases in than to the expat. So you're asking, how do we do it? Basically, what we have done is, right now we have about 50 horizontal and vertical specific templates. We're going to get it 200 very easily post funding. The main thing that takes us time is we create these generic templates that we are deriving from so that we have much better maintainability. And I'll come back to that, because as underlying models changes, as new GPUs come out and new processors come out, we have to maintain these. Each of these templates are optimized all the way to the hardware level, so we know how to run it optimally for different hardwares. The agents have these reasoning steps that users provide feedback on. This is where we are doing very efficient adjustment to the users preferences. And one of the good things here is, the moment they give feedback, the agent gets better for them. So it's not give feedback, and six months later, maybe it'll get better. It is you give feedback, and the thing does a better job for you right now. So we have unlocked that. The big problem there was always business users are not incentivized to give feedback because they're not getting the payoff from it. We will the payoff into it. A lot of our work is actually around user behavior. So Clay Christensen, the guy who wrote Innovators Dilemma, Clay, was my faculty sponsor for the research that became beyond core. It actually started out of the HBS business plan contest, and so you'll see a lot of k's research in what we do. So this is kind of the job to be done, if you're in all that stuff is built into it. Then as we give feedback, those generalized templates are becoming specialized templates, and then those can be deployed wherever you want. You can deploy it at the edge in the Super chip, you can deploy it in DGX cloud. You can deploy it in any clouds. And that's our basic premises, like wherever the customer needs it to be. And just remember that the edge stuff we just announced. So we don't have customers on the edge yet. We have customers on these two
guys have deployed three agents in production, right? Got it so that was the first one. This was the first agent. We sold it to them at 250 K, yeah. Then we sold them two more agents for 250 K more that was in their contract. So the way we structured them is we land with the use cases, so that, rather than trying to tell them about our platform and this, that and the other, we just say, Okay, you we have a use case, we'll, we'll sell you that use case, right? And it's a recurring revenue, so it's like that amount every year, right? And you have the option to buy up to a few more, and then very quickly they say, Okay, why don't you give me the platform? Because you're doing these so fast. My guys can do it themselves. Yeah, so. But what we found was, if you go and try to sell them a platform first, it is lot more confusing. It's easier to land a couple of use cases in than to the expat. So you're asking, how do we do it? Basically, what we have done is, right now we have about 50 horizontal and vertical specific templates. We're going to get it 200 very easily post funding. The main thing that takes us time is we create these generic templates that we are deriving from so that we have much better maintainability. And I'll come back to that, because as underlying models changes, as new GPUs come out and new processors come out, we have to maintain these. Each of these templates are optimized all the way to the hardware level, so we know how to run it optimally for different hardwares. The agents have these reasoning steps that users provide feedback on. This is where we are doing very efficient adjustment to the users preferences. And one of the good things here is, the moment they give feedback, the agent gets better for them. So it's not give feedback, and six months later, maybe it'll get better. It is you give feedback, and the thing does a better job for you right now. So we have unlocked that. The big problem there was always business users are not incentivized to give feedback because they're not getting the payoff from it. We will the payoff into it. A lot of our work is actually around user behavior. So Clay Christensen, the guy who wrote Innovators Dilemma, Clay, was my faculty sponsor for the research that became beyond core. It actually started out of the HBS business plan contest, and so you'll see a lot of k's research in what we do. So this is kind of the job to be done, if you're in all that stuff is built into it. Then as we give feedback, those generalized templates are becoming specialized templates, and then those can be deployed wherever you want. You can deploy it at the edge in the Super chip, you can deploy it in DGX cloud. You can deploy it in any clouds. And that's our basic premises, like wherever the customer needs it to be. And just remember that the edge stuff we just announced. So we don't have customers on the edge yet. We have customers on these two
guys have deployed three agents in production, right? Got it so that was the first one. This was the first agent. We sold it to them at 250 K, yeah. Then we sold them two more agents for 250 K more that was in their contract. So the way we structured them is we land with the use cases, so that, rather than trying to tell them about our platform and this, that and the other, we just say, Okay, you we have a use case, we'll, we'll sell you that use case, right? And it's a recurring revenue, so it's like that amount every year, right? And you have the option to buy up to a few more, and then very quickly they say, Okay, why don't you give me the platform? Because you're doing these so fast. My guys can do it themselves. Yeah, so. But what we found was, if you go and try to sell them a platform first, it is lot more confusing. It's easier to land a couple of use cases in than to the expat. So you're asking, how do we do it? Basically, what we have done is, right now we have about 50 horizontal and vertical specific templates. We're going to get it 200 very easily post funding. The main thing that takes us time is we create these generic templates that we are deriving from so that we have much better maintainability. And I'll come back to that, because as underlying models changes, as new GPUs come out and new processors come out, we have to maintain these. Each of these templates are optimized all the way to the hardware level, so we know how to run it optimally for different hardwares. The agents have these reasoning steps that users provide feedback on. This is where we are doing very efficient adjustment to the users preferences. And one of the good things here is, the moment they give feedback, the agent gets better for them. So it's not give feedback, and six months later, maybe it'll get better. It is you give feedback, and the thing does a better job for you right now. So we have unlocked that. The big problem there was always business users are not incentivized to give feedback because they're not getting the payoff from it. We will the payoff into it. A lot of our work is actually around user behavior. So Clay Christensen, the guy who wrote Innovators Dilemma, Clay, was my faculty sponsor for the research that became beyond core. It actually started out of the HBS business plan contest, and so you'll see a lot of k's research in what we do. So this is kind of the job to be done, if you're in all that stuff is built into it. Then as we give feedback, those generalized templates are becoming specialized templates, and then those can be deployed wherever you want. You can deploy it at the edge in the Super chip, you can deploy it in DGX cloud. You can deploy it in any clouds. And that's our basic premises, like wherever the customer needs it to be. And just remember that the edge stuff we just announced. So we don't have customers on the edge yet. We have customers on these two
guys have deployed three agents in production, right? Got it so that was the first one. This was the first agent. We sold it to them at 250 K, yeah. Then we sold them two more agents for 250 K more that was in their contract. So the way we structured them is we land with the use cases, so that, rather than trying to tell them about our platform and this, that and the other, we just say, Okay, you we have a use case, we'll, we'll sell you that use case, right? And it's a recurring revenue, so it's like that amount every year, right? And you have the option to buy up to a few more, and then very quickly they say, Okay, why don't you give me the platform? Because you're doing these so fast. My guys can do it themselves. Yeah, so. But what we found was, if you go and try to sell them a platform first, it is lot more confusing. It's easier to land a couple of use cases in than to the expat. So you're asking, how do we do it? Basically, what we have done is, right now we have about 50 horizontal and vertical specific templates. We're going to get it 200 very easily post funding. The main thing that takes us time is we create these generic templates that we are deriving from so that we have much better maintainability. And I'll come back to that, because as underlying models changes, as new GPUs come out and new processors come out, we have to maintain these. Each of these templates are optimized all the way to the hardware level, so we know how to run it optimally for different hardwares. The agents have these reasoning steps that users provide feedback on. This is where we are doing very efficient adjustment to the users preferences. And one of the good things here is, the moment they give feedback, the agent gets better for them. So it's not give feedback, and six months later, maybe it'll get better. It is you give feedback, and the thing does a better job for you right now. So we have unlocked that. The big problem there was always business users are not incentivized to give feedback because they're not getting the payoff from it. We will the payoff into it. A lot of our work is actually around user behavior. So Clay Christensen, the guy who wrote Innovators Dilemma, Clay, was my faculty sponsor for the research that became beyond core. It actually started out of the HBS business plan contest, and so you'll see a lot of k's research in what we do. So this is kind of the job to be done, if you're in all that stuff is built into it. Then as we give feedback, those generalized templates are becoming specialized templates, and then those can be deployed wherever you want. You can deploy it at the edge in the Super chip, you can deploy it in DGX cloud. You can deploy it in any clouds. And that's our basic premises, like wherever the customer needs it to be. And just remember that the edge stuff we just announced. So we don't have customers on the edge yet. We have customers on these two
30:07and then what we do
S Speaker 230:08is, as new tech, new models show up, we update the parent templates, and all the children templates automatically update themselves. And this is very important, otherwise, in a constantly changing tech environment you'll be running after just doing updates of every agent, and you can never evolve out of that. Any questions comments on this? Does this make sense?
is, as new tech, new models show up, we update the parent templates, and all the children templates automatically update themselves. And this is very important, otherwise, in a constantly changing tech environment you'll be running after just doing updates of every agent, and you can never evolve out of that. Any questions comments on this? Does this make sense?
is, as new tech, new models show up, we update the parent templates, and all the children templates automatically update themselves. And this is very important, otherwise, in a constantly changing tech environment you'll be running after just doing updates of every agent, and you can never evolve out of that. Any questions comments on this? Does this make sense?
is, as new tech, new models show up, we update the parent templates, and all the children templates automatically update themselves. And this is very important, otherwise, in a constantly changing tech environment you'll be running after just doing updates of every agent, and you can never evolve out of that. Any questions comments on this? Does this make sense?
30:34Makes sense. So the good thing is, the questions
Makes sense. So the good thing is, the questions
Makes sense. So the good thing is, the questions
Makes sense. So the good thing is, the questions
30:35you were asking, they were genuine in the next few slides. So,
you were asking, they were genuine in the next few slides. So,
you were asking, they were genuine in the next few slides. So,
you were asking, they were genuine in the next few slides. So,
S Speaker 230:41and the way we did this is we basically said, look, let's take the superset of the kinds of things you need to build different kinds of agents, right? So there is a bunch of presentation there stuff. There's a bunch of stuff that is around specializing to that business. Of course, there are a bunch of different kinds of AI models and what we call data value extractors, like you don't want generative AI to be doing math, for example, it will hallucinate even up whatever people say, even if it's a five person hallucination, that is not acceptable to an enterprise. So we really built in all of these pieces. Every piece is built serverless, and it comes up in the customers cloud as needed, or in that processor as needed, which is why we can stuff a lot of capabilities in small spaces. And you might have an extremely simple use case, like the summarization with Q and A sure that works very simply. You can have a more complicated thing, the Gen ai plus augmented analytics. This is a three step, actually four step, three model use case. Or you can end up with a really complex end to end agent use case. This is a procure to pay, right? Actually, sorry, this is a this order to cash. This is not a procure to pay. This is an order to cash and just the right components get used. That's it. This is why we can be so flexible with customers. We are not hand coding solutions for customers. We are getting them started on the right combination of pre existing tech that they are then customizing through their feedback. Does that make sense? And all the data stays where it is. We are never moving data. This is another really important part of it. So from a market opportunity part, of course, we are going after the Gen AI market. So
and the way we did this is we basically said, look, let's take the superset of the kinds of things you need to build different kinds of agents, right? So there is a bunch of presentation there stuff. There's a bunch of stuff that is around specializing to that business. Of course, there are a bunch of different kinds of AI models and what we call data value extractors, like you don't want generative AI to be doing math, for example, it will hallucinate even up whatever people say, even if it's a five person hallucination, that is not acceptable to an enterprise. So we really built in all of these pieces. Every piece is built serverless, and it comes up in the customers cloud as needed, or in that processor as needed, which is why we can stuff a lot of capabilities in small spaces. And you might have an extremely simple use case, like the summarization with Q and A sure that works very simply. You can have a more complicated thing, the Gen ai plus augmented analytics. This is a three step, actually four step, three model use case. Or you can end up with a really complex end to end agent use case. This is a procure to pay, right? Actually, sorry, this is a this order to cash. This is not a procure to pay. This is an order to cash and just the right components get used. That's it. This is why we can be so flexible with customers. We are not hand coding solutions for customers. We are getting them started on the right combination of pre existing tech that they are then customizing through their feedback. Does that make sense? And all the data stays where it is. We are never moving data. This is another really important part of it. So from a market opportunity part, of course, we are going after the Gen AI market. So
and the way we did this is we basically said, look, let's take the superset of the kinds of things you need to build different kinds of agents, right? So there is a bunch of presentation there stuff. There's a bunch of stuff that is around specializing to that business. Of course, there are a bunch of different kinds of AI models and what we call data value extractors, like you don't want generative AI to be doing math, for example, it will hallucinate even up whatever people say, even if it's a five person hallucination, that is not acceptable to an enterprise. So we really built in all of these pieces. Every piece is built serverless, and it comes up in the customers cloud as needed, or in that processor as needed, which is why we can stuff a lot of capabilities in small spaces. And you might have an extremely simple use case, like the summarization with Q and A sure that works very simply. You can have a more complicated thing, the Gen ai plus augmented analytics. This is a three step, actually four step, three model use case. Or you can end up with a really complex end to end agent use case. This is a procure to pay, right? Actually, sorry, this is a this order to cash. This is not a procure to pay. This is an order to cash and just the right components get used. That's it. This is why we can be so flexible with customers. We are not hand coding solutions for customers. We are getting them started on the right combination of pre existing tech that they are then customizing through their feedback. Does that make sense? And all the data stays where it is. We are never moving data. This is another really important part of it. So from a market opportunity part, of course, we are going after the Gen AI market. So
and the way we did this is we basically said, look, let's take the superset of the kinds of things you need to build different kinds of agents, right? So there is a bunch of presentation there stuff. There's a bunch of stuff that is around specializing to that business. Of course, there are a bunch of different kinds of AI models and what we call data value extractors, like you don't want generative AI to be doing math, for example, it will hallucinate even up whatever people say, even if it's a five person hallucination, that is not acceptable to an enterprise. So we really built in all of these pieces. Every piece is built serverless, and it comes up in the customers cloud as needed, or in that processor as needed, which is why we can stuff a lot of capabilities in small spaces. And you might have an extremely simple use case, like the summarization with Q and A sure that works very simply. You can have a more complicated thing, the Gen ai plus augmented analytics. This is a three step, actually four step, three model use case. Or you can end up with a really complex end to end agent use case. This is a procure to pay, right? Actually, sorry, this is a this order to cash. This is not a procure to pay. This is an order to cash and just the right components get used. That's it. This is why we can be so flexible with customers. We are not hand coding solutions for customers. We are getting them started on the right combination of pre existing tech that they are then customizing through their feedback. Does that make sense? And all the data stays where it is. We are never moving data. This is another really important part of it. So from a market opportunity part, of course, we are going after the Gen AI market. So
32:27good market. I We all agree, market opportunities big,
good market. I We all agree, market opportunities big,
good market. I We all agree, market opportunities big,
good market. I We all agree, market opportunities big,
S Speaker 232:30okay, I was told, have to put it in. I'm glad you. Glad you feel that bit. So let's move on to this one where, from our perspective, AI agents, post training. Reasoning models and super chips are coming together right now. Now, the thing is, the agents are slow, expensive, and they need to be customized. The post training is way too dependent on data scientists right now. Reasoning models are great, but they're slow, expensive, and the ones from open, AI and Google are not adjustable. We can't actually finding them, and whether deep seek does not take instruction well. So these things actually are not working too well. And then the super chips are very powerful technology, but you have to rethink your code. If you just did something that worked on the h1 100 and put it on the G, h2 100, it just won't work. It you won't get any benefit, like you'll be paying more for something and not get a benefit, right? So what we did is we went in and we really got to the combination of these four things. So firstly, we did this custom reasoning NLP benchmark where we were able to go in for less than five bucks, add a reasoning to that model and beat the biggest models on that use case. And the reason we're able to do it is because this is a very, very specific model. But when I can completely automate this and the cost is just by bugs, customers will actually end up with many, many specialized models. And in our full research paper, you'll see we also worked on, how can I serve many, many specialized models at the same time, like, you know, what happens if I have 10 different Lora layers? We actually were able to show that the cost concurrency curve doesn't get damaged, just the appropriate concurrency setting has to be changed. So we did a lot of work to do this from the fine tuning side, from the optimization side to make this possible. And what we do is, through that same process, we're also adding reasoning to base models, including 3.1 which does not do reasoning. We have also done it with 3.3 but we deliberately started from 311 3.1 to prove the point that we can add it to things that didn't happen. Right on the right hand side, this is the feedback cycle, where you go in and you provide feedback on the reasoning steps themselves. So in this case, the bag is too far away from people. The model got it wrong. You provided feedback and say, hey, it's too far away. The people are too far away, and they interns from them. Now the result of that is, we can go in and build these wide variety of use cases with just a little bit of a feedback cycle, right? It's not years of consulting. It is just little bit of feedback. And the last part of this is we are efficiently using the super chips to get two to five times faster agentic flows. And so
okay, I was told, have to put it in. I'm glad you. Glad you feel that bit. So let's move on to this one where, from our perspective, AI agents, post training. Reasoning models and super chips are coming together right now. Now, the thing is, the agents are slow, expensive, and they need to be customized. The post training is way too dependent on data scientists right now. Reasoning models are great, but they're slow, expensive, and the ones from open, AI and Google are not adjustable. We can't actually finding them, and whether deep seek does not take instruction well. So these things actually are not working too well. And then the super chips are very powerful technology, but you have to rethink your code. If you just did something that worked on the h1 100 and put it on the G, h2 100, it just won't work. It you won't get any benefit, like you'll be paying more for something and not get a benefit, right? So what we did is we went in and we really got to the combination of these four things. So firstly, we did this custom reasoning NLP benchmark where we were able to go in for less than five bucks, add a reasoning to that model and beat the biggest models on that use case. And the reason we're able to do it is because this is a very, very specific model. But when I can completely automate this and the cost is just by bugs, customers will actually end up with many, many specialized models. And in our full research paper, you'll see we also worked on, how can I serve many, many specialized models at the same time, like, you know, what happens if I have 10 different Lora layers? We actually were able to show that the cost concurrency curve doesn't get damaged, just the appropriate concurrency setting has to be changed. So we did a lot of work to do this from the fine tuning side, from the optimization side to make this possible. And what we do is, through that same process, we're also adding reasoning to base models, including 3.1 which does not do reasoning. We have also done it with 3.3 but we deliberately started from 311 3.1 to prove the point that we can add it to things that didn't happen. Right on the right hand side, this is the feedback cycle, where you go in and you provide feedback on the reasoning steps themselves. So in this case, the bag is too far away from people. The model got it wrong. You provided feedback and say, hey, it's too far away. The people are too far away, and they interns from them. Now the result of that is, we can go in and build these wide variety of use cases with just a little bit of a feedback cycle, right? It's not years of consulting. It is just little bit of feedback. And the last part of this is we are efficiently using the super chips to get two to five times faster agentic flows. And so
okay, I was told, have to put it in. I'm glad you. Glad you feel that bit. So let's move on to this one where, from our perspective, AI agents, post training. Reasoning models and super chips are coming together right now. Now, the thing is, the agents are slow, expensive, and they need to be customized. The post training is way too dependent on data scientists right now. Reasoning models are great, but they're slow, expensive, and the ones from open, AI and Google are not adjustable. We can't actually finding them, and whether deep seek does not take instruction well. So these things actually are not working too well. And then the super chips are very powerful technology, but you have to rethink your code. If you just did something that worked on the h1 100 and put it on the G, h2 100, it just won't work. It you won't get any benefit, like you'll be paying more for something and not get a benefit, right? So what we did is we went in and we really got to the combination of these four things. So firstly, we did this custom reasoning NLP benchmark where we were able to go in for less than five bucks, add a reasoning to that model and beat the biggest models on that use case. And the reason we're able to do it is because this is a very, very specific model. But when I can completely automate this and the cost is just by bugs, customers will actually end up with many, many specialized models. And in our full research paper, you'll see we also worked on, how can I serve many, many specialized models at the same time, like, you know, what happens if I have 10 different Lora layers? We actually were able to show that the cost concurrency curve doesn't get damaged, just the appropriate concurrency setting has to be changed. So we did a lot of work to do this from the fine tuning side, from the optimization side to make this possible. And what we do is, through that same process, we're also adding reasoning to base models, including 3.1 which does not do reasoning. We have also done it with 3.3 but we deliberately started from 311 3.1 to prove the point that we can add it to things that didn't happen. Right on the right hand side, this is the feedback cycle, where you go in and you provide feedback on the reasoning steps themselves. So in this case, the bag is too far away from people. The model got it wrong. You provided feedback and say, hey, it's too far away. The people are too far away, and they interns from them. Now the result of that is, we can go in and build these wide variety of use cases with just a little bit of a feedback cycle, right? It's not years of consulting. It is just little bit of feedback. And the last part of this is we are efficiently using the super chips to get two to five times faster agentic flows. And so
okay, I was told, have to put it in. I'm glad you. Glad you feel that bit. So let's move on to this one where, from our perspective, AI agents, post training. Reasoning models and super chips are coming together right now. Now, the thing is, the agents are slow, expensive, and they need to be customized. The post training is way too dependent on data scientists right now. Reasoning models are great, but they're slow, expensive, and the ones from open, AI and Google are not adjustable. We can't actually finding them, and whether deep seek does not take instruction well. So these things actually are not working too well. And then the super chips are very powerful technology, but you have to rethink your code. If you just did something that worked on the h1 100 and put it on the G, h2 100, it just won't work. It you won't get any benefit, like you'll be paying more for something and not get a benefit, right? So what we did is we went in and we really got to the combination of these four things. So firstly, we did this custom reasoning NLP benchmark where we were able to go in for less than five bucks, add a reasoning to that model and beat the biggest models on that use case. And the reason we're able to do it is because this is a very, very specific model. But when I can completely automate this and the cost is just by bugs, customers will actually end up with many, many specialized models. And in our full research paper, you'll see we also worked on, how can I serve many, many specialized models at the same time, like, you know, what happens if I have 10 different Lora layers? We actually were able to show that the cost concurrency curve doesn't get damaged, just the appropriate concurrency setting has to be changed. So we did a lot of work to do this from the fine tuning side, from the optimization side to make this possible. And what we do is, through that same process, we're also adding reasoning to base models, including 3.1 which does not do reasoning. We have also done it with 3.3 but we deliberately started from 311 3.1 to prove the point that we can add it to things that didn't happen. Right on the right hand side, this is the feedback cycle, where you go in and you provide feedback on the reasoning steps themselves. So in this case, the bag is too far away from people. The model got it wrong. You provided feedback and say, hey, it's too far away. The people are too far away, and they interns from them. Now the result of that is, we can go in and build these wide variety of use cases with just a little bit of a feedback cycle, right? It's not years of consulting. It is just little bit of feedback. And the last part of this is we are efficiently using the super chips to get two to five times faster agentic flows. And so
S Speaker 135:24on. The on this benchmark, which is so this is your version of Lama, 8 billion instruct, post trained. And then on the,
on. The on this benchmark, which is so this is your version of Lama, 8 billion instruct, post trained. And then on the,
on. The on this benchmark, which is so this is your version of Lama, 8 billion instruct, post trained. And then on the,
on. The on this benchmark, which is so this is your version of Lama, 8 billion instruct, post trained. And then on the,
35:40those are the generic ones. Got it
those are the generic ones. Got it
those are the generic ones. Got it
those are the generic ones. Got it
S Speaker 135:50is that an example of a model that you're deploying, or is an example
is that an example of a model that you're deploying, or is an example
is that an example of a model that you're deploying, or is an example
is that an example of a model that you're deploying, or is an example
S Speaker 235:54of a model? So what we do is we have the ABLE base intern model for SQL. Yeah, we have a able base intern model that we are working on for a few other ones, like for analytics. We are right now using the same base intern, but we'll specialize it. It's just a matter of having the resources like we have, you know, and then we specialize it to the use case. This is once we have specialized to the use case.
of a model? So what we do is we have the ABLE base intern model for SQL. Yeah, we have a able base intern model that we are working on for a few other ones, like for analytics. We are right now using the same base intern, but we'll specialize it. It's just a matter of having the resources like we have, you know, and then we specialize it to the use case. This is once we have specialized to the use case.
of a model? So what we do is we have the ABLE base intern model for SQL. Yeah, we have a able base intern model that we are working on for a few other ones, like for analytics. We are right now using the same base intern, but we'll specialize it. It's just a matter of having the resources like we have, you know, and then we specialize it to the use case. This is once we have specialized to the use case.
of a model? So what we do is we have the ABLE base intern model for SQL. Yeah, we have a able base intern model that we are working on for a few other ones, like for analytics. We are right now using the same base intern, but we'll specialize it. It's just a matter of having the resources like we have, you know, and then we specialize it to the use case. This is once we have specialized to the use case.
36:18Okay, right? Yeah.
S Speaker 236:22Thanks. Now, one of the important things here is, if you think about the mode, we always say that one model to rule them all actually doesn't give you a mode. Because if you really got Artificial General Intelligence, I could move from one AGI to another AGI, right? The AGI does it all, but what happens in the real world is having that serverless infrastructure so I can run 1000s of agents at scale, efficiently. That becomes very important. This ability to do the template lineage, so that when models change, I'm not going into 1000 agents and trying to update them. I just do it at the parent and everything gets handled. That becomes very important. The automated post training, we are constantly improving. The moment you give feedback, the thing improves the moment we get 100 bits of feedback. We do fine tuning, the moment we get 1000 bits of feedback, we do reinforcement learning. And we are constantly automatically testing. Because just because you have fine tuned a model doesn't mean it's better. You actually have to test it and do that AZ testing, and we have all of that built in. Now we do let customers take out their training data, so we are doing this in their cloud. And our basic premise is we'll tell customers, hey guys, you can always take out the training data. But the reality is, they can't replicate the system of value, right? Because if, if I, if I had a cake that was working, our card that is working, or give you all the ingredients and the blueprint you got to assemble the card again, and how you going to go assemble 1000 agents from scratch, even if I've given you every agents fine tuning data.
Thanks. Now, one of the important things here is, if you think about the mode, we always say that one model to rule them all actually doesn't give you a mode. Because if you really got Artificial General Intelligence, I could move from one AGI to another AGI, right? The AGI does it all, but what happens in the real world is having that serverless infrastructure so I can run 1000s of agents at scale, efficiently. That becomes very important. This ability to do the template lineage, so that when models change, I'm not going into 1000 agents and trying to update them. I just do it at the parent and everything gets handled. That becomes very important. The automated post training, we are constantly improving. The moment you give feedback, the thing improves the moment we get 100 bits of feedback. We do fine tuning, the moment we get 1000 bits of feedback, we do reinforcement learning. And we are constantly automatically testing. Because just because you have fine tuned a model doesn't mean it's better. You actually have to test it and do that AZ testing, and we have all of that built in. Now we do let customers take out their training data, so we are doing this in their cloud. And our basic premise is we'll tell customers, hey guys, you can always take out the training data. But the reality is, they can't replicate the system of value, right? Because if, if I, if I had a cake that was working, our card that is working, or give you all the ingredients and the blueprint you got to assemble the card again, and how you going to go assemble 1000 agents from scratch, even if I've given you every agents fine tuning data.
Thanks. Now, one of the important things here is, if you think about the mode, we always say that one model to rule them all actually doesn't give you a mode. Because if you really got Artificial General Intelligence, I could move from one AGI to another AGI, right? The AGI does it all, but what happens in the real world is having that serverless infrastructure so I can run 1000s of agents at scale, efficiently. That becomes very important. This ability to do the template lineage, so that when models change, I'm not going into 1000 agents and trying to update them. I just do it at the parent and everything gets handled. That becomes very important. The automated post training, we are constantly improving. The moment you give feedback, the thing improves the moment we get 100 bits of feedback. We do fine tuning, the moment we get 1000 bits of feedback, we do reinforcement learning. And we are constantly automatically testing. Because just because you have fine tuned a model doesn't mean it's better. You actually have to test it and do that AZ testing, and we have all of that built in. Now we do let customers take out their training data, so we are doing this in their cloud. And our basic premise is we'll tell customers, hey guys, you can always take out the training data. But the reality is, they can't replicate the system of value, right? Because if, if I, if I had a cake that was working, our card that is working, or give you all the ingredients and the blueprint you got to assemble the card again, and how you going to go assemble 1000 agents from scratch, even if I've given you every agents fine tuning data.
Thanks. Now, one of the important things here is, if you think about the mode, we always say that one model to rule them all actually doesn't give you a mode. Because if you really got Artificial General Intelligence, I could move from one AGI to another AGI, right? The AGI does it all, but what happens in the real world is having that serverless infrastructure so I can run 1000s of agents at scale, efficiently. That becomes very important. This ability to do the template lineage, so that when models change, I'm not going into 1000 agents and trying to update them. I just do it at the parent and everything gets handled. That becomes very important. The automated post training, we are constantly improving. The moment you give feedback, the thing improves the moment we get 100 bits of feedback. We do fine tuning, the moment we get 1000 bits of feedback, we do reinforcement learning. And we are constantly automatically testing. Because just because you have fine tuned a model doesn't mean it's better. You actually have to test it and do that AZ testing, and we have all of that built in. Now we do let customers take out their training data, so we are doing this in their cloud. And our basic premise is we'll tell customers, hey guys, you can always take out the training data. But the reality is, they can't replicate the system of value, right? Because if, if I, if I had a cake that was working, our card that is working, or give you all the ingredients and the blueprint you got to assemble the card again, and how you going to go assemble 1000 agents from scratch, even if I've given you every agents fine tuning data.
37:52Does that make sense?
Does that make sense?
Does that make sense?
Does that make sense?
S Speaker 137:54So then here every agent that you like for the use cases that you describe. What do you produce? Like, is there one interface that you provide to everybody, which is you ingest your data? Here's a way to ingest your data. And then, like, for example, you had use cases for again, I believe that healthcare provider wanted to learn about reviews. And then you had some other use cases across, Jonathan, I'm gonna
So then here every agent that you like for the use cases that you describe. What do you produce? Like, is there one interface that you provide to everybody, which is you ingest your data? Here's a way to ingest your data. And then, like, for example, you had use cases for again, I believe that healthcare provider wanted to learn about reviews. And then you had some other use cases across, Jonathan, I'm gonna
So then here every agent that you like for the use cases that you describe. What do you produce? Like, is there one interface that you provide to everybody, which is you ingest your data? Here's a way to ingest your data. And then, like, for example, you had use cases for again, I believe that healthcare provider wanted to learn about reviews. And then you had some other use cases across, Jonathan, I'm gonna
So then here every agent that you like for the use cases that you describe. What do you produce? Like, is there one interface that you provide to everybody, which is you ingest your data? Here's a way to ingest your data. And then, like, for example, you had use cases for again, I believe that healthcare provider wanted to learn about reviews. And then you had some other use cases across, Jonathan, I'm gonna
38:21Are you still there?
38:25Okay, demo, finish your
Okay, demo, finish your
Okay, demo, finish your
Okay, demo, finish your
S Speaker 238:27thought. But I just wanted to make sure he was still there and getting the demo set up while he was
thought. But I just wanted to make sure he was still there and getting the demo set up while he was
thought. But I just wanted to make sure he was still there and getting the demo set up while he was
thought. But I just wanted to make sure he was still there and getting the demo set up while he was
38:33speaking, that would be good, yeah, but
speaking, that would be good, yeah, but
speaking, that would be good, yeah, but
speaking, that would be good, yeah, but
S Speaker 238:35finish your thought please. I didn't want to interrupt. No, I think what? Which
finish your thought please. I didn't want to interrupt. No, I think what? Which
finish your thought please. I didn't want to interrupt. No, I think what? Which
finish your thought please. I didn't want to interrupt. No, I think what? Which
S Speaker 138:39is the customers they all use, which is for customers. They say, Okay, here's an agent that does what I wanted to do, and here's a way to ingest my data. Or do you have to customize the interface for each one of these such that they can, they can generate and create
is the customers they all use, which is for customers. They say, Okay, here's an agent that does what I wanted to do, and here's a way to ingest my data. Or do you have to customize the interface for each one of these such that they can, they can generate and create
is the customers they all use, which is for customers. They say, Okay, here's an agent that does what I wanted to do, and here's a way to ingest my data. Or do you have to customize the interface for each one of these such that they can, they can generate and create
is the customers they all use, which is for customers. They say, Okay, here's an agent that does what I wanted to do, and here's a way to ingest my data. Or do you have to customize the interface for each one of these such that they can, they can generate and create
38:57their own agents. So Jonathan,
their own agents. So Jonathan,
their own agents. So Jonathan,
their own agents. So Jonathan,
S Speaker 239:00Do an analytics one, and then do a image one, because that combination is so different that you'll get how flexible it is. Okay,
Do an analytics one, and then do a image one, because that combination is so different that you'll get how flexible it is. Okay,
Do an analytics one, and then do a image one, because that combination is so different that you'll get how flexible it is. Okay,
Do an analytics one, and then do a image one, because that combination is so different that you'll get how flexible it is. Okay,
S Speaker 339:07yeah, sorry, I'm set up for my customer call next. Let me I just need to change
yeah, sorry, I'm set up for my customer call next. Let me I just need to change
yeah, sorry, I'm set up for my customer call next. Let me I just need to change
yeah, sorry, I'm set up for my customer call next. Let me I just need to change
S Speaker 239:11it all up. Sorry, my fault, actually, while, while you're doing that, why don't I show
it all up. Sorry, my fault, actually, while, while you're doing that, why don't I show
it all up. Sorry, my fault, actually, while, while you're doing that, why don't I show
it all up. Sorry, my fault, actually, while, while you're doing that, why don't I show
S Speaker 139:16a little bit off? So already maybe we, well, that's getting set up. So which is, we can perhaps
a little bit off? So already maybe we, well, that's getting set up. So which is, we can perhaps
a little bit off? So already maybe we, well, that's getting set up. So which is, we can perhaps
a little bit off? So already maybe we, well, that's getting set up. So which is, we can perhaps
39:26I have to jump at 1255, to start driving. But
I have to jump at 1255, to start driving. But
I have to jump at 1255, to start driving. But
I have to jump at 1255, to start driving. But
39:33we can talk about
39:36business, which is,
39:40what? Well, what
S Speaker 139:42was the revenue last year? What are you planning this year? What's the current state of the business? And then, how much are you raising? And why? Also, why did you raise from institutionals Up until now?
was the revenue last year? What are you planning this year? What's the current state of the business? And then, how much are you raising? And why? Also, why did you raise from institutionals Up until now?
was the revenue last year? What are you planning this year? What's the current state of the business? And then, how much are you raising? And why? Also, why did you raise from institutionals Up until now?
was the revenue last year? What are you planning this year? What's the current state of the business? And then, how much are you raising? And why? Also, why did you raise from institutionals Up until now?
S Speaker 239:55So the key reason we didn't raise from institutionals is the stuff we were doing was very counter to what the market believed was the right answer. So pretty much everybody wanted us to use the large language models and stuff like that, and we were sitting there saying, No, that's the wrong path, right? So I made enough people money that they put in money into my new one. So we have raised 50 million. By the way, we didn't raise a small amount of money, five, zero, yeah, okay, oh, you have raised 50, yeah, but we didn't take it from institution. So there are multiple Midas list investors who have invested in us, people like Ed Zander that he would know that have invested in us, like we took money from like crazy top tier people. John Jarvi from Mendel Priyesh would know is an investor. Personally, some of the founders of Evercore are investors. But what we did is we took money from really smart people. We just didn't take it from the institutions, right? And then that was, was that part of a round or so that was around pre COVID, I raised 25 mil then. And then we raised 25 mil in safes and convertibles,
So the key reason we didn't raise from institutionals is the stuff we were doing was very counter to what the market believed was the right answer. So pretty much everybody wanted us to use the large language models and stuff like that, and we were sitting there saying, No, that's the wrong path, right? So I made enough people money that they put in money into my new one. So we have raised 50 million. By the way, we didn't raise a small amount of money, five, zero, yeah, okay, oh, you have raised 50, yeah, but we didn't take it from institution. So there are multiple Midas list investors who have invested in us, people like Ed Zander that he would know that have invested in us, like we took money from like crazy top tier people. John Jarvi from Mendel Priyesh would know is an investor. Personally, some of the founders of Evercore are investors. But what we did is we took money from really smart people. We just didn't take it from the institutions, right? And then that was, was that part of a round or so that was around pre COVID, I raised 25 mil then. And then we raised 25 mil in safes and convertibles,
So the key reason we didn't raise from institutionals is the stuff we were doing was very counter to what the market believed was the right answer. So pretty much everybody wanted us to use the large language models and stuff like that, and we were sitting there saying, No, that's the wrong path, right? So I made enough people money that they put in money into my new one. So we have raised 50 million. By the way, we didn't raise a small amount of money, five, zero, yeah, okay, oh, you have raised 50, yeah, but we didn't take it from institution. So there are multiple Midas list investors who have invested in us, people like Ed Zander that he would know that have invested in us, like we took money from like crazy top tier people. John Jarvi from Mendel Priyesh would know is an investor. Personally, some of the founders of Evercore are investors. But what we did is we took money from really smart people. We just didn't take it from the institutions, right? And then that was, was that part of a round or so that was around pre COVID, I raised 25 mil then. And then we raised 25 mil in safes and convertibles,
So the key reason we didn't raise from institutionals is the stuff we were doing was very counter to what the market believed was the right answer. So pretty much everybody wanted us to use the large language models and stuff like that, and we were sitting there saying, No, that's the wrong path, right? So I made enough people money that they put in money into my new one. So we have raised 50 million. By the way, we didn't raise a small amount of money, five, zero, yeah, okay, oh, you have raised 50, yeah, but we didn't take it from institution. So there are multiple Midas list investors who have invested in us, people like Ed Zander that he would know that have invested in us, like we took money from like crazy top tier people. John Jarvi from Mendel Priyesh would know is an investor. Personally, some of the founders of Evercore are investors. But what we did is we took money from really smart people. We just didn't take it from the institutions, right? And then that was, was that part of a round or so that was around pre COVID, I raised 25 mil then. And then we raised 25 mil in safes and convertibles,
41:00got it. And then that cab tent
got it. And then that cab tent
got it. And then that cab tent
got it. And then that cab tent
41:04capped at something like 400 mil lesser
capped at something like 400 mil lesser
capped at something like 400 mil lesser
capped at something like 400 mil lesser
41:08right? 400 mil pre, yeah.
right? 400 mil pre, yeah.
right? 400 mil pre, yeah.
right? 400 mil pre, yeah.
41:13you're raising a Series A. Now is that you're
you're raising a Series A. Now is that you're
you're raising a Series A. Now is that you're
you're raising a Series A. Now is that you're
S Speaker 241:16raising our first institution around, I don't know whether to call it a Series A, Series B, I don't know. So it is the first institutional round, if you will, got
raising our first institution around, I don't know whether to call it a Series A, Series B, I don't know. So it is the first institutional round, if you will, got
raising our first institution around, I don't know whether to call it a Series A, Series B, I don't know. So it is the first institutional round, if you will, got
raising our first institution around, I don't know whether to call it a Series A, Series B, I don't know. So it is the first institutional round, if you will, got
S Speaker 141:24it and then, so this, you Rick how, what's the size of that round? So
it and then, so this, you Rick how, what's the size of that round? So
it and then, so this, you Rick how, what's the size of that round? So
it and then, so this, you Rick how, what's the size of that round? So
S Speaker 241:28again, we are talking to the lead investor and discussing that we think it should be 50 to 100 but there are people who are asking us to raise a lot more. Like for me, it's a matter of talking to the lead investor and finding would be on a board and having a very open conversation, because we don't want to raise unnecessarily, but we don't. We do want to raise enough that we can prosecute the opportunity. Okay, so I'll just skip past this. This is CVS is a customer. Baptist Health is a customer. State of Nebraska is a customer. I'll skip past the team, but it's a very experienced team that has worked together for a while, and it was on Google's board for 15 years. Audrey, you might know some of you. So we are raising our first institution around as I mentioned, right? And one of the business model that we are going after is that use case based land, then they expand. So this is one fortune 50 basically. The thing I wanted to point out is that this is the path we are on, and they are currently spending their 2025 budget is 500 million in year. So that 10 million number could be massively underestimated. That's our path, though. It's like, we get in, get quick successes, get the senior execs excited, then go down this one. Yeah, right.
again, we are talking to the lead investor and discussing that we think it should be 50 to 100 but there are people who are asking us to raise a lot more. Like for me, it's a matter of talking to the lead investor and finding would be on a board and having a very open conversation, because we don't want to raise unnecessarily, but we don't. We do want to raise enough that we can prosecute the opportunity. Okay, so I'll just skip past this. This is CVS is a customer. Baptist Health is a customer. State of Nebraska is a customer. I'll skip past the team, but it's a very experienced team that has worked together for a while, and it was on Google's board for 15 years. Audrey, you might know some of you. So we are raising our first institution around as I mentioned, right? And one of the business model that we are going after is that use case based land, then they expand. So this is one fortune 50 basically. The thing I wanted to point out is that this is the path we are on, and they are currently spending their 2025 budget is 500 million in year. So that 10 million number could be massively underestimated. That's our path, though. It's like, we get in, get quick successes, get the senior execs excited, then go down this one. Yeah, right.
again, we are talking to the lead investor and discussing that we think it should be 50 to 100 but there are people who are asking us to raise a lot more. Like for me, it's a matter of talking to the lead investor and finding would be on a board and having a very open conversation, because we don't want to raise unnecessarily, but we don't. We do want to raise enough that we can prosecute the opportunity. Okay, so I'll just skip past this. This is CVS is a customer. Baptist Health is a customer. State of Nebraska is a customer. I'll skip past the team, but it's a very experienced team that has worked together for a while, and it was on Google's board for 15 years. Audrey, you might know some of you. So we are raising our first institution around as I mentioned, right? And one of the business model that we are going after is that use case based land, then they expand. So this is one fortune 50 basically. The thing I wanted to point out is that this is the path we are on, and they are currently spending their 2025 budget is 500 million in year. So that 10 million number could be massively underestimated. That's our path, though. It's like, we get in, get quick successes, get the senior execs excited, then go down this one. Yeah, right.
again, we are talking to the lead investor and discussing that we think it should be 50 to 100 but there are people who are asking us to raise a lot more. Like for me, it's a matter of talking to the lead investor and finding would be on a board and having a very open conversation, because we don't want to raise unnecessarily, but we don't. We do want to raise enough that we can prosecute the opportunity. Okay, so I'll just skip past this. This is CVS is a customer. Baptist Health is a customer. State of Nebraska is a customer. I'll skip past the team, but it's a very experienced team that has worked together for a while, and it was on Google's board for 15 years. Audrey, you might know some of you. So we are raising our first institution around as I mentioned, right? And one of the business model that we are going after is that use case based land, then they expand. So this is one fortune 50 basically. The thing I wanted to point out is that this is the path we are on, and they are currently spending their 2025 budget is 500 million in year. So that 10 million number could be massively underestimated. That's our path, though. It's like, we get in, get quick successes, get the senior execs excited, then go down this one. Yeah, right.
42:49But the total err is not
But the total err is not
But the total err is not
But the total err is not
S Speaker 242:51huge. It's just a four mil. It's going to be at five mill next month, right, right. But the key thing here is we have been working on the tech most of our customers are large customers, because we were trying to prove out that we will meet the security needs of these huge customers, right? We are on Air Force Research Lab security. We are, you know, we have worked with some of the largest customers because of that, but that's where we're going to raise the money. And we actually have a pretty huge current pipeline. So we believe we can hit 100 million ARR in 18 months, just off of the backs of our current customers. If we go into what we are looking at, we can get 50 million ARR just
huge. It's just a four mil. It's going to be at five mill next month, right, right. But the key thing here is we have been working on the tech most of our customers are large customers, because we were trying to prove out that we will meet the security needs of these huge customers, right? We are on Air Force Research Lab security. We are, you know, we have worked with some of the largest customers because of that, but that's where we're going to raise the money. And we actually have a pretty huge current pipeline. So we believe we can hit 100 million ARR in 18 months, just off of the backs of our current customers. If we go into what we are looking at, we can get 50 million ARR just
huge. It's just a four mil. It's going to be at five mill next month, right, right. But the key thing here is we have been working on the tech most of our customers are large customers, because we were trying to prove out that we will meet the security needs of these huge customers, right? We are on Air Force Research Lab security. We are, you know, we have worked with some of the largest customers because of that, but that's where we're going to raise the money. And we actually have a pretty huge current pipeline. So we believe we can hit 100 million ARR in 18 months, just off of the backs of our current customers. If we go into what we are looking at, we can get 50 million ARR just
huge. It's just a four mil. It's going to be at five mill next month, right, right. But the key thing here is we have been working on the tech most of our customers are large customers, because we were trying to prove out that we will meet the security needs of these huge customers, right? We are on Air Force Research Lab security. We are, you know, we have worked with some of the largest customers because of that, but that's where we're going to raise the money. And we actually have a pretty huge current pipeline. So we believe we can hit 100 million ARR in 18 months, just off of the backs of our current customers. If we go into what we are looking at, we can get 50 million ARR just
43:35because of the size point
because of the size point
because of the size point
because of the size point
S Speaker 343:57Alright, so this one is this demonstration is going to show you know, different clips from cameras around an airport. And the idea here is trying to identify if there's any abandoned luggage. And so we see here an image where it's identifying correctly. No, this is baggage on a carousel. It's the other people attending. So this is definitely not abandoned.
Alright, so this one is this demonstration is going to show you know, different clips from cameras around an airport. And the idea here is trying to identify if there's any abandoned luggage. And so we see here an image where it's identifying correctly. No, this is baggage on a carousel. It's the other people attending. So this is definitely not abandoned.
Alright, so this one is this demonstration is going to show you know, different clips from cameras around an airport. And the idea here is trying to identify if there's any abandoned luggage. And so we see here an image where it's identifying correctly. No, this is baggage on a carousel. It's the other people attending. So this is definitely not abandoned.
Alright, so this one is this demonstration is going to show you know, different clips from cameras around an airport. And the idea here is trying to identify if there's any abandoned luggage. And so we see here an image where it's identifying correctly. No, this is baggage on a carousel. It's the other people attending. So this is definitely not abandoned.
44:24However, when I load
S Speaker 344:32this also says no. It says no, because it's identified that there are people in the background. And with this I have the ability to come back and update what we're doing, and here it's going to take a look at the image and try to figure out why it's or how it's made an issue or made a mistake. If it doesn't correctly understand I
this also says no. It says no, because it's identified that there are people in the background. And with this I have the ability to come back and update what we're doing, and here it's going to take a look at the image and try to figure out why it's or how it's made an issue or made a mistake. If it doesn't correctly understand I
this also says no. It says no, because it's identified that there are people in the background. And with this I have the ability to come back and update what we're doing, and here it's going to take a look at the image and try to figure out why it's or how it's made an issue or made a mistake. If it doesn't correctly understand I
this also says no. It says no, because it's identified that there are people in the background. And with this I have the ability to come back and update what we're doing, and here it's going to take a look at the image and try to figure out why it's or how it's made an issue or made a mistake. If it doesn't correctly understand I
S Speaker 345:06so I can correct this as well. In this case, I don't like how this has been translated. So I can say, yeah, the problem here is nobody is actually near the luggage, right? I
so I can correct this as well. In this case, I don't like how this has been translated. So I can say, yeah, the problem here is nobody is actually near the luggage, right? I
so I can correct this as well. In this case, I don't like how this has been translated. So I can say, yeah, the problem here is nobody is actually near the luggage, right? I
so I can correct this as well. In this case, I don't like how this has been translated. So I can say, yeah, the problem here is nobody is actually near the luggage, right? I
S Speaker 345:27and then I can approve the updated prompt and use this to go ahead and try it again and
and then I can approve the updated prompt and use this to go ahead and try it again and
and then I can approve the updated prompt and use this to go ahead and try it again and
and then I can approve the updated prompt and use this to go ahead and try it again and
S Speaker 246:15can use the reasoning steps, if you could, maybe in the NLP or something, if you could show the feedback on the he used the different technique, which is the adjusting the prompt based on the feedback. The different techniques we have. Could you show him one on the reasoning steps?
can use the reasoning steps, if you could, maybe in the NLP or something, if you could show the feedback on the he used the different technique, which is the adjusting the prompt based on the feedback. The different techniques we have. Could you show him one on the reasoning steps?
can use the reasoning steps, if you could, maybe in the NLP or something, if you could show the feedback on the he used the different technique, which is the adjusting the prompt based on the feedback. The different techniques we have. Could you show him one on the reasoning steps?
can use the reasoning steps, if you could, maybe in the NLP or something, if you could show the feedback on the he used the different technique, which is the adjusting the prompt based on the feedback. The different techniques we have. Could you show him one on the reasoning steps?
S Speaker 346:30Yeah, I could have provided it right within the reasoning steps as well. I don't have a separate reasoning queued up, but I can get that queued up
Yeah, I could have provided it right within the reasoning steps as well. I don't have a separate reasoning queued up, but I can get that queued up
Yeah, I could have provided it right within the reasoning steps as well. I don't have a separate reasoning queued up, but I can get that queued up
Yeah, I could have provided it right within the reasoning steps as well. I don't have a separate reasoning queued up, but I can get that queued up
S Speaker 246:36without an LQ or something, yeah, yeah.
without an LQ or something, yeah, yeah.
without an LQ or something, yeah, yeah.
without an LQ or something, yeah, yeah.
46:40I don't have give me a second.
I don't have give me a second.
I don't have give me a second.
I don't have give me a second.
S Speaker 346:44Sorry, I'm all I was also my customer call, so I wasn't ready to I
Sorry, I'm all I was also my customer call, so I wasn't ready to I
Sorry, I'm all I was also my customer call, so I wasn't ready to I
Sorry, I'm all I was also my customer call, so I wasn't ready to I
46:49totally understand. I mean, I see
totally understand. I mean, I see
totally understand. I mean, I see
totally understand. I mean, I see
S Speaker 146:52this. This is one interface that you provide to a particular customer for Yeah,
this. This is one interface that you provide to a particular customer for Yeah,
this. This is one interface that you provide to a particular customer for Yeah,
this. This is one interface that you provide to a particular customer for Yeah,
47:02it image based test, yeah, let me grab one.
it image based test, yeah, let me grab one.
it image based test, yeah, let me grab one.
it image based test, yeah, let me grab one.
47:13Of course, the zoom bar is covering my ability to select it.
Of course, the zoom bar is covering my ability to select it.
Of course, the zoom bar is covering my ability to select it.
Of course, the zoom bar is covering my ability to select it.
S Speaker 647:35Alright, so here's a natural language query. Just Gonna have to grab my questions set so
Alright, so here's a natural language query. Just Gonna have to grab my questions set so
Alright, so here's a natural language query. Just Gonna have to grab my questions set so
Alright, so here's a natural language query. Just Gonna have to grab my questions set so
48:14so this case, I'm
S Speaker 250:57could you go to the template library? Because Jonathan started in the middle of agents. So if you notice, depending on what he was doing, the right stuff was happening, right? He started just the question of, How can improve sales? It went in there. But if you want to create template by template, if you want to go build agents, you start out here. Here are your templates. You go in and search by your function, by your industry, answer a few questions, and you end up with your own agent.
could you go to the template library? Because Jonathan started in the middle of agents. So if you notice, depending on what he was doing, the right stuff was happening, right? He started just the question of, How can improve sales? It went in there. But if you want to create template by template, if you want to go build agents, you start out here. Here are your templates. You go in and search by your function, by your industry, answer a few questions, and you end up with your own agent.
could you go to the template library? Because Jonathan started in the middle of agents. So if you notice, depending on what he was doing, the right stuff was happening, right? He started just the question of, How can improve sales? It went in there. But if you want to create template by template, if you want to go build agents, you start out here. Here are your templates. You go in and search by your function, by your industry, answer a few questions, and you end up with your own agent.
could you go to the template library? Because Jonathan started in the middle of agents. So if you notice, depending on what he was doing, the right stuff was happening, right? He started just the question of, How can improve sales? It went in there. But if you want to create template by template, if you want to go build agents, you start out here. Here are your templates. You go in and search by your function, by your industry, answer a few questions, and you end up with your own agent.
S Speaker 151:23Got it right, but that's the which is, this is one platform for different use cases. So yeah, and
Got it right, but that's the which is, this is one platform for different use cases. So yeah, and
Got it right, but that's the which is, this is one platform for different use cases. So yeah, and
Got it right, but that's the which is, this is one platform for different use cases. So yeah, and
51:30like, like, if you have the order
like, like, if you have the order
like, like, if you have the order
like, like, if you have the order
S Speaker 251:32to cache available, Jonathan, that will look very different. So the, the way this is set up is, even though he's changing user interfaces, going from NLQ to analytics
to cache available, Jonathan, that will look very different. So the, the way this is set up is, even though he's changing user interfaces, going from NLQ to analytics
to cache available, Jonathan, that will look very different. So the, the way this is set up is, even though he's changing user interfaces, going from NLQ to analytics
to cache available, Jonathan, that will look very different. So the, the way this is set up is, even though he's changing user interfaces, going from NLQ to analytics
51:39to to image and stuff.
to to image and stuff.
to to image and stuff.
to to image and stuff.
S Speaker 251:43It's the same underlying set of services that are working right? That's one of the key things. Okay, so that makes it
It's the same underlying set of services that are working right? That's one of the key things. Okay, so that makes it
It's the same underlying set of services that are working right? That's one of the key things. Okay, so that makes it
It's the same underlying set of services that are working right? That's one of the key things. Okay, so that makes it
51:56Thanks. Thanks, Jonathan, with the demo,
Thanks. Thanks, Jonathan, with the demo,
Thanks. Thanks, Jonathan, with the demo,
Thanks. Thanks, Jonathan, with the demo,
S Speaker 152:00I do have to drop in a minute, so maybe, if you can give us some view into what the business looks like for this year and next year, and Then the confidence level on the business. The reason I'm asking is the hurdle for us would be the valuation for our check size
I do have to drop in a minute, so maybe, if you can give us some view into what the business looks like for this year and next year, and Then the confidence level on the business. The reason I'm asking is the hurdle for us would be the valuation for our check size
I do have to drop in a minute, so maybe, if you can give us some view into what the business looks like for this year and next year, and Then the confidence level on the business. The reason I'm asking is the hurdle for us would be the valuation for our check size
I do have to drop in a minute, so maybe, if you can give us some view into what the business looks like for this year and next year, and Then the confidence level on the business. The reason I'm asking is the hurdle for us would be the valuation for our check size
52:32and and then we'll have to get some comfort into
and and then we'll have to get some comfort into
and and then we'll have to get some comfort into
and and then we'll have to get some comfort into
S Speaker 152:37what the business looks like for this year and next year, and can we get comfortable on that to participate? Because I'm guessing you're going to raise a higher valuation from from the note,
what the business looks like for this year and next year, and can we get comfortable on that to participate? Because I'm guessing you're going to raise a higher valuation from from the note,
what the business looks like for this year and next year, and can we get comfortable on that to participate? Because I'm guessing you're going to raise a higher valuation from from the note,
what the business looks like for this year and next year, and can we get comfortable on that to participate? Because I'm guessing you're going to raise a higher valuation from from the note,
S Speaker 252:50yeah, either way, you guys are going to not be the leads for what we are talking about. So this was more a matter of me giving you guys a sense of it, because Priyesh, give me some good advice at the booth, which was like, Hey, if you're talking to Nvidia, why you're not talking to us, kind of a thing, and we are working with different clouds, so as we get our lead identified, why don't I come back to you? Because we're seeing what the staff is, and we are going to end up having some of the clouds invest as well as my expectation. So you guys would be a very valuable part of that
yeah, either way, you guys are going to not be the leads for what we are talking about. So this was more a matter of me giving you guys a sense of it, because Priyesh, give me some good advice at the booth, which was like, Hey, if you're talking to Nvidia, why you're not talking to us, kind of a thing, and we are working with different clouds, so as we get our lead identified, why don't I come back to you? Because we're seeing what the staff is, and we are going to end up having some of the clouds invest as well as my expectation. So you guys would be a very valuable part of that
yeah, either way, you guys are going to not be the leads for what we are talking about. So this was more a matter of me giving you guys a sense of it, because Priyesh, give me some good advice at the booth, which was like, Hey, if you're talking to Nvidia, why you're not talking to us, kind of a thing, and we are working with different clouds, so as we get our lead identified, why don't I come back to you? Because we're seeing what the staff is, and we are going to end up having some of the clouds invest as well as my expectation. So you guys would be a very valuable part of that
yeah, either way, you guys are going to not be the leads for what we are talking about. So this was more a matter of me giving you guys a sense of it, because Priyesh, give me some good advice at the booth, which was like, Hey, if you're talking to Nvidia, why you're not talking to us, kind of a thing, and we are working with different clouds, so as we get our lead identified, why don't I come back to you? Because we're seeing what the staff is, and we are going to end up having some of the clouds invest as well as my expectation. So you guys would be a very valuable part of that
53:28system you talk to
53:45Let you know, and then I guess, are you talking to Mela?
Let you know, and then I guess, are you talking to Mela?
Let you know, and then I guess, are you talking to Mela?
Let you know, and then I guess, are you talking to Mela?