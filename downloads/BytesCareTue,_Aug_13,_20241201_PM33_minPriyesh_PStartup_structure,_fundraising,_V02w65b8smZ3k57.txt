Meeting: BytesCare
Tue, Aug 13, 2024
12:01 PM
33 min
Priyesh P
Startup structure, fundraising, and content pr
URL: https://otter.ai/u/V02w65b8smZ3k5756xAjPxR4YEs
Downloaded: 2025-12-22T14:51:48.251446
Method: text_extraction
============================================================

0:01Connecting again, same, yeah, absolutely. Where
S Speaker 10:08and remind me again. Have Have you shifted completely to the US I remember you were in India. Let me
and remind me again. Have Have you shifted completely to the US I remember you were in India. Let me
and remind me again. Have Have you shifted completely to the US I remember you were in India. Let me
and remind me again. Have Have you shifted completely to the US I remember you were in India. Let me
S Speaker 20:17give you overview, like of how the company is currently structured. We started the company in the Indian India with the name of Next up technology. It was like it was registered way back in 2016 because we was working alone. And he registered some company when he was in college. So he continued with the same activity, but wide scale came into existence in 2019 how? But scares as the product or as a brand. In 2022 when we were raising our angel round, mostly the angels were like us, citizens and everything so and NRIs and everything. So it was easier for us to, like set up an in between the US, and based from the US, because we had the plans from the day one that we want to target that entity, and we registered one entity in the US. We transferred all the IPs to the US entity, yeah. And so currently, how we are operating. Most of the payroll is coming from the Indian entity. Most of the revenues is coming from the Indian entity. Currently we have set up like we have not made any parent or child entity right now, but we are happy to look into the structures once we receive this, right? Yeah, yeah, yeah. So that's how we are structured right now. Got
give you overview, like of how the company is currently structured. We started the company in the Indian India with the name of Next up technology. It was like it was registered way back in 2016 because we was working alone. And he registered some company when he was in college. So he continued with the same activity, but wide scale came into existence in 2019 how? But scares as the product or as a brand. In 2022 when we were raising our angel round, mostly the angels were like us, citizens and everything so and NRIs and everything. So it was easier for us to, like set up an in between the US, and based from the US, because we had the plans from the day one that we want to target that entity, and we registered one entity in the US. We transferred all the IPs to the US entity, yeah. And so currently, how we are operating. Most of the payroll is coming from the Indian entity. Most of the revenues is coming from the Indian entity. Currently we have set up like we have not made any parent or child entity right now, but we are happy to look into the structures once we receive this, right? Yeah, yeah, yeah. So that's how we are structured right now. Got
give you overview, like of how the company is currently structured. We started the company in the Indian India with the name of Next up technology. It was like it was registered way back in 2016 because we was working alone. And he registered some company when he was in college. So he continued with the same activity, but wide scale came into existence in 2019 how? But scares as the product or as a brand. In 2022 when we were raising our angel round, mostly the angels were like us, citizens and everything so and NRIs and everything. So it was easier for us to, like set up an in between the US, and based from the US, because we had the plans from the day one that we want to target that entity, and we registered one entity in the US. We transferred all the IPs to the US entity, yeah. And so currently, how we are operating. Most of the payroll is coming from the Indian entity. Most of the revenues is coming from the Indian entity. Currently we have set up like we have not made any parent or child entity right now, but we are happy to look into the structures once we receive this, right? Yeah, yeah, yeah. So that's how we are structured right now. Got
give you overview, like of how the company is currently structured. We started the company in the Indian India with the name of Next up technology. It was like it was registered way back in 2016 because we was working alone. And he registered some company when he was in college. So he continued with the same activity, but wide scale came into existence in 2019 how? But scares as the product or as a brand. In 2022 when we were raising our angel round, mostly the angels were like us, citizens and everything so and NRIs and everything. So it was easier for us to, like set up an in between the US, and based from the US, because we had the plans from the day one that we want to target that entity, and we registered one entity in the US. We transferred all the IPs to the US entity, yeah. And so currently, how we are operating. Most of the payroll is coming from the Indian entity. Most of the revenues is coming from the Indian entity. Currently we have set up like we have not made any parent or child entity right now, but we are happy to look into the structures once we receive this, right? Yeah, yeah, yeah. So that's how we are structured right now. Got
S Speaker 11:38it. Got it, and is Winnie based out of India right now.
it. Got it, and is Winnie based out of India right now.
it. Got it, and is Winnie based out of India right now.
it. Got it, and is Winnie based out of India right now.
S Speaker 21:44Yes, both of us like live in us. I shifted to us couple of months ago for that accelerator program, Falcon x, and we now have plans to move. One of us will move here with the oven, or as soon as we get our oven,
Yes, both of us like live in us. I shifted to us couple of months ago for that accelerator program, Falcon x, and we now have plans to move. One of us will move here with the oven, or as soon as we get our oven,
Yes, both of us like live in us. I shifted to us couple of months ago for that accelerator program, Falcon x, and we now have plans to move. One of us will move here with the oven, or as soon as we get our oven,
Yes, both of us like live in us. I shifted to us couple of months ago for that accelerator program, Falcon x, and we now have plans to move. One of us will move here with the oven, or as soon as we get our oven,
S Speaker 22:20like preliminary tech. I'm still working on my tech, but absolutely it has, like, quite a lot of information, so I can walk you through our progress and what we are planning to build and all those things. And there are a couple of components are still missing, but we will,
like preliminary tech. I'm still working on my tech, but absolutely it has, like, quite a lot of information, so I can walk you through our progress and what we are planning to build and all those things. And there are a couple of components are still missing, but we will,
like preliminary tech. I'm still working on my tech, but absolutely it has, like, quite a lot of information, so I can walk you through our progress and what we are planning to build and all those things. And there are a couple of components are still missing, but we will,
like preliminary tech. I'm still working on my tech, but absolutely it has, like, quite a lot of information, so I can walk you through our progress and what we are planning to build and all those things. And there are a couple of components are still missing, but we will,
S Speaker 12:54You can see my screen, right? Yeah, initially, I was a little bit confused if it is bite scare or it's bytes care.
You can see my screen, right? Yeah, initially, I was a little bit confused if it is bite scare or it's bytes care.
You can see my screen, right? Yeah, initially, I was a little bit confused if it is bite scare or it's bytes care.
You can see my screen, right? Yeah, initially, I was a little bit confused if it is bite scare or it's bytes care.
S Speaker 23:02There is a word play around it. So basically, we started with a company with a mindset of like just protecting the content. So bites come from the bits and bytes we are producing the bites and we care for your content, so that's why. But soon we also realize that it's also bite scare. So it is like we are scaring the people who are scaring your bike.
There is a word play around it. So basically, we started with a company with a mindset of like just protecting the content. So bites come from the bits and bytes we are producing the bites and we care for your content, so that's why. But soon we also realize that it's also bite scare. So it is like we are scaring the people who are scaring your bike.
There is a word play around it. So basically, we started with a company with a mindset of like just protecting the content. So bites come from the bits and bytes we are producing the bites and we care for your content, so that's why. But soon we also realize that it's also bite scare. So it is like we are scaring the people who are scaring your bike.
There is a word play around it. So basically, we started with a company with a mindset of like just protecting the content. So bites come from the bits and bytes we are producing the bites and we care for your content, so that's why. But soon we also realize that it's also bite scare. So it is like we are scaring the people who are scaring your bike.
S Speaker 23:31name, and we love it right now, because it sends both the messages and however you so it's a totally resonate what we are doing. Yeah, it. So I will start with a question, have you ever watched a pirated movie or read a paraded
name, and we love it right now, because it sends both the messages and however you so it's a totally resonate what we are doing. Yeah, it. So I will start with a question, have you ever watched a pirated movie or read a paraded
name, and we love it right now, because it sends both the messages and however you so it's a totally resonate what we are doing. Yeah, it. So I will start with a question, have you ever watched a pirated movie or read a paraded
name, and we love it right now, because it sends both the messages and however you so it's a totally resonate what we are doing. Yeah, it. So I will start with a question, have you ever watched a pirated movie or read a paraded
S Speaker 23:50So do you know like the pirated websites are getting over two 30 billion views every year from people like us and people from different parts of the world. And to give you a perspective, Netflix is only getting 24 million that's mean that it's 10x more like viewers are getting on different pirated website, as compared Netflix being the largest streaming company, and this is only for the video content, and there are other content as well. So comparing, trying to compare only, and the most of the companies like who are operating in content or like brand and come like selling, they are losing this much of money because of the unauthorized distribution of their content or products like digital privacy companies are losing over $400 billion in impersonation in US alone. I think they are losing more than like this because of the scams. People are losing money, and the counterfeit is a much, much bigger problem. Like every company that sells our products on E commerce, non e commerce, everybody visits those counterfeits problem.
So do you know like the pirated websites are getting over two 30 billion views every year from people like us and people from different parts of the world. And to give you a perspective, Netflix is only getting 24 million that's mean that it's 10x more like viewers are getting on different pirated website, as compared Netflix being the largest streaming company, and this is only for the video content, and there are other content as well. So comparing, trying to compare only, and the most of the companies like who are operating in content or like brand and come like selling, they are losing this much of money because of the unauthorized distribution of their content or products like digital privacy companies are losing over $400 billion in impersonation in US alone. I think they are losing more than like this because of the scams. People are losing money, and the counterfeit is a much, much bigger problem. Like every company that sells our products on E commerce, non e commerce, everybody visits those counterfeits problem.
So do you know like the pirated websites are getting over two 30 billion views every year from people like us and people from different parts of the world. And to give you a perspective, Netflix is only getting 24 million that's mean that it's 10x more like viewers are getting on different pirated website, as compared Netflix being the largest streaming company, and this is only for the video content, and there are other content as well. So comparing, trying to compare only, and the most of the companies like who are operating in content or like brand and come like selling, they are losing this much of money because of the unauthorized distribution of their content or products like digital privacy companies are losing over $400 billion in impersonation in US alone. I think they are losing more than like this because of the scams. People are losing money, and the counterfeit is a much, much bigger problem. Like every company that sells our products on E commerce, non e commerce, everybody visits those counterfeits problem.
So do you know like the pirated websites are getting over two 30 billion views every year from people like us and people from different parts of the world. And to give you a perspective, Netflix is only getting 24 million that's mean that it's 10x more like viewers are getting on different pirated website, as compared Netflix being the largest streaming company, and this is only for the video content, and there are other content as well. So comparing, trying to compare only, and the most of the companies like who are operating in content or like brand and come like selling, they are losing this much of money because of the unauthorized distribution of their content or products like digital privacy companies are losing over $400 billion in impersonation in US alone. I think they are losing more than like this because of the scams. People are losing money, and the counterfeit is a much, much bigger problem. Like every company that sells our products on E commerce, non e commerce, everybody visits those counterfeits problem.
S Speaker 112:10if we can go back to the technology side, Manish, I want you to understand the web crawler side of it a little bit better. How is it any different from a lot of other web crawlers, which are more focused on, let's say, user research, and how do you specifically design it so that it finds counterfeit links for you? Yeah,
if we can go back to the technology side, Manish, I want you to understand the web crawler side of it a little bit better. How is it any different from a lot of other web crawlers, which are more focused on, let's say, user research, and how do you specifically design it so that it finds counterfeit links for you? Yeah,
if we can go back to the technology side, Manish, I want you to understand the web crawler side of it a little bit better. How is it any different from a lot of other web crawlers, which are more focused on, let's say, user research, and how do you specifically design it so that it finds counterfeit links for you? Yeah,
if we can go back to the technology side, Manish, I want you to understand the web crawler side of it a little bit better. How is it any different from a lot of other web crawlers, which are more focused on, let's say, user research, and how do you specifically design it so that it finds counterfeit links for you? Yeah,
S Speaker 212:34so web crawlers, the underlying technology, is the same, like they're all the user research people, or any person who's running a web crawler, like for E commerce platform people are running for price finding or product finding and all those things. The underlying technology is the same, the crawling mechanism, but the training, which has been done like what kind of data they have to extract, first thing, what kind of website they have to crawl. Because we have been crawling Google, Google results, Facebook results, Instagram profiles, e commerce websites. So we have identified, like, what are the more prominent website which we need to crawl, and we have defined in a way that what kind of information just not just the link, there is an evidence. For example, if we find like 10 websites on Google search and like five are genuine, five are not. So we are able to identify which are genuine, which are not genuine by understanding the footprint of the websites. Like, generally a pirated website, if you have visited recently, they are running hundreds of ads. There are more like, download buttons, P to P, network links and all those things. So we accept those kind of information make a decision based on like, what kind of evidence we have been able to identify, if this is a genuine website or it's a priority website, even if it's a genuine websites, are they part of their official partner channel? Like we collect when we are collecting the information from the client, the metadata and everything. We also collect the like, the what are the their official channels, official websites, official social media handles, so that we are know that these are the official partners and everything and whatever else, wherever their content or products are available, they they needs to be taken down. So that's how we are understanding the problem statement based on our use cases
so web crawlers, the underlying technology, is the same, like they're all the user research people, or any person who's running a web crawler, like for E commerce platform people are running for price finding or product finding and all those things. The underlying technology is the same, the crawling mechanism, but the training, which has been done like what kind of data they have to extract, first thing, what kind of website they have to crawl. Because we have been crawling Google, Google results, Facebook results, Instagram profiles, e commerce websites. So we have identified, like, what are the more prominent website which we need to crawl, and we have defined in a way that what kind of information just not just the link, there is an evidence. For example, if we find like 10 websites on Google search and like five are genuine, five are not. So we are able to identify which are genuine, which are not genuine by understanding the footprint of the websites. Like, generally a pirated website, if you have visited recently, they are running hundreds of ads. There are more like, download buttons, P to P, network links and all those things. So we accept those kind of information make a decision based on like, what kind of evidence we have been able to identify, if this is a genuine website or it's a priority website, even if it's a genuine websites, are they part of their official partner channel? Like we collect when we are collecting the information from the client, the metadata and everything. We also collect the like, the what are the their official channels, official websites, official social media handles, so that we are know that these are the official partners and everything and whatever else, wherever their content or products are available, they they needs to be taken down. So that's how we are understanding the problem statement based on our use cases
so web crawlers, the underlying technology, is the same, like they're all the user research people, or any person who's running a web crawler, like for E commerce platform people are running for price finding or product finding and all those things. The underlying technology is the same, the crawling mechanism, but the training, which has been done like what kind of data they have to extract, first thing, what kind of website they have to crawl. Because we have been crawling Google, Google results, Facebook results, Instagram profiles, e commerce websites. So we have identified, like, what are the more prominent website which we need to crawl, and we have defined in a way that what kind of information just not just the link, there is an evidence. For example, if we find like 10 websites on Google search and like five are genuine, five are not. So we are able to identify which are genuine, which are not genuine by understanding the footprint of the websites. Like, generally a pirated website, if you have visited recently, they are running hundreds of ads. There are more like, download buttons, P to P, network links and all those things. So we accept those kind of information make a decision based on like, what kind of evidence we have been able to identify, if this is a genuine website or it's a priority website, even if it's a genuine websites, are they part of their official partner channel? Like we collect when we are collecting the information from the client, the metadata and everything. We also collect the like, the what are the their official channels, official websites, official social media handles, so that we are know that these are the official partners and everything and whatever else, wherever their content or products are available, they they needs to be taken down. So that's how we are understanding the problem statement based on our use cases
so web crawlers, the underlying technology, is the same, like they're all the user research people, or any person who's running a web crawler, like for E commerce platform people are running for price finding or product finding and all those things. The underlying technology is the same, the crawling mechanism, but the training, which has been done like what kind of data they have to extract, first thing, what kind of website they have to crawl. Because we have been crawling Google, Google results, Facebook results, Instagram profiles, e commerce websites. So we have identified, like, what are the more prominent website which we need to crawl, and we have defined in a way that what kind of information just not just the link, there is an evidence. For example, if we find like 10 websites on Google search and like five are genuine, five are not. So we are able to identify which are genuine, which are not genuine by understanding the footprint of the websites. Like, generally a pirated website, if you have visited recently, they are running hundreds of ads. There are more like, download buttons, P to P, network links and all those things. So we accept those kind of information make a decision based on like, what kind of evidence we have been able to identify, if this is a genuine website or it's a priority website, even if it's a genuine websites, are they part of their official partner channel? Like we collect when we are collecting the information from the client, the metadata and everything. We also collect the like, the what are the their official channels, official websites, official social media handles, so that we are know that these are the official partners and everything and whatever else, wherever their content or products are available, they they needs to be taken down. So that's how we are understanding the problem statement based on our use cases
S Speaker 215:01So currently, if I like, if I give you an explanation, but client, our client, are looking for, it will I should be able to answer your question. So basically, most of these companies what they're looking for, like, if somebody is looking for their content, they should come to our official channels. Yeah. So how we approach this process as like mimicking the human behavior. For example, if you want to watch a Spider Man movie, but you will go, you will go on Google, Bing, or any other search engine or and you will search for Spider Man, let's say no way home, download or stream or something like that. So we are mimicking that behavior. So whatever being flagged by Google, we can extract all the information if, let's say somebody has put some other title or like letter instead of Spider Man, they put like, let's say Iron Man, yeah, but when I'm searching for Spider Man, I'm looking for more Spider Man, I'm getting a results of iron and it doesn't make sense. So we are trying to mimic the human behavior, to reduce the impact of these infringements on our clients. We can do a deeper analysis, whatever you mentioned, like, do a deeper like putting a bottom up to understand, like, how the content is available. Those technologies can be built, but these, these are the technologies that require extensive resources, research and all those things. So we have, we have all the plans, I will come to that point as well, where we are doing those kind of research as well. But currently it's a, I would say more like a keyword research, because we are trying to maybe the human behavior, and that is what, like 95 to 98% problem lies for our clients. Something is available on a website which is not indexable on Google. It's not really their issues. If it's indeed web like less than point one or point zero, 1% of user effects has direct access to those websites or those platforms. They are not with some they are more like people problem. So they want to solve the mask. So we are solving the problem from their perspective,
So currently, if I like, if I give you an explanation, but client, our client, are looking for, it will I should be able to answer your question. So basically, most of these companies what they're looking for, like, if somebody is looking for their content, they should come to our official channels. Yeah. So how we approach this process as like mimicking the human behavior. For example, if you want to watch a Spider Man movie, but you will go, you will go on Google, Bing, or any other search engine or and you will search for Spider Man, let's say no way home, download or stream or something like that. So we are mimicking that behavior. So whatever being flagged by Google, we can extract all the information if, let's say somebody has put some other title or like letter instead of Spider Man, they put like, let's say Iron Man, yeah, but when I'm searching for Spider Man, I'm looking for more Spider Man, I'm getting a results of iron and it doesn't make sense. So we are trying to mimic the human behavior, to reduce the impact of these infringements on our clients. We can do a deeper analysis, whatever you mentioned, like, do a deeper like putting a bottom up to understand, like, how the content is available. Those technologies can be built, but these, these are the technologies that require extensive resources, research and all those things. So we have, we have all the plans, I will come to that point as well, where we are doing those kind of research as well. But currently it's a, I would say more like a keyword research, because we are trying to maybe the human behavior, and that is what, like 95 to 98% problem lies for our clients. Something is available on a website which is not indexable on Google. It's not really their issues. If it's indeed web like less than point one or point zero, 1% of user effects has direct access to those websites or those platforms. They are not with some they are more like people problem. So they want to solve the mask. So we are solving the problem from their perspective,
So currently, if I like, if I give you an explanation, but client, our client, are looking for, it will I should be able to answer your question. So basically, most of these companies what they're looking for, like, if somebody is looking for their content, they should come to our official channels. Yeah. So how we approach this process as like mimicking the human behavior. For example, if you want to watch a Spider Man movie, but you will go, you will go on Google, Bing, or any other search engine or and you will search for Spider Man, let's say no way home, download or stream or something like that. So we are mimicking that behavior. So whatever being flagged by Google, we can extract all the information if, let's say somebody has put some other title or like letter instead of Spider Man, they put like, let's say Iron Man, yeah, but when I'm searching for Spider Man, I'm looking for more Spider Man, I'm getting a results of iron and it doesn't make sense. So we are trying to mimic the human behavior, to reduce the impact of these infringements on our clients. We can do a deeper analysis, whatever you mentioned, like, do a deeper like putting a bottom up to understand, like, how the content is available. Those technologies can be built, but these, these are the technologies that require extensive resources, research and all those things. So we have, we have all the plans, I will come to that point as well, where we are doing those kind of research as well. But currently it's a, I would say more like a keyword research, because we are trying to maybe the human behavior, and that is what, like 95 to 98% problem lies for our clients. Something is available on a website which is not indexable on Google. It's not really their issues. If it's indeed web like less than point one or point zero, 1% of user effects has direct access to those websites or those platforms. They are not with some they are more like people problem. So they want to solve the mask. So we are solving the problem from their perspective,
So currently, if I like, if I give you an explanation, but client, our client, are looking for, it will I should be able to answer your question. So basically, most of these companies what they're looking for, like, if somebody is looking for their content, they should come to our official channels. Yeah. So how we approach this process as like mimicking the human behavior. For example, if you want to watch a Spider Man movie, but you will go, you will go on Google, Bing, or any other search engine or and you will search for Spider Man, let's say no way home, download or stream or something like that. So we are mimicking that behavior. So whatever being flagged by Google, we can extract all the information if, let's say somebody has put some other title or like letter instead of Spider Man, they put like, let's say Iron Man, yeah, but when I'm searching for Spider Man, I'm looking for more Spider Man, I'm getting a results of iron and it doesn't make sense. So we are trying to mimic the human behavior, to reduce the impact of these infringements on our clients. We can do a deeper analysis, whatever you mentioned, like, do a deeper like putting a bottom up to understand, like, how the content is available. Those technologies can be built, but these, these are the technologies that require extensive resources, research and all those things. So we have, we have all the plans, I will come to that point as well, where we are doing those kind of research as well. But currently it's a, I would say more like a keyword research, because we are trying to maybe the human behavior, and that is what, like 95 to 98% problem lies for our clients. Something is available on a website which is not indexable on Google. It's not really their issues. If it's indeed web like less than point one or point zero, 1% of user effects has direct access to those websites or those platforms. They are not with some they are more like people problem. So they want to solve the mask. So we are solving the problem from their perspective,
S Speaker 217:28it, got it. So filtering is auto external. Like, let's say we are searching for like, a PDF, if there is no PDF available on that particular home page. So we accept like, we remove it like, because there is no PDF label image. There is a name use that it doesn't make sense. Ranking, it's depend on, like, when we, for example, we are searching for a video content and audio content, so we try to analyze the home page, then the closeness to like the pilot separated websites or not ranking is decided like, because the more the piloted website, there more chances people are visiting that because it's available for free. So we have parameters for that. We have parameter on the Google search like, at what page or at what location the website is coming so that is another ranking. Then the third is how closely it is when we are looking for a PDF, we can we understand, like, the size of the PDF and everything, like, how these things are done. So we do there are other factors. We also include case to case basis, like, what is important for the closeness? Like, we try to identify, when we say ranking, we try to under closeness to the original content. And ranking is based on those parameters, Google ranking, video content, websites, content, which other, let's say, if I find a particular website that only shows movies and TV series, and I'm looking for an educational content so it will have a reduced ranking on those things. So all those parameters are considered, and we have been collecting data of those things for last three, four years, so we have an we understand the parameters in more deeper perspective, but parameters have been relevant for what kind of analysis. For example, we also provide newspaper companies the article protection services where we are protecting the articles. As soon as a newspaper company publish an article on their digital platform, some boards come to their website, copy the entire data and paste it on third party websites. To provide a ranking, we provide a ranking like, how closely like, for example, the original article, I have 100 words the but how many words are there? For example, like, how, how closely like? What is the overlap? Percentage of both the articles is 100% match, 90% match, or 50% match. Similarly, what kind of website, it's a partner websites, it's a pirated website, it's is it a repeat offender or not? All those vectors are considered for that kind of virus. So similarly, different factors are involved for different kind of content. Yes. Operating
it, got it. So filtering is auto external. Like, let's say we are searching for like, a PDF, if there is no PDF available on that particular home page. So we accept like, we remove it like, because there is no PDF label image. There is a name use that it doesn't make sense. Ranking, it's depend on, like, when we, for example, we are searching for a video content and audio content, so we try to analyze the home page, then the closeness to like the pilot separated websites or not ranking is decided like, because the more the piloted website, there more chances people are visiting that because it's available for free. So we have parameters for that. We have parameter on the Google search like, at what page or at what location the website is coming so that is another ranking. Then the third is how closely it is when we are looking for a PDF, we can we understand, like, the size of the PDF and everything, like, how these things are done. So we do there are other factors. We also include case to case basis, like, what is important for the closeness? Like, we try to identify, when we say ranking, we try to under closeness to the original content. And ranking is based on those parameters, Google ranking, video content, websites, content, which other, let's say, if I find a particular website that only shows movies and TV series, and I'm looking for an educational content so it will have a reduced ranking on those things. So all those parameters are considered, and we have been collecting data of those things for last three, four years, so we have an we understand the parameters in more deeper perspective, but parameters have been relevant for what kind of analysis. For example, we also provide newspaper companies the article protection services where we are protecting the articles. As soon as a newspaper company publish an article on their digital platform, some boards come to their website, copy the entire data and paste it on third party websites. To provide a ranking, we provide a ranking like, how closely like, for example, the original article, I have 100 words the but how many words are there? For example, like, how, how closely like? What is the overlap? Percentage of both the articles is 100% match, 90% match, or 50% match. Similarly, what kind of website, it's a partner websites, it's a pirated website, it's is it a repeat offender or not? All those vectors are considered for that kind of virus. So similarly, different factors are involved for different kind of content. Yes. Operating
it, got it. So filtering is auto external. Like, let's say we are searching for like, a PDF, if there is no PDF available on that particular home page. So we accept like, we remove it like, because there is no PDF label image. There is a name use that it doesn't make sense. Ranking, it's depend on, like, when we, for example, we are searching for a video content and audio content, so we try to analyze the home page, then the closeness to like the pilot separated websites or not ranking is decided like, because the more the piloted website, there more chances people are visiting that because it's available for free. So we have parameters for that. We have parameter on the Google search like, at what page or at what location the website is coming so that is another ranking. Then the third is how closely it is when we are looking for a PDF, we can we understand, like, the size of the PDF and everything, like, how these things are done. So we do there are other factors. We also include case to case basis, like, what is important for the closeness? Like, we try to identify, when we say ranking, we try to under closeness to the original content. And ranking is based on those parameters, Google ranking, video content, websites, content, which other, let's say, if I find a particular website that only shows movies and TV series, and I'm looking for an educational content so it will have a reduced ranking on those things. So all those parameters are considered, and we have been collecting data of those things for last three, four years, so we have an we understand the parameters in more deeper perspective, but parameters have been relevant for what kind of analysis. For example, we also provide newspaper companies the article protection services where we are protecting the articles. As soon as a newspaper company publish an article on their digital platform, some boards come to their website, copy the entire data and paste it on third party websites. To provide a ranking, we provide a ranking like, how closely like, for example, the original article, I have 100 words the but how many words are there? For example, like, how, how closely like? What is the overlap? Percentage of both the articles is 100% match, 90% match, or 50% match. Similarly, what kind of website, it's a partner websites, it's a pirated website, it's is it a repeat offender or not? All those vectors are considered for that kind of virus. So similarly, different factors are involved for different kind of content. Yes. Operating
it, got it. So filtering is auto external. Like, let's say we are searching for like, a PDF, if there is no PDF available on that particular home page. So we accept like, we remove it like, because there is no PDF label image. There is a name use that it doesn't make sense. Ranking, it's depend on, like, when we, for example, we are searching for a video content and audio content, so we try to analyze the home page, then the closeness to like the pilot separated websites or not ranking is decided like, because the more the piloted website, there more chances people are visiting that because it's available for free. So we have parameters for that. We have parameter on the Google search like, at what page or at what location the website is coming so that is another ranking. Then the third is how closely it is when we are looking for a PDF, we can we understand, like, the size of the PDF and everything, like, how these things are done. So we do there are other factors. We also include case to case basis, like, what is important for the closeness? Like, we try to identify, when we say ranking, we try to under closeness to the original content. And ranking is based on those parameters, Google ranking, video content, websites, content, which other, let's say, if I find a particular website that only shows movies and TV series, and I'm looking for an educational content so it will have a reduced ranking on those things. So all those parameters are considered, and we have been collecting data of those things for last three, four years, so we have an we understand the parameters in more deeper perspective, but parameters have been relevant for what kind of analysis. For example, we also provide newspaper companies the article protection services where we are protecting the articles. As soon as a newspaper company publish an article on their digital platform, some boards come to their website, copy the entire data and paste it on third party websites. To provide a ranking, we provide a ranking like, how closely like, for example, the original article, I have 100 words the but how many words are there? For example, like, how, how closely like? What is the overlap? Percentage of both the articles is 100% match, 90% match, or 50% match. Similarly, what kind of website, it's a partner websites, it's a pirated website, it's is it a repeat offender or not? All those vectors are considered for that kind of virus. So similarly, different factors are involved for different kind of content. Yes. Operating
S Speaker 120:15makes sense. Makes sense. And the infringement reporting, I think, is a very simple reporting back to the reporting
makes sense. Makes sense. And the infringement reporting, I think, is a very simple reporting back to the reporting
makes sense. Makes sense. And the infringement reporting, I think, is a very simple reporting back to the reporting
makes sense. Makes sense. And the infringement reporting, I think, is a very simple reporting back to the reporting
S Speaker 220:22back to the platforms. Like, Google has their own channels, like we can report it through a filling a form Facebook has similar. So what we have done, like, if you open a Google reporting, it's like a 20 field long form. So what we have done, we have built a Chrome extension that will feel automatically you choose the like in the Chrome extension when you're using it, you can choose like I'm reporting, for example, on the behalf of Indian experts, you can choose and the form will automatically get filled, and it will extend data from our dashboard, and all the evidences, all the link proofs, and everything, will be automatically filled, and we just have to press submit for them. And the next phase, we are trying to build it in our own platform so that nobody has to visit even the Google Form outside on a Google Chrome so we will extract all those capabilities on our dashboard. So that's the product region we are
back to the platforms. Like, Google has their own channels, like we can report it through a filling a form Facebook has similar. So what we have done, like, if you open a Google reporting, it's like a 20 field long form. So what we have done, we have built a Chrome extension that will feel automatically you choose the like in the Chrome extension when you're using it, you can choose like I'm reporting, for example, on the behalf of Indian experts, you can choose and the form will automatically get filled, and it will extend data from our dashboard, and all the evidences, all the link proofs, and everything, will be automatically filled, and we just have to press submit for them. And the next phase, we are trying to build it in our own platform so that nobody has to visit even the Google Form outside on a Google Chrome so we will extract all those capabilities on our dashboard. So that's the product region we are
back to the platforms. Like, Google has their own channels, like we can report it through a filling a form Facebook has similar. So what we have done, like, if you open a Google reporting, it's like a 20 field long form. So what we have done, we have built a Chrome extension that will feel automatically you choose the like in the Chrome extension when you're using it, you can choose like I'm reporting, for example, on the behalf of Indian experts, you can choose and the form will automatically get filled, and it will extend data from our dashboard, and all the evidences, all the link proofs, and everything, will be automatically filled, and we just have to press submit for them. And the next phase, we are trying to build it in our own platform so that nobody has to visit even the Google Form outside on a Google Chrome so we will extract all those capabilities on our dashboard. So that's the product region we are
back to the platforms. Like, Google has their own channels, like we can report it through a filling a form Facebook has similar. So what we have done, like, if you open a Google reporting, it's like a 20 field long form. So what we have done, we have built a Chrome extension that will feel automatically you choose the like in the Chrome extension when you're using it, you can choose like I'm reporting, for example, on the behalf of Indian experts, you can choose and the form will automatically get filled, and it will extend data from our dashboard, and all the evidences, all the link proofs, and everything, will be automatically filled, and we just have to press submit for them. And the next phase, we are trying to build it in our own platform so that nobody has to visit even the Google Form outside on a Google Chrome so we will extract all those capabilities on our dashboard. So that's the product region we are
S Speaker 221:21Yeah. So these are the major pain points. Like, these are the key use cases, digital piracy, brand improvement, all those factor either to the scam or revenue losses, subscriber losses or brand payment losses. So these are the use cases that have the similar process. Like, the underlying technology is different kind of content, and there is a different mechanism to report different kind of content, but the crawling mechanism and all those things like there is an overlap of the technology. That's why we have so many use cases we have identified. We started with just the digital piracy, but over the period, we have find brand infringement. When a brand infringement is somebody has created a fake website using your logo, without your permission, or all those things, or even if your image fake profile is like social media, fake profile in on Instagram, LinkedIn, or anywhere on social media platform, deep fake like, we are still working with deep fakes trying to identify, like, if we are able to identify using the thing which you discuss, The understanding the what, what is the content of video, and if you're able to extract the information about the video, like, how much is it edited or not? So it's under development, counterfeits, like, it's a similar technology for the E commerce platform. So we have been, we have built rollers for Amazon, Flipkart, Misha and and other websites. Similarly, we are also a great role as also people we were working with LEAs as a client who were facing those infringement on these food commerce websites as well. So that's the experiment we try to run. But we see a lot of companies in the food business, or you can say consumer businesses like Palestine, then cosmetics, consumer electronics and all those things, these, those. We can target those kind of companies as well. Phishing is name is clearly if somebody is trying to scam people in your name, like inviting them to your website, if you know of your website, something like that. Online scams are happening. Like people are selling fake currency, fake stock market, tapes, cryptocurrency and all those things, those online scams we are reporting and different things. So all those have the like, if you see like, we have to identify the infringement and then report it. So all the have the underlying thought process is same for all those users
Yeah. So these are the major pain points. Like, these are the key use cases, digital piracy, brand improvement, all those factor either to the scam or revenue losses, subscriber losses or brand payment losses. So these are the use cases that have the similar process. Like, the underlying technology is different kind of content, and there is a different mechanism to report different kind of content, but the crawling mechanism and all those things like there is an overlap of the technology. That's why we have so many use cases we have identified. We started with just the digital piracy, but over the period, we have find brand infringement. When a brand infringement is somebody has created a fake website using your logo, without your permission, or all those things, or even if your image fake profile is like social media, fake profile in on Instagram, LinkedIn, or anywhere on social media platform, deep fake like, we are still working with deep fakes trying to identify, like, if we are able to identify using the thing which you discuss, The understanding the what, what is the content of video, and if you're able to extract the information about the video, like, how much is it edited or not? So it's under development, counterfeits, like, it's a similar technology for the E commerce platform. So we have been, we have built rollers for Amazon, Flipkart, Misha and and other websites. Similarly, we are also a great role as also people we were working with LEAs as a client who were facing those infringement on these food commerce websites as well. So that's the experiment we try to run. But we see a lot of companies in the food business, or you can say consumer businesses like Palestine, then cosmetics, consumer electronics and all those things, these, those. We can target those kind of companies as well. Phishing is name is clearly if somebody is trying to scam people in your name, like inviting them to your website, if you know of your website, something like that. Online scams are happening. Like people are selling fake currency, fake stock market, tapes, cryptocurrency and all those things, those online scams we are reporting and different things. So all those have the like, if you see like, we have to identify the infringement and then report it. So all the have the underlying thought process is same for all those users
Yeah. So these are the major pain points. Like, these are the key use cases, digital piracy, brand improvement, all those factor either to the scam or revenue losses, subscriber losses or brand payment losses. So these are the use cases that have the similar process. Like, the underlying technology is different kind of content, and there is a different mechanism to report different kind of content, but the crawling mechanism and all those things like there is an overlap of the technology. That's why we have so many use cases we have identified. We started with just the digital piracy, but over the period, we have find brand infringement. When a brand infringement is somebody has created a fake website using your logo, without your permission, or all those things, or even if your image fake profile is like social media, fake profile in on Instagram, LinkedIn, or anywhere on social media platform, deep fake like, we are still working with deep fakes trying to identify, like, if we are able to identify using the thing which you discuss, The understanding the what, what is the content of video, and if you're able to extract the information about the video, like, how much is it edited or not? So it's under development, counterfeits, like, it's a similar technology for the E commerce platform. So we have been, we have built rollers for Amazon, Flipkart, Misha and and other websites. Similarly, we are also a great role as also people we were working with LEAs as a client who were facing those infringement on these food commerce websites as well. So that's the experiment we try to run. But we see a lot of companies in the food business, or you can say consumer businesses like Palestine, then cosmetics, consumer electronics and all those things, these, those. We can target those kind of companies as well. Phishing is name is clearly if somebody is trying to scam people in your name, like inviting them to your website, if you know of your website, something like that. Online scams are happening. Like people are selling fake currency, fake stock market, tapes, cryptocurrency and all those things, those online scams we are reporting and different things. So all those have the like, if you see like, we have to identify the infringement and then report it. So all the have the underlying thought process is same for all those users
Yeah. So these are the major pain points. Like, these are the key use cases, digital piracy, brand improvement, all those factor either to the scam or revenue losses, subscriber losses or brand payment losses. So these are the use cases that have the similar process. Like, the underlying technology is different kind of content, and there is a different mechanism to report different kind of content, but the crawling mechanism and all those things like there is an overlap of the technology. That's why we have so many use cases we have identified. We started with just the digital piracy, but over the period, we have find brand infringement. When a brand infringement is somebody has created a fake website using your logo, without your permission, or all those things, or even if your image fake profile is like social media, fake profile in on Instagram, LinkedIn, or anywhere on social media platform, deep fake like, we are still working with deep fakes trying to identify, like, if we are able to identify using the thing which you discuss, The understanding the what, what is the content of video, and if you're able to extract the information about the video, like, how much is it edited or not? So it's under development, counterfeits, like, it's a similar technology for the E commerce platform. So we have been, we have built rollers for Amazon, Flipkart, Misha and and other websites. Similarly, we are also a great role as also people we were working with LEAs as a client who were facing those infringement on these food commerce websites as well. So that's the experiment we try to run. But we see a lot of companies in the food business, or you can say consumer businesses like Palestine, then cosmetics, consumer electronics and all those things, these, those. We can target those kind of companies as well. Phishing is name is clearly if somebody is trying to scam people in your name, like inviting them to your website, if you know of your website, something like that. Online scams are happening. Like people are selling fake currency, fake stock market, tapes, cryptocurrency and all those things, those online scams we are reporting and different things. So all those have the like, if you see like, we have to identify the infringement and then report it. So all the have the underlying thought process is same for all those users
S Speaker 123:39and Manish here, I can imagine for let's say, if you you mentioned that currently you are also providing brand infringement and fake profile, and you're working on deep fake for brand infringement and fake profile, you would need to sort of like match on image data. So we have that technology
and Manish here, I can imagine for let's say, if you you mentioned that currently you are also providing brand infringement and fake profile, and you're working on deep fake for brand infringement and fake profile, you would need to sort of like match on image data. So we have that technology
and Manish here, I can imagine for let's say, if you you mentioned that currently you are also providing brand infringement and fake profile, and you're working on deep fake for brand infringement and fake profile, you would need to sort of like match on image data. So we have that technology
and Manish here, I can imagine for let's say, if you you mentioned that currently you are also providing brand infringement and fake profile, and you're working on deep fake for brand infringement and fake profile, you would need to sort of like match on image data. So we have that technology
S Speaker 224:00available currently. We do not want to implement it on a larger scale, because it's little expensive. We have to define it more. Yeah, more. But what we are doing right now for fake profile, mostly, we are working with the social media influencers, so it is easier to identify those images, because they are more popular names and everything. So that is where the we are providing the solution. But yes, you are correct, like we in future we might require the image processing and all those things. So those are also in the
available currently. We do not want to implement it on a larger scale, because it's little expensive. We have to define it more. Yeah, more. But what we are doing right now for fake profile, mostly, we are working with the social media influencers, so it is easier to identify those images, because they are more popular names and everything. So that is where the we are providing the solution. But yes, you are correct, like we in future we might require the image processing and all those things. So those are also in the
available currently. We do not want to implement it on a larger scale, because it's little expensive. We have to define it more. Yeah, more. But what we are doing right now for fake profile, mostly, we are working with the social media influencers, so it is easier to identify those images, because they are more popular names and everything. So that is where the we are providing the solution. But yes, you are correct, like we in future we might require the image processing and all those things. So those are also in the
available currently. We do not want to implement it on a larger scale, because it's little expensive. We have to define it more. Yeah, more. But what we are doing right now for fake profile, mostly, we are working with the social media influencers, so it is easier to identify those images, because they are more popular names and everything. So that is where the we are providing the solution. But yes, you are correct, like we in future we might require the image processing and all those things. So those are also in the
S Speaker 124:32part of future plans. Makes sense. Makes sense. Okay,
part of future plans. Makes sense. Makes sense. Okay,
part of future plans. Makes sense. Makes sense. Okay,
part of future plans. Makes sense. Makes sense. Okay,
S Speaker 224:35yeah, so we have underlined like we have built smaller, smaller like components for image processing, for even live streaming as well. We have built some prototype or something, but we have not used those, because those are not very efficient right now we have to make efficient. So for last 18 months, you can say that we are trying to, like to do two things. One is like, increase our revenue, so we are focusing on the revenue goals. And the second is improvement in the current use cases, which we are currently solving, to improve them in a way that the problem gets solved for our plan. So when we have a happy client, then it will be easier for us to fund our next set of operations.
yeah, so we have underlined like we have built smaller, smaller like components for image processing, for even live streaming as well. We have built some prototype or something, but we have not used those, because those are not very efficient right now we have to make efficient. So for last 18 months, you can say that we are trying to, like to do two things. One is like, increase our revenue, so we are focusing on the revenue goals. And the second is improvement in the current use cases, which we are currently solving, to improve them in a way that the problem gets solved for our plan. So when we have a happy client, then it will be easier for us to fund our next set of operations.
yeah, so we have underlined like we have built smaller, smaller like components for image processing, for even live streaming as well. We have built some prototype or something, but we have not used those, because those are not very efficient right now we have to make efficient. So for last 18 months, you can say that we are trying to, like to do two things. One is like, increase our revenue, so we are focusing on the revenue goals. And the second is improvement in the current use cases, which we are currently solving, to improve them in a way that the problem gets solved for our plan. So when we have a happy client, then it will be easier for us to fund our next set of operations.
yeah, so we have underlined like we have built smaller, smaller like components for image processing, for even live streaming as well. We have built some prototype or something, but we have not used those, because those are not very efficient right now we have to make efficient. So for last 18 months, you can say that we are trying to, like to do two things. One is like, increase our revenue, so we are focusing on the revenue goals. And the second is improvement in the current use cases, which we are currently solving, to improve them in a way that the problem gets solved for our plan. So when we have a happy client, then it will be easier for us to fund our next set of operations.
S Speaker 125:17No, that makes sense, and I think you are thinking about it the rank way, just having a deep penetration in one niche that you have identified, where there is a willingness to pay and then expanding from there makes a lot of sense.
No, that makes sense, and I think you are thinking about it the rank way, just having a deep penetration in one niche that you have identified, where there is a willingness to pay and then expanding from there makes a lot of sense.
No, that makes sense, and I think you are thinking about it the rank way, just having a deep penetration in one niche that you have identified, where there is a willingness to pay and then expanding from there makes a lot of sense.
No, that makes sense, and I think you are thinking about it the rank way, just having a deep penetration in one niche that you have identified, where there is a willingness to pay and then expanding from there makes a lot of sense.
S Speaker 225:32Yes, so this is like the slides are under development. But just to give you, like, how we are doing it, so it's a force like four step process, like, what is data collection? So when, when we say data collection is the preliminary data collection, which we are getting from our clients, so it includes the metadata, set information, official listings, primary keywords, and the secondary information, secondary research is that MIT. So all those things put in are put into our system so that we know, like, what what to look for and how to so it's after data collection. The key keywords are generated from like we get the primary listing, but we generate our own keywords based on the our Model and Model like, because we know, for example, if it's financial influencer. So we know that what kind of prefixes and suffixes we have to add so that we can get wider coverage for that particular client. So those once keyword is done, we put it all the keywords into our system, and that gives the like we if we want to run a single crawler only on Google, we can do that. If we want to run all the 10 to 20 crawlers, we can that as well. So we can be selective, like, what kind of and these crawlers and scrapers, like, identify all the information infringement they collect the evidence. And this evidence collection is, again, it's optional, like, we do not want to collect evidence, because then, if we take screenshot of everything, like, it becomes a data heavy project as well. So we understand when to take screenshot, when not to take it, and all those things. So we can do that, and we do all the website footprint analysis, group analysis, and all those things, so we identify those things. Then once we get the Raw Data, our filtering and ranking mechanism does their work like they filter out the false positive as much as possible, and then we thank them based on the priority order. And we do verification like cross verification with the client, so that we do not end up sending take down strike on their one of their official listing, which are which they forgot to mention. So we have found the cases like, we collect all the official listings. Still there are few missing because the team member was not aware of it. And this is the case when we are working with the larger organization. Like, not everybody knows, like, what kind of platforms, and especially the education client, because the teachers are changing every month and so and new teachers are getting added. So it becomes really difficult for them as well to monitor that. So we give that cross verification option. If they want to do that, they can do that once the links are locked, like, whenever they have, like, give us the headset that those are the links. Let's say we have done 100 links, and these 90 links need to be reported. Our team will shoot all the reporting mechanism and all those things will be done. So we send direct notices, platform notices. We do follows, we do remove content from social media platforms, search engines, from direct websites, from Cloud Storage websites, and all those things. So we remove all those things. If you want to go deeper, we can be we have capabilities to remove it from dark web data as well. But again, those comes with an expense. So currently, we are providing those service activity, and we do a constant monitoring if the website comes back or the content comes back. So what steps need to be taken? So we do a constant monitoring of those things as well. So where we want to add more like where we want to invest more is to develop our analysis engines more by introducing newer AI Gen AI technologies and ml technologies to improve this raw data into a better so that we can reduce the false positive as much as possible, and so that the manual effort even done by us or my client does not become a part of it, become a constant part of it. So we want to reduce the dependency on manual as much as possible. We have already reduced it to some extent, but we want to reduce it to 95 to 98% so that's how we want to build the company, and that's how we will be using the AI model to understand the comparisons and the data collection, which we have done, and also we will training those models and get that desired out. So that's where we are trying to build that's
Yes, so this is like the slides are under development. But just to give you, like, how we are doing it, so it's a force like four step process, like, what is data collection? So when, when we say data collection is the preliminary data collection, which we are getting from our clients, so it includes the metadata, set information, official listings, primary keywords, and the secondary information, secondary research is that MIT. So all those things put in are put into our system so that we know, like, what what to look for and how to so it's after data collection. The key keywords are generated from like we get the primary listing, but we generate our own keywords based on the our Model and Model like, because we know, for example, if it's financial influencer. So we know that what kind of prefixes and suffixes we have to add so that we can get wider coverage for that particular client. So those once keyword is done, we put it all the keywords into our system, and that gives the like we if we want to run a single crawler only on Google, we can do that. If we want to run all the 10 to 20 crawlers, we can that as well. So we can be selective, like, what kind of and these crawlers and scrapers, like, identify all the information infringement they collect the evidence. And this evidence collection is, again, it's optional, like, we do not want to collect evidence, because then, if we take screenshot of everything, like, it becomes a data heavy project as well. So we understand when to take screenshot, when not to take it, and all those things. So we can do that, and we do all the website footprint analysis, group analysis, and all those things, so we identify those things. Then once we get the Raw Data, our filtering and ranking mechanism does their work like they filter out the false positive as much as possible, and then we thank them based on the priority order. And we do verification like cross verification with the client, so that we do not end up sending take down strike on their one of their official listing, which are which they forgot to mention. So we have found the cases like, we collect all the official listings. Still there are few missing because the team member was not aware of it. And this is the case when we are working with the larger organization. Like, not everybody knows, like, what kind of platforms, and especially the education client, because the teachers are changing every month and so and new teachers are getting added. So it becomes really difficult for them as well to monitor that. So we give that cross verification option. If they want to do that, they can do that once the links are locked, like, whenever they have, like, give us the headset that those are the links. Let's say we have done 100 links, and these 90 links need to be reported. Our team will shoot all the reporting mechanism and all those things will be done. So we send direct notices, platform notices. We do follows, we do remove content from social media platforms, search engines, from direct websites, from Cloud Storage websites, and all those things. So we remove all those things. If you want to go deeper, we can be we have capabilities to remove it from dark web data as well. But again, those comes with an expense. So currently, we are providing those service activity, and we do a constant monitoring if the website comes back or the content comes back. So what steps need to be taken? So we do a constant monitoring of those things as well. So where we want to add more like where we want to invest more is to develop our analysis engines more by introducing newer AI Gen AI technologies and ml technologies to improve this raw data into a better so that we can reduce the false positive as much as possible, and so that the manual effort even done by us or my client does not become a part of it, become a constant part of it. So we want to reduce the dependency on manual as much as possible. We have already reduced it to some extent, but we want to reduce it to 95 to 98% so that's how we want to build the company, and that's how we will be using the AI model to understand the comparisons and the data collection, which we have done, and also we will training those models and get that desired out. So that's where we are trying to build that's
Yes, so this is like the slides are under development. But just to give you, like, how we are doing it, so it's a force like four step process, like, what is data collection? So when, when we say data collection is the preliminary data collection, which we are getting from our clients, so it includes the metadata, set information, official listings, primary keywords, and the secondary information, secondary research is that MIT. So all those things put in are put into our system so that we know, like, what what to look for and how to so it's after data collection. The key keywords are generated from like we get the primary listing, but we generate our own keywords based on the our Model and Model like, because we know, for example, if it's financial influencer. So we know that what kind of prefixes and suffixes we have to add so that we can get wider coverage for that particular client. So those once keyword is done, we put it all the keywords into our system, and that gives the like we if we want to run a single crawler only on Google, we can do that. If we want to run all the 10 to 20 crawlers, we can that as well. So we can be selective, like, what kind of and these crawlers and scrapers, like, identify all the information infringement they collect the evidence. And this evidence collection is, again, it's optional, like, we do not want to collect evidence, because then, if we take screenshot of everything, like, it becomes a data heavy project as well. So we understand when to take screenshot, when not to take it, and all those things. So we can do that, and we do all the website footprint analysis, group analysis, and all those things, so we identify those things. Then once we get the Raw Data, our filtering and ranking mechanism does their work like they filter out the false positive as much as possible, and then we thank them based on the priority order. And we do verification like cross verification with the client, so that we do not end up sending take down strike on their one of their official listing, which are which they forgot to mention. So we have found the cases like, we collect all the official listings. Still there are few missing because the team member was not aware of it. And this is the case when we are working with the larger organization. Like, not everybody knows, like, what kind of platforms, and especially the education client, because the teachers are changing every month and so and new teachers are getting added. So it becomes really difficult for them as well to monitor that. So we give that cross verification option. If they want to do that, they can do that once the links are locked, like, whenever they have, like, give us the headset that those are the links. Let's say we have done 100 links, and these 90 links need to be reported. Our team will shoot all the reporting mechanism and all those things will be done. So we send direct notices, platform notices. We do follows, we do remove content from social media platforms, search engines, from direct websites, from Cloud Storage websites, and all those things. So we remove all those things. If you want to go deeper, we can be we have capabilities to remove it from dark web data as well. But again, those comes with an expense. So currently, we are providing those service activity, and we do a constant monitoring if the website comes back or the content comes back. So what steps need to be taken? So we do a constant monitoring of those things as well. So where we want to add more like where we want to invest more is to develop our analysis engines more by introducing newer AI Gen AI technologies and ml technologies to improve this raw data into a better so that we can reduce the false positive as much as possible, and so that the manual effort even done by us or my client does not become a part of it, become a constant part of it. So we want to reduce the dependency on manual as much as possible. We have already reduced it to some extent, but we want to reduce it to 95 to 98% so that's how we want to build the company, and that's how we will be using the AI model to understand the comparisons and the data collection, which we have done, and also we will training those models and get that desired out. So that's where we are trying to build that's
Yes, so this is like the slides are under development. But just to give you, like, how we are doing it, so it's a force like four step process, like, what is data collection? So when, when we say data collection is the preliminary data collection, which we are getting from our clients, so it includes the metadata, set information, official listings, primary keywords, and the secondary information, secondary research is that MIT. So all those things put in are put into our system so that we know, like, what what to look for and how to so it's after data collection. The key keywords are generated from like we get the primary listing, but we generate our own keywords based on the our Model and Model like, because we know, for example, if it's financial influencer. So we know that what kind of prefixes and suffixes we have to add so that we can get wider coverage for that particular client. So those once keyword is done, we put it all the keywords into our system, and that gives the like we if we want to run a single crawler only on Google, we can do that. If we want to run all the 10 to 20 crawlers, we can that as well. So we can be selective, like, what kind of and these crawlers and scrapers, like, identify all the information infringement they collect the evidence. And this evidence collection is, again, it's optional, like, we do not want to collect evidence, because then, if we take screenshot of everything, like, it becomes a data heavy project as well. So we understand when to take screenshot, when not to take it, and all those things. So we can do that, and we do all the website footprint analysis, group analysis, and all those things, so we identify those things. Then once we get the Raw Data, our filtering and ranking mechanism does their work like they filter out the false positive as much as possible, and then we thank them based on the priority order. And we do verification like cross verification with the client, so that we do not end up sending take down strike on their one of their official listing, which are which they forgot to mention. So we have found the cases like, we collect all the official listings. Still there are few missing because the team member was not aware of it. And this is the case when we are working with the larger organization. Like, not everybody knows, like, what kind of platforms, and especially the education client, because the teachers are changing every month and so and new teachers are getting added. So it becomes really difficult for them as well to monitor that. So we give that cross verification option. If they want to do that, they can do that once the links are locked, like, whenever they have, like, give us the headset that those are the links. Let's say we have done 100 links, and these 90 links need to be reported. Our team will shoot all the reporting mechanism and all those things will be done. So we send direct notices, platform notices. We do follows, we do remove content from social media platforms, search engines, from direct websites, from Cloud Storage websites, and all those things. So we remove all those things. If you want to go deeper, we can be we have capabilities to remove it from dark web data as well. But again, those comes with an expense. So currently, we are providing those service activity, and we do a constant monitoring if the website comes back or the content comes back. So what steps need to be taken? So we do a constant monitoring of those things as well. So where we want to add more like where we want to invest more is to develop our analysis engines more by introducing newer AI Gen AI technologies and ml technologies to improve this raw data into a better so that we can reduce the false positive as much as possible, and so that the manual effort even done by us or my client does not become a part of it, become a constant part of it. So we want to reduce the dependency on manual as much as possible. We have already reduced it to some extent, but we want to reduce it to 95 to 98% so that's how we want to build the company, and that's how we will be using the AI model to understand the comparisons and the data collection, which we have done, and also we will training those models and get that desired out. So that's where we are trying to build that's
S Speaker 129:55very interesting. MANISH, and like, I know we are about time. I have to go in about five minutes. But can you give me a brief overview on what the traction has been so far, and are you raising currently, and how would the funds be used? Something like that? Yeah,
very interesting. MANISH, and like, I know we are about time. I have to go in about five minutes. But can you give me a brief overview on what the traction has been so far, and are you raising currently, and how would the funds be used? Something like that? Yeah,
very interesting. MANISH, and like, I know we are about time. I have to go in about five minutes. But can you give me a brief overview on what the traction has been so far, and are you raising currently, and how would the funds be used? Something like that? Yeah,
very interesting. MANISH, and like, I know we are about time. I have to go in about five minutes. But can you give me a brief overview on what the traction has been so far, and are you raising currently, and how would the funds be used? Something like that? Yeah,
S Speaker 230:12so if you see about the current traction, so we are at almost a million dollar. Arr, so in last two years, we have increased our ARR by 7x we have more than 150 active clients around you can say 90% are from India, and less start from Middle East and other few other parts of the world. We want to focus on the US and the bigger markets we have identified the US is obviously the obvious choice, but we have seen some traction in the Middle East, which we were not envisioning when we, like couple of years ago. But now we are getting a lot of traction there. So we are getting trying to thinking of setting up something there.
so if you see about the current traction, so we are at almost a million dollar. Arr, so in last two years, we have increased our ARR by 7x we have more than 150 active clients around you can say 90% are from India, and less start from Middle East and other few other parts of the world. We want to focus on the US and the bigger markets we have identified the US is obviously the obvious choice, but we have seen some traction in the Middle East, which we were not envisioning when we, like couple of years ago. But now we are getting a lot of traction there. So we are getting trying to thinking of setting up something there.
so if you see about the current traction, so we are at almost a million dollar. Arr, so in last two years, we have increased our ARR by 7x we have more than 150 active clients around you can say 90% are from India, and less start from Middle East and other few other parts of the world. We want to focus on the US and the bigger markets we have identified the US is obviously the obvious choice, but we have seen some traction in the Middle East, which we were not envisioning when we, like couple of years ago. But now we are getting a lot of traction there. So we are getting trying to thinking of setting up something there.
so if you see about the current traction, so we are at almost a million dollar. Arr, so in last two years, we have increased our ARR by 7x we have more than 150 active clients around you can say 90% are from India, and less start from Middle East and other few other parts of the world. We want to focus on the US and the bigger markets we have identified the US is obviously the obvious choice, but we have seen some traction in the Middle East, which we were not envisioning when we, like couple of years ago. But now we are getting a lot of traction there. So we are getting trying to thinking of setting up something there.
S Speaker 131:14Absolutely. Yeah, yeah. And any update on the fundraising side,
Absolutely. Yeah, yeah. And any update on the fundraising side,
Absolutely. Yeah, yeah. And any update on the fundraising side,
Absolutely. Yeah, yeah. And any update on the fundraising side,
S Speaker 231:18fundraise like we have not started, started officially, but like, we are preparing, like, I'm still working on the deck, you can see mostly and we are, we still have to find out the final number, like, should we go for a smaller seed round of 2 million, or should we go for a Series A around seven to 8 million, maybe 10 million? So we are figuring out, like, what should be the right approach or right step for a company? So we are discussing this internally, and we should be able to, we should be able to project all those like, what are the use uses of those funders and everything. So we, in a week's time, or maybe in 10 days, we will have all those
fundraise like we have not started, started officially, but like, we are preparing, like, I'm still working on the deck, you can see mostly and we are, we still have to find out the final number, like, should we go for a smaller seed round of 2 million, or should we go for a Series A around seven to 8 million, maybe 10 million? So we are figuring out, like, what should be the right approach or right step for a company? So we are discussing this internally, and we should be able to, we should be able to project all those like, what are the use uses of those funders and everything. So we, in a week's time, or maybe in 10 days, we will have all those
fundraise like we have not started, started officially, but like, we are preparing, like, I'm still working on the deck, you can see mostly and we are, we still have to find out the final number, like, should we go for a smaller seed round of 2 million, or should we go for a Series A around seven to 8 million, maybe 10 million? So we are figuring out, like, what should be the right approach or right step for a company? So we are discussing this internally, and we should be able to, we should be able to project all those like, what are the use uses of those funders and everything. So we, in a week's time, or maybe in 10 days, we will have all those
fundraise like we have not started, started officially, but like, we are preparing, like, I'm still working on the deck, you can see mostly and we are, we still have to find out the final number, like, should we go for a smaller seed round of 2 million, or should we go for a Series A around seven to 8 million, maybe 10 million? So we are figuring out, like, what should be the right approach or right step for a company? So we are discussing this internally, and we should be able to, we should be able to project all those like, what are the use uses of those funders and everything. So we, in a week's time, or maybe in 10 days, we will have all those
S Speaker 132:19it's it's in San Francisco, and I know there are a lot of UAE based companies coming there. It was organized by a UAE based accelerator, so it might be a good one for you to go. I will be there. And in case you do plan to go, let me know I might be able to get you a ticket without the cost, because I'm going there as an investor, so I think it'll be good, but otherwise, yeah, keep me posted about everything. And
it's it's in San Francisco, and I know there are a lot of UAE based companies coming there. It was organized by a UAE based accelerator, so it might be a good one for you to go. I will be there. And in case you do plan to go, let me know I might be able to get you a ticket without the cost, because I'm going there as an investor, so I think it'll be good, but otherwise, yeah, keep me posted about everything. And
it's it's in San Francisco, and I know there are a lot of UAE based companies coming there. It was organized by a UAE based accelerator, so it might be a good one for you to go. I will be there. And in case you do plan to go, let me know I might be able to get you a ticket without the cost, because I'm going there as an investor, so I think it'll be good, but otherwise, yeah, keep me posted about everything. And
it's it's in San Francisco, and I know there are a lot of UAE based companies coming there. It was organized by a UAE based accelerator, so it might be a good one for you to go. I will be there. And in case you do plan to go, let me know I might be able to get you a ticket without the cost, because I'm going there as an investor, so I think it'll be good, but otherwise, yeah, keep me posted about everything. And
S Speaker 132:55it's on Thursday this week. Okay, yeah, Thursday. Okay, perfect. Perfect, man, I drop your map. your message on LinkedIn, and keep me posted. Thanks, man. Thanks a lot for the update. Yeah, great catching up. Bye. Bye. Bye. Bye.
it's on Thursday this week. Okay, yeah, Thursday. Okay, perfect. Perfect, man, I drop your map. your message on LinkedIn, and keep me posted. Thanks, man. Thanks a lot for the update. Yeah, great catching up. Bye. Bye. Bye. Bye.
it's on Thursday this week. Okay, yeah, Thursday. Okay, perfect. Perfect, man, I drop your map. your message on LinkedIn, and keep me posted. Thanks, man. Thanks a lot for the update. Yeah, great catching up. Bye. Bye. Bye. Bye.
it's on Thursday this week. Okay, yeah, Thursday. Okay, perfect. Perfect, man, I drop your map. your message on LinkedIn, and keep me posted. Thanks, man. Thanks a lot for the update. Yeah, great catching up. Bye. Bye. Bye. Bye.