Meeting: Note
Fri, Oct 4, 2024
3:33 PM
8 min
Priyesh P
Local Model Execution for Avatar Companies
0:00
Partne
URL: https://otter.ai/u/UiynN6otBlEygonfFltlh2MhOUk
Downloaded: 2025-12-22T14:12:55.467953
Method: text_extraction
============================================================

S Speaker 10:00Now they want to sell their avatar, making things like you know, to different consumers, as well as like different enterprises. Now that requires you to capture even micro expressions, voice right, like everything, and there's so many companies which are trying to do like this, to voice aspect, right? And these enterprises and their employees are not comfortable sharing their voice samples with these third party applications for fine tuning, right? They would rather have a model that runs locally. So we go ahead and enable like these Gen AI avatar companies to run their models like locally and enable them to fine tune like so that it is able to, like, you know, capture your video data entirely, and then have like, very well, fine tuned, you know, avatar for different use cases. And could be like, you know, for just your gaming purposes. It could be like, you know, for taking any customer call, or like, you know, your meeting avatar or anything. So that's where like is another intersection, so that's where we do very well. And Tom, do you want to talk about the chip makers and cloud providers? Because that's more significant?
Now they want to sell their avatar, making things like you know, to different consumers, as well as like different enterprises. Now that requires you to capture even micro expressions, voice right, like everything, and there's so many companies which are trying to do like this, to voice aspect, right? And these enterprises and their employees are not comfortable sharing their voice samples with these third party applications for fine tuning, right? They would rather have a model that runs locally. So we go ahead and enable like these Gen AI avatar companies to run their models like locally and enable them to fine tune like so that it is able to, like, you know, capture your video data entirely, and then have like, very well, fine tuned, you know, avatar for different use cases. And could be like, you know, for just your gaming purposes. It could be like, you know, for taking any customer call, or like, you know, your meeting avatar or anything. So that's where like is another intersection, so that's where we do very well. And Tom, do you want to talk about the chip makers and cloud providers? Because that's more significant?
Now they want to sell their avatar, making things like you know, to different consumers, as well as like different enterprises. Now that requires you to capture even micro expressions, voice right, like everything, and there's so many companies which are trying to do like this, to voice aspect, right? And these enterprises and their employees are not comfortable sharing their voice samples with these third party applications for fine tuning, right? They would rather have a model that runs locally. So we go ahead and enable like these Gen AI avatar companies to run their models like locally and enable them to fine tune like so that it is able to, like, you know, capture your video data entirely, and then have like, very well, fine tuned, you know, avatar for different use cases. And could be like, you know, for just your gaming purposes. It could be like, you know, for taking any customer call, or like, you know, your meeting avatar or anything. So that's where like is another intersection, so that's where we do very well. And Tom, do you want to talk about the chip makers and cloud providers? Because that's more significant?
Now they want to sell their avatar, making things like you know, to different consumers, as well as like different enterprises. Now that requires you to capture even micro expressions, voice right, like everything, and there's so many companies which are trying to do like this, to voice aspect, right? And these enterprises and their employees are not comfortable sharing their voice samples with these third party applications for fine tuning, right? They would rather have a model that runs locally. So we go ahead and enable like these Gen AI avatar companies to run their models like locally and enable them to fine tune like so that it is able to, like, you know, capture your video data entirely, and then have like, very well, fine tuned, you know, avatar for different use cases. And could be like, you know, for just your gaming purposes. It could be like, you know, for taking any customer call, or like, you know, your meeting avatar or anything. So that's where like is another intersection, so that's where we do very well. And Tom, do you want to talk about the chip makers and cloud providers? Because that's more significant?
S Speaker 21:11Yeah, so we have some existing partnerships in place. One of the ones that's further along is with salvonoma. So they're building custom hardware that they're deploying into the cloud, and some of their customers also have some edge needs as well, which they don't currently support. So the workflow that we're envisioning is you still have pre training happens in the cloud, and fine tuning and inference on the private data is then handled on prem, and that's where we come in. So someone who has already sent their customers our way. That's how our partnership with Canvas got started, and we're looking at recent building a unified workflow. Cerebrus has similar designs, just with different hardware under the hood, and then flex AI is also looking to solve some of the same technical issues that we are so they're doing cloud based training as well, but like us, they're looking to address basically building a universal Compute Engine to handle heterogeneity hardware. So there's likely opportunities for us to work together on solving this issue, primarily looking at going through the compiler stack to be able to generate code for multiple back ends. This kind of ties back to your earlier question about what components are running on different laptops and how we how we plan to handle
Yeah, so we have some existing partnerships in place. One of the ones that's further along is with salvonoma. So they're building custom hardware that they're deploying into the cloud, and some of their customers also have some edge needs as well, which they don't currently support. So the workflow that we're envisioning is you still have pre training happens in the cloud, and fine tuning and inference on the private data is then handled on prem, and that's where we come in. So someone who has already sent their customers our way. That's how our partnership with Canvas got started, and we're looking at recent building a unified workflow. Cerebrus has similar designs, just with different hardware under the hood, and then flex AI is also looking to solve some of the same technical issues that we are so they're doing cloud based training as well, but like us, they're looking to address basically building a universal Compute Engine to handle heterogeneity hardware. So there's likely opportunities for us to work together on solving this issue, primarily looking at going through the compiler stack to be able to generate code for multiple back ends. This kind of ties back to your earlier question about what components are running on different laptops and how we how we plan to handle
Yeah, so we have some existing partnerships in place. One of the ones that's further along is with salvonoma. So they're building custom hardware that they're deploying into the cloud, and some of their customers also have some edge needs as well, which they don't currently support. So the workflow that we're envisioning is you still have pre training happens in the cloud, and fine tuning and inference on the private data is then handled on prem, and that's where we come in. So someone who has already sent their customers our way. That's how our partnership with Canvas got started, and we're looking at recent building a unified workflow. Cerebrus has similar designs, just with different hardware under the hood, and then flex AI is also looking to solve some of the same technical issues that we are so they're doing cloud based training as well, but like us, they're looking to address basically building a universal Compute Engine to handle heterogeneity hardware. So there's likely opportunities for us to work together on solving this issue, primarily looking at going through the compiler stack to be able to generate code for multiple back ends. This kind of ties back to your earlier question about what components are running on different laptops and how we how we plan to handle
Yeah, so we have some existing partnerships in place. One of the ones that's further along is with salvonoma. So they're building custom hardware that they're deploying into the cloud, and some of their customers also have some edge needs as well, which they don't currently support. So the workflow that we're envisioning is you still have pre training happens in the cloud, and fine tuning and inference on the private data is then handled on prem, and that's where we come in. So someone who has already sent their customers our way. That's how our partnership with Canvas got started, and we're looking at recent building a unified workflow. Cerebrus has similar designs, just with different hardware under the hood, and then flex AI is also looking to solve some of the same technical issues that we are so they're doing cloud based training as well, but like us, they're looking to address basically building a universal Compute Engine to handle heterogeneity hardware. So there's likely opportunities for us to work together on solving this issue, primarily looking at going through the compiler stack to be able to generate code for multiple back ends. This kind of ties back to your earlier question about what components are running on different laptops and how we how we plan to handle
S Speaker 12:36that. These are like use cases for dokigami as well, like for example, for dokigami or Accenture, we are trying to enable their on device modules for continuous training and fine tuning for their document generation platform. So you might have, like, you know, some document locally, right, like sitting on my laptop, and then I want to generate, like, similar documents based on five or 10 different templates that I have, and then automatically serve as, like, Grammarly for your legal documents. So now there is a lot of rlhf data that goes into play, right? So that's where continuous training also comes in. So we enable their continuous training, like module, they're like, you know, composite of different models to enable their document generation. Like, happen locally on device for these customers, on prem. Now, looking at some competitive analysis, like scale does a lot of rlhf stuff, but it's very manual. We do enable continuous training, so that's why they are in the matrix. Anyscale is another one. Now they do distributed ml on Cloud very well, but they don't support the edge device aspect and the new cloud that we envision. It's not just like combination of just, you know, your heterogeneous cloud, but it also should account like, you know, these edge devices as well in the future, as they become more and more like, you know, compute capable. So that's why, like we think, we have a leverage over any scale as well. Then mosaic like, they don't focus on on prem. The same with like, you know, Apple, Microsoft and Google as well, because most of these people are making things which are specific to their like, you know, ecosystem, rather than like, building it across and solve the heterogeneity problem in general for us, like we saw the heterogeneity problem, we want to enable this, like, across different chip types, across different device types. That's why, like, you know, an OS type. So that gives us, like, you know, a different edge in the market. So we are raising $5 million right now. Of course, like, we will close it as well as possible. But we do want, like, some you know, strategic checks as well. And we think, like, Qualcomm is going to be a great partner here, especially from, you know, the hardware angle that we talked about and the heterogeneity solution angle as well, could be very interesting for Qualcomm, because we do understand, like, Okay, we have different type of OS types, right? So we like, you know, for example, Mac works with MPs, Windows works with CUDA. And then as you moved, like, you know, different chip type level and different kernel level, then there is much more like heterogeneity there. So abstraction is needed in each of these layers. And I think, like this problem would be an interesting angle, just this problem, like none of anything else like this. This problem itself would be of great strategic angle to call com. So we think, like you know, this is something we can co collaborate, or, like, you know, work together. That would be amazing. And of course, like this, other forms of partnerships that we do as well,
that. These are like use cases for dokigami as well, like for example, for dokigami or Accenture, we are trying to enable their on device modules for continuous training and fine tuning for their document generation platform. So you might have, like, you know, some document locally, right, like sitting on my laptop, and then I want to generate, like, similar documents based on five or 10 different templates that I have, and then automatically serve as, like, Grammarly for your legal documents. So now there is a lot of rlhf data that goes into play, right? So that's where continuous training also comes in. So we enable their continuous training, like module, they're like, you know, composite of different models to enable their document generation. Like, happen locally on device for these customers, on prem. Now, looking at some competitive analysis, like scale does a lot of rlhf stuff, but it's very manual. We do enable continuous training, so that's why they are in the matrix. Anyscale is another one. Now they do distributed ml on Cloud very well, but they don't support the edge device aspect and the new cloud that we envision. It's not just like combination of just, you know, your heterogeneous cloud, but it also should account like, you know, these edge devices as well in the future, as they become more and more like, you know, compute capable. So that's why, like we think, we have a leverage over any scale as well. Then mosaic like, they don't focus on on prem. The same with like, you know, Apple, Microsoft and Google as well, because most of these people are making things which are specific to their like, you know, ecosystem, rather than like, building it across and solve the heterogeneity problem in general for us, like we saw the heterogeneity problem, we want to enable this, like, across different chip types, across different device types. That's why, like, you know, an OS type. So that gives us, like, you know, a different edge in the market. So we are raising $5 million right now. Of course, like, we will close it as well as possible. But we do want, like, some you know, strategic checks as well. And we think, like, Qualcomm is going to be a great partner here, especially from, you know, the hardware angle that we talked about and the heterogeneity solution angle as well, could be very interesting for Qualcomm, because we do understand, like, Okay, we have different type of OS types, right? So we like, you know, for example, Mac works with MPs, Windows works with CUDA. And then as you moved, like, you know, different chip type level and different kernel level, then there is much more like heterogeneity there. So abstraction is needed in each of these layers. And I think, like this problem would be an interesting angle, just this problem, like none of anything else like this. This problem itself would be of great strategic angle to call com. So we think, like you know, this is something we can co collaborate, or, like, you know, work together. That would be amazing. And of course, like this, other forms of partnerships that we do as well,
that. These are like use cases for dokigami as well, like for example, for dokigami or Accenture, we are trying to enable their on device modules for continuous training and fine tuning for their document generation platform. So you might have, like, you know, some document locally, right, like sitting on my laptop, and then I want to generate, like, similar documents based on five or 10 different templates that I have, and then automatically serve as, like, Grammarly for your legal documents. So now there is a lot of rlhf data that goes into play, right? So that's where continuous training also comes in. So we enable their continuous training, like module, they're like, you know, composite of different models to enable their document generation. Like, happen locally on device for these customers, on prem. Now, looking at some competitive analysis, like scale does a lot of rlhf stuff, but it's very manual. We do enable continuous training, so that's why they are in the matrix. Anyscale is another one. Now they do distributed ml on Cloud very well, but they don't support the edge device aspect and the new cloud that we envision. It's not just like combination of just, you know, your heterogeneous cloud, but it also should account like, you know, these edge devices as well in the future, as they become more and more like, you know, compute capable. So that's why, like we think, we have a leverage over any scale as well. Then mosaic like, they don't focus on on prem. The same with like, you know, Apple, Microsoft and Google as well, because most of these people are making things which are specific to their like, you know, ecosystem, rather than like, building it across and solve the heterogeneity problem in general for us, like we saw the heterogeneity problem, we want to enable this, like, across different chip types, across different device types. That's why, like, you know, an OS type. So that gives us, like, you know, a different edge in the market. So we are raising $5 million right now. Of course, like, we will close it as well as possible. But we do want, like, some you know, strategic checks as well. And we think, like, Qualcomm is going to be a great partner here, especially from, you know, the hardware angle that we talked about and the heterogeneity solution angle as well, could be very interesting for Qualcomm, because we do understand, like, Okay, we have different type of OS types, right? So we like, you know, for example, Mac works with MPs, Windows works with CUDA. And then as you moved, like, you know, different chip type level and different kernel level, then there is much more like heterogeneity there. So abstraction is needed in each of these layers. And I think, like this problem would be an interesting angle, just this problem, like none of anything else like this. This problem itself would be of great strategic angle to call com. So we think, like you know, this is something we can co collaborate, or, like, you know, work together. That would be amazing. And of course, like this, other forms of partnerships that we do as well,
that. These are like use cases for dokigami as well, like for example, for dokigami or Accenture, we are trying to enable their on device modules for continuous training and fine tuning for their document generation platform. So you might have, like, you know, some document locally, right, like sitting on my laptop, and then I want to generate, like, similar documents based on five or 10 different templates that I have, and then automatically serve as, like, Grammarly for your legal documents. So now there is a lot of rlhf data that goes into play, right? So that's where continuous training also comes in. So we enable their continuous training, like module, they're like, you know, composite of different models to enable their document generation. Like, happen locally on device for these customers, on prem. Now, looking at some competitive analysis, like scale does a lot of rlhf stuff, but it's very manual. We do enable continuous training, so that's why they are in the matrix. Anyscale is another one. Now they do distributed ml on Cloud very well, but they don't support the edge device aspect and the new cloud that we envision. It's not just like combination of just, you know, your heterogeneous cloud, but it also should account like, you know, these edge devices as well in the future, as they become more and more like, you know, compute capable. So that's why, like we think, we have a leverage over any scale as well. Then mosaic like, they don't focus on on prem. The same with like, you know, Apple, Microsoft and Google as well, because most of these people are making things which are specific to their like, you know, ecosystem, rather than like, building it across and solve the heterogeneity problem in general for us, like we saw the heterogeneity problem, we want to enable this, like, across different chip types, across different device types. That's why, like, you know, an OS type. So that gives us, like, you know, a different edge in the market. So we are raising $5 million right now. Of course, like, we will close it as well as possible. But we do want, like, some you know, strategic checks as well. And we think, like, Qualcomm is going to be a great partner here, especially from, you know, the hardware angle that we talked about and the heterogeneity solution angle as well, could be very interesting for Qualcomm, because we do understand, like, Okay, we have different type of OS types, right? So we like, you know, for example, Mac works with MPs, Windows works with CUDA. And then as you moved, like, you know, different chip type level and different kernel level, then there is much more like heterogeneity there. So abstraction is needed in each of these layers. And I think, like this problem would be an interesting angle, just this problem, like none of anything else like this. This problem itself would be of great strategic angle to call com. So we think, like you know, this is something we can co collaborate, or, like, you know, work together. That would be amazing. And of course, like this, other forms of partnerships that we do as well,
S Speaker 37:53okay, like, whatever you're comfortable with sharing, and so happy to facilitate, you know, a call and let's figure out if there are opportunities to work together. I would also ask you to think about the GTM, because it is a very noisy, noisy market to get customers. So let's do that as two next steps, if that makes sense, yeah, sounds like yeah. Thank you so much. Thank you for taking the time. Okay? Thank
okay, like, whatever you're comfortable with sharing, and so happy to facilitate, you know, a call and let's figure out if there are opportunities to work together. I would also ask you to think about the GTM, because it is a very noisy, noisy market to get customers. So let's do that as two next steps, if that makes sense, yeah, sounds like yeah. Thank you so much. Thank you for taking the time. Okay? Thank
okay, like, whatever you're comfortable with sharing, and so happy to facilitate, you know, a call and let's figure out if there are opportunities to work together. I would also ask you to think about the GTM, because it is a very noisy, noisy market to get customers. So let's do that as two next steps, if that makes sense, yeah, sounds like yeah. Thank you so much. Thank you for taking the time. Okay? Thank
okay, like, whatever you're comfortable with sharing, and so happy to facilitate, you know, a call and let's figure out if there are opportunities to work together. I would also ask you to think about the GTM, because it is a very noisy, noisy market to get customers. So let's do that as two next steps, if that makes sense, yeah, sounds like yeah. Thank you so much. Thank you for taking the time. Okay? Thank