Meeting: Featurely AI
Wed, Nov 26
10:15 AM
35 min
Priyesh P
Meeting Introduction and Apologies
0:01
Qualcomm'
URL: https://otter.ai/u/MfSvp_QHNqJqQ3BzJoOfMYsYq6A
Downloaded: 2025-12-21T19:33:19.056378
Method: text_extraction
============================================================

S Speaker 10:01Okay, hey, hi, Krishna. Hey, how's it going? First thing off the bat, I'm really sorry the Parent Teachers meeting didn't go as I thought it would go. So there's a long delay. So everything got pushed out.
Okay, hey, hi, Krishna. Hey, how's it going? First thing off the bat, I'm really sorry the Parent Teachers meeting didn't go as I thought it would go. So there's a long delay. So everything got pushed out.
Okay, hey, hi, Krishna. Hey, how's it going? First thing off the bat, I'm really sorry the Parent Teachers meeting didn't go as I thought it would go. So there's a long delay. So everything got pushed out.
Okay, hey, hi, Krishna. Hey, how's it going? First thing off the bat, I'm really sorry the Parent Teachers meeting didn't go as I thought it would go. So there's a long delay. So everything got pushed out.
S Speaker 20:37No, no worries at all. Krishna, things like this always happens, so you don't have to worry. And thanks a lot for taking the for taking the time. I know it seems like you're traveling, but thanks a lot for joining in still.
No, no worries at all. Krishna, things like this always happens, so you don't have to worry. And thanks a lot for taking the for taking the time. I know it seems like you're traveling, but thanks a lot for joining in still.
No, no worries at all. Krishna, things like this always happens, so you don't have to worry. And thanks a lot for taking the for taking the time. I know it seems like you're traveling, but thanks a lot for joining in still.
No, no worries at all. Krishna, things like this always happens, so you don't have to worry. And thanks a lot for taking the for taking the time. I know it seems like you're traveling, but thanks a lot for joining in still.
S Speaker 10:47Yeah, no worries. No, I just came out of the school, I drove to a parking lot, and now I'm sitting here.
Yeah, no worries. No, I just came out of the school, I drove to a parking lot, and now I'm sitting here.
Yeah, no worries. No, I just came out of the school, I drove to a parking lot, and now I'm sitting here.
Yeah, no worries. No, I just came out of the school, I drove to a parking lot, and now I'm sitting here.
S Speaker 20:52Yeah, do you think it's the right time to meet, or I can even do this later today? What would you prefer?
Yeah, do you think it's the right time to meet, or I can even do this later today? What would you prefer?
Yeah, do you think it's the right time to meet, or I can even do this later today? What would you prefer?
Yeah, do you think it's the right time to meet, or I can even do this later today? What would you prefer?
S Speaker 10:58No, that's okay. We can start because, like, like, we already been postponing it so we can start talking, and then, you know, I can always do a follow up call. But again, my deepest apologies. I didn't mean to be Tati.
No, that's okay. We can start because, like, like, we already been postponing it so we can start talking, and then, you know, I can always do a follow up call. But again, my deepest apologies. I didn't mean to be Tati.
No, that's okay. We can start because, like, like, we already been postponing it so we can start talking, and then, you know, I can always do a follow up call. But again, my deepest apologies. I didn't mean to be Tati.
No, that's okay. We can start because, like, like, we already been postponing it so we can start talking, and then, you know, I can always do a follow up call. But again, my deepest apologies. I didn't mean to be Tati.
S Speaker 21:10Absolutely no worries. No worries at all about that. Krishna. And thanks a lot for taking your time. Like, like you said. We've been trying to have this for such a long time. So glad that we finally got to make it. Krishna, I wanted to specifically also chat, because I've been following this space a little bit more recently, and seems like there is interest within Qualcomm as well. What is broadly happening is that new new companies are emerging in this space, broadly, sort of say, artificial society simulation, if I call it right, and could be the use cases could be in product, UX testing, market research, and a lot of other space adjacent spaces. We believe there could be a possibility that down the line, some of these AI workloads could run on device, even, even a part of these workloads could run on device today. And if that happens, Qualcomm is interested in sort of working with some companies to take it to some customers
Absolutely no worries. No worries at all about that. Krishna. And thanks a lot for taking your time. Like, like you said. We've been trying to have this for such a long time. So glad that we finally got to make it. Krishna, I wanted to specifically also chat, because I've been following this space a little bit more recently, and seems like there is interest within Qualcomm as well. What is broadly happening is that new new companies are emerging in this space, broadly, sort of say, artificial society simulation, if I call it right, and could be the use cases could be in product, UX testing, market research, and a lot of other space adjacent spaces. We believe there could be a possibility that down the line, some of these AI workloads could run on device, even, even a part of these workloads could run on device today. And if that happens, Qualcomm is interested in sort of working with some companies to take it to some customers
Absolutely no worries. No worries at all about that. Krishna. And thanks a lot for taking your time. Like, like you said. We've been trying to have this for such a long time. So glad that we finally got to make it. Krishna, I wanted to specifically also chat, because I've been following this space a little bit more recently, and seems like there is interest within Qualcomm as well. What is broadly happening is that new new companies are emerging in this space, broadly, sort of say, artificial society simulation, if I call it right, and could be the use cases could be in product, UX testing, market research, and a lot of other space adjacent spaces. We believe there could be a possibility that down the line, some of these AI workloads could run on device, even, even a part of these workloads could run on device today. And if that happens, Qualcomm is interested in sort of working with some companies to take it to some customers
Absolutely no worries. No worries at all about that. Krishna. And thanks a lot for taking your time. Like, like you said. We've been trying to have this for such a long time. So glad that we finally got to make it. Krishna, I wanted to specifically also chat, because I've been following this space a little bit more recently, and seems like there is interest within Qualcomm as well. What is broadly happening is that new new companies are emerging in this space, broadly, sort of say, artificial society simulation, if I call it right, and could be the use cases could be in product, UX testing, market research, and a lot of other space adjacent spaces. We believe there could be a possibility that down the line, some of these AI workloads could run on device, even, even a part of these workloads could run on device today. And if that happens, Qualcomm is interested in sort of working with some companies to take it to some customers
2:08as as a packet solution with our
as as a packet solution with our
as as a packet solution with our
as as a packet solution with our
S Speaker 22:12platforms, which now so the platforms that we have, say, the Snapdragon PCs, the Snapdragon mobile phones, all of them are have the ability to run AI workloads on edge, and we sort of want to work with few startups to take them to the market as a package device, saying that, hey, we have our platforms, and these are some companies which already work on our platform, buy our PCs, buy our mobiles, things like that, right? So I do see some interesting partnership propositions, and wanted to sort of chat about that as well, and then planning to meet, meet the business unit teams fairly recently, say, mid December. So wanted to sort of have a chat before that, so that I'm well a world and then we can take it from there. Sounds great. That sounds great. So that's broadly context setting Krishna, but been following this space, very interesting space, for sure. I would love to make an investment in this area. I've spoken to one or two more companies in this space, and would love to understand a little bit more. I know we had very detailed conversation during the conference, and I really enjoyed what what you were building. So we'd love to get an overview from you on where you are. You mentioned a few design partners at that time, so would be great to get a update on the business, absolutely.
platforms, which now so the platforms that we have, say, the Snapdragon PCs, the Snapdragon mobile phones, all of them are have the ability to run AI workloads on edge, and we sort of want to work with few startups to take them to the market as a package device, saying that, hey, we have our platforms, and these are some companies which already work on our platform, buy our PCs, buy our mobiles, things like that, right? So I do see some interesting partnership propositions, and wanted to sort of chat about that as well, and then planning to meet, meet the business unit teams fairly recently, say, mid December. So wanted to sort of have a chat before that, so that I'm well a world and then we can take it from there. Sounds great. That sounds great. So that's broadly context setting Krishna, but been following this space, very interesting space, for sure. I would love to make an investment in this area. I've spoken to one or two more companies in this space, and would love to understand a little bit more. I know we had very detailed conversation during the conference, and I really enjoyed what what you were building. So we'd love to get an overview from you on where you are. You mentioned a few design partners at that time, so would be great to get a update on the business, absolutely.
platforms, which now so the platforms that we have, say, the Snapdragon PCs, the Snapdragon mobile phones, all of them are have the ability to run AI workloads on edge, and we sort of want to work with few startups to take them to the market as a package device, saying that, hey, we have our platforms, and these are some companies which already work on our platform, buy our PCs, buy our mobiles, things like that, right? So I do see some interesting partnership propositions, and wanted to sort of chat about that as well, and then planning to meet, meet the business unit teams fairly recently, say, mid December. So wanted to sort of have a chat before that, so that I'm well a world and then we can take it from there. Sounds great. That sounds great. So that's broadly context setting Krishna, but been following this space, very interesting space, for sure. I would love to make an investment in this area. I've spoken to one or two more companies in this space, and would love to understand a little bit more. I know we had very detailed conversation during the conference, and I really enjoyed what what you were building. So we'd love to get an overview from you on where you are. You mentioned a few design partners at that time, so would be great to get a update on the business, absolutely.
platforms, which now so the platforms that we have, say, the Snapdragon PCs, the Snapdragon mobile phones, all of them are have the ability to run AI workloads on edge, and we sort of want to work with few startups to take them to the market as a package device, saying that, hey, we have our platforms, and these are some companies which already work on our platform, buy our PCs, buy our mobiles, things like that, right? So I do see some interesting partnership propositions, and wanted to sort of chat about that as well, and then planning to meet, meet the business unit teams fairly recently, say, mid December. So wanted to sort of have a chat before that, so that I'm well a world and then we can take it from there. Sounds great. That sounds great. So that's broadly context setting Krishna, but been following this space, very interesting space, for sure. I would love to make an investment in this area. I've spoken to one or two more companies in this space, and would love to understand a little bit more. I know we had very detailed conversation during the conference, and I really enjoyed what what you were building. So we'd love to get an overview from you on where you are. You mentioned a few design partners at that time, so would be great to get a update on the business, absolutely.
S Speaker 13:31So I think from where we are, since I last spoke to you, there's been quite a big, would you say, increase in number of our design partners, yeah. And also in size. Now we are starting to work with the likes of companies like FreshWorks, which are more, I want to say, mid market, and that kind HSBC, Deutsche Telecom, which tend to be, I don't know what size you call them, but they're like, generally larger companies. Yeah, right. And we are also have a huge bunch of like, wait list on the smaller company side, who are trying to get into the product and try to use it. So that's what is happening on the business side. We are now coming to a point where we are in monetization conversations, yeah, with some of these guys in terms of in terms of pricing and things of that sort. But we're also doing a lot of pricing experiments, you know, with each of these design partners to kind of see what is the best model. We are going back and forth between usage based pricing like outcome based pricing, basically, you know, along with the platform access fee versus PR SaaS like model, they probably are not going to do the pure SAS like model. They probably going to the farmer. That's kind of what we're learning from the business. We have made quite some changes and improvements to the product that earlier we were getting feedback from our design partners that the simulations are great, but there's an onboarding curve involved. So now we have a layer on top of it where you just come and tell us what your problem is, and then we run all the simulations on your behalf. We give you a final report, we email you, and the report is ready. So then, that way, it becomes more democratized, lesser. What do you say? Time to start using the product and find value. The final thing is, we have been focused a lot on improving the quality of the simulations. So that's been getting better and better, and we've come to a point where we just started raising seed yesterday, and I think we are hoping to close it pretty soon, and then, you know, continue down the process. Yeah. So that's kind of the state of business right now.
So I think from where we are, since I last spoke to you, there's been quite a big, would you say, increase in number of our design partners, yeah. And also in size. Now we are starting to work with the likes of companies like FreshWorks, which are more, I want to say, mid market, and that kind HSBC, Deutsche Telecom, which tend to be, I don't know what size you call them, but they're like, generally larger companies. Yeah, right. And we are also have a huge bunch of like, wait list on the smaller company side, who are trying to get into the product and try to use it. So that's what is happening on the business side. We are now coming to a point where we are in monetization conversations, yeah, with some of these guys in terms of in terms of pricing and things of that sort. But we're also doing a lot of pricing experiments, you know, with each of these design partners to kind of see what is the best model. We are going back and forth between usage based pricing like outcome based pricing, basically, you know, along with the platform access fee versus PR SaaS like model, they probably are not going to do the pure SAS like model. They probably going to the farmer. That's kind of what we're learning from the business. We have made quite some changes and improvements to the product that earlier we were getting feedback from our design partners that the simulations are great, but there's an onboarding curve involved. So now we have a layer on top of it where you just come and tell us what your problem is, and then we run all the simulations on your behalf. We give you a final report, we email you, and the report is ready. So then, that way, it becomes more democratized, lesser. What do you say? Time to start using the product and find value. The final thing is, we have been focused a lot on improving the quality of the simulations. So that's been getting better and better, and we've come to a point where we just started raising seed yesterday, and I think we are hoping to close it pretty soon, and then, you know, continue down the process. Yeah. So that's kind of the state of business right now.
So I think from where we are, since I last spoke to you, there's been quite a big, would you say, increase in number of our design partners, yeah. And also in size. Now we are starting to work with the likes of companies like FreshWorks, which are more, I want to say, mid market, and that kind HSBC, Deutsche Telecom, which tend to be, I don't know what size you call them, but they're like, generally larger companies. Yeah, right. And we are also have a huge bunch of like, wait list on the smaller company side, who are trying to get into the product and try to use it. So that's what is happening on the business side. We are now coming to a point where we are in monetization conversations, yeah, with some of these guys in terms of in terms of pricing and things of that sort. But we're also doing a lot of pricing experiments, you know, with each of these design partners to kind of see what is the best model. We are going back and forth between usage based pricing like outcome based pricing, basically, you know, along with the platform access fee versus PR SaaS like model, they probably are not going to do the pure SAS like model. They probably going to the farmer. That's kind of what we're learning from the business. We have made quite some changes and improvements to the product that earlier we were getting feedback from our design partners that the simulations are great, but there's an onboarding curve involved. So now we have a layer on top of it where you just come and tell us what your problem is, and then we run all the simulations on your behalf. We give you a final report, we email you, and the report is ready. So then, that way, it becomes more democratized, lesser. What do you say? Time to start using the product and find value. The final thing is, we have been focused a lot on improving the quality of the simulations. So that's been getting better and better, and we've come to a point where we just started raising seed yesterday, and I think we are hoping to close it pretty soon, and then, you know, continue down the process. Yeah. So that's kind of the state of business right now.
So I think from where we are, since I last spoke to you, there's been quite a big, would you say, increase in number of our design partners, yeah. And also in size. Now we are starting to work with the likes of companies like FreshWorks, which are more, I want to say, mid market, and that kind HSBC, Deutsche Telecom, which tend to be, I don't know what size you call them, but they're like, generally larger companies. Yeah, right. And we are also have a huge bunch of like, wait list on the smaller company side, who are trying to get into the product and try to use it. So that's what is happening on the business side. We are now coming to a point where we are in monetization conversations, yeah, with some of these guys in terms of in terms of pricing and things of that sort. But we're also doing a lot of pricing experiments, you know, with each of these design partners to kind of see what is the best model. We are going back and forth between usage based pricing like outcome based pricing, basically, you know, along with the platform access fee versus PR SaaS like model, they probably are not going to do the pure SAS like model. They probably going to the farmer. That's kind of what we're learning from the business. We have made quite some changes and improvements to the product that earlier we were getting feedback from our design partners that the simulations are great, but there's an onboarding curve involved. So now we have a layer on top of it where you just come and tell us what your problem is, and then we run all the simulations on your behalf. We give you a final report, we email you, and the report is ready. So then, that way, it becomes more democratized, lesser. What do you say? Time to start using the product and find value. The final thing is, we have been focused a lot on improving the quality of the simulations. So that's been getting better and better, and we've come to a point where we just started raising seed yesterday, and I think we are hoping to close it pretty soon, and then, you know, continue down the process. Yeah. So that's kind of the state of business right now.
S Speaker 25:41Perfect, perfect timing. Krishna, so I think Glad we could have this conversation from from the fundraise side, if you have, say, a deck prepared, or some data room, I'd love to take a look at it, have a chat with the team early next week and come back to you very quickly on where we stand.
Perfect, perfect timing. Krishna, so I think Glad we could have this conversation from from the fundraise side, if you have, say, a deck prepared, or some data room, I'd love to take a look at it, have a chat with the team early next week and come back to you very quickly on where we stand.
Perfect, perfect timing. Krishna, so I think Glad we could have this conversation from from the fundraise side, if you have, say, a deck prepared, or some data room, I'd love to take a look at it, have a chat with the team early next week and come back to you very quickly on where we stand.
Perfect, perfect timing. Krishna, so I think Glad we could have this conversation from from the fundraise side, if you have, say, a deck prepared, or some data room, I'd love to take a look at it, have a chat with the team early next week and come back to you very quickly on where we stand.
S Speaker 15:58So I'm happy to send you the deck. Let me know if there's specific items or questions you want me to address. I just plug it into the appendix or send you a notion document.
So I'm happy to send you the deck. Let me know if there's specific items or questions you want me to address. I just plug it into the appendix or send you a notion document.
So I'm happy to send you the deck. Let me know if there's specific items or questions you want me to address. I just plug it into the appendix or send you a notion document.
So I'm happy to send you the deck. Let me know if there's specific items or questions you want me to address. I just plug it into the appendix or send you a notion document.
S Speaker 26:07Got it? Got it. Yes. Can we? Can we? Can do that? Krishna and and Krishna just taking a step back, say, from what I remember you, you mentioned an onboarding curve. I have a few initial say, macro level questions. On one, how, what's the ground truth for the simulations you run? How do you sort of attest that these are the right insights that you're giving? And yes, we can start from there.
Got it? Got it. Yes. Can we? Can we? Can do that? Krishna and and Krishna just taking a step back, say, from what I remember you, you mentioned an onboarding curve. I have a few initial say, macro level questions. On one, how, what's the ground truth for the simulations you run? How do you sort of attest that these are the right insights that you're giving? And yes, we can start from there.
Got it? Got it. Yes. Can we? Can we? Can do that? Krishna and and Krishna just taking a step back, say, from what I remember you, you mentioned an onboarding curve. I have a few initial say, macro level questions. On one, how, what's the ground truth for the simulations you run? How do you sort of attest that these are the right insights that you're giving? And yes, we can start from there.
Got it? Got it. Yes. Can we? Can we? Can do that? Krishna and and Krishna just taking a step back, say, from what I remember you, you mentioned an onboarding curve. I have a few initial say, macro level questions. On one, how, what's the ground truth for the simulations you run? How do you sort of attest that these are the right insights that you're giving? And yes, we can start from there.
S Speaker 16:38So basically, there are three levels of that, right? And I'm happy to send you detailed written material on this, but the three levels first is the structural model itself, right? Unlike a lot of the new competition in the space, we aren't prompting llms. We are kind of building reasoning engines on top, so structurally, we are trying to mimic how people think, as opposed to mimic the outcome of what people think. It's a nuance, but it's a big nuance. So where do the data come from? It comes from priors. So as an example, we're looking at a lot of public data. We are looking at data where people have performed an action like people buying stuff, Clickstream data, you know, as opposed to people saying things do versus say, has been a huge part in tuning out initial models. Secondly, what we do is we are constantly running our own synthetic human versus human tests to kind of calibrate our own model internally, you know, like week over week. Yeah, when we reach a customer, what ends up happening, typically, is they first use us out of the box, and then they start giving us their PRDs, their customer logs, a bunch of their proprietary data that goes into a multi tenant architecture, you know, and that is used to further tune the models to make sure we are accurate as possible. In our roadmap, we are planning to pull in even mixed panel data in so then that further kind of gets the quality of the data. So those are the three stages, or three levels in which we are doing synthetic human validation, happy to kind of get into detail of exact sources and everything, yeah, but I can do written material on that, and you can, you guys can take a look
So basically, there are three levels of that, right? And I'm happy to send you detailed written material on this, but the three levels first is the structural model itself, right? Unlike a lot of the new competition in the space, we aren't prompting llms. We are kind of building reasoning engines on top, so structurally, we are trying to mimic how people think, as opposed to mimic the outcome of what people think. It's a nuance, but it's a big nuance. So where do the data come from? It comes from priors. So as an example, we're looking at a lot of public data. We are looking at data where people have performed an action like people buying stuff, Clickstream data, you know, as opposed to people saying things do versus say, has been a huge part in tuning out initial models. Secondly, what we do is we are constantly running our own synthetic human versus human tests to kind of calibrate our own model internally, you know, like week over week. Yeah, when we reach a customer, what ends up happening, typically, is they first use us out of the box, and then they start giving us their PRDs, their customer logs, a bunch of their proprietary data that goes into a multi tenant architecture, you know, and that is used to further tune the models to make sure we are accurate as possible. In our roadmap, we are planning to pull in even mixed panel data in so then that further kind of gets the quality of the data. So those are the three stages, or three levels in which we are doing synthetic human validation, happy to kind of get into detail of exact sources and everything, yeah, but I can do written material on that, and you can, you guys can take a look
So basically, there are three levels of that, right? And I'm happy to send you detailed written material on this, but the three levels first is the structural model itself, right? Unlike a lot of the new competition in the space, we aren't prompting llms. We are kind of building reasoning engines on top, so structurally, we are trying to mimic how people think, as opposed to mimic the outcome of what people think. It's a nuance, but it's a big nuance. So where do the data come from? It comes from priors. So as an example, we're looking at a lot of public data. We are looking at data where people have performed an action like people buying stuff, Clickstream data, you know, as opposed to people saying things do versus say, has been a huge part in tuning out initial models. Secondly, what we do is we are constantly running our own synthetic human versus human tests to kind of calibrate our own model internally, you know, like week over week. Yeah, when we reach a customer, what ends up happening, typically, is they first use us out of the box, and then they start giving us their PRDs, their customer logs, a bunch of their proprietary data that goes into a multi tenant architecture, you know, and that is used to further tune the models to make sure we are accurate as possible. In our roadmap, we are planning to pull in even mixed panel data in so then that further kind of gets the quality of the data. So those are the three stages, or three levels in which we are doing synthetic human validation, happy to kind of get into detail of exact sources and everything, yeah, but I can do written material on that, and you can, you guys can take a look
So basically, there are three levels of that, right? And I'm happy to send you detailed written material on this, but the three levels first is the structural model itself, right? Unlike a lot of the new competition in the space, we aren't prompting llms. We are kind of building reasoning engines on top, so structurally, we are trying to mimic how people think, as opposed to mimic the outcome of what people think. It's a nuance, but it's a big nuance. So where do the data come from? It comes from priors. So as an example, we're looking at a lot of public data. We are looking at data where people have performed an action like people buying stuff, Clickstream data, you know, as opposed to people saying things do versus say, has been a huge part in tuning out initial models. Secondly, what we do is we are constantly running our own synthetic human versus human tests to kind of calibrate our own model internally, you know, like week over week. Yeah, when we reach a customer, what ends up happening, typically, is they first use us out of the box, and then they start giving us their PRDs, their customer logs, a bunch of their proprietary data that goes into a multi tenant architecture, you know, and that is used to further tune the models to make sure we are accurate as possible. In our roadmap, we are planning to pull in even mixed panel data in so then that further kind of gets the quality of the data. So those are the three stages, or three levels in which we are doing synthetic human validation, happy to kind of get into detail of exact sources and everything, yeah, but I can do written material on that, and you can, you guys can take a look
S Speaker 28:33absolutely that would that would be great. Krishna, so from what I understand, you have an out of box solution that you have already tuned based on publicly available data. You then take, sort of, it is
absolutely that would that would be great. Krishna, so from what I understand, you have an out of box solution that you have already tuned based on publicly available data. You then take, sort of, it is
absolutely that would that would be great. Krishna, so from what I understand, you have an out of box solution that you have already tuned based on publicly available data. You then take, sort of, it is
absolutely that would that would be great. Krishna, so from what I understand, you have an out of box solution that you have already tuned based on publicly available data. You then take, sort of, it is
S Speaker 18:45not exactly publicly available. Some of this you like, kind of get, like, extreme data is, I guess you can buy it, but it's not public like, you know. So there's even that, and then there's, of course, Reddit and LinkedIn data. So yeah,
not exactly publicly available. Some of this you like, kind of get, like, extreme data is, I guess you can buy it, but it's not public like, you know. So there's even that, and then there's, of course, Reddit and LinkedIn data. So yeah,
not exactly publicly available. Some of this you like, kind of get, like, extreme data is, I guess you can buy it, but it's not public like, you know. So there's even that, and then there's, of course, Reddit and LinkedIn data. So yeah,
not exactly publicly available. Some of this you like, kind of get, like, extreme data is, I guess you can buy it, but it's not public like, you know. So there's even that, and then there's, of course, Reddit and LinkedIn data. So yeah,
S Speaker 29:00do you sort of then segregate it based on demographics, user behavior, things like that, and then segment those based on what are you? What a specific say use case would be?
do you sort of then segregate it based on demographics, user behavior, things like that, and then segment those based on what are you? What a specific say use case would be?
do you sort of then segregate it based on demographics, user behavior, things like that, and then segment those based on what are you? What a specific say use case would be?
do you sort of then segregate it based on demographics, user behavior, things like that, and then segment those based on what are you? What a specific say use case would be?
S Speaker 19:12So, yeah, so you should think of us like this. Like to mimic cognition, we have close to 15 learning models change back to back, and each of those models have their own tuning and data set and data privates, and so then it's a wide variety of data for each thing. So I'll give you a specific example. Let's talk about standing patterns. Okay, so when you think about UX, yeah, most people think that, oh, this is my UX. So everybody can see which is everything, which is there on the UX. But in reality, we have got data from National Institute of Health that, depending upon your
So, yeah, so you should think of us like this. Like to mimic cognition, we have close to 15 learning models change back to back, and each of those models have their own tuning and data set and data privates, and so then it's a wide variety of data for each thing. So I'll give you a specific example. Let's talk about standing patterns. Okay, so when you think about UX, yeah, most people think that, oh, this is my UX. So everybody can see which is everything, which is there on the UX. But in reality, we have got data from National Institute of Health that, depending upon your
So, yeah, so you should think of us like this. Like to mimic cognition, we have close to 15 learning models change back to back, and each of those models have their own tuning and data set and data privates, and so then it's a wide variety of data for each thing. So I'll give you a specific example. Let's talk about standing patterns. Okay, so when you think about UX, yeah, most people think that, oh, this is my UX. So everybody can see which is everything, which is there on the UX. But in reality, we have got data from National Institute of Health that, depending upon your
So, yeah, so you should think of us like this. Like to mimic cognition, we have close to 15 learning models change back to back, and each of those models have their own tuning and data set and data privates, and so then it's a wide variety of data for each thing. So I'll give you a specific example. Let's talk about standing patterns. Okay, so when you think about UX, yeah, most people think that, oh, this is my UX. So everybody can see which is everything, which is there on the UX. But in reality, we have got data from National Institute of Health that, depending upon your
9:58age, the way you look at
age, the way you look at
age, the way you look at
age, the way you look at
10:01stimulus, the way you stand stimulus is very different.
stimulus, the way you stand stimulus is very different.
stimulus, the way you stand stimulus is very different.
stimulus, the way you stand stimulus is very different.
S Speaker 110:06So we use that data to kind of decide how we filter the incoming stimulus before it reaches the eye of the synthetic hue. Yeah. Okay, so that data is very different from the data required related to, will I spend money on this? Yes, that's a different kind of data. So, which is why? So I think that's our core thesis, right? We are not trying to say, act like this person and tell me what you'll do. Rather, we are trying to build part by part of the entire cognitive pipeline. Do training data for each one of them, chain them back to back, and then have larger training data for the entire output of the pipeline? Yeah, to see, because my thesis is the person with the best quality synthetic user is going to win this market eventually. Yes, it is nothing else, right? And so we think we have a unique approach of solving the
So we use that data to kind of decide how we filter the incoming stimulus before it reaches the eye of the synthetic hue. Yeah. Okay, so that data is very different from the data required related to, will I spend money on this? Yes, that's a different kind of data. So, which is why? So I think that's our core thesis, right? We are not trying to say, act like this person and tell me what you'll do. Rather, we are trying to build part by part of the entire cognitive pipeline. Do training data for each one of them, chain them back to back, and then have larger training data for the entire output of the pipeline? Yeah, to see, because my thesis is the person with the best quality synthetic user is going to win this market eventually. Yes, it is nothing else, right? And so we think we have a unique approach of solving the
So we use that data to kind of decide how we filter the incoming stimulus before it reaches the eye of the synthetic hue. Yeah. Okay, so that data is very different from the data required related to, will I spend money on this? Yes, that's a different kind of data. So, which is why? So I think that's our core thesis, right? We are not trying to say, act like this person and tell me what you'll do. Rather, we are trying to build part by part of the entire cognitive pipeline. Do training data for each one of them, chain them back to back, and then have larger training data for the entire output of the pipeline? Yeah, to see, because my thesis is the person with the best quality synthetic user is going to win this market eventually. Yes, it is nothing else, right? And so we think we have a unique approach of solving the
So we use that data to kind of decide how we filter the incoming stimulus before it reaches the eye of the synthetic hue. Yeah. Okay, so that data is very different from the data required related to, will I spend money on this? Yes, that's a different kind of data. So, which is why? So I think that's our core thesis, right? We are not trying to say, act like this person and tell me what you'll do. Rather, we are trying to build part by part of the entire cognitive pipeline. Do training data for each one of them, chain them back to back, and then have larger training data for the entire output of the pipeline? Yeah, to see, because my thesis is the person with the best quality synthetic user is going to win this market eventually. Yes, it is nothing else, right? And so we think we have a unique approach of solving the
S Speaker 211:08problem that's that's certainly very, very interesting. Krishna just, just the cognitive part of it.
problem that's that's certainly very, very interesting. Krishna just, just the cognitive part of it.
problem that's that's certainly very, very interesting. Krishna just, just the cognitive part of it.
problem that's that's certainly very, very interesting. Krishna just, just the cognitive part of it.
11:13And you mentioned 15 more
And you mentioned 15 more
And you mentioned 15 more
And you mentioned 15 more
S Speaker 111:17challenge we are facing right now. Priyesh is computer. Yeah, yeah, we are because, like, what we are thinking is, right now we're using like, heavy GPUs, right? But at some point of time, from a privacy perspective and whatnot, we want to do it such that it's on the customers devices, right, so that privacy is maintained. They're not worried about this data going anywhere else, and also we are able to speed up the computer. Yeah.
challenge we are facing right now. Priyesh is computer. Yeah, yeah, we are because, like, what we are thinking is, right now we're using like, heavy GPUs, right? But at some point of time, from a privacy perspective and whatnot, we want to do it such that it's on the customers devices, right, so that privacy is maintained. They're not worried about this data going anywhere else, and also we are able to speed up the computer. Yeah.
challenge we are facing right now. Priyesh is computer. Yeah, yeah, we are because, like, what we are thinking is, right now we're using like, heavy GPUs, right? But at some point of time, from a privacy perspective and whatnot, we want to do it such that it's on the customers devices, right, so that privacy is maintained. They're not worried about this data going anywhere else, and also we are able to speed up the computer. Yeah.
challenge we are facing right now. Priyesh is computer. Yeah, yeah, we are because, like, what we are thinking is, right now we're using like, heavy GPUs, right? But at some point of time, from a privacy perspective and whatnot, we want to do it such that it's on the customers devices, right, so that privacy is maintained. They're not worried about this data going anywhere else, and also we are able to speed up the computer. Yeah.
S Speaker 112:09and one more model is like, listen, like, let's say you go to zoom info, you have a lot of customer data, yeah, but the customer data has got very high level characteristics, like demographics and all that stuff, right? It doesn't have so for us, every synthetic human has 35 attributes, demographic, geographic, behavioral, psychographic, but in general, you don't have all of them. So let us say you're building for let's say you're selling your Qualcomm Snapdragon to people in worldwide zip code. I'm just making them right? So now a typical persona engine would be like, okay, an average person in Irvine makes so much money, this is kind of what they need, and this is what they're willingness to pay for a cell phone is, and therefore that you can backtrack Qualcomm, start driving, right? But what we do instead is we say that, Oh, we have a model called customer enrichment model, which takes that information of Irvine and then does predictive ranges on all the other attributes, and then we use that to generate synthetic humans, which are point combinations of those attributes and life experiences. Then you ask them that question, and then you will see each person thinks differently based upon you know, their specifics, right? So that's another model. We call it, the patient, customer enrichment model, right? Which is very similar to what Cambridge Analytica used to do. Then we have a model for cognition that's a little bit of a secret sauce where I'm hoping to patent it. I'm doing the paperwork for that, yeah, but, you know, the way we think of cognition, we have broken cognition into multiple models. Then we have models for the emotional state of a human where we came across a lot of literature and real world data, which says that whenever you're thinking about simulating humans, the mistake, which a lot of people are doing is they're simulating a human as if they're in vacuum. But they're not, let us say you're using a product like, let's say, workday tomorrow. You're not just using workday. You have 23 other tabs open, yes. And therefore you're constantly switching, right? And that makes it very difficult, right? And that affects your emotional state. That changes the way you react to the product, right? So they're models for that, they're models for actions, they're models for frustration. So it's a bunch of models. We are also trying to get into the way people move the mouse models, right? So, for example, a person with Parkinson's, or a person who's older, the way they move and interact with a thing is very different than a person who's young like you, right? So this is what I mean when I say model changing, right?
and one more model is like, listen, like, let's say you go to zoom info, you have a lot of customer data, yeah, but the customer data has got very high level characteristics, like demographics and all that stuff, right? It doesn't have so for us, every synthetic human has 35 attributes, demographic, geographic, behavioral, psychographic, but in general, you don't have all of them. So let us say you're building for let's say you're selling your Qualcomm Snapdragon to people in worldwide zip code. I'm just making them right? So now a typical persona engine would be like, okay, an average person in Irvine makes so much money, this is kind of what they need, and this is what they're willingness to pay for a cell phone is, and therefore that you can backtrack Qualcomm, start driving, right? But what we do instead is we say that, Oh, we have a model called customer enrichment model, which takes that information of Irvine and then does predictive ranges on all the other attributes, and then we use that to generate synthetic humans, which are point combinations of those attributes and life experiences. Then you ask them that question, and then you will see each person thinks differently based upon you know, their specifics, right? So that's another model. We call it, the patient, customer enrichment model, right? Which is very similar to what Cambridge Analytica used to do. Then we have a model for cognition that's a little bit of a secret sauce where I'm hoping to patent it. I'm doing the paperwork for that, yeah, but, you know, the way we think of cognition, we have broken cognition into multiple models. Then we have models for the emotional state of a human where we came across a lot of literature and real world data, which says that whenever you're thinking about simulating humans, the mistake, which a lot of people are doing is they're simulating a human as if they're in vacuum. But they're not, let us say you're using a product like, let's say, workday tomorrow. You're not just using workday. You have 23 other tabs open, yes. And therefore you're constantly switching, right? And that makes it very difficult, right? And that affects your emotional state. That changes the way you react to the product, right? So they're models for that, they're models for actions, they're models for frustration. So it's a bunch of models. We are also trying to get into the way people move the mouse models, right? So, for example, a person with Parkinson's, or a person who's older, the way they move and interact with a thing is very different than a person who's young like you, right? So this is what I mean when I say model changing, right?
and one more model is like, listen, like, let's say you go to zoom info, you have a lot of customer data, yeah, but the customer data has got very high level characteristics, like demographics and all that stuff, right? It doesn't have so for us, every synthetic human has 35 attributes, demographic, geographic, behavioral, psychographic, but in general, you don't have all of them. So let us say you're building for let's say you're selling your Qualcomm Snapdragon to people in worldwide zip code. I'm just making them right? So now a typical persona engine would be like, okay, an average person in Irvine makes so much money, this is kind of what they need, and this is what they're willingness to pay for a cell phone is, and therefore that you can backtrack Qualcomm, start driving, right? But what we do instead is we say that, Oh, we have a model called customer enrichment model, which takes that information of Irvine and then does predictive ranges on all the other attributes, and then we use that to generate synthetic humans, which are point combinations of those attributes and life experiences. Then you ask them that question, and then you will see each person thinks differently based upon you know, their specifics, right? So that's another model. We call it, the patient, customer enrichment model, right? Which is very similar to what Cambridge Analytica used to do. Then we have a model for cognition that's a little bit of a secret sauce where I'm hoping to patent it. I'm doing the paperwork for that, yeah, but, you know, the way we think of cognition, we have broken cognition into multiple models. Then we have models for the emotional state of a human where we came across a lot of literature and real world data, which says that whenever you're thinking about simulating humans, the mistake, which a lot of people are doing is they're simulating a human as if they're in vacuum. But they're not, let us say you're using a product like, let's say, workday tomorrow. You're not just using workday. You have 23 other tabs open, yes. And therefore you're constantly switching, right? And that makes it very difficult, right? And that affects your emotional state. That changes the way you react to the product, right? So they're models for that, they're models for actions, they're models for frustration. So it's a bunch of models. We are also trying to get into the way people move the mouse models, right? So, for example, a person with Parkinson's, or a person who's older, the way they move and interact with a thing is very different than a person who's young like you, right? So this is what I mean when I say model changing, right?
and one more model is like, listen, like, let's say you go to zoom info, you have a lot of customer data, yeah, but the customer data has got very high level characteristics, like demographics and all that stuff, right? It doesn't have so for us, every synthetic human has 35 attributes, demographic, geographic, behavioral, psychographic, but in general, you don't have all of them. So let us say you're building for let's say you're selling your Qualcomm Snapdragon to people in worldwide zip code. I'm just making them right? So now a typical persona engine would be like, okay, an average person in Irvine makes so much money, this is kind of what they need, and this is what they're willingness to pay for a cell phone is, and therefore that you can backtrack Qualcomm, start driving, right? But what we do instead is we say that, Oh, we have a model called customer enrichment model, which takes that information of Irvine and then does predictive ranges on all the other attributes, and then we use that to generate synthetic humans, which are point combinations of those attributes and life experiences. Then you ask them that question, and then you will see each person thinks differently based upon you know, their specifics, right? So that's another model. We call it, the patient, customer enrichment model, right? Which is very similar to what Cambridge Analytica used to do. Then we have a model for cognition that's a little bit of a secret sauce where I'm hoping to patent it. I'm doing the paperwork for that, yeah, but, you know, the way we think of cognition, we have broken cognition into multiple models. Then we have models for the emotional state of a human where we came across a lot of literature and real world data, which says that whenever you're thinking about simulating humans, the mistake, which a lot of people are doing is they're simulating a human as if they're in vacuum. But they're not, let us say you're using a product like, let's say, workday tomorrow. You're not just using workday. You have 23 other tabs open, yes. And therefore you're constantly switching, right? And that makes it very difficult, right? And that affects your emotional state. That changes the way you react to the product, right? So they're models for that, they're models for actions, they're models for frustration. So it's a bunch of models. We are also trying to get into the way people move the mouse models, right? So, for example, a person with Parkinson's, or a person who's older, the way they move and interact with a thing is very different than a person who's young like you, right? So this is what I mean when I say model changing, right?
S Speaker 215:10Krishna, super interesting. It seems like you are approaching this in certainly a lot more nuanced than a couple of other stage companies I've spoken to. How do you think about, say, closing the loop here? Do you think just, just a tie up with a mix panel or a post hog and getting actual customer logs would close the loop for you? Do you want to do more than that, and is this important?
Krishna, super interesting. It seems like you are approaching this in certainly a lot more nuanced than a couple of other stage companies I've spoken to. How do you think about, say, closing the loop here? Do you think just, just a tie up with a mix panel or a post hog and getting actual customer logs would close the loop for you? Do you want to do more than that, and is this important?
Krishna, super interesting. It seems like you are approaching this in certainly a lot more nuanced than a couple of other stage companies I've spoken to. How do you think about, say, closing the loop here? Do you think just, just a tie up with a mix panel or a post hog and getting actual customer logs would close the loop for you? Do you want to do more than that, and is this important?
Krishna, super interesting. It seems like you are approaching this in certainly a lot more nuanced than a couple of other stage companies I've spoken to. How do you think about, say, closing the loop here? Do you think just, just a tie up with a mix panel or a post hog and getting actual customer logs would close the loop for you? Do you want to do more than that, and is this important?
16:58That is the holy grail I'm going out.
That is the holy grail I'm going out.
That is the holy grail I'm going out.
That is the holy grail I'm going out.
S Speaker 117:10please stop me if I'm, if I, if I'm, if I'm starting to rant too much. But the fundamental problem is, when you simulate a persona, yeah, you're simulating a segment size of 10,000 people or 100,000 people, right? And that's what llms do very well. Okay, the challenge with that is you are not getting all the nuances within a persona, okay, so you want to get more and more accurate where a synthetic human is accurate for the equivalent human counterpart? Yes, so that is n equal to 10,000 to n equal to one. But that said, if I simulate 10,000 of these individual people, and then if I look at the group behavior, that should match what I would get when I do a quantitative AB study, right? So I should be able to So on one hand, I should be able to say that a user with this, this, this, this, this is going to behave this way. On the other hand, I should be able to say, if you're going to launch this feature against this sort of Persona, we predict your onboarding rate is going to be 18% and that aligns with what AB testing
please stop me if I'm, if I, if I'm, if I'm starting to rant too much. But the fundamental problem is, when you simulate a persona, yeah, you're simulating a segment size of 10,000 people or 100,000 people, right? And that's what llms do very well. Okay, the challenge with that is you are not getting all the nuances within a persona, okay, so you want to get more and more accurate where a synthetic human is accurate for the equivalent human counterpart? Yes, so that is n equal to 10,000 to n equal to one. But that said, if I simulate 10,000 of these individual people, and then if I look at the group behavior, that should match what I would get when I do a quantitative AB study, right? So I should be able to So on one hand, I should be able to say that a user with this, this, this, this, this is going to behave this way. On the other hand, I should be able to say, if you're going to launch this feature against this sort of Persona, we predict your onboarding rate is going to be 18% and that aligns with what AB testing
please stop me if I'm, if I, if I'm, if I'm starting to rant too much. But the fundamental problem is, when you simulate a persona, yeah, you're simulating a segment size of 10,000 people or 100,000 people, right? And that's what llms do very well. Okay, the challenge with that is you are not getting all the nuances within a persona, okay, so you want to get more and more accurate where a synthetic human is accurate for the equivalent human counterpart? Yes, so that is n equal to 10,000 to n equal to one. But that said, if I simulate 10,000 of these individual people, and then if I look at the group behavior, that should match what I would get when I do a quantitative AB study, right? So I should be able to So on one hand, I should be able to say that a user with this, this, this, this, this is going to behave this way. On the other hand, I should be able to say, if you're going to launch this feature against this sort of Persona, we predict your onboarding rate is going to be 18% and that aligns with what AB testing
please stop me if I'm, if I, if I'm, if I'm starting to rant too much. But the fundamental problem is, when you simulate a persona, yeah, you're simulating a segment size of 10,000 people or 100,000 people, right? And that's what llms do very well. Okay, the challenge with that is you are not getting all the nuances within a persona, okay, so you want to get more and more accurate where a synthetic human is accurate for the equivalent human counterpart? Yes, so that is n equal to 10,000 to n equal to one. But that said, if I simulate 10,000 of these individual people, and then if I look at the group behavior, that should match what I would get when I do a quantitative AB study, right? So I should be able to So on one hand, I should be able to say that a user with this, this, this, this, this is going to behave this way. On the other hand, I should be able to say, if you're going to launch this feature against this sort of Persona, we predict your onboarding rate is going to be 18% and that aligns with what AB testing
18:28tools do. Interesting,
tools do. Interesting,
tools do. Interesting,
tools do. Interesting,
S Speaker 218:30yeah, so it goes both this way and this way. Yeah, I get that now. Thanks. Thanks a lot for that. Krishna and Krishna, can you sort of so what kind of design partners do you have today, and what are the use cases that they are adopting it for? You also mentioned some pretty big names, and my initial assumption was that this would be for, say, early product development, but seems like some of the companies you mentioned are using it for other user usage as well. So how? So what's happening is,
yeah, so it goes both this way and this way. Yeah, I get that now. Thanks. Thanks a lot for that. Krishna and Krishna, can you sort of so what kind of design partners do you have today, and what are the use cases that they are adopting it for? You also mentioned some pretty big names, and my initial assumption was that this would be for, say, early product development, but seems like some of the companies you mentioned are using it for other user usage as well. So how? So what's happening is,
yeah, so it goes both this way and this way. Yeah, I get that now. Thanks. Thanks a lot for that. Krishna and Krishna, can you sort of so what kind of design partners do you have today, and what are the use cases that they are adopting it for? You also mentioned some pretty big names, and my initial assumption was that this would be for, say, early product development, but seems like some of the companies you mentioned are using it for other user usage as well. So how? So what's happening is,
yeah, so it goes both this way and this way. Yeah, I get that now. Thanks. Thanks a lot for that. Krishna and Krishna, can you sort of so what kind of design partners do you have today, and what are the use cases that they are adopting it for? You also mentioned some pretty big names, and my initial assumption was that this would be for, say, early product development, but seems like some of the companies you mentioned are using it for other user usage as well. So how? So what's happening is,
S Speaker 119:03let's take FreshWorks. FreshWorks is using, they're building a brand new product within FreshWorks, okay? And they are using us to kind of figure out their designs, the user workflows, usability, everything else. Okay? Again, all these are designed partnership stage, yeah, I want to be clear, yeah. Zora, we became an approved software the POC is yet to start, but the POC is primarily going to be around improving usability, improving messaging, then HSBC, HSBC, we are waiting on GDPR. They are very interesting. They want to use us for customer research, for digital products, for all their marketing content for all their micro sites, because they do a lot of AB testing, they want to get it right before they even go out to AB testing. Same with Deutsche Telekom, again, very similar thing. Morocco, we are in early talks with them. They're a massive advertising company. They keep creating digital playable ads, which are like mini products in themselves and every point percentage point conversion rate bump increases their revenue like crazy, yes, so they want to test again for synthetic humans before they go to market. So those are all these big companies in they are currently using Qualtrics or user testing to do what they do. Yeah, that takes too much time. That takes too much money, and it's very biased, agreed. So for them, they're replacing that Qualtrics budget for smaller companies like Merlin profit, all those guys. For those guys, it's like before I would never do customer understanding before. I needed a lot of product people, user research people, all that. Today, I don't need all that. I just hire feature Lee and feature he takes care of it.
let's take FreshWorks. FreshWorks is using, they're building a brand new product within FreshWorks, okay? And they are using us to kind of figure out their designs, the user workflows, usability, everything else. Okay? Again, all these are designed partnership stage, yeah, I want to be clear, yeah. Zora, we became an approved software the POC is yet to start, but the POC is primarily going to be around improving usability, improving messaging, then HSBC, HSBC, we are waiting on GDPR. They are very interesting. They want to use us for customer research, for digital products, for all their marketing content for all their micro sites, because they do a lot of AB testing, they want to get it right before they even go out to AB testing. Same with Deutsche Telekom, again, very similar thing. Morocco, we are in early talks with them. They're a massive advertising company. They keep creating digital playable ads, which are like mini products in themselves and every point percentage point conversion rate bump increases their revenue like crazy, yes, so they want to test again for synthetic humans before they go to market. So those are all these big companies in they are currently using Qualtrics or user testing to do what they do. Yeah, that takes too much time. That takes too much money, and it's very biased, agreed. So for them, they're replacing that Qualtrics budget for smaller companies like Merlin profit, all those guys. For those guys, it's like before I would never do customer understanding before. I needed a lot of product people, user research people, all that. Today, I don't need all that. I just hire feature Lee and feature he takes care of it.
let's take FreshWorks. FreshWorks is using, they're building a brand new product within FreshWorks, okay? And they are using us to kind of figure out their designs, the user workflows, usability, everything else. Okay? Again, all these are designed partnership stage, yeah, I want to be clear, yeah. Zora, we became an approved software the POC is yet to start, but the POC is primarily going to be around improving usability, improving messaging, then HSBC, HSBC, we are waiting on GDPR. They are very interesting. They want to use us for customer research, for digital products, for all their marketing content for all their micro sites, because they do a lot of AB testing, they want to get it right before they even go out to AB testing. Same with Deutsche Telekom, again, very similar thing. Morocco, we are in early talks with them. They're a massive advertising company. They keep creating digital playable ads, which are like mini products in themselves and every point percentage point conversion rate bump increases their revenue like crazy, yes, so they want to test again for synthetic humans before they go to market. So those are all these big companies in they are currently using Qualtrics or user testing to do what they do. Yeah, that takes too much time. That takes too much money, and it's very biased, agreed. So for them, they're replacing that Qualtrics budget for smaller companies like Merlin profit, all those guys. For those guys, it's like before I would never do customer understanding before. I needed a lot of product people, user research people, all that. Today, I don't need all that. I just hire feature Lee and feature he takes care of it.
let's take FreshWorks. FreshWorks is using, they're building a brand new product within FreshWorks, okay? And they are using us to kind of figure out their designs, the user workflows, usability, everything else. Okay? Again, all these are designed partnership stage, yeah, I want to be clear, yeah. Zora, we became an approved software the POC is yet to start, but the POC is primarily going to be around improving usability, improving messaging, then HSBC, HSBC, we are waiting on GDPR. They are very interesting. They want to use us for customer research, for digital products, for all their marketing content for all their micro sites, because they do a lot of AB testing, they want to get it right before they even go out to AB testing. Same with Deutsche Telekom, again, very similar thing. Morocco, we are in early talks with them. They're a massive advertising company. They keep creating digital playable ads, which are like mini products in themselves and every point percentage point conversion rate bump increases their revenue like crazy, yes, so they want to test again for synthetic humans before they go to market. So those are all these big companies in they are currently using Qualtrics or user testing to do what they do. Yeah, that takes too much time. That takes too much money, and it's very biased, agreed. So for them, they're replacing that Qualtrics budget for smaller companies like Merlin profit, all those guys. For those guys, it's like before I would never do customer understanding before. I needed a lot of product people, user research people, all that. Today, I don't need all that. I just hire feature Lee and feature he takes care of it.
20:48Yeah, right. So the way I think of it is,
Yeah, right. So the way I think of it is,
Yeah, right. So the way I think of it is,
Yeah, right. So the way I think of it is,
S Speaker 120:53I want higher and higher ECBs. And the higher ECBs come from the Deutsche telekoms of the world, right? And I know they're paying Qualtrics a million dollars today annual contract. I want to get that, but then to get that, I like those guys are slow, like they're generally slow. In the meantime, I'm constantly improving my sensitive humans. For that. I need these startups to keep, you know, the data, slightly. Yeah, right. So that's my dual strategy, right?
I want higher and higher ECBs. And the higher ECBs come from the Deutsche telekoms of the world, right? And I know they're paying Qualtrics a million dollars today annual contract. I want to get that, but then to get that, I like those guys are slow, like they're generally slow. In the meantime, I'm constantly improving my sensitive humans. For that. I need these startups to keep, you know, the data, slightly. Yeah, right. So that's my dual strategy, right?
I want higher and higher ECBs. And the higher ECBs come from the Deutsche telekoms of the world, right? And I know they're paying Qualtrics a million dollars today annual contract. I want to get that, but then to get that, I like those guys are slow, like they're generally slow. In the meantime, I'm constantly improving my sensitive humans. For that. I need these startups to keep, you know, the data, slightly. Yeah, right. So that's my dual strategy, right?
I want higher and higher ECBs. And the higher ECBs come from the Deutsche telekoms of the world, right? And I know they're paying Qualtrics a million dollars today annual contract. I want to get that, but then to get that, I like those guys are slow, like they're generally slow. In the meantime, I'm constantly improving my sensitive humans. For that. I need these startups to keep, you know, the data, slightly. Yeah, right. So that's my dual strategy, right?
S Speaker 221:22Makes a lot of sense. Krishna, I've seen that strategy play out in a few other spaces. Totally believe in that and
Makes a lot of sense. Krishna, I've seen that strategy play out in a few other spaces. Totally believe in that and
Makes a lot of sense. Krishna, I've seen that strategy play out in a few other spaces. Totally believe in that and
Makes a lot of sense. Krishna, I've seen that strategy play out in a few other spaces. Totally believe in that and
21:29agree with how you're thinking about
agree with how you're thinking about
agree with how you're thinking about
agree with how you're thinking about
S Speaker 221:33GDM in this space, for sure. You Krishna, you mentioned closing the loops across three three different verticals out of the box. What's the level of accuracy, roughly that you reach today?
GDM in this space, for sure. You Krishna, you mentioned closing the loops across three three different verticals out of the box. What's the level of accuracy, roughly that you reach today?
GDM in this space, for sure. You Krishna, you mentioned closing the loops across three three different verticals out of the box. What's the level of accuracy, roughly that you reach today?
GDM in this space, for sure. You Krishna, you mentioned closing the loops across three three different verticals out of the box. What's the level of accuracy, roughly that you reach today?
S Speaker 121:46Again, let's take all this with Pinterest thought, because we have talked a lot about benchmarking synthetic humans, yeah, and what we realized is it is use case dependent, but typically for usability, I want to say we are
Again, let's take all this with Pinterest thought, because we have talked a lot about benchmarking synthetic humans, yeah, and what we realized is it is use case dependent, but typically for usability, I want to say we are
Again, let's take all this with Pinterest thought, because we have talked a lot about benchmarking synthetic humans, yeah, and what we realized is it is use case dependent, but typically for usability, I want to say we are
Again, let's take all this with Pinterest thought, because we have talked a lot about benchmarking synthetic humans, yeah, and what we realized is it is use case dependent, but typically for usability, I want to say we are
21:58close to 80% that's not bad. That's pretty good. Yes,
close to 80% that's not bad. That's pretty good. Yes,
close to 80% that's not bad. That's pretty good. Yes,
close to 80% that's not bad. That's pretty good. Yes,
S Speaker 122:01you know. But now tomorrow, if you ask me to test whether people like pink tomatoes or red tomatoes, I'm sure the percentage would be closer to 65 to 70% so it's all use case like, you know, it's going to be a range of use cases. Yeah, right. But generally we think out of the box, we are like 75% there on average, right? Once you start giving us more data, I think we can go to 80% Yeah, and this, I think we are probably under representing ourselves, because Dr park at Stanford believes that he can get 80% accuracy with just llms that we haven't seen out. But this is a problem with benchmarks, right? It's very easy to
you know. But now tomorrow, if you ask me to test whether people like pink tomatoes or red tomatoes, I'm sure the percentage would be closer to 65 to 70% so it's all use case like, you know, it's going to be a range of use cases. Yeah, right. But generally we think out of the box, we are like 75% there on average, right? Once you start giving us more data, I think we can go to 80% Yeah, and this, I think we are probably under representing ourselves, because Dr park at Stanford believes that he can get 80% accuracy with just llms that we haven't seen out. But this is a problem with benchmarks, right? It's very easy to
you know. But now tomorrow, if you ask me to test whether people like pink tomatoes or red tomatoes, I'm sure the percentage would be closer to 65 to 70% so it's all use case like, you know, it's going to be a range of use cases. Yeah, right. But generally we think out of the box, we are like 75% there on average, right? Once you start giving us more data, I think we can go to 80% Yeah, and this, I think we are probably under representing ourselves, because Dr park at Stanford believes that he can get 80% accuracy with just llms that we haven't seen out. But this is a problem with benchmarks, right? It's very easy to
you know. But now tomorrow, if you ask me to test whether people like pink tomatoes or red tomatoes, I'm sure the percentage would be closer to 65 to 70% so it's all use case like, you know, it's going to be a range of use cases. Yeah, right. But generally we think out of the box, we are like 75% there on average, right? Once you start giving us more data, I think we can go to 80% Yeah, and this, I think we are probably under representing ourselves, because Dr park at Stanford believes that he can get 80% accuracy with just llms that we haven't seen out. But this is a problem with benchmarks, right? It's very easy to
22:43manipulate benchmark.
manipulate benchmark.
manipulate benchmark.
manipulate benchmark.
22:44Yes, so which is why
S Speaker 222:48makes sense. Makes sense. Krishna got it and from from a platform perspective, again, Krishna, do you see a particular use case that would sort of take 80% of your revenue through like is UX a particular use case. Do you also see market you mentioned marketing, testing, user research, things like that. So I
makes sense. Makes sense. Krishna got it and from from a platform perspective, again, Krishna, do you see a particular use case that would sort of take 80% of your revenue through like is UX a particular use case. Do you also see market you mentioned marketing, testing, user research, things like that. So I
makes sense. Makes sense. Krishna got it and from from a platform perspective, again, Krishna, do you see a particular use case that would sort of take 80% of your revenue through like is UX a particular use case. Do you also see market you mentioned marketing, testing, user research, things like that. So I
makes sense. Makes sense. Krishna got it and from from a platform perspective, again, Krishna, do you see a particular use case that would sort of take 80% of your revenue through like is UX a particular use case. Do you also see market you mentioned marketing, testing, user research, things like that. So I
23:08think the Holy Grail,
think the Holy Grail,
think the Holy Grail,
think the Holy Grail,
S Speaker 123:11okay, in the future is, I believe in two thesis. I believe that in the upcoming years, product market fit is going to be replaced with product user fit. Okay? I even wrote a small article about that, okay. And so I think that means that I should be able to build experiences which learn the user as opposed to vice versa. And for that, you need authentic any equal to one simulations, yeah, and I think that you charge on every time a customer logs into the product. You simulate something, you give some advice, that experience gets generated on their side, and you take a percentage money for yourself. So I think that is the long pole right right now I think the biggest money is in PR research, right where people are already spending, like these big companies are already spending, like million dollar ECBs. Yeah, right. I want to get that. But the way you get there is my wedge is the digital teams. Why is that? Because today, if you go and ask a strategy team to say, hey, simulate against synthetic human, civil artificial search, like, Hey, am I going to make a billion dollar decision based on synthetic humans? No, right. So first go to the digital product teams, you kind of get them to iterate a lot with your synthetic humans, yeah. And at some point that becomes like universe of validated synthetic humans, which becomes the customer proxies for the company. When it becomes a customer's proxies for the company, any organization strategy, market research, customer research, customer support, they all start testing against that universe of synthetic humans, which is common across the organization. And that is when I think our ECB just go through the roof. That's been the way I've been thinking about this problem.
okay, in the future is, I believe in two thesis. I believe that in the upcoming years, product market fit is going to be replaced with product user fit. Okay? I even wrote a small article about that, okay. And so I think that means that I should be able to build experiences which learn the user as opposed to vice versa. And for that, you need authentic any equal to one simulations, yeah, and I think that you charge on every time a customer logs into the product. You simulate something, you give some advice, that experience gets generated on their side, and you take a percentage money for yourself. So I think that is the long pole right right now I think the biggest money is in PR research, right where people are already spending, like these big companies are already spending, like million dollar ECBs. Yeah, right. I want to get that. But the way you get there is my wedge is the digital teams. Why is that? Because today, if you go and ask a strategy team to say, hey, simulate against synthetic human, civil artificial search, like, Hey, am I going to make a billion dollar decision based on synthetic humans? No, right. So first go to the digital product teams, you kind of get them to iterate a lot with your synthetic humans, yeah. And at some point that becomes like universe of validated synthetic humans, which becomes the customer proxies for the company. When it becomes a customer's proxies for the company, any organization strategy, market research, customer research, customer support, they all start testing against that universe of synthetic humans, which is common across the organization. And that is when I think our ECB just go through the roof. That's been the way I've been thinking about this problem.
okay, in the future is, I believe in two thesis. I believe that in the upcoming years, product market fit is going to be replaced with product user fit. Okay? I even wrote a small article about that, okay. And so I think that means that I should be able to build experiences which learn the user as opposed to vice versa. And for that, you need authentic any equal to one simulations, yeah, and I think that you charge on every time a customer logs into the product. You simulate something, you give some advice, that experience gets generated on their side, and you take a percentage money for yourself. So I think that is the long pole right right now I think the biggest money is in PR research, right where people are already spending, like these big companies are already spending, like million dollar ECBs. Yeah, right. I want to get that. But the way you get there is my wedge is the digital teams. Why is that? Because today, if you go and ask a strategy team to say, hey, simulate against synthetic human, civil artificial search, like, Hey, am I going to make a billion dollar decision based on synthetic humans? No, right. So first go to the digital product teams, you kind of get them to iterate a lot with your synthetic humans, yeah. And at some point that becomes like universe of validated synthetic humans, which becomes the customer proxies for the company. When it becomes a customer's proxies for the company, any organization strategy, market research, customer research, customer support, they all start testing against that universe of synthetic humans, which is common across the organization. And that is when I think our ECB just go through the roof. That's been the way I've been thinking about this problem.
okay, in the future is, I believe in two thesis. I believe that in the upcoming years, product market fit is going to be replaced with product user fit. Okay? I even wrote a small article about that, okay. And so I think that means that I should be able to build experiences which learn the user as opposed to vice versa. And for that, you need authentic any equal to one simulations, yeah, and I think that you charge on every time a customer logs into the product. You simulate something, you give some advice, that experience gets generated on their side, and you take a percentage money for yourself. So I think that is the long pole right right now I think the biggest money is in PR research, right where people are already spending, like these big companies are already spending, like million dollar ECBs. Yeah, right. I want to get that. But the way you get there is my wedge is the digital teams. Why is that? Because today, if you go and ask a strategy team to say, hey, simulate against synthetic human, civil artificial search, like, Hey, am I going to make a billion dollar decision based on synthetic humans? No, right. So first go to the digital product teams, you kind of get them to iterate a lot with your synthetic humans, yeah. And at some point that becomes like universe of validated synthetic humans, which becomes the customer proxies for the company. When it becomes a customer's proxies for the company, any organization strategy, market research, customer research, customer support, they all start testing against that universe of synthetic humans, which is common across the organization. And that is when I think our ECB just go through the roof. That's been the way I've been thinking about this problem.
S Speaker 225:06Makes sense. Krishna, I definitely believe in your initial thesis as well on NS to one experiences across all digital platforms. I've been also tracking a lot of generative UI applications, which will sort of then become the front end of your your back end solution down the line, when, when that thesis plays out? So 100% agreed there. And from a platform perspective again, are you building it very horizontally in terms of the use cases it could support it down the line, I have very accurate synthetic humans generated through feature like, Can I also test it for election campaigns? Can I also test it for launching my book and things like that? Do you approach it from a very horizontal platform perspective or so?
Makes sense. Krishna, I definitely believe in your initial thesis as well on NS to one experiences across all digital platforms. I've been also tracking a lot of generative UI applications, which will sort of then become the front end of your your back end solution down the line, when, when that thesis plays out? So 100% agreed there. And from a platform perspective again, are you building it very horizontally in terms of the use cases it could support it down the line, I have very accurate synthetic humans generated through feature like, Can I also test it for election campaigns? Can I also test it for launching my book and things like that? Do you approach it from a very horizontal platform perspective or so?
Makes sense. Krishna, I definitely believe in your initial thesis as well on NS to one experiences across all digital platforms. I've been also tracking a lot of generative UI applications, which will sort of then become the front end of your your back end solution down the line, when, when that thesis plays out? So 100% agreed there. And from a platform perspective again, are you building it very horizontally in terms of the use cases it could support it down the line, I have very accurate synthetic humans generated through feature like, Can I also test it for election campaigns? Can I also test it for launching my book and things like that? Do you approach it from a very horizontal platform perspective or so?
Makes sense. Krishna, I definitely believe in your initial thesis as well on NS to one experiences across all digital platforms. I've been also tracking a lot of generative UI applications, which will sort of then become the front end of your your back end solution down the line, when, when that thesis plays out? So 100% agreed there. And from a platform perspective again, are you building it very horizontally in terms of the use cases it could support it down the line, I have very accurate synthetic humans generated through feature like, Can I also test it for election campaigns? Can I also test it for launching my book and things like that? Do you approach it from a very horizontal platform perspective or so?
27:02super interesting. The
super interesting. The
super interesting. The
super interesting. The
S Speaker 127:03reason why we're doing this is also pricing, right? Because I know generally people think of whatever SaaS pricing and all that, but I'm like, No, guys, SaaS pricing is not going to work. There's going to be a time when we need to have usage based pricing. And if you need usage based pricing, you need to have a API service where anybody can call a synthetic human. I can call a synthetic human into a zoom call. I can call a synthetic human into a Slack conversation. I can call a synthetic human into a user testing panel. I can call a synthetic human into anything,
reason why we're doing this is also pricing, right? Because I know generally people think of whatever SaaS pricing and all that, but I'm like, No, guys, SaaS pricing is not going to work. There's going to be a time when we need to have usage based pricing. And if you need usage based pricing, you need to have a API service where anybody can call a synthetic human. I can call a synthetic human into a zoom call. I can call a synthetic human into a Slack conversation. I can call a synthetic human into a user testing panel. I can call a synthetic human into anything,
reason why we're doing this is also pricing, right? Because I know generally people think of whatever SaaS pricing and all that, but I'm like, No, guys, SaaS pricing is not going to work. There's going to be a time when we need to have usage based pricing. And if you need usage based pricing, you need to have a API service where anybody can call a synthetic human. I can call a synthetic human into a zoom call. I can call a synthetic human into a Slack conversation. I can call a synthetic human into a user testing panel. I can call a synthetic human into anything,
reason why we're doing this is also pricing, right? Because I know generally people think of whatever SaaS pricing and all that, but I'm like, No, guys, SaaS pricing is not going to work. There's going to be a time when we need to have usage based pricing. And if you need usage based pricing, you need to have a API service where anybody can call a synthetic human. I can call a synthetic human into a zoom call. I can call a synthetic human into a Slack conversation. I can call a synthetic human into a user testing panel. I can call a synthetic human into anything,
27:35right? And then you pay for that.
right? And then you pay for that.
right? And then you pay for that.
right? And then you pay for that.
S Speaker 227:37I agree. I agree on that 100% Krishna, especially as the initial markets that you would be targeting would be around experimentation. You would like to push more frequency of usage further and and then usage based pricing makes the most sense, 100% Krishna, what's your thought on, I think this is last question on my end. What's your thought on some of the competitors today, there's artificial society, there's a new YC company. I recently met sanctum, a few other uxia Ai, UX, AI, some, some of these companies also building synthetic humans. Thoughts on those and those approaches,
I agree. I agree on that 100% Krishna, especially as the initial markets that you would be targeting would be around experimentation. You would like to push more frequency of usage further and and then usage based pricing makes the most sense, 100% Krishna, what's your thought on, I think this is last question on my end. What's your thought on some of the competitors today, there's artificial society, there's a new YC company. I recently met sanctum, a few other uxia Ai, UX, AI, some, some of these companies also building synthetic humans. Thoughts on those and those approaches,
I agree. I agree on that 100% Krishna, especially as the initial markets that you would be targeting would be around experimentation. You would like to push more frequency of usage further and and then usage based pricing makes the most sense, 100% Krishna, what's your thought on, I think this is last question on my end. What's your thought on some of the competitors today, there's artificial society, there's a new YC company. I recently met sanctum, a few other uxia Ai, UX, AI, some, some of these companies also building synthetic humans. Thoughts on those and those approaches,
I agree. I agree on that 100% Krishna, especially as the initial markets that you would be targeting would be around experimentation. You would like to push more frequency of usage further and and then usage based pricing makes the most sense, 100% Krishna, what's your thought on, I think this is last question on my end. What's your thought on some of the competitors today, there's artificial society, there's a new YC company. I recently met sanctum, a few other uxia Ai, UX, AI, some, some of these companies also building synthetic humans. Thoughts on those and those approaches,
28:11yeah. So I think primary two things right
yeah. So I think primary two things right
yeah. So I think primary two things right
yeah. So I think primary two things right
28:15for me, my thesis remains strong that
for me, my thesis remains strong that
for me, my thesis remains strong that
for me, my thesis remains strong that
S Speaker 128:18there's probably going to be a couple of eventual giants in this market. And the giants are going to be the people, non negotiable. You need to have high quality synthetic units, yeah, if not, you're just generating LLM slop. No one is going to buy your product. Second, you need to be deeply integrated into the workflows. Those are the two things. And you need to have a datafly effect that every single time a customer uses you, they're getting better and better and better and better. Now most of my competition, they use llms. You understand how you tune llms. You basically re prompt engineer them. So tuning has only so much value right now, when we change models back to back, you actually tune the models right so we know we are getting better with every user who logs on to our product. Second big differentiation is most of my competitors are simulating or mimicking the output of synthetic units. We are taking a exact 180 degree shift approach that we are saying that we are not going to output the output. We're not going to mimic the output. We're going to mimic the process of cognition so that we give the right output, even in places where it's a new kind of user or a new kind of stimulus, which there's been no data in the algorithm world before. Yeah, right. So that's the second thing. The third biggest thing is, my thesis is, I've been on a lot of customer research panels. The quality of customer insights when customers play with a product or experience is very different from when they say things and with feature Lee, all our synthetic humans play with the stimulus. They log on, they try things, they do things, they explore stuff, and then they kind of, you know, articulate their feedback. Yeah. So the level of insights is very different, right? The first use case which we have vertically integrated into his product. So today, if you come to futurely, we have a layer called Chi where you can literally come to chi and say, Hey, Kai, I'm building this experience. My onboarding rate is not very good. So what chi does is Chi kind of generates a hypothesis of what could be wrong. It goes and tests the hypothesis against the synthetic humans. It comes back with
there's probably going to be a couple of eventual giants in this market. And the giants are going to be the people, non negotiable. You need to have high quality synthetic units, yeah, if not, you're just generating LLM slop. No one is going to buy your product. Second, you need to be deeply integrated into the workflows. Those are the two things. And you need to have a datafly effect that every single time a customer uses you, they're getting better and better and better and better. Now most of my competition, they use llms. You understand how you tune llms. You basically re prompt engineer them. So tuning has only so much value right now, when we change models back to back, you actually tune the models right so we know we are getting better with every user who logs on to our product. Second big differentiation is most of my competitors are simulating or mimicking the output of synthetic units. We are taking a exact 180 degree shift approach that we are saying that we are not going to output the output. We're not going to mimic the output. We're going to mimic the process of cognition so that we give the right output, even in places where it's a new kind of user or a new kind of stimulus, which there's been no data in the algorithm world before. Yeah, right. So that's the second thing. The third biggest thing is, my thesis is, I've been on a lot of customer research panels. The quality of customer insights when customers play with a product or experience is very different from when they say things and with feature Lee, all our synthetic humans play with the stimulus. They log on, they try things, they do things, they explore stuff, and then they kind of, you know, articulate their feedback. Yeah. So the level of insights is very different, right? The first use case which we have vertically integrated into his product. So today, if you come to futurely, we have a layer called Chi where you can literally come to chi and say, Hey, Kai, I'm building this experience. My onboarding rate is not very good. So what chi does is Chi kind of generates a hypothesis of what could be wrong. It goes and tests the hypothesis against the synthetic humans. It comes back with
there's probably going to be a couple of eventual giants in this market. And the giants are going to be the people, non negotiable. You need to have high quality synthetic units, yeah, if not, you're just generating LLM slop. No one is going to buy your product. Second, you need to be deeply integrated into the workflows. Those are the two things. And you need to have a datafly effect that every single time a customer uses you, they're getting better and better and better and better. Now most of my competition, they use llms. You understand how you tune llms. You basically re prompt engineer them. So tuning has only so much value right now, when we change models back to back, you actually tune the models right so we know we are getting better with every user who logs on to our product. Second big differentiation is most of my competitors are simulating or mimicking the output of synthetic units. We are taking a exact 180 degree shift approach that we are saying that we are not going to output the output. We're not going to mimic the output. We're going to mimic the process of cognition so that we give the right output, even in places where it's a new kind of user or a new kind of stimulus, which there's been no data in the algorithm world before. Yeah, right. So that's the second thing. The third biggest thing is, my thesis is, I've been on a lot of customer research panels. The quality of customer insights when customers play with a product or experience is very different from when they say things and with feature Lee, all our synthetic humans play with the stimulus. They log on, they try things, they do things, they explore stuff, and then they kind of, you know, articulate their feedback. Yeah. So the level of insights is very different, right? The first use case which we have vertically integrated into his product. So today, if you come to futurely, we have a layer called Chi where you can literally come to chi and say, Hey, Kai, I'm building this experience. My onboarding rate is not very good. So what chi does is Chi kind of generates a hypothesis of what could be wrong. It goes and tests the hypothesis against the synthetic humans. It comes back with
there's probably going to be a couple of eventual giants in this market. And the giants are going to be the people, non negotiable. You need to have high quality synthetic units, yeah, if not, you're just generating LLM slop. No one is going to buy your product. Second, you need to be deeply integrated into the workflows. Those are the two things. And you need to have a datafly effect that every single time a customer uses you, they're getting better and better and better and better. Now most of my competition, they use llms. You understand how you tune llms. You basically re prompt engineer them. So tuning has only so much value right now, when we change models back to back, you actually tune the models right so we know we are getting better with every user who logs on to our product. Second big differentiation is most of my competitors are simulating or mimicking the output of synthetic units. We are taking a exact 180 degree shift approach that we are saying that we are not going to output the output. We're not going to mimic the output. We're going to mimic the process of cognition so that we give the right output, even in places where it's a new kind of user or a new kind of stimulus, which there's been no data in the algorithm world before. Yeah, right. So that's the second thing. The third biggest thing is, my thesis is, I've been on a lot of customer research panels. The quality of customer insights when customers play with a product or experience is very different from when they say things and with feature Lee, all our synthetic humans play with the stimulus. They log on, they try things, they do things, they explore stuff, and then they kind of, you know, articulate their feedback. Yeah. So the level of insights is very different, right? The first use case which we have vertically integrated into his product. So today, if you come to futurely, we have a layer called Chi where you can literally come to chi and say, Hey, Kai, I'm building this experience. My onboarding rate is not very good. So what chi does is Chi kind of generates a hypothesis of what could be wrong. It goes and tests the hypothesis against the synthetic humans. It comes back with
30:41evidence for or against.
evidence for or against.
evidence for or against.
evidence for or against.
S Speaker 130:44It generates a ranked list of user stories, and you can generate the React code for those user stories. So that's end to end workflow integration so that we get more and more tougher to displace from the product.
It generates a ranked list of user stories, and you can generate the React code for those user stories. So that's end to end workflow integration so that we get more and more tougher to displace from the product.
It generates a ranked list of user stories, and you can generate the React code for those user stories. So that's end to end workflow integration so that we get more and more tougher to displace from the product.
It generates a ranked list of user stories, and you can generate the React code for those user stories. So that's end to end workflow integration so that we get more and more tougher to displace from the product.
S Speaker 230:59Makes sense? Krishna, really, really enjoyed this conversation. Would definitely love to dig deeper. Where how big a round Are you raising for your seat? We are raising
Makes sense? Krishna, really, really enjoyed this conversation. Would definitely love to dig deeper. Where how big a round Are you raising for your seat? We are raising
Makes sense? Krishna, really, really enjoyed this conversation. Would definitely love to dig deeper. Where how big a round Are you raising for your seat? We are raising
Makes sense? Krishna, really, really enjoyed this conversation. Would definitely love to dig deeper. Where how big a round Are you raising for your seat? We are raising
S Speaker 131:11four to six. Okay, okay, that's kind of what we're raising right now. And we are we just started the process, but I think things are moving slightly faster than we thought it would, yeah, so, you know, we just see how it goes.
four to six. Okay, okay, that's kind of what we're raising right now. And we are we just started the process, but I think things are moving slightly faster than we thought it would, yeah, so, you know, we just see how it goes.
four to six. Okay, okay, that's kind of what we're raising right now. And we are we just started the process, but I think things are moving slightly faster than we thought it would, yeah, so, you know, we just see how it goes.
four to six. Okay, okay, that's kind of what we're raising right now. And we are we just started the process, but I think things are moving slightly faster than we thought it would, yeah, so, you know, we just see how it goes.
S Speaker 231:27And when you say slightly fast, what's the rough type line that you're looking at here?
And when you say slightly fast, what's the rough type line that you're looking at here?
And when you say slightly fast, what's the rough type line that you're looking at here?
And when you say slightly fast, what's the rough type line that you're looking at here?
S Speaker 131:35I think we had originally thought that we will be looking to, you know, do the main raising post December holidays, okay, but I think some of the people we're talking to are moving fast, and they're looking at term sheets by like, first week of December, kind of term sheets. So we'll have to kind of figure out how the like this is. In some ways, this is new to me as well, so I'm working with bling to try to figure out, like we didn't expect this. So we are now trying to figure out what to do with it.
I think we had originally thought that we will be looking to, you know, do the main raising post December holidays, okay, but I think some of the people we're talking to are moving fast, and they're looking at term sheets by like, first week of December, kind of term sheets. So we'll have to kind of figure out how the like this is. In some ways, this is new to me as well, so I'm working with bling to try to figure out, like we didn't expect this. So we are now trying to figure out what to do with it.
I think we had originally thought that we will be looking to, you know, do the main raising post December holidays, okay, but I think some of the people we're talking to are moving fast, and they're looking at term sheets by like, first week of December, kind of term sheets. So we'll have to kind of figure out how the like this is. In some ways, this is new to me as well, so I'm working with bling to try to figure out, like we didn't expect this. So we are now trying to figure out what to do with it.
I think we had originally thought that we will be looking to, you know, do the main raising post December holidays, okay, but I think some of the people we're talking to are moving fast, and they're looking at term sheets by like, first week of December, kind of term sheets. So we'll have to kind of figure out how the like this is. In some ways, this is new to me as well, so I'm working with bling to try to figure out, like we didn't expect this. So we are now trying to figure out what to do with it.
S Speaker 232:11100% understand that Krishna, this is a pretty interesting market, and I'm not very surprised that the timeline could be that short. Typically, for us, it takes a little bit more time. We would do anything, say for four weeks, four to six weeks from the first call to coming to a decision, just because corporate venture fund have to sort of go through a lot of loops.
100% understand that Krishna, this is a pretty interesting market, and I'm not very surprised that the timeline could be that short. Typically, for us, it takes a little bit more time. We would do anything, say for four weeks, four to six weeks from the first call to coming to a decision, just because corporate venture fund have to sort of go through a lot of loops.
100% understand that Krishna, this is a pretty interesting market, and I'm not very surprised that the timeline could be that short. Typically, for us, it takes a little bit more time. We would do anything, say for four weeks, four to six weeks from the first call to coming to a decision, just because corporate venture fund have to sort of go through a lot of loops.
100% understand that Krishna, this is a pretty interesting market, and I'm not very surprised that the timeline could be that short. Typically, for us, it takes a little bit more time. We would do anything, say for four weeks, four to six weeks from the first call to coming to a decision, just because corporate venture fund have to sort of go through a lot of loops.
S Speaker 233:42It's correct? Yes, we have say 20% of our investments, we would lead Krishna, but those will be highly strategic in this space. It's slightly adjacent to the work that we do, so it fits our thesis, but it wouldn't be a space where we would lead. And from a check size perspective as well, the lowest check that we have written is has been $2 million and we are seed muscle is slightly younger, say, in the last couple of years, we have started writing seed checks, especially in AI companies, where it's more difficult to sort of get in on later stages. So that's why we have started doing seed more. So that's that's a space that I'll also have to think about. But would be great Krishna, if you can send me the materials as soon as you can. The next two days are off, so I'd like to have a chat with the team today, if possible, put together some things on my end, have a chat with the team, and I will also like to have a conversation between you and Tushar. Tushar leads Qualcomm ventures us, investment teams. He's the managing director here, and I work directly with him. Would like to have a call with him as well, so I'll come back to you with some of his availability as well.
It's correct? Yes, we have say 20% of our investments, we would lead Krishna, but those will be highly strategic in this space. It's slightly adjacent to the work that we do, so it fits our thesis, but it wouldn't be a space where we would lead. And from a check size perspective as well, the lowest check that we have written is has been $2 million and we are seed muscle is slightly younger, say, in the last couple of years, we have started writing seed checks, especially in AI companies, where it's more difficult to sort of get in on later stages. So that's why we have started doing seed more. So that's that's a space that I'll also have to think about. But would be great Krishna, if you can send me the materials as soon as you can. The next two days are off, so I'd like to have a chat with the team today, if possible, put together some things on my end, have a chat with the team, and I will also like to have a conversation between you and Tushar. Tushar leads Qualcomm ventures us, investment teams. He's the managing director here, and I work directly with him. Would like to have a call with him as well, so I'll come back to you with some of his availability as well.
It's correct? Yes, we have say 20% of our investments, we would lead Krishna, but those will be highly strategic in this space. It's slightly adjacent to the work that we do, so it fits our thesis, but it wouldn't be a space where we would lead. And from a check size perspective as well, the lowest check that we have written is has been $2 million and we are seed muscle is slightly younger, say, in the last couple of years, we have started writing seed checks, especially in AI companies, where it's more difficult to sort of get in on later stages. So that's why we have started doing seed more. So that's that's a space that I'll also have to think about. But would be great Krishna, if you can send me the materials as soon as you can. The next two days are off, so I'd like to have a chat with the team today, if possible, put together some things on my end, have a chat with the team, and I will also like to have a conversation between you and Tushar. Tushar leads Qualcomm ventures us, investment teams. He's the managing director here, and I work directly with him. Would like to have a call with him as well, so I'll come back to you with some of his availability as well.
It's correct? Yes, we have say 20% of our investments, we would lead Krishna, but those will be highly strategic in this space. It's slightly adjacent to the work that we do, so it fits our thesis, but it wouldn't be a space where we would lead. And from a check size perspective as well, the lowest check that we have written is has been $2 million and we are seed muscle is slightly younger, say, in the last couple of years, we have started writing seed checks, especially in AI companies, where it's more difficult to sort of get in on later stages. So that's why we have started doing seed more. So that's that's a space that I'll also have to think about. But would be great Krishna, if you can send me the materials as soon as you can. The next two days are off, so I'd like to have a chat with the team today, if possible, put together some things on my end, have a chat with the team, and I will also like to have a conversation between you and Tushar. Tushar leads Qualcomm ventures us, investment teams. He's the managing director here, and I work directly with him. Would like to have a call with him as well, so I'll come back to you with some of his availability as well.
34:50Okay, sounds great.
S Speaker 134:51Sounds good, Priyesh, tonight. Okay, I'll drive back home now from school, and then I'll start getting all this into works, and I'll send stuff over to you.
Sounds good, Priyesh, tonight. Okay, I'll drive back home now from school, and then I'll start getting all this into works, and I'll send stuff over to you.
Sounds good, Priyesh, tonight. Okay, I'll drive back home now from school, and then I'll start getting all this into works, and I'll send stuff over to you.
Sounds good, Priyesh, tonight. Okay, I'll drive back home now from school, and then I'll start getting all this into works, and I'll send stuff over to you.
S Speaker 234:59Absolutely, the Parkside works. Yeah, thanks. Krishna, really enjoyed the conversation. Glad we could connect at the right time again. Priyesh, I apologize for the delay. Hey, no worries at all. No worries at all about that. Thanks. Thanks. Krishna, see you soon. Guys. You.
Absolutely, the Parkside works. Yeah, thanks. Krishna, really enjoyed the conversation. Glad we could connect at the right time again. Priyesh, I apologize for the delay. Hey, no worries at all. No worries at all about that. Thanks. Thanks. Krishna, see you soon. Guys. You.
Absolutely, the Parkside works. Yeah, thanks. Krishna, really enjoyed the conversation. Glad we could connect at the right time again. Priyesh, I apologize for the delay. Hey, no worries at all. No worries at all about that. Thanks. Thanks. Krishna, see you soon. Guys. You.
Absolutely, the Parkside works. Yeah, thanks. Krishna, really enjoyed the conversation. Glad we could connect at the right time again. Priyesh, I apologize for the delay. Hey, no worries at all. No worries at all about that. Thanks. Thanks. Krishna, see you soon. Guys. You.