Meeting: Stellon Labs
Wed, Sep 3
12:59 PM
28 min
Priyesh P
Introduction and Team Background
0:00
Overview of 
URL: https://otter.ai/u/EmvHyluJIUYQuJS07fByQiOkr4s
Downloaded: 2025-12-21T20:48:43.792447
Method: text_extraction
============================================================

0:00I You is Like I cover Here.
I You is Like I cover Here.
I You is Like I cover Here.
I You is Like I cover Here.
2:02Hello, hi, Priyesh, how are you?
Hello, hi, Priyesh, how are you?
Hello, hi, Priyesh, how are you?
Hello, hi, Priyesh, how are you?
S Speaker 12:17Hey, hi everyone. I'm doing great. Man. How you doing? Very good. Where are you calling in from? I'm in the Bay Area. We are based like the team. We are in split between San Diego and the bay. I'm currently in San Mateo. Where are you from?
Hey, hi everyone. I'm doing great. Man. How you doing? Very good. Where are you calling in from? I'm in the Bay Area. We are based like the team. We are in split between San Diego and the bay. I'm currently in San Mateo. Where are you from?
Hey, hi everyone. I'm doing great. Man. How you doing? Very good. Where are you calling in from? I'm in the Bay Area. We are based like the team. We are in split between San Diego and the bay. I'm currently in San Mateo. Where are you from?
Hey, hi everyone. I'm doing great. Man. How you doing? Very good. Where are you calling in from? I'm in the Bay Area. We are based like the team. We are in split between San Diego and the bay. I'm currently in San Mateo. Where are you from?
2:32I'm calling in from San Francisco. Okay,
I'm calling in from San Francisco. Okay,
I'm calling in from San Francisco. Okay,
I'm calling in from San Francisco. Okay,
S Speaker 12:36yeah, that is there, where the team is based, yeah. And is it just the two of you guys know, or how big is the team now?
yeah, that is there, where the team is based, yeah. And is it just the two of you guys know, or how big is the team now?
yeah, that is there, where the team is based, yeah. And is it just the two of you guys know, or how big is the team now?
yeah, that is there, where the team is based, yeah. And is it just the two of you guys know, or how big is the team now?
S Speaker 22:44Yeah, which is the two of us at the moment? We're just getting started?
Yeah, which is the two of us at the moment? We're just getting started?
Yeah, which is the two of us at the moment? We're just getting started?
Yeah, which is the two of us at the moment? We're just getting started?
S Speaker 24:13So, yeah, we're an AI research lab that's training tiny frontier models that are so small that they can run on edge devices. So we are trying to solve edge AI from the model layer, because models today are not small enough to run on all edge devices. So we think we have the expertise to bring them to be super efficient and run on CPU. They can run on Raspberry Pis, or they can run on npus and other hardware accelerators that are available on edge, and we're going to do this for all modalities. We started with speech, because there's already a huge market for on device speech, but we're doing vlms, vlms next and more like vision based stuff, like image models, video models, yeah. And after that, we start doing like, more task specific llms, general purpose llms. So that's the current roadmap we are. Our main priority is to get these models to run on edge devices like smartphones, IoT devices, embedded systems. And I think that we are probably, like, gonna build the like we already on track to have the best text to speech model that is super tiny. And I think we are gonna like, we have the expertise to bring the same trend for BLMs as well, which will be a huge unlock for manufacturing, for security, surveillance, for mobile phones. And then, if you think of like, text for voice, like, we're gonna have the full stack for voice agents that can run on device, so like speech to text, tool calling and text to speech, and that'll unlock like on device voice agents. Then we do a vlms, where you can have custom vlms for manufacturing or other IoT devices, and then trans specific llms and general purpose llms. So that's our current roadmap.
So, yeah, we're an AI research lab that's training tiny frontier models that are so small that they can run on edge devices. So we are trying to solve edge AI from the model layer, because models today are not small enough to run on all edge devices. So we think we have the expertise to bring them to be super efficient and run on CPU. They can run on Raspberry Pis, or they can run on npus and other hardware accelerators that are available on edge, and we're going to do this for all modalities. We started with speech, because there's already a huge market for on device speech, but we're doing vlms, vlms next and more like vision based stuff, like image models, video models, yeah. And after that, we start doing like, more task specific llms, general purpose llms. So that's the current roadmap we are. Our main priority is to get these models to run on edge devices like smartphones, IoT devices, embedded systems. And I think that we are probably, like, gonna build the like we already on track to have the best text to speech model that is super tiny. And I think we are gonna like, we have the expertise to bring the same trend for BLMs as well, which will be a huge unlock for manufacturing, for security, surveillance, for mobile phones. And then, if you think of like, text for voice, like, we're gonna have the full stack for voice agents that can run on device, so like speech to text, tool calling and text to speech, and that'll unlock like on device voice agents. Then we do a vlms, where you can have custom vlms for manufacturing or other IoT devices, and then trans specific llms and general purpose llms. So that's our current roadmap.
So, yeah, we're an AI research lab that's training tiny frontier models that are so small that they can run on edge devices. So we are trying to solve edge AI from the model layer, because models today are not small enough to run on all edge devices. So we think we have the expertise to bring them to be super efficient and run on CPU. They can run on Raspberry Pis, or they can run on npus and other hardware accelerators that are available on edge, and we're going to do this for all modalities. We started with speech, because there's already a huge market for on device speech, but we're doing vlms, vlms next and more like vision based stuff, like image models, video models, yeah. And after that, we start doing like, more task specific llms, general purpose llms. So that's the current roadmap we are. Our main priority is to get these models to run on edge devices like smartphones, IoT devices, embedded systems. And I think that we are probably, like, gonna build the like we already on track to have the best text to speech model that is super tiny. And I think we are gonna like, we have the expertise to bring the same trend for BLMs as well, which will be a huge unlock for manufacturing, for security, surveillance, for mobile phones. And then, if you think of like, text for voice, like, we're gonna have the full stack for voice agents that can run on device, so like speech to text, tool calling and text to speech, and that'll unlock like on device voice agents. Then we do a vlms, where you can have custom vlms for manufacturing or other IoT devices, and then trans specific llms and general purpose llms. So that's our current roadmap.
So, yeah, we're an AI research lab that's training tiny frontier models that are so small that they can run on edge devices. So we are trying to solve edge AI from the model layer, because models today are not small enough to run on all edge devices. So we think we have the expertise to bring them to be super efficient and run on CPU. They can run on Raspberry Pis, or they can run on npus and other hardware accelerators that are available on edge, and we're going to do this for all modalities. We started with speech, because there's already a huge market for on device speech, but we're doing vlms, vlms next and more like vision based stuff, like image models, video models, yeah. And after that, we start doing like, more task specific llms, general purpose llms. So that's the current roadmap we are. Our main priority is to get these models to run on edge devices like smartphones, IoT devices, embedded systems. And I think that we are probably, like, gonna build the like we already on track to have the best text to speech model that is super tiny. And I think we are gonna like, we have the expertise to bring the same trend for BLMs as well, which will be a huge unlock for manufacturing, for security, surveillance, for mobile phones. And then, if you think of like, text for voice, like, we're gonna have the full stack for voice agents that can run on device, so like speech to text, tool calling and text to speech, and that'll unlock like on device voice agents. Then we do a vlms, where you can have custom vlms for manufacturing or other IoT devices, and then trans specific llms and general purpose llms. So that's our current roadmap.
S Speaker 16:06So that's a, that's a big, big vision Rowan and super interesting stuff, actually, let's, let's just unpack the voice model today, right? You mentioned you have a very tiny text to speech model right. Speech model today, 25 MB, something like that, and you're already beating some of the frontier voice model companies. How are you thinking of in terms of or maybe three, three parameters, right, cost, latency and accuracy across all of these three vectors, where do you see yourself placed, and what? Maybe that's the next question. But yeah, just on these three vectors, where do you see yourself comparing to some of the other models?
So that's a, that's a big, big vision Rowan and super interesting stuff, actually, let's, let's just unpack the voice model today, right? You mentioned you have a very tiny text to speech model right. Speech model today, 25 MB, something like that, and you're already beating some of the frontier voice model companies. How are you thinking of in terms of or maybe three, three parameters, right, cost, latency and accuracy across all of these three vectors, where do you see yourself placed, and what? Maybe that's the next question. But yeah, just on these three vectors, where do you see yourself comparing to some of the other models?
So that's a, that's a big, big vision Rowan and super interesting stuff, actually, let's, let's just unpack the voice model today, right? You mentioned you have a very tiny text to speech model right. Speech model today, 25 MB, something like that, and you're already beating some of the frontier voice model companies. How are you thinking of in terms of or maybe three, three parameters, right, cost, latency and accuracy across all of these three vectors, where do you see yourself placed, and what? Maybe that's the next question. But yeah, just on these three vectors, where do you see yourself comparing to some of the other models?
So that's a, that's a big, big vision Rowan and super interesting stuff, actually, let's, let's just unpack the voice model today, right? You mentioned you have a very tiny text to speech model right. Speech model today, 25 MB, something like that, and you're already beating some of the frontier voice model companies. How are you thinking of in terms of or maybe three, three parameters, right, cost, latency and accuracy across all of these three vectors, where do you see yourself placed, and what? Maybe that's the next question. But yeah, just on these three vectors, where do you see yourself comparing to some of the other models?
S Speaker 26:51Yeah. So for latency, we will have the best latency out of all Yeah, of course, like the kind of devices that we will support are not even accessible to most of these models. So they can't even run and fit on these devices or run in real time, whereas ours are so small, faster than real time, if you have a hardware accelerator, but even if you don't, you can run them in like a reasonable amount. So like most of our users, want to run us like yo and Android devices, which don't have the kind of hardware acceleration, like that iPhone does, yeah. So I think in terms of latency, we will, like, have the best model there. In terms of quality, like, we think by the end, by middle of next year, like, for 90% of the use cases, we will be matching closed source models which fit on the cloud and yeah. So I think these are, like, the two main parameters of quality and latency, like we just got started, like four weeks ago. Yeah, before that, we were working on a different idea, and we pivoted to this, like a little over four weeks ago, and since, and we launched, like a little over two weeks ago. And when we launched, like, we are one of the fastest growing open source projects out of Y Combinator. We have like, more than 90,000 model downloads on hugging face already, like three weeks. And so I think the community is very excited for our scale of models, and they see, like, a lot of potential in us just keeping on improving the quality after every iteration. So yeah, I think that's that gave us the conviction that we should continue building for this space. And we think that we are an existential threat for cloud AI companies. If you look at cloud AI companies, if you can match that quality in the form factors that I have, there is no reason for them to exist like because you don't like if you can run these models on device, you can see you don't need to have GPU servers on the cloud. You don't need to have like, site reliability engineers that make sure that the API is available. So all the headache of handle having like, large GPU infrastructures taken away, if you can bring that quality to on device, and we have like, very clear experiments that we see even in house that we're able to scale the quality very predictably, and we are also working on like, core technology that allows us to bring that quality down. So one example is of ternary neural networks for like, like, one reason why neural networks are so slow is that you have a lot of these floating point matrix multiplications. But if you have, like, one bit neural networks, you can replace that multiplication with just addition, which is much faster on hardware. So we're training like, the first one bit speech model, which will be out in like, maybe two months. And these kind like, having a team that is able to build this kind of core technology is what you need if you want to really unlock AGI. I think everyone thinks that AGI is in the future, but like, I think it's time to make it happen now. So you need these kind of labs that are focused on this, whereas we see all the existing labs are just trying to scale models one after the other. And as you scale models like you make them more inaccessible to edge devices. So we want to just focus on edge devices and start unlocking AI applications that were not possible before.
Yeah. So for latency, we will have the best latency out of all Yeah, of course, like the kind of devices that we will support are not even accessible to most of these models. So they can't even run and fit on these devices or run in real time, whereas ours are so small, faster than real time, if you have a hardware accelerator, but even if you don't, you can run them in like a reasonable amount. So like most of our users, want to run us like yo and Android devices, which don't have the kind of hardware acceleration, like that iPhone does, yeah. So I think in terms of latency, we will, like, have the best model there. In terms of quality, like, we think by the end, by middle of next year, like, for 90% of the use cases, we will be matching closed source models which fit on the cloud and yeah. So I think these are, like, the two main parameters of quality and latency, like we just got started, like four weeks ago. Yeah, before that, we were working on a different idea, and we pivoted to this, like a little over four weeks ago, and since, and we launched, like a little over two weeks ago. And when we launched, like, we are one of the fastest growing open source projects out of Y Combinator. We have like, more than 90,000 model downloads on hugging face already, like three weeks. And so I think the community is very excited for our scale of models, and they see, like, a lot of potential in us just keeping on improving the quality after every iteration. So yeah, I think that's that gave us the conviction that we should continue building for this space. And we think that we are an existential threat for cloud AI companies. If you look at cloud AI companies, if you can match that quality in the form factors that I have, there is no reason for them to exist like because you don't like if you can run these models on device, you can see you don't need to have GPU servers on the cloud. You don't need to have like, site reliability engineers that make sure that the API is available. So all the headache of handle having like, large GPU infrastructures taken away, if you can bring that quality to on device, and we have like, very clear experiments that we see even in house that we're able to scale the quality very predictably, and we are also working on like, core technology that allows us to bring that quality down. So one example is of ternary neural networks for like, like, one reason why neural networks are so slow is that you have a lot of these floating point matrix multiplications. But if you have, like, one bit neural networks, you can replace that multiplication with just addition, which is much faster on hardware. So we're training like, the first one bit speech model, which will be out in like, maybe two months. And these kind like, having a team that is able to build this kind of core technology is what you need if you want to really unlock AGI. I think everyone thinks that AGI is in the future, but like, I think it's time to make it happen now. So you need these kind of labs that are focused on this, whereas we see all the existing labs are just trying to scale models one after the other. And as you scale models like you make them more inaccessible to edge devices. So we want to just focus on edge devices and start unlocking AI applications that were not possible before.
Yeah. So for latency, we will have the best latency out of all Yeah, of course, like the kind of devices that we will support are not even accessible to most of these models. So they can't even run and fit on these devices or run in real time, whereas ours are so small, faster than real time, if you have a hardware accelerator, but even if you don't, you can run them in like a reasonable amount. So like most of our users, want to run us like yo and Android devices, which don't have the kind of hardware acceleration, like that iPhone does, yeah. So I think in terms of latency, we will, like, have the best model there. In terms of quality, like, we think by the end, by middle of next year, like, for 90% of the use cases, we will be matching closed source models which fit on the cloud and yeah. So I think these are, like, the two main parameters of quality and latency, like we just got started, like four weeks ago. Yeah, before that, we were working on a different idea, and we pivoted to this, like a little over four weeks ago, and since, and we launched, like a little over two weeks ago. And when we launched, like, we are one of the fastest growing open source projects out of Y Combinator. We have like, more than 90,000 model downloads on hugging face already, like three weeks. And so I think the community is very excited for our scale of models, and they see, like, a lot of potential in us just keeping on improving the quality after every iteration. So yeah, I think that's that gave us the conviction that we should continue building for this space. And we think that we are an existential threat for cloud AI companies. If you look at cloud AI companies, if you can match that quality in the form factors that I have, there is no reason for them to exist like because you don't like if you can run these models on device, you can see you don't need to have GPU servers on the cloud. You don't need to have like, site reliability engineers that make sure that the API is available. So all the headache of handle having like, large GPU infrastructures taken away, if you can bring that quality to on device, and we have like, very clear experiments that we see even in house that we're able to scale the quality very predictably, and we are also working on like, core technology that allows us to bring that quality down. So one example is of ternary neural networks for like, like, one reason why neural networks are so slow is that you have a lot of these floating point matrix multiplications. But if you have, like, one bit neural networks, you can replace that multiplication with just addition, which is much faster on hardware. So we're training like, the first one bit speech model, which will be out in like, maybe two months. And these kind like, having a team that is able to build this kind of core technology is what you need if you want to really unlock AGI. I think everyone thinks that AGI is in the future, but like, I think it's time to make it happen now. So you need these kind of labs that are focused on this, whereas we see all the existing labs are just trying to scale models one after the other. And as you scale models like you make them more inaccessible to edge devices. So we want to just focus on edge devices and start unlocking AI applications that were not possible before.
Yeah. So for latency, we will have the best latency out of all Yeah, of course, like the kind of devices that we will support are not even accessible to most of these models. So they can't even run and fit on these devices or run in real time, whereas ours are so small, faster than real time, if you have a hardware accelerator, but even if you don't, you can run them in like a reasonable amount. So like most of our users, want to run us like yo and Android devices, which don't have the kind of hardware acceleration, like that iPhone does, yeah. So I think in terms of latency, we will, like, have the best model there. In terms of quality, like, we think by the end, by middle of next year, like, for 90% of the use cases, we will be matching closed source models which fit on the cloud and yeah. So I think these are, like, the two main parameters of quality and latency, like we just got started, like four weeks ago. Yeah, before that, we were working on a different idea, and we pivoted to this, like a little over four weeks ago, and since, and we launched, like a little over two weeks ago. And when we launched, like, we are one of the fastest growing open source projects out of Y Combinator. We have like, more than 90,000 model downloads on hugging face already, like three weeks. And so I think the community is very excited for our scale of models, and they see, like, a lot of potential in us just keeping on improving the quality after every iteration. So yeah, I think that's that gave us the conviction that we should continue building for this space. And we think that we are an existential threat for cloud AI companies. If you look at cloud AI companies, if you can match that quality in the form factors that I have, there is no reason for them to exist like because you don't like if you can run these models on device, you can see you don't need to have GPU servers on the cloud. You don't need to have like, site reliability engineers that make sure that the API is available. So all the headache of handle having like, large GPU infrastructures taken away, if you can bring that quality to on device, and we have like, very clear experiments that we see even in house that we're able to scale the quality very predictably, and we are also working on like, core technology that allows us to bring that quality down. So one example is of ternary neural networks for like, like, one reason why neural networks are so slow is that you have a lot of these floating point matrix multiplications. But if you have, like, one bit neural networks, you can replace that multiplication with just addition, which is much faster on hardware. So we're training like, the first one bit speech model, which will be out in like, maybe two months. And these kind like, having a team that is able to build this kind of core technology is what you need if you want to really unlock AGI. I think everyone thinks that AGI is in the future, but like, I think it's time to make it happen now. So you need these kind of labs that are focused on this, whereas we see all the existing labs are just trying to scale models one after the other. And as you scale models like you make them more inaccessible to edge devices. So we want to just focus on edge devices and start unlocking AI applications that were not possible before.
S Speaker 110:04Super interesting. Rohan, and I think first of all, great progress in the four weeks that you've started that's really amazing. I was smiling because what you said, if someone in the Qualcomm's business unit would have heard they would have loved it. That's exactly the kind of, I would say, modus operandi that we want to focus on, but I am in the, I would say, camp, which still believes that there is time before edge. AI is really practical, but we are sort of putting a lot of our or some part of workflows of a lot of our portfolio companies onto edge. And seems like it makes a lot of sense, maybe today, the reasoning for it might not be just accuracy, cost. A lot of market pull is coming from privacy preserving scenarios, from very sensitive industries, and that's where we see a lot of our portfolio companies trying to get on and sort of put a lot of their models on the PC itself. Sell the PCs directly to some of these enterprises which are privacy sensitive, but I see the future you're building for super interesting. How do you think about commercialization? Let's if we are just speaking about the voice model part of things. How do you see the commercialize? Or maybe a primer question to that 90k downloads on hugging faces huge you you already have a good community build to use like, what kind of use cases are some of these members using it for?
Super interesting. Rohan, and I think first of all, great progress in the four weeks that you've started that's really amazing. I was smiling because what you said, if someone in the Qualcomm's business unit would have heard they would have loved it. That's exactly the kind of, I would say, modus operandi that we want to focus on, but I am in the, I would say, camp, which still believes that there is time before edge. AI is really practical, but we are sort of putting a lot of our or some part of workflows of a lot of our portfolio companies onto edge. And seems like it makes a lot of sense, maybe today, the reasoning for it might not be just accuracy, cost. A lot of market pull is coming from privacy preserving scenarios, from very sensitive industries, and that's where we see a lot of our portfolio companies trying to get on and sort of put a lot of their models on the PC itself. Sell the PCs directly to some of these enterprises which are privacy sensitive, but I see the future you're building for super interesting. How do you think about commercialization? Let's if we are just speaking about the voice model part of things. How do you see the commercialize? Or maybe a primer question to that 90k downloads on hugging faces huge you you already have a good community build to use like, what kind of use cases are some of these members using it for?
Super interesting. Rohan, and I think first of all, great progress in the four weeks that you've started that's really amazing. I was smiling because what you said, if someone in the Qualcomm's business unit would have heard they would have loved it. That's exactly the kind of, I would say, modus operandi that we want to focus on, but I am in the, I would say, camp, which still believes that there is time before edge. AI is really practical, but we are sort of putting a lot of our or some part of workflows of a lot of our portfolio companies onto edge. And seems like it makes a lot of sense, maybe today, the reasoning for it might not be just accuracy, cost. A lot of market pull is coming from privacy preserving scenarios, from very sensitive industries, and that's where we see a lot of our portfolio companies trying to get on and sort of put a lot of their models on the PC itself. Sell the PCs directly to some of these enterprises which are privacy sensitive, but I see the future you're building for super interesting. How do you think about commercialization? Let's if we are just speaking about the voice model part of things. How do you see the commercialize? Or maybe a primer question to that 90k downloads on hugging faces huge you you already have a good community build to use like, what kind of use cases are some of these members using it for?
Super interesting. Rohan, and I think first of all, great progress in the four weeks that you've started that's really amazing. I was smiling because what you said, if someone in the Qualcomm's business unit would have heard they would have loved it. That's exactly the kind of, I would say, modus operandi that we want to focus on, but I am in the, I would say, camp, which still believes that there is time before edge. AI is really practical, but we are sort of putting a lot of our or some part of workflows of a lot of our portfolio companies onto edge. And seems like it makes a lot of sense, maybe today, the reasoning for it might not be just accuracy, cost. A lot of market pull is coming from privacy preserving scenarios, from very sensitive industries, and that's where we see a lot of our portfolio companies trying to get on and sort of put a lot of their models on the PC itself. Sell the PCs directly to some of these enterprises which are privacy sensitive, but I see the future you're building for super interesting. How do you think about commercialization? Let's if we are just speaking about the voice model part of things. How do you see the commercialize? Or maybe a primer question to that 90k downloads on hugging faces huge you you already have a good community build to use like, what kind of use cases are some of these members using it for?
S Speaker 211:33A lot of them are for running on mobile applications on device compute. So a lot of these devices are Android devices, where you can't run other open source models, so iPhones are still very powerful. So you can run models that are slightly more powerful on iPhones versus on other Androids, then that is so like we are getting inbound in three categories, right? One is like hardware and robotics companies that want to run our model on their hardware, so a model can run on CPU, so it allows them to run other AI models, on the GPU, on device. So that's like a big unlock, having super efficient models, which are efficient by structure, like just inherently more cost effective, cost efficient. So that is one category of inbound for hardware companies. The second category of inbound are mobile, mobile applications where they want to start providing voice features to their users, and most of their users are on Android devices, so you need a very efficient model that can run on Android phones. And the third is kind of use case that we did not expect, but people want to take our model, put it up on the cloud, and run it on on cloud servers. So yeah, I think that is like, of course, like then you can downgrade from each 100 GPU to, like, a very low end GPU or even CPU servers. So like, that cost arbitrage that you get by just switching to a more efficient model is why people want to run us. So these are the three categories right now. We think that category one and two, where for hardware, robotics and for mobile applications, we have, like, a very clear right to win, because no one else is building for it global use cases like I think it's more complicated. It's mostly outside of North America and Western Europe, where this use case will be, we'll start displacing cloud usage first. So, yeah, I think because there the cost, like in the US and in Western Europe, voice AI and cloud AI is competing with the cost of human labor, which is very high. Outside of North America and Western Europe. The cost of human labor is very low to the extent very few you pick 11 labs for making voice agents. The cost of your voice agents will be more than the cost of this hiring someone to do it. So so the way you have voice agent costs go down lower than the cost of human labor is by having on device compute, which is why people are excited to use our models, because they're so efficient that you can start getting the benefits of AI, even in these more, stricter cost markets. And, yeah, I think these are, like the three categories that we're seeing in private.
A lot of them are for running on mobile applications on device compute. So a lot of these devices are Android devices, where you can't run other open source models, so iPhones are still very powerful. So you can run models that are slightly more powerful on iPhones versus on other Androids, then that is so like we are getting inbound in three categories, right? One is like hardware and robotics companies that want to run our model on their hardware, so a model can run on CPU, so it allows them to run other AI models, on the GPU, on device. So that's like a big unlock, having super efficient models, which are efficient by structure, like just inherently more cost effective, cost efficient. So that is one category of inbound for hardware companies. The second category of inbound are mobile, mobile applications where they want to start providing voice features to their users, and most of their users are on Android devices, so you need a very efficient model that can run on Android phones. And the third is kind of use case that we did not expect, but people want to take our model, put it up on the cloud, and run it on on cloud servers. So yeah, I think that is like, of course, like then you can downgrade from each 100 GPU to, like, a very low end GPU or even CPU servers. So like, that cost arbitrage that you get by just switching to a more efficient model is why people want to run us. So these are the three categories right now. We think that category one and two, where for hardware, robotics and for mobile applications, we have, like, a very clear right to win, because no one else is building for it global use cases like I think it's more complicated. It's mostly outside of North America and Western Europe, where this use case will be, we'll start displacing cloud usage first. So, yeah, I think because there the cost, like in the US and in Western Europe, voice AI and cloud AI is competing with the cost of human labor, which is very high. Outside of North America and Western Europe. The cost of human labor is very low to the extent very few you pick 11 labs for making voice agents. The cost of your voice agents will be more than the cost of this hiring someone to do it. So so the way you have voice agent costs go down lower than the cost of human labor is by having on device compute, which is why people are excited to use our models, because they're so efficient that you can start getting the benefits of AI, even in these more, stricter cost markets. And, yeah, I think these are, like the three categories that we're seeing in private.
A lot of them are for running on mobile applications on device compute. So a lot of these devices are Android devices, where you can't run other open source models, so iPhones are still very powerful. So you can run models that are slightly more powerful on iPhones versus on other Androids, then that is so like we are getting inbound in three categories, right? One is like hardware and robotics companies that want to run our model on their hardware, so a model can run on CPU, so it allows them to run other AI models, on the GPU, on device. So that's like a big unlock, having super efficient models, which are efficient by structure, like just inherently more cost effective, cost efficient. So that is one category of inbound for hardware companies. The second category of inbound are mobile, mobile applications where they want to start providing voice features to their users, and most of their users are on Android devices, so you need a very efficient model that can run on Android phones. And the third is kind of use case that we did not expect, but people want to take our model, put it up on the cloud, and run it on on cloud servers. So yeah, I think that is like, of course, like then you can downgrade from each 100 GPU to, like, a very low end GPU or even CPU servers. So like, that cost arbitrage that you get by just switching to a more efficient model is why people want to run us. So these are the three categories right now. We think that category one and two, where for hardware, robotics and for mobile applications, we have, like, a very clear right to win, because no one else is building for it global use cases like I think it's more complicated. It's mostly outside of North America and Western Europe, where this use case will be, we'll start displacing cloud usage first. So, yeah, I think because there the cost, like in the US and in Western Europe, voice AI and cloud AI is competing with the cost of human labor, which is very high. Outside of North America and Western Europe. The cost of human labor is very low to the extent very few you pick 11 labs for making voice agents. The cost of your voice agents will be more than the cost of this hiring someone to do it. So so the way you have voice agent costs go down lower than the cost of human labor is by having on device compute, which is why people are excited to use our models, because they're so efficient that you can start getting the benefits of AI, even in these more, stricter cost markets. And, yeah, I think these are, like the three categories that we're seeing in private.
A lot of them are for running on mobile applications on device compute. So a lot of these devices are Android devices, where you can't run other open source models, so iPhones are still very powerful. So you can run models that are slightly more powerful on iPhones versus on other Androids, then that is so like we are getting inbound in three categories, right? One is like hardware and robotics companies that want to run our model on their hardware, so a model can run on CPU, so it allows them to run other AI models, on the GPU, on device. So that's like a big unlock, having super efficient models, which are efficient by structure, like just inherently more cost effective, cost efficient. So that is one category of inbound for hardware companies. The second category of inbound are mobile, mobile applications where they want to start providing voice features to their users, and most of their users are on Android devices, so you need a very efficient model that can run on Android phones. And the third is kind of use case that we did not expect, but people want to take our model, put it up on the cloud, and run it on on cloud servers. So yeah, I think that is like, of course, like then you can downgrade from each 100 GPU to, like, a very low end GPU or even CPU servers. So like, that cost arbitrage that you get by just switching to a more efficient model is why people want to run us. So these are the three categories right now. We think that category one and two, where for hardware, robotics and for mobile applications, we have, like, a very clear right to win, because no one else is building for it global use cases like I think it's more complicated. It's mostly outside of North America and Western Europe, where this use case will be, we'll start displacing cloud usage first. So, yeah, I think because there the cost, like in the US and in Western Europe, voice AI and cloud AI is competing with the cost of human labor, which is very high. Outside of North America and Western Europe. The cost of human labor is very low to the extent very few you pick 11 labs for making voice agents. The cost of your voice agents will be more than the cost of this hiring someone to do it. So so the way you have voice agent costs go down lower than the cost of human labor is by having on device compute, which is why people are excited to use our models, because they're so efficient that you can start getting the benefits of AI, even in these more, stricter cost markets. And, yeah, I think these are, like the three categories that we're seeing in private.
S Speaker 114:08Interesting. Rowan and so how do you think about commercialization? And I asked that question because we had earlier invested in a company called well said labs, if you, if you've heard about them, it's a complex, well said labs. So they develop voice models. It's a it was one of the early ones. Came around, probably even earlier than 11 laps. And by when during the time we were investing in them, they had probably much better accuracy than a quality than 11 labs. 11 Labs has done much better since then, and more recently, you would see a lot of voice AI models come on the market, many open source versions, many closes versions, many approaching the market from different angles. There is different architectures. You have space state, space models, etc, etc, right? Yeah. Thinking in macro terms, the model, seem, the technology behind model seems to be commoditizing very soon, and a lot of these companies are moving into applications, trying to get closer to customers, trying to get value from there. What's your thought around the entire commercialization space? How you're thinking about
Interesting. Rowan and so how do you think about commercialization? And I asked that question because we had earlier invested in a company called well said labs, if you, if you've heard about them, it's a complex, well said labs. So they develop voice models. It's a it was one of the early ones. Came around, probably even earlier than 11 laps. And by when during the time we were investing in them, they had probably much better accuracy than a quality than 11 labs. 11 Labs has done much better since then, and more recently, you would see a lot of voice AI models come on the market, many open source versions, many closes versions, many approaching the market from different angles. There is different architectures. You have space state, space models, etc, etc, right? Yeah. Thinking in macro terms, the model, seem, the technology behind model seems to be commoditizing very soon, and a lot of these companies are moving into applications, trying to get closer to customers, trying to get value from there. What's your thought around the entire commercialization space? How you're thinking about
Interesting. Rowan and so how do you think about commercialization? And I asked that question because we had earlier invested in a company called well said labs, if you, if you've heard about them, it's a complex, well said labs. So they develop voice models. It's a it was one of the early ones. Came around, probably even earlier than 11 laps. And by when during the time we were investing in them, they had probably much better accuracy than a quality than 11 labs. 11 Labs has done much better since then, and more recently, you would see a lot of voice AI models come on the market, many open source versions, many closes versions, many approaching the market from different angles. There is different architectures. You have space state, space models, etc, etc, right? Yeah. Thinking in macro terms, the model, seem, the technology behind model seems to be commoditizing very soon, and a lot of these companies are moving into applications, trying to get closer to customers, trying to get value from there. What's your thought around the entire commercialization space? How you're thinking about
Interesting. Rowan and so how do you think about commercialization? And I asked that question because we had earlier invested in a company called well said labs, if you, if you've heard about them, it's a complex, well said labs. So they develop voice models. It's a it was one of the early ones. Came around, probably even earlier than 11 laps. And by when during the time we were investing in them, they had probably much better accuracy than a quality than 11 labs. 11 Labs has done much better since then, and more recently, you would see a lot of voice AI models come on the market, many open source versions, many closes versions, many approaching the market from different angles. There is different architectures. You have space state, space models, etc, etc, right? Yeah. Thinking in macro terms, the model, seem, the technology behind model seems to be commoditizing very soon, and a lot of these companies are moving into applications, trying to get closer to customers, trying to get value from there. What's your thought around the entire commercialization space? How you're thinking about
S Speaker 215:21it? Yeah, so I think that a lot of the text to speech use cases are still focused on data center GPUs and cloud GPUs. We are focusing on hardware and on mobile applications for running on device there. There is actually very little competition
it? Yeah, so I think that a lot of the text to speech use cases are still focused on data center GPUs and cloud GPUs. We are focusing on hardware and on mobile applications for running on device there. There is actually very little competition
it? Yeah, so I think that a lot of the text to speech use cases are still focused on data center GPUs and cloud GPUs. We are focusing on hardware and on mobile applications for running on device there. There is actually very little competition
it? Yeah, so I think that a lot of the text to speech use cases are still focused on data center GPUs and cloud GPUs. We are focusing on hardware and on mobile applications for running on device there. There is actually very little competition
S Speaker 215:40like the way we the way we think about our products, is by making it easy for developers to run voice AI into their applications and build voice agents. So we have to build SDKs for Android. SDKs for different backend accelerators, like for npus, or if you want to run on a neural engine, like, we have to support these kind of backend accelerations. And like, funnily enough, like, people are using checkpoints from research papers as an alternative because they want to run tiny models, but they don't exist. Like, none of the big labs are focusing on this part. Yeah, they want to. They are focusing on Cloud AI and for running on data center GPUs that cloud access is mandatory because that's where most of the revenue is today, and also that's where they've invested so much capex into their models that if they want an ROI in the short term, like they have to go after cloud for us, like we want to be the kings of AGI, where our models are the models you use if you want to run anything on devices. We are not competing. We are trying to get to AGI so we don't need to keep scaling our models. We are the best models that can fit on a mobile phone. You can fit on a wearable, fit on robotics, and for that, the strategy has to be very different. You can't if you can't rely on the fact that you will build like your technology behind these models will get commoditized, and in any case, like a model itself will become outdated after six or 12 months. So what you are building is a team that can keep iterating and get improving the quality of these models every six months, and that, compounding over the next few years, builds an ecosystem which is ubiquitous, then it's almost you can't get outside of, like, for example, if you look in the cloud space, the cost of switching models is very high. Like, if this is very low, sorry, you can switch from one model to another without having to worry much about, like, it'll take you, like, less than a day to switch your model provider, whereas, for edge devices, now you like, you're in like, low level system implementation, where, if you provide the best user experience for integrating your model, an ecosystem starts building around you. Like, whereas, on the cloud, almost all the data, all the GPUs are like Nilda GPUs and all the cloud inference structure is based around NVIDIA GPUs and maybe some AMD GPUs for edge devices. Every edge device is different. Your mode ends up being like just distribution and your ease of integrating into a new edge device. And then you and you only support your models to be used that easily. So I think, I think that's why, also, like a lot of labs, are not focusing on this market, is because you can't just stop at building models. You have to still cover the last mile of getting that. We're getting users to run your models on different edge devices. And we actually very excited to do, to do that allow people to run so just right before talking to you, I was talking with the CEO of Raspberry Pi, they also make their own chips, and to see how, like, you know what partnerships we could be exploring, yeah, because running AI on edge is something that everyone prefers. Now, only question, only reason they have not, is that none of the labs are focused on bringing the quality down to edge AI, all the labs are going up for AGI, which makes like a clear gap in the market for lab that can just take these large models and get it to be as small as possible. So I think that's how we are thinking
like the way we the way we think about our products, is by making it easy for developers to run voice AI into their applications and build voice agents. So we have to build SDKs for Android. SDKs for different backend accelerators, like for npus, or if you want to run on a neural engine, like, we have to support these kind of backend accelerations. And like, funnily enough, like, people are using checkpoints from research papers as an alternative because they want to run tiny models, but they don't exist. Like, none of the big labs are focusing on this part. Yeah, they want to. They are focusing on Cloud AI and for running on data center GPUs that cloud access is mandatory because that's where most of the revenue is today, and also that's where they've invested so much capex into their models that if they want an ROI in the short term, like they have to go after cloud for us, like we want to be the kings of AGI, where our models are the models you use if you want to run anything on devices. We are not competing. We are trying to get to AGI so we don't need to keep scaling our models. We are the best models that can fit on a mobile phone. You can fit on a wearable, fit on robotics, and for that, the strategy has to be very different. You can't if you can't rely on the fact that you will build like your technology behind these models will get commoditized, and in any case, like a model itself will become outdated after six or 12 months. So what you are building is a team that can keep iterating and get improving the quality of these models every six months, and that, compounding over the next few years, builds an ecosystem which is ubiquitous, then it's almost you can't get outside of, like, for example, if you look in the cloud space, the cost of switching models is very high. Like, if this is very low, sorry, you can switch from one model to another without having to worry much about, like, it'll take you, like, less than a day to switch your model provider, whereas, for edge devices, now you like, you're in like, low level system implementation, where, if you provide the best user experience for integrating your model, an ecosystem starts building around you. Like, whereas, on the cloud, almost all the data, all the GPUs are like Nilda GPUs and all the cloud inference structure is based around NVIDIA GPUs and maybe some AMD GPUs for edge devices. Every edge device is different. Your mode ends up being like just distribution and your ease of integrating into a new edge device. And then you and you only support your models to be used that easily. So I think, I think that's why, also, like a lot of labs, are not focusing on this market, is because you can't just stop at building models. You have to still cover the last mile of getting that. We're getting users to run your models on different edge devices. And we actually very excited to do, to do that allow people to run so just right before talking to you, I was talking with the CEO of Raspberry Pi, they also make their own chips, and to see how, like, you know what partnerships we could be exploring, yeah, because running AI on edge is something that everyone prefers. Now, only question, only reason they have not, is that none of the labs are focused on bringing the quality down to edge AI, all the labs are going up for AGI, which makes like a clear gap in the market for lab that can just take these large models and get it to be as small as possible. So I think that's how we are thinking
like the way we the way we think about our products, is by making it easy for developers to run voice AI into their applications and build voice agents. So we have to build SDKs for Android. SDKs for different backend accelerators, like for npus, or if you want to run on a neural engine, like, we have to support these kind of backend accelerations. And like, funnily enough, like, people are using checkpoints from research papers as an alternative because they want to run tiny models, but they don't exist. Like, none of the big labs are focusing on this part. Yeah, they want to. They are focusing on Cloud AI and for running on data center GPUs that cloud access is mandatory because that's where most of the revenue is today, and also that's where they've invested so much capex into their models that if they want an ROI in the short term, like they have to go after cloud for us, like we want to be the kings of AGI, where our models are the models you use if you want to run anything on devices. We are not competing. We are trying to get to AGI so we don't need to keep scaling our models. We are the best models that can fit on a mobile phone. You can fit on a wearable, fit on robotics, and for that, the strategy has to be very different. You can't if you can't rely on the fact that you will build like your technology behind these models will get commoditized, and in any case, like a model itself will become outdated after six or 12 months. So what you are building is a team that can keep iterating and get improving the quality of these models every six months, and that, compounding over the next few years, builds an ecosystem which is ubiquitous, then it's almost you can't get outside of, like, for example, if you look in the cloud space, the cost of switching models is very high. Like, if this is very low, sorry, you can switch from one model to another without having to worry much about, like, it'll take you, like, less than a day to switch your model provider, whereas, for edge devices, now you like, you're in like, low level system implementation, where, if you provide the best user experience for integrating your model, an ecosystem starts building around you. Like, whereas, on the cloud, almost all the data, all the GPUs are like Nilda GPUs and all the cloud inference structure is based around NVIDIA GPUs and maybe some AMD GPUs for edge devices. Every edge device is different. Your mode ends up being like just distribution and your ease of integrating into a new edge device. And then you and you only support your models to be used that easily. So I think, I think that's why, also, like a lot of labs, are not focusing on this market, is because you can't just stop at building models. You have to still cover the last mile of getting that. We're getting users to run your models on different edge devices. And we actually very excited to do, to do that allow people to run so just right before talking to you, I was talking with the CEO of Raspberry Pi, they also make their own chips, and to see how, like, you know what partnerships we could be exploring, yeah, because running AI on edge is something that everyone prefers. Now, only question, only reason they have not, is that none of the labs are focused on bringing the quality down to edge AI, all the labs are going up for AGI, which makes like a clear gap in the market for lab that can just take these large models and get it to be as small as possible. So I think that's how we are thinking
like the way we the way we think about our products, is by making it easy for developers to run voice AI into their applications and build voice agents. So we have to build SDKs for Android. SDKs for different backend accelerators, like for npus, or if you want to run on a neural engine, like, we have to support these kind of backend accelerations. And like, funnily enough, like, people are using checkpoints from research papers as an alternative because they want to run tiny models, but they don't exist. Like, none of the big labs are focusing on this part. Yeah, they want to. They are focusing on Cloud AI and for running on data center GPUs that cloud access is mandatory because that's where most of the revenue is today, and also that's where they've invested so much capex into their models that if they want an ROI in the short term, like they have to go after cloud for us, like we want to be the kings of AGI, where our models are the models you use if you want to run anything on devices. We are not competing. We are trying to get to AGI so we don't need to keep scaling our models. We are the best models that can fit on a mobile phone. You can fit on a wearable, fit on robotics, and for that, the strategy has to be very different. You can't if you can't rely on the fact that you will build like your technology behind these models will get commoditized, and in any case, like a model itself will become outdated after six or 12 months. So what you are building is a team that can keep iterating and get improving the quality of these models every six months, and that, compounding over the next few years, builds an ecosystem which is ubiquitous, then it's almost you can't get outside of, like, for example, if you look in the cloud space, the cost of switching models is very high. Like, if this is very low, sorry, you can switch from one model to another without having to worry much about, like, it'll take you, like, less than a day to switch your model provider, whereas, for edge devices, now you like, you're in like, low level system implementation, where, if you provide the best user experience for integrating your model, an ecosystem starts building around you. Like, whereas, on the cloud, almost all the data, all the GPUs are like Nilda GPUs and all the cloud inference structure is based around NVIDIA GPUs and maybe some AMD GPUs for edge devices. Every edge device is different. Your mode ends up being like just distribution and your ease of integrating into a new edge device. And then you and you only support your models to be used that easily. So I think, I think that's why, also, like a lot of labs, are not focusing on this market, is because you can't just stop at building models. You have to still cover the last mile of getting that. We're getting users to run your models on different edge devices. And we actually very excited to do, to do that allow people to run so just right before talking to you, I was talking with the CEO of Raspberry Pi, they also make their own chips, and to see how, like, you know what partnerships we could be exploring, yeah, because running AI on edge is something that everyone prefers. Now, only question, only reason they have not, is that none of the labs are focused on bringing the quality down to edge AI, all the labs are going up for AGI, which makes like a clear gap in the market for lab that can just take these large models and get it to be as small as possible. So I think that's how we are thinking
S Speaker 119:13about commercialization. That makes sense. Rohan, that makes a lot of sense. It's also a cannabis organization problem for some of these large models, if they increase, or if they really work on the quality of some of these small models, they are essentially cannibalizing their own business. So essence, makes a lot of sense. Maybe a follow up question on something you mentioned, we have seen a lot of problems with first hand, seen a lot of problems of deploying models on edge, like that's one of the biggest the entire software stack behind it is something that Qualcomm internally is working quite a lot on. It's a tough problem to solve. There is not a lot of compiler talent you have to work on low level languages, things like that. How are you thinking about that, especially given that it also feels like you've undertaken a very, very broad vision. The product map we initially discussed was huge, which is exciting at the same time, but also quite challenging. So thinking, how are you thinking about that?
about commercialization. That makes sense. Rohan, that makes a lot of sense. It's also a cannabis organization problem for some of these large models, if they increase, or if they really work on the quality of some of these small models, they are essentially cannibalizing their own business. So essence, makes a lot of sense. Maybe a follow up question on something you mentioned, we have seen a lot of problems with first hand, seen a lot of problems of deploying models on edge, like that's one of the biggest the entire software stack behind it is something that Qualcomm internally is working quite a lot on. It's a tough problem to solve. There is not a lot of compiler talent you have to work on low level languages, things like that. How are you thinking about that, especially given that it also feels like you've undertaken a very, very broad vision. The product map we initially discussed was huge, which is exciting at the same time, but also quite challenging. So thinking, how are you thinking about that?
about commercialization. That makes sense. Rohan, that makes a lot of sense. It's also a cannabis organization problem for some of these large models, if they increase, or if they really work on the quality of some of these small models, they are essentially cannibalizing their own business. So essence, makes a lot of sense. Maybe a follow up question on something you mentioned, we have seen a lot of problems with first hand, seen a lot of problems of deploying models on edge, like that's one of the biggest the entire software stack behind it is something that Qualcomm internally is working quite a lot on. It's a tough problem to solve. There is not a lot of compiler talent you have to work on low level languages, things like that. How are you thinking about that, especially given that it also feels like you've undertaken a very, very broad vision. The product map we initially discussed was huge, which is exciting at the same time, but also quite challenging. So thinking, how are you thinking about that?
about commercialization. That makes sense. Rohan, that makes a lot of sense. It's also a cannabis organization problem for some of these large models, if they increase, or if they really work on the quality of some of these small models, they are essentially cannibalizing their own business. So essence, makes a lot of sense. Maybe a follow up question on something you mentioned, we have seen a lot of problems with first hand, seen a lot of problems of deploying models on edge, like that's one of the biggest the entire software stack behind it is something that Qualcomm internally is working quite a lot on. It's a tough problem to solve. There is not a lot of compiler talent you have to work on low level languages, things like that. How are you thinking about that, especially given that it also feels like you've undertaken a very, very broad vision. The product map we initially discussed was huge, which is exciting at the same time, but also quite challenging. So thinking, how are you thinking about that?
S Speaker 220:13Yes, I think we'll solve it one step at a time, where as soon as we get, like, inbound customers, so like we will prioritize based on revenue and the back ends that get us the most distribution. We think that the way we become ubiquitous in this space is through bottoms up adoption. So we start by providing like an Android SDK or an iOS SDK. This unlocks the use of our models for mobile applications, then we can maybe, like, for example, one of the robotics companies we're building, like, they're not using, I think they're using Intel GPUs or some like, other kind of GPUs. So like, of course, that because if that converts to a commercial contract, then we will prioritize that for now, because I think our core competency is building the model. A lot of the implementation is like, more often than not, undertaken by the user itself, especially in hardware companies. It's mostly for app developers where there's a very clear distinction between low level optimizations and like app generation. So they have like, for example, like a mobile app developer might have expertise in building React Native apps. He may not know like, how to optimize it, to use the optimizer model, to use the neural engine, or optimize device to use like npus, for example, that like, so that is where we should be doing most of the heavy lifting. So I think that's how we will prioritize just mostly on what kind of inbound we get, what kind of commercial contract we see for what back ends. And I think over like, the vision we have is a multi year, like a decade long vision. So that is plenty of time to accomplish this. So for next year, it seems like we will be doubling down on voice and maybe BLMs. And I think there is enough of a market today to even voice AI is exploring. So there's, I think that'll that's enough to take us to a CBC.
Yes, I think we'll solve it one step at a time, where as soon as we get, like, inbound customers, so like we will prioritize based on revenue and the back ends that get us the most distribution. We think that the way we become ubiquitous in this space is through bottoms up adoption. So we start by providing like an Android SDK or an iOS SDK. This unlocks the use of our models for mobile applications, then we can maybe, like, for example, one of the robotics companies we're building, like, they're not using, I think they're using Intel GPUs or some like, other kind of GPUs. So like, of course, that because if that converts to a commercial contract, then we will prioritize that for now, because I think our core competency is building the model. A lot of the implementation is like, more often than not, undertaken by the user itself, especially in hardware companies. It's mostly for app developers where there's a very clear distinction between low level optimizations and like app generation. So they have like, for example, like a mobile app developer might have expertise in building React Native apps. He may not know like, how to optimize it, to use the optimizer model, to use the neural engine, or optimize device to use like npus, for example, that like, so that is where we should be doing most of the heavy lifting. So I think that's how we will prioritize just mostly on what kind of inbound we get, what kind of commercial contract we see for what back ends. And I think over like, the vision we have is a multi year, like a decade long vision. So that is plenty of time to accomplish this. So for next year, it seems like we will be doubling down on voice and maybe BLMs. And I think there is enough of a market today to even voice AI is exploring. So there's, I think that'll that's enough to take us to a CBC.
Yes, I think we'll solve it one step at a time, where as soon as we get, like, inbound customers, so like we will prioritize based on revenue and the back ends that get us the most distribution. We think that the way we become ubiquitous in this space is through bottoms up adoption. So we start by providing like an Android SDK or an iOS SDK. This unlocks the use of our models for mobile applications, then we can maybe, like, for example, one of the robotics companies we're building, like, they're not using, I think they're using Intel GPUs or some like, other kind of GPUs. So like, of course, that because if that converts to a commercial contract, then we will prioritize that for now, because I think our core competency is building the model. A lot of the implementation is like, more often than not, undertaken by the user itself, especially in hardware companies. It's mostly for app developers where there's a very clear distinction between low level optimizations and like app generation. So they have like, for example, like a mobile app developer might have expertise in building React Native apps. He may not know like, how to optimize it, to use the optimizer model, to use the neural engine, or optimize device to use like npus, for example, that like, so that is where we should be doing most of the heavy lifting. So I think that's how we will prioritize just mostly on what kind of inbound we get, what kind of commercial contract we see for what back ends. And I think over like, the vision we have is a multi year, like a decade long vision. So that is plenty of time to accomplish this. So for next year, it seems like we will be doubling down on voice and maybe BLMs. And I think there is enough of a market today to even voice AI is exploring. So there's, I think that'll that's enough to take us to a CBC.
Yes, I think we'll solve it one step at a time, where as soon as we get, like, inbound customers, so like we will prioritize based on revenue and the back ends that get us the most distribution. We think that the way we become ubiquitous in this space is through bottoms up adoption. So we start by providing like an Android SDK or an iOS SDK. This unlocks the use of our models for mobile applications, then we can maybe, like, for example, one of the robotics companies we're building, like, they're not using, I think they're using Intel GPUs or some like, other kind of GPUs. So like, of course, that because if that converts to a commercial contract, then we will prioritize that for now, because I think our core competency is building the model. A lot of the implementation is like, more often than not, undertaken by the user itself, especially in hardware companies. It's mostly for app developers where there's a very clear distinction between low level optimizations and like app generation. So they have like, for example, like a mobile app developer might have expertise in building React Native apps. He may not know like, how to optimize it, to use the optimizer model, to use the neural engine, or optimize device to use like npus, for example, that like, so that is where we should be doing most of the heavy lifting. So I think that's how we will prioritize just mostly on what kind of inbound we get, what kind of commercial contract we see for what back ends. And I think over like, the vision we have is a multi year, like a decade long vision. So that is plenty of time to accomplish this. So for next year, it seems like we will be doubling down on voice and maybe BLMs. And I think there is enough of a market today to even voice AI is exploring. So there's, I think that'll that's enough to take us to a CBC.
S Speaker 122:15Absolutely Rohan makes sense. I would love to have you speak to one of my partners as well. He has spent quite a lot of time on gi works internally with Qualcomm. Could have a lot of valuable solutions for you guys as you build towards this. Maybe I can do that as a follow up call with Tushar, who leads our venture investment in us, just just on voice Rohan, today, what's what's the roadmap? What are the next milestones that you're planning to do this year?
Absolutely Rohan makes sense. I would love to have you speak to one of my partners as well. He has spent quite a lot of time on gi works internally with Qualcomm. Could have a lot of valuable solutions for you guys as you build towards this. Maybe I can do that as a follow up call with Tushar, who leads our venture investment in us, just just on voice Rohan, today, what's what's the roadmap? What are the next milestones that you're planning to do this year?
Absolutely Rohan makes sense. I would love to have you speak to one of my partners as well. He has spent quite a lot of time on gi works internally with Qualcomm. Could have a lot of valuable solutions for you guys as you build towards this. Maybe I can do that as a follow up call with Tushar, who leads our venture investment in us, just just on voice Rohan, today, what's what's the roadmap? What are the next milestones that you're planning to do this year?
Absolutely Rohan makes sense. I would love to have you speak to one of my partners as well. He has spent quite a lot of time on gi works internally with Qualcomm. Could have a lot of valuable solutions for you guys as you build towards this. Maybe I can do that as a follow up call with Tushar, who leads our venture investment in us, just just on voice Rohan, today, what's what's the roadmap? What are the next milestones that you're planning to do this year?
24:05applications and server use cases.
applications and server use cases.
applications and server use cases.
applications and server use cases.
S Speaker 224:40Yeah, so we just started raising last week. We are raising 5 million at the moment, and we've already hand shaken on 4.4 so far. We closed the round last Thursday itself, but we want to keep the round open for strategics, which is why we thought this would be an interesting conversation. So yeah, I think if there are some partnerships that could be
Yeah, so we just started raising last week. We are raising 5 million at the moment, and we've already hand shaken on 4.4 so far. We closed the round last Thursday itself, but we want to keep the round open for strategics, which is why we thought this would be an interesting conversation. So yeah, I think if there are some partnerships that could be
Yeah, so we just started raising last week. We are raising 5 million at the moment, and we've already hand shaken on 4.4 so far. We closed the round last Thursday itself, but we want to keep the round open for strategics, which is why we thought this would be an interesting conversation. So yeah, I think if there are some partnerships that could be
Yeah, so we just started raising last week. We are raising 5 million at the moment, and we've already hand shaken on 4.4 so far. We closed the round last Thursday itself, but we want to keep the round open for strategics, which is why we thought this would be an interesting conversation. So yeah, I think if there are some partnerships that could be
25:04initiated, that would be very exciting for us too,
initiated, that would be very exciting for us too,
initiated, that would be very exciting for us too,
initiated, that would be very exciting for us too,
S Speaker 125:06absolutely. Rohan, I can get back to you early. The problem from our end would be our smallest check size would be 2 million. We wouldn't want to go below that. So is that something that you can accommodate for the current round, or do you think that may be stretching it too far,
absolutely. Rohan, I can get back to you early. The problem from our end would be our smallest check size would be 2 million. We wouldn't want to go below that. So is that something that you can accommodate for the current round, or do you think that may be stretching it too far,
absolutely. Rohan, I can get back to you early. The problem from our end would be our smallest check size would be 2 million. We wouldn't want to go below that. So is that something that you can accommodate for the current round, or do you think that may be stretching it too far,
absolutely. Rohan, I can get back to you early. The problem from our end would be our smallest check size would be 2 million. We wouldn't want to go below that. So is that something that you can accommodate for the current round, or do you think that may be stretching it too far,
S Speaker 225:21yeah, I think if there, if it makes sense, if we could totally accommodate that, yeah, I think yeah, if it makes sense, yeah, totally we could
yeah, I think if there, if it makes sense, if we could totally accommodate that, yeah, I think yeah, if it makes sense, yeah, totally we could
yeah, I think if there, if it makes sense, if we could totally accommodate that, yeah, I think yeah, if it makes sense, yeah, totally we could
yeah, I think if there, if it makes sense, if we could totally accommodate that, yeah, I think yeah, if it makes sense, yeah, totally we could
S Speaker 125:31do that. Understood. Let me have a chat with the team. I would like we can, in parallel, initiate some of these partnership discussions, irrespective of where the fundraising discussion go for the current round, we can always come back for the next round. Series A is preferably our sweet spot as well, with a check size of five to seven, something around that. But we've started doing seed more recently, especially for companies, which, because the seed rounds itself, have expanded quite a lot, but let me, let me start the partnership discussions immediately. Have a chart with the team. I'd like to have one more to chart like I mentioned, speak with you on that. And then in terms of timeline, you mentioned you could have closed last week for us to come to a decision. Rohan, it would typically take four to five weeks. It's kind of a slow process given via corporate fund, and there's some more gas to move internally. So initially where we are, I would quickly get back to you, and if it makes sense, we can have a discussion on, how do we see this moving from there?
do that. Understood. Let me have a chat with the team. I would like we can, in parallel, initiate some of these partnership discussions, irrespective of where the fundraising discussion go for the current round, we can always come back for the next round. Series A is preferably our sweet spot as well, with a check size of five to seven, something around that. But we've started doing seed more recently, especially for companies, which, because the seed rounds itself, have expanded quite a lot, but let me, let me start the partnership discussions immediately. Have a chart with the team. I'd like to have one more to chart like I mentioned, speak with you on that. And then in terms of timeline, you mentioned you could have closed last week for us to come to a decision. Rohan, it would typically take four to five weeks. It's kind of a slow process given via corporate fund, and there's some more gas to move internally. So initially where we are, I would quickly get back to you, and if it makes sense, we can have a discussion on, how do we see this moving from there?
do that. Understood. Let me have a chat with the team. I would like we can, in parallel, initiate some of these partnership discussions, irrespective of where the fundraising discussion go for the current round, we can always come back for the next round. Series A is preferably our sweet spot as well, with a check size of five to seven, something around that. But we've started doing seed more recently, especially for companies, which, because the seed rounds itself, have expanded quite a lot, but let me, let me start the partnership discussions immediately. Have a chart with the team. I'd like to have one more to chart like I mentioned, speak with you on that. And then in terms of timeline, you mentioned you could have closed last week for us to come to a decision. Rohan, it would typically take four to five weeks. It's kind of a slow process given via corporate fund, and there's some more gas to move internally. So initially where we are, I would quickly get back to you, and if it makes sense, we can have a discussion on, how do we see this moving from there?
do that. Understood. Let me have a chat with the team. I would like we can, in parallel, initiate some of these partnership discussions, irrespective of where the fundraising discussion go for the current round, we can always come back for the next round. Series A is preferably our sweet spot as well, with a check size of five to seven, something around that. But we've started doing seed more recently, especially for companies, which, because the seed rounds itself, have expanded quite a lot, but let me, let me start the partnership discussions immediately. Have a chart with the team. I'd like to have one more to chart like I mentioned, speak with you on that. And then in terms of timeline, you mentioned you could have closed last week for us to come to a decision. Rohan, it would typically take four to five weeks. It's kind of a slow process given via corporate fund, and there's some more gas to move internally. So initially where we are, I would quickly get back to you, and if it makes sense, we can have a discussion on, how do we see this moving from there?
S Speaker 126:55Absolutely super interesting. Man. Thanks a lot for this. I will get back to you quickly on that. I don't want to hold up the entire discussion. I'm sure you have amazing momentum right now, and great progress, great progress in the small period of time that you've pivoted from.
Absolutely super interesting. Man. Thanks a lot for this. I will get back to you quickly on that. I don't want to hold up the entire discussion. I'm sure you have amazing momentum right now, and great progress, great progress in the small period of time that you've pivoted from.
Absolutely super interesting. Man. Thanks a lot for this. I will get back to you quickly on that. I don't want to hold up the entire discussion. I'm sure you have amazing momentum right now, and great progress, great progress in the small period of time that you've pivoted from.
Absolutely super interesting. Man. Thanks a lot for this. I will get back to you quickly on that. I don't want to hold up the entire discussion. I'm sure you have amazing momentum right now, and great progress, great progress in the small period of time that you've pivoted from.