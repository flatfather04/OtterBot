Meeting: MoonValley Pitch
Tue, Apr 22
1:34 PM
46 min
Priyesh P
Overview of Moon Valley's Focus and Unique Val
URL: https://otter.ai/u/Gx3OgH7CrzZSkPC-jVJMMiImCy0
Downloaded: 2025-12-22T11:40:37.351660
Method: text_extraction
============================================================

0:00So yeah, thank you.
S Speaker 10:02Over to you, we'll have Quinn, who runs poker matches also join. In the meantime, awesome. Sounds great.
Over to you, we'll have Quinn, who runs poker matches also join. In the meantime, awesome. Sounds great.
Over to you, we'll have Quinn, who runs poker matches also join. In the meantime, awesome. Sounds great.
Over to you, we'll have Quinn, who runs poker matches also join. In the meantime, awesome. Sounds great.
S Speaker 20:13Well, Richard Priyesh and Albert, really great to meet you guys, and thanks for taking the time. I can give kind of like 1000 foot view of Moon Valley, and then you can kind of jump into wherever is interesting. You know, at a high, excuse me, at a high level, our focus is production grade generative video models. So we're a lab primarily focused on what are the models, and also the accompanying software and technology that's necessary to kind of power the next generation of kind of the highest orders of media and entertainment? What are the models that studios will use to make movies, to make them, the next big Oscar winning movie? What are the models that brands will use to make Super Bowl commercials? And that's essentially our entire focus. And so, you know, I think that that the reason that I call that out is that a lot of the companies today that are operating in generative video, they're much they are much more consumer focused than they are kind of enterprise facing. So they're, you know, building social applications, things that allow you to make like Tik Tok videos and things like that, which, while really interesting, I think that, like the much larger enterprise opportunities are where you haven't really seen as much as much kind of focus. And that was kind of the MO of us coming in enterprise. Great video models, and also image models, they require levels of kind of features and and capabilities that you don't really have or need in there, and more consumer ones. So the first big piece is you need controllability. That's probably the most important thing, right? Is you need to be able to build models that you can enable eventually, you know, effectively, the same level of control and granular control as you can have over an actual film set. You need to be able to have over, over the outputs of these models. And they can't just be like a coin toss in what you're generating. And so building models really around that controllability is is incredibly important. The second piece is, there's also, at the enterprise level, just so many kind of workflow, logistical and legal requirements that need to be met in order for these tools to actually be be used. So being able to, you know, just figuring out how you deploy these massive models into these studio environments, for instance, is is kind of a work workload in and of itself. But there's also other factors involved. So for instance, our model, our foundational video model that we released a month ago is the first model that's ever been trained entirely on clean data. So we've licensed all of the images and videos that this model is trained on. And this ends up being critically important at the enterprise level, because it's the first model that's legally indemnified from copyright issues, and that's one of the biggest challenges to the application of these models, certainly with large brands, like we're chatting with Walmart and Nike and similar ones that have historically not been able to use these models for that reason. And certainly in the entertainment industry where, you know, we're working with all of the top studios now, really, I would say part five of the top six or seven. And you know, though, they've been clamoring to use this tech for a very long time. The legal teams have never allowed it. And so this is kind of the first real path to, you know, enterprise feasibility with these with these models. So that's essentially us, in a nutshell. We've assembled really the strongest research team in the world, in the space. My co founders, matish and Mick pioneered video research at DeepMind, and they started the VO project, if you're familiar with DeepMind, which is their video generation model that is state of the art today. And prior to that, matesh And also, he essentially built the world's first VLM, which is kind of the foundation of a lot of the multimodal and certainly understanding that you see in models today. And so since they've joined, we've recruited a team of over 30 just some of the most senior and most well known researchers in the space from DeepMind and OpenAI and Nvidia and similar places. And so our focus for the last just under a year has been building our foundational model, the Mary model, which we officially launched about a month ago. And it's the biggest video foundational model that's ever been built. It's the first model with deep controllability on camera and motion and these different things, as well as being the first clean data model. And so now are we're basically going into market. So we're essentially approaching a kind of going to market across a few different sort of verticals, but largely we're bringing our model into movie studios, and we're building bringing our model directly to brands as well as kind of intermediate agencies. So we're working on a partnership with WPP, with Publicis, Omnicon, and a few of those as well. And so, yeah, that's sort of us in in a nutshell. I figure I'll pause there and happy to dive into the areas that are that are kind of interesting for y'all
Well, Richard Priyesh and Albert, really great to meet you guys, and thanks for taking the time. I can give kind of like 1000 foot view of Moon Valley, and then you can kind of jump into wherever is interesting. You know, at a high, excuse me, at a high level, our focus is production grade generative video models. So we're a lab primarily focused on what are the models, and also the accompanying software and technology that's necessary to kind of power the next generation of kind of the highest orders of media and entertainment? What are the models that studios will use to make movies, to make them, the next big Oscar winning movie? What are the models that brands will use to make Super Bowl commercials? And that's essentially our entire focus. And so, you know, I think that that the reason that I call that out is that a lot of the companies today that are operating in generative video, they're much they are much more consumer focused than they are kind of enterprise facing. So they're, you know, building social applications, things that allow you to make like Tik Tok videos and things like that, which, while really interesting, I think that, like the much larger enterprise opportunities are where you haven't really seen as much as much kind of focus. And that was kind of the MO of us coming in enterprise. Great video models, and also image models, they require levels of kind of features and and capabilities that you don't really have or need in there, and more consumer ones. So the first big piece is you need controllability. That's probably the most important thing, right? Is you need to be able to build models that you can enable eventually, you know, effectively, the same level of control and granular control as you can have over an actual film set. You need to be able to have over, over the outputs of these models. And they can't just be like a coin toss in what you're generating. And so building models really around that controllability is is incredibly important. The second piece is, there's also, at the enterprise level, just so many kind of workflow, logistical and legal requirements that need to be met in order for these tools to actually be be used. So being able to, you know, just figuring out how you deploy these massive models into these studio environments, for instance, is is kind of a work workload in and of itself. But there's also other factors involved. So for instance, our model, our foundational video model that we released a month ago is the first model that's ever been trained entirely on clean data. So we've licensed all of the images and videos that this model is trained on. And this ends up being critically important at the enterprise level, because it's the first model that's legally indemnified from copyright issues, and that's one of the biggest challenges to the application of these models, certainly with large brands, like we're chatting with Walmart and Nike and similar ones that have historically not been able to use these models for that reason. And certainly in the entertainment industry where, you know, we're working with all of the top studios now, really, I would say part five of the top six or seven. And you know, though, they've been clamoring to use this tech for a very long time. The legal teams have never allowed it. And so this is kind of the first real path to, you know, enterprise feasibility with these with these models. So that's essentially us, in a nutshell. We've assembled really the strongest research team in the world, in the space. My co founders, matish and Mick pioneered video research at DeepMind, and they started the VO project, if you're familiar with DeepMind, which is their video generation model that is state of the art today. And prior to that, matesh And also, he essentially built the world's first VLM, which is kind of the foundation of a lot of the multimodal and certainly understanding that you see in models today. And so since they've joined, we've recruited a team of over 30 just some of the most senior and most well known researchers in the space from DeepMind and OpenAI and Nvidia and similar places. And so our focus for the last just under a year has been building our foundational model, the Mary model, which we officially launched about a month ago. And it's the biggest video foundational model that's ever been built. It's the first model with deep controllability on camera and motion and these different things, as well as being the first clean data model. And so now are we're basically going into market. So we're essentially approaching a kind of going to market across a few different sort of verticals, but largely we're bringing our model into movie studios, and we're building bringing our model directly to brands as well as kind of intermediate agencies. So we're working on a partnership with WPP, with Publicis, Omnicon, and a few of those as well. And so, yeah, that's sort of us in in a nutshell. I figure I'll pause there and happy to dive into the areas that are that are kind of interesting for y'all
Well, Richard Priyesh and Albert, really great to meet you guys, and thanks for taking the time. I can give kind of like 1000 foot view of Moon Valley, and then you can kind of jump into wherever is interesting. You know, at a high, excuse me, at a high level, our focus is production grade generative video models. So we're a lab primarily focused on what are the models, and also the accompanying software and technology that's necessary to kind of power the next generation of kind of the highest orders of media and entertainment? What are the models that studios will use to make movies, to make them, the next big Oscar winning movie? What are the models that brands will use to make Super Bowl commercials? And that's essentially our entire focus. And so, you know, I think that that the reason that I call that out is that a lot of the companies today that are operating in generative video, they're much they are much more consumer focused than they are kind of enterprise facing. So they're, you know, building social applications, things that allow you to make like Tik Tok videos and things like that, which, while really interesting, I think that, like the much larger enterprise opportunities are where you haven't really seen as much as much kind of focus. And that was kind of the MO of us coming in enterprise. Great video models, and also image models, they require levels of kind of features and and capabilities that you don't really have or need in there, and more consumer ones. So the first big piece is you need controllability. That's probably the most important thing, right? Is you need to be able to build models that you can enable eventually, you know, effectively, the same level of control and granular control as you can have over an actual film set. You need to be able to have over, over the outputs of these models. And they can't just be like a coin toss in what you're generating. And so building models really around that controllability is is incredibly important. The second piece is, there's also, at the enterprise level, just so many kind of workflow, logistical and legal requirements that need to be met in order for these tools to actually be be used. So being able to, you know, just figuring out how you deploy these massive models into these studio environments, for instance, is is kind of a work workload in and of itself. But there's also other factors involved. So for instance, our model, our foundational video model that we released a month ago is the first model that's ever been trained entirely on clean data. So we've licensed all of the images and videos that this model is trained on. And this ends up being critically important at the enterprise level, because it's the first model that's legally indemnified from copyright issues, and that's one of the biggest challenges to the application of these models, certainly with large brands, like we're chatting with Walmart and Nike and similar ones that have historically not been able to use these models for that reason. And certainly in the entertainment industry where, you know, we're working with all of the top studios now, really, I would say part five of the top six or seven. And you know, though, they've been clamoring to use this tech for a very long time. The legal teams have never allowed it. And so this is kind of the first real path to, you know, enterprise feasibility with these with these models. So that's essentially us, in a nutshell. We've assembled really the strongest research team in the world, in the space. My co founders, matish and Mick pioneered video research at DeepMind, and they started the VO project, if you're familiar with DeepMind, which is their video generation model that is state of the art today. And prior to that, matesh And also, he essentially built the world's first VLM, which is kind of the foundation of a lot of the multimodal and certainly understanding that you see in models today. And so since they've joined, we've recruited a team of over 30 just some of the most senior and most well known researchers in the space from DeepMind and OpenAI and Nvidia and similar places. And so our focus for the last just under a year has been building our foundational model, the Mary model, which we officially launched about a month ago. And it's the biggest video foundational model that's ever been built. It's the first model with deep controllability on camera and motion and these different things, as well as being the first clean data model. And so now are we're basically going into market. So we're essentially approaching a kind of going to market across a few different sort of verticals, but largely we're bringing our model into movie studios, and we're building bringing our model directly to brands as well as kind of intermediate agencies. So we're working on a partnership with WPP, with Publicis, Omnicon, and a few of those as well. And so, yeah, that's sort of us in in a nutshell. I figure I'll pause there and happy to dive into the areas that are that are kind of interesting for y'all
Well, Richard Priyesh and Albert, really great to meet you guys, and thanks for taking the time. I can give kind of like 1000 foot view of Moon Valley, and then you can kind of jump into wherever is interesting. You know, at a high, excuse me, at a high level, our focus is production grade generative video models. So we're a lab primarily focused on what are the models, and also the accompanying software and technology that's necessary to kind of power the next generation of kind of the highest orders of media and entertainment? What are the models that studios will use to make movies, to make them, the next big Oscar winning movie? What are the models that brands will use to make Super Bowl commercials? And that's essentially our entire focus. And so, you know, I think that that the reason that I call that out is that a lot of the companies today that are operating in generative video, they're much they are much more consumer focused than they are kind of enterprise facing. So they're, you know, building social applications, things that allow you to make like Tik Tok videos and things like that, which, while really interesting, I think that, like the much larger enterprise opportunities are where you haven't really seen as much as much kind of focus. And that was kind of the MO of us coming in enterprise. Great video models, and also image models, they require levels of kind of features and and capabilities that you don't really have or need in there, and more consumer ones. So the first big piece is you need controllability. That's probably the most important thing, right? Is you need to be able to build models that you can enable eventually, you know, effectively, the same level of control and granular control as you can have over an actual film set. You need to be able to have over, over the outputs of these models. And they can't just be like a coin toss in what you're generating. And so building models really around that controllability is is incredibly important. The second piece is, there's also, at the enterprise level, just so many kind of workflow, logistical and legal requirements that need to be met in order for these tools to actually be be used. So being able to, you know, just figuring out how you deploy these massive models into these studio environments, for instance, is is kind of a work workload in and of itself. But there's also other factors involved. So for instance, our model, our foundational video model that we released a month ago is the first model that's ever been trained entirely on clean data. So we've licensed all of the images and videos that this model is trained on. And this ends up being critically important at the enterprise level, because it's the first model that's legally indemnified from copyright issues, and that's one of the biggest challenges to the application of these models, certainly with large brands, like we're chatting with Walmart and Nike and similar ones that have historically not been able to use these models for that reason. And certainly in the entertainment industry where, you know, we're working with all of the top studios now, really, I would say part five of the top six or seven. And you know, though, they've been clamoring to use this tech for a very long time. The legal teams have never allowed it. And so this is kind of the first real path to, you know, enterprise feasibility with these with these models. So that's essentially us, in a nutshell. We've assembled really the strongest research team in the world, in the space. My co founders, matish and Mick pioneered video research at DeepMind, and they started the VO project, if you're familiar with DeepMind, which is their video generation model that is state of the art today. And prior to that, matesh And also, he essentially built the world's first VLM, which is kind of the foundation of a lot of the multimodal and certainly understanding that you see in models today. And so since they've joined, we've recruited a team of over 30 just some of the most senior and most well known researchers in the space from DeepMind and OpenAI and Nvidia and similar places. And so our focus for the last just under a year has been building our foundational model, the Mary model, which we officially launched about a month ago. And it's the biggest video foundational model that's ever been built. It's the first model with deep controllability on camera and motion and these different things, as well as being the first clean data model. And so now are we're basically going into market. So we're essentially approaching a kind of going to market across a few different sort of verticals, but largely we're bringing our model into movie studios, and we're building bringing our model directly to brands as well as kind of intermediate agencies. So we're working on a partnership with WPP, with Publicis, Omnicon, and a few of those as well. And so, yeah, that's sort of us in in a nutshell. I figure I'll pause there and happy to dive into the areas that are that are kind of interesting for y'all
S Speaker 15:36name that clean data. Is that the data collection for that is that primarily coming from the studio acquisition that you had made, or where data coming from for training?
name that clean data. Is that the data collection for that is that primarily coming from the studio acquisition that you had made, or where data coming from for training?
name that clean data. Is that the data collection for that is that primarily coming from the studio acquisition that you had made, or where data coming from for training?
name that clean data. Is that the data collection for that is that primarily coming from the studio acquisition that you had made, or where data coming from for training?
S Speaker 17:55Are these relationships with studios exclusive to you? Or
Are these relationships with studios exclusive to you? Or
Are these relationships with studios exclusive to you? Or
Are these relationships with studios exclusive to you? Or
S Speaker 28:00that's a good question, I think. And Andrew, correct me if I'm wrong today, most of them aren't exclusive. Think we have a couple that are, or that we're building towards that we haven't pushed for exclusivity. The option is there, like, if we structure the deals economically in the right way, we've had a number of providers that have approached us to do exclusivity. The reason we haven't yet is because this is still our first model, and so once we have a better sense of which data is the most performance, then our plan is to go in and lock it in for longer terms and, you know, kind of build more exclusivity that way. But today, we haven't focused on that. Okay, yeah. Thanks.
that's a good question, I think. And Andrew, correct me if I'm wrong today, most of them aren't exclusive. Think we have a couple that are, or that we're building towards that we haven't pushed for exclusivity. The option is there, like, if we structure the deals economically in the right way, we've had a number of providers that have approached us to do exclusivity. The reason we haven't yet is because this is still our first model, and so once we have a better sense of which data is the most performance, then our plan is to go in and lock it in for longer terms and, you know, kind of build more exclusivity that way. But today, we haven't focused on that. Okay, yeah. Thanks.
that's a good question, I think. And Andrew, correct me if I'm wrong today, most of them aren't exclusive. Think we have a couple that are, or that we're building towards that we haven't pushed for exclusivity. The option is there, like, if we structure the deals economically in the right way, we've had a number of providers that have approached us to do exclusivity. The reason we haven't yet is because this is still our first model, and so once we have a better sense of which data is the most performance, then our plan is to go in and lock it in for longer terms and, you know, kind of build more exclusivity that way. But today, we haven't focused on that. Okay, yeah. Thanks.
that's a good question, I think. And Andrew, correct me if I'm wrong today, most of them aren't exclusive. Think we have a couple that are, or that we're building towards that we haven't pushed for exclusivity. The option is there, like, if we structure the deals economically in the right way, we've had a number of providers that have approached us to do exclusivity. The reason we haven't yet is because this is still our first model, and so once we have a better sense of which data is the most performance, then our plan is to go in and lock it in for longer terms and, you know, kind of build more exclusivity that way. But today, we haven't focused on that. Okay, yeah. Thanks.
S Speaker 38:49I'd love to, you know, maybe especially from the enterprise use cases, you know, where kind of in the value chain you sit, or even, you know, what are the remaining bottlenecks to fulfilling this, you know, vision of generative video for these studios, right? Because this is, it's a tool, but it's cobbled together with a bunch of other tools, just trying to understand, you know, where are we today, as far as kind of bringing some of these experiences out to the market? Yeah,
I'd love to, you know, maybe especially from the enterprise use cases, you know, where kind of in the value chain you sit, or even, you know, what are the remaining bottlenecks to fulfilling this, you know, vision of generative video for these studios, right? Because this is, it's a tool, but it's cobbled together with a bunch of other tools, just trying to understand, you know, where are we today, as far as kind of bringing some of these experiences out to the market? Yeah,
I'd love to, you know, maybe especially from the enterprise use cases, you know, where kind of in the value chain you sit, or even, you know, what are the remaining bottlenecks to fulfilling this, you know, vision of generative video for these studios, right? Because this is, it's a tool, but it's cobbled together with a bunch of other tools, just trying to understand, you know, where are we today, as far as kind of bringing some of these experiences out to the market? Yeah,
I'd love to, you know, maybe especially from the enterprise use cases, you know, where kind of in the value chain you sit, or even, you know, what are the remaining bottlenecks to fulfilling this, you know, vision of generative video for these studios, right? Because this is, it's a tool, but it's cobbled together with a bunch of other tools, just trying to understand, you know, where are we today, as far as kind of bringing some of these experiences out to the market? Yeah,
S Speaker 29:22that's a great question. So right now, what we're seeing is, so on the on the studio side, the line item that we're most frequently replacing in these discussions is just the VFX line item. So most things that you you would do with VFX today, those are some of the easiest ones to replace with generative workloads. And it's also a very like Logical Switch, just because the work streams are kind of all very similar, are using a lot of the same technology, the areas that are tougher and that are going to build more long term, or, for instance, like more pre production things, or like within, you know, actual like scene building, those are the ones where there's still a lot of, you know, this is the first time anybody has used these tech at these studios. And so the way we're looking at it is for so I would say the majority of our kind of studio POCs that we're doing right now, it's kind of starts at VFX. So the idea is, like, let's get our feet wet and let's train a model. Let's see how this works within VFX, and then if that all works, then we can start to kind of expand and do broader ones. There's a few that are later stage, that are further up the chain. So for instance, with Amazon Prime studios, we're talking about taking an entire show. Basically, they have this house of David production, which is one of their biggest TV shows on the network today. And essentially we're going to train a custom model for that, for that show, and essentially go and kind of hard launch, implement across pre, post and production. And so there's gonna be a lot of effects, but we'll be doing B roll. We'll be doing, you know, kind of early planning and prototyping. There's a whole set of different activities. And you know, our goal with some of these early deals is to figure out exactly that it's like, what are the repeatable pieces that we can then just take and slot into every other production? Our goal is, you know, we want to just become a reliable line item in every production, right? So we can go in and say, you've got a $70 million production, you carve out $5 million for a custom deployment of Mary model, right? And so that's kind of the repeatability we're building towards with brands. It's a lot simpler and it's a lot more usable today already. So what brands are using our models for is for creating ads. So you can make like video ads or image ads with our models. There, we already have a few 100 brands that are in production, some very large ones as well. Good example is Steve Madden. So they're doing there's like, live campaigns and ads that they already have up that are using footage from our models. And so that one is much more kind of production ready and developed. I think what we're generally finding on that side is it's less of a technology question, more of figuring out the right buying pattern. So we're learning about like, what's the sales cycle there? Are you selling to marketing? Are you selling to creative? You know, these are kind of the things that we're trying to figure out right
that's a great question. So right now, what we're seeing is, so on the on the studio side, the line item that we're most frequently replacing in these discussions is just the VFX line item. So most things that you you would do with VFX today, those are some of the easiest ones to replace with generative workloads. And it's also a very like Logical Switch, just because the work streams are kind of all very similar, are using a lot of the same technology, the areas that are tougher and that are going to build more long term, or, for instance, like more pre production things, or like within, you know, actual like scene building, those are the ones where there's still a lot of, you know, this is the first time anybody has used these tech at these studios. And so the way we're looking at it is for so I would say the majority of our kind of studio POCs that we're doing right now, it's kind of starts at VFX. So the idea is, like, let's get our feet wet and let's train a model. Let's see how this works within VFX, and then if that all works, then we can start to kind of expand and do broader ones. There's a few that are later stage, that are further up the chain. So for instance, with Amazon Prime studios, we're talking about taking an entire show. Basically, they have this house of David production, which is one of their biggest TV shows on the network today. And essentially we're going to train a custom model for that, for that show, and essentially go and kind of hard launch, implement across pre, post and production. And so there's gonna be a lot of effects, but we'll be doing B roll. We'll be doing, you know, kind of early planning and prototyping. There's a whole set of different activities. And you know, our goal with some of these early deals is to figure out exactly that it's like, what are the repeatable pieces that we can then just take and slot into every other production? Our goal is, you know, we want to just become a reliable line item in every production, right? So we can go in and say, you've got a $70 million production, you carve out $5 million for a custom deployment of Mary model, right? And so that's kind of the repeatability we're building towards with brands. It's a lot simpler and it's a lot more usable today already. So what brands are using our models for is for creating ads. So you can make like video ads or image ads with our models. There, we already have a few 100 brands that are in production, some very large ones as well. Good example is Steve Madden. So they're doing there's like, live campaigns and ads that they already have up that are using footage from our models. And so that one is much more kind of production ready and developed. I think what we're generally finding on that side is it's less of a technology question, more of figuring out the right buying pattern. So we're learning about like, what's the sales cycle there? Are you selling to marketing? Are you selling to creative? You know, these are kind of the things that we're trying to figure out right
that's a great question. So right now, what we're seeing is, so on the on the studio side, the line item that we're most frequently replacing in these discussions is just the VFX line item. So most things that you you would do with VFX today, those are some of the easiest ones to replace with generative workloads. And it's also a very like Logical Switch, just because the work streams are kind of all very similar, are using a lot of the same technology, the areas that are tougher and that are going to build more long term, or, for instance, like more pre production things, or like within, you know, actual like scene building, those are the ones where there's still a lot of, you know, this is the first time anybody has used these tech at these studios. And so the way we're looking at it is for so I would say the majority of our kind of studio POCs that we're doing right now, it's kind of starts at VFX. So the idea is, like, let's get our feet wet and let's train a model. Let's see how this works within VFX, and then if that all works, then we can start to kind of expand and do broader ones. There's a few that are later stage, that are further up the chain. So for instance, with Amazon Prime studios, we're talking about taking an entire show. Basically, they have this house of David production, which is one of their biggest TV shows on the network today. And essentially we're going to train a custom model for that, for that show, and essentially go and kind of hard launch, implement across pre, post and production. And so there's gonna be a lot of effects, but we'll be doing B roll. We'll be doing, you know, kind of early planning and prototyping. There's a whole set of different activities. And you know, our goal with some of these early deals is to figure out exactly that it's like, what are the repeatable pieces that we can then just take and slot into every other production? Our goal is, you know, we want to just become a reliable line item in every production, right? So we can go in and say, you've got a $70 million production, you carve out $5 million for a custom deployment of Mary model, right? And so that's kind of the repeatability we're building towards with brands. It's a lot simpler and it's a lot more usable today already. So what brands are using our models for is for creating ads. So you can make like video ads or image ads with our models. There, we already have a few 100 brands that are in production, some very large ones as well. Good example is Steve Madden. So they're doing there's like, live campaigns and ads that they already have up that are using footage from our models. And so that one is much more kind of production ready and developed. I think what we're generally finding on that side is it's less of a technology question, more of figuring out the right buying pattern. So we're learning about like, what's the sales cycle there? Are you selling to marketing? Are you selling to creative? You know, these are kind of the things that we're trying to figure out right
that's a great question. So right now, what we're seeing is, so on the on the studio side, the line item that we're most frequently replacing in these discussions is just the VFX line item. So most things that you you would do with VFX today, those are some of the easiest ones to replace with generative workloads. And it's also a very like Logical Switch, just because the work streams are kind of all very similar, are using a lot of the same technology, the areas that are tougher and that are going to build more long term, or, for instance, like more pre production things, or like within, you know, actual like scene building, those are the ones where there's still a lot of, you know, this is the first time anybody has used these tech at these studios. And so the way we're looking at it is for so I would say the majority of our kind of studio POCs that we're doing right now, it's kind of starts at VFX. So the idea is, like, let's get our feet wet and let's train a model. Let's see how this works within VFX, and then if that all works, then we can start to kind of expand and do broader ones. There's a few that are later stage, that are further up the chain. So for instance, with Amazon Prime studios, we're talking about taking an entire show. Basically, they have this house of David production, which is one of their biggest TV shows on the network today. And essentially we're going to train a custom model for that, for that show, and essentially go and kind of hard launch, implement across pre, post and production. And so there's gonna be a lot of effects, but we'll be doing B roll. We'll be doing, you know, kind of early planning and prototyping. There's a whole set of different activities. And you know, our goal with some of these early deals is to figure out exactly that it's like, what are the repeatable pieces that we can then just take and slot into every other production? Our goal is, you know, we want to just become a reliable line item in every production, right? So we can go in and say, you've got a $70 million production, you carve out $5 million for a custom deployment of Mary model, right? And so that's kind of the repeatability we're building towards with brands. It's a lot simpler and it's a lot more usable today already. So what brands are using our models for is for creating ads. So you can make like video ads or image ads with our models. There, we already have a few 100 brands that are in production, some very large ones as well. Good example is Steve Madden. So they're doing there's like, live campaigns and ads that they already have up that are using footage from our models. And so that one is much more kind of production ready and developed. I think what we're generally finding on that side is it's less of a technology question, more of figuring out the right buying pattern. So we're learning about like, what's the sales cycle there? Are you selling to marketing? Are you selling to creative? You know, these are kind of the things that we're trying to figure out right
S Speaker 112:19now. And typically, how do you structure these contracts? Like, what's the business model?
now. And typically, how do you structure these contracts? Like, what's the business model?
now. And typically, how do you structure these contracts? Like, what's the business model?
now. And typically, how do you structure these contracts? Like, what's the business model?
S Speaker 212:24So that's the other area there. You know, lots of rapid experimentation happening. So I can tell you a few different deals that we're kind of working on at the moment to get a to get an idea. So on the commercial side, one of our very late stage deals, which we're hoping to close in the next week or two, is with the brand Alexander Wang, and it's a $250,000 kind of pilot deal, basically. And it's structured essentially around products. So I can't remember the exact product count on the current deal. I think we have it at 200 so essentially, 200 SKUs, basically that Alexander Wang has, they'll be able to take those and generate creatives for those products using our model. And so, you know, they have a shoe or whatever, they can generate an ad with somebody wearing that shoe and walking around right now. And you know this is going to change, but right now, we don't charge for just like creative usage itself. So you can, Alexander Wang will be able to generate essentially as many videos as they want. What they're limited by is the number of products that they can actually create videos for. So that's how that deal looks like. And we have a number of other deals, some that are about to close, some that we've already closed, that are all kind of similar on the commercial side. And the goal is, you know, Alexander Wang, they have north of 5000 total SKUs. They're one of the smaller brands from that regard, because they're much more bespoke. You know, we're talking Walmart less 500,000 SKUs, obviously, but there are 5000 SKUs. So the idea is, you know, if this pilot works well for their 200 SKUs, then by middle to late of next year, we want to start expanding that to the entire range, basically. And then every product that Alexander Wang has, they can do a deployment and get custom creators for that. So, you know, ultimately, we expect that to be a seven, potentially even eight figure account if we're able to deliver obviously. And then on the studio side, still fermentation there the Amazon Prime deal, the house of David, one that I mentioned, is a million dollars. So essentially, they're giving us a million dollars to train a model and essentially work on that show for this season. So it's a couple of episodes, basically. And the goal is to just go in and figure out what that actually means. I believe the budget for each season of House of David is around $15 million and so that's kind of the, you know, the calculus of those early deals that we're seeing. But like I said, the goal is that, you know, after this, once this goes well, we'll then branch out and we'll do several more, you know, kind of productions, and it'll be the same thing, a few million for a custom model for this show, few million for a custom model for this movie, and then start to expand that. Basically, you
So that's the other area there. You know, lots of rapid experimentation happening. So I can tell you a few different deals that we're kind of working on at the moment to get a to get an idea. So on the commercial side, one of our very late stage deals, which we're hoping to close in the next week or two, is with the brand Alexander Wang, and it's a $250,000 kind of pilot deal, basically. And it's structured essentially around products. So I can't remember the exact product count on the current deal. I think we have it at 200 so essentially, 200 SKUs, basically that Alexander Wang has, they'll be able to take those and generate creatives for those products using our model. And so, you know, they have a shoe or whatever, they can generate an ad with somebody wearing that shoe and walking around right now. And you know this is going to change, but right now, we don't charge for just like creative usage itself. So you can, Alexander Wang will be able to generate essentially as many videos as they want. What they're limited by is the number of products that they can actually create videos for. So that's how that deal looks like. And we have a number of other deals, some that are about to close, some that we've already closed, that are all kind of similar on the commercial side. And the goal is, you know, Alexander Wang, they have north of 5000 total SKUs. They're one of the smaller brands from that regard, because they're much more bespoke. You know, we're talking Walmart less 500,000 SKUs, obviously, but there are 5000 SKUs. So the idea is, you know, if this pilot works well for their 200 SKUs, then by middle to late of next year, we want to start expanding that to the entire range, basically. And then every product that Alexander Wang has, they can do a deployment and get custom creators for that. So, you know, ultimately, we expect that to be a seven, potentially even eight figure account if we're able to deliver obviously. And then on the studio side, still fermentation there the Amazon Prime deal, the house of David, one that I mentioned, is a million dollars. So essentially, they're giving us a million dollars to train a model and essentially work on that show for this season. So it's a couple of episodes, basically. And the goal is to just go in and figure out what that actually means. I believe the budget for each season of House of David is around $15 million and so that's kind of the, you know, the calculus of those early deals that we're seeing. But like I said, the goal is that, you know, after this, once this goes well, we'll then branch out and we'll do several more, you know, kind of productions, and it'll be the same thing, a few million for a custom model for this show, few million for a custom model for this movie, and then start to expand that. Basically, you
So that's the other area there. You know, lots of rapid experimentation happening. So I can tell you a few different deals that we're kind of working on at the moment to get a to get an idea. So on the commercial side, one of our very late stage deals, which we're hoping to close in the next week or two, is with the brand Alexander Wang, and it's a $250,000 kind of pilot deal, basically. And it's structured essentially around products. So I can't remember the exact product count on the current deal. I think we have it at 200 so essentially, 200 SKUs, basically that Alexander Wang has, they'll be able to take those and generate creatives for those products using our model. And so, you know, they have a shoe or whatever, they can generate an ad with somebody wearing that shoe and walking around right now. And you know this is going to change, but right now, we don't charge for just like creative usage itself. So you can, Alexander Wang will be able to generate essentially as many videos as they want. What they're limited by is the number of products that they can actually create videos for. So that's how that deal looks like. And we have a number of other deals, some that are about to close, some that we've already closed, that are all kind of similar on the commercial side. And the goal is, you know, Alexander Wang, they have north of 5000 total SKUs. They're one of the smaller brands from that regard, because they're much more bespoke. You know, we're talking Walmart less 500,000 SKUs, obviously, but there are 5000 SKUs. So the idea is, you know, if this pilot works well for their 200 SKUs, then by middle to late of next year, we want to start expanding that to the entire range, basically. And then every product that Alexander Wang has, they can do a deployment and get custom creators for that. So, you know, ultimately, we expect that to be a seven, potentially even eight figure account if we're able to deliver obviously. And then on the studio side, still fermentation there the Amazon Prime deal, the house of David, one that I mentioned, is a million dollars. So essentially, they're giving us a million dollars to train a model and essentially work on that show for this season. So it's a couple of episodes, basically. And the goal is to just go in and figure out what that actually means. I believe the budget for each season of House of David is around $15 million and so that's kind of the, you know, the calculus of those early deals that we're seeing. But like I said, the goal is that, you know, after this, once this goes well, we'll then branch out and we'll do several more, you know, kind of productions, and it'll be the same thing, a few million for a custom model for this show, few million for a custom model for this movie, and then start to expand that. Basically, you
So that's the other area there. You know, lots of rapid experimentation happening. So I can tell you a few different deals that we're kind of working on at the moment to get a to get an idea. So on the commercial side, one of our very late stage deals, which we're hoping to close in the next week or two, is with the brand Alexander Wang, and it's a $250,000 kind of pilot deal, basically. And it's structured essentially around products. So I can't remember the exact product count on the current deal. I think we have it at 200 so essentially, 200 SKUs, basically that Alexander Wang has, they'll be able to take those and generate creatives for those products using our model. And so, you know, they have a shoe or whatever, they can generate an ad with somebody wearing that shoe and walking around right now. And you know this is going to change, but right now, we don't charge for just like creative usage itself. So you can, Alexander Wang will be able to generate essentially as many videos as they want. What they're limited by is the number of products that they can actually create videos for. So that's how that deal looks like. And we have a number of other deals, some that are about to close, some that we've already closed, that are all kind of similar on the commercial side. And the goal is, you know, Alexander Wang, they have north of 5000 total SKUs. They're one of the smaller brands from that regard, because they're much more bespoke. You know, we're talking Walmart less 500,000 SKUs, obviously, but there are 5000 SKUs. So the idea is, you know, if this pilot works well for their 200 SKUs, then by middle to late of next year, we want to start expanding that to the entire range, basically. And then every product that Alexander Wang has, they can do a deployment and get custom creators for that. So, you know, ultimately, we expect that to be a seven, potentially even eight figure account if we're able to deliver obviously. And then on the studio side, still fermentation there the Amazon Prime deal, the house of David, one that I mentioned, is a million dollars. So essentially, they're giving us a million dollars to train a model and essentially work on that show for this season. So it's a couple of episodes, basically. And the goal is to just go in and figure out what that actually means. I believe the budget for each season of House of David is around $15 million and so that's kind of the, you know, the calculus of those early deals that we're seeing. But like I said, the goal is that, you know, after this, once this goes well, we'll then branch out and we'll do several more, you know, kind of productions, and it'll be the same thing, a few million for a custom model for this show, few million for a custom model for this movie, and then start to expand that. Basically, you
S Speaker 114:40said custom model, which is for each show, you have to do a custom model, like how, why and how, not a model in that
said custom model, which is for each show, you have to do a custom model, like how, why and how, not a model in that
said custom model, which is for each show, you have to do a custom model, like how, why and how, not a model in that
said custom model, which is for each show, you have to do a custom model, like how, why and how, not a model in that
S Speaker 214:46sense, essentially, just fine tuning our model on their material. So with House of David, training their characters, their footage, their scenes, into the model, you know, it takes at most, probably 48 hours generally, to train, you know, to that of like, that much footage, essentially. So it's a very short kind of period, but there is a period but there is a period where we have to say, Okay, give us a week. Give us your materials. We train the model, and then that becomes just a deployment that they they get, they have on prem that's like, this is your model for House of David that's been trained with your footage as well. So it knows your characters and your style and those kind of things. So that's what they're paying for on top of the, you know, just like the baseline model, obviously. So when you approach customers, what was the typical key value position differentiation, given there so many models out there, right? So, yeah, why do they choose value? Yeah, other than your training data for
sense, essentially, just fine tuning our model on their material. So with House of David, training their characters, their footage, their scenes, into the model, you know, it takes at most, probably 48 hours generally, to train, you know, to that of like, that much footage, essentially. So it's a very short kind of period, but there is a period but there is a period where we have to say, Okay, give us a week. Give us your materials. We train the model, and then that becomes just a deployment that they they get, they have on prem that's like, this is your model for House of David that's been trained with your footage as well. So it knows your characters and your style and those kind of things. So that's what they're paying for on top of the, you know, just like the baseline model, obviously. So when you approach customers, what was the typical key value position differentiation, given there so many models out there, right? So, yeah, why do they choose value? Yeah, other than your training data for
sense, essentially, just fine tuning our model on their material. So with House of David, training their characters, their footage, their scenes, into the model, you know, it takes at most, probably 48 hours generally, to train, you know, to that of like, that much footage, essentially. So it's a very short kind of period, but there is a period but there is a period where we have to say, Okay, give us a week. Give us your materials. We train the model, and then that becomes just a deployment that they they get, they have on prem that's like, this is your model for House of David that's been trained with your footage as well. So it knows your characters and your style and those kind of things. So that's what they're paying for on top of the, you know, just like the baseline model, obviously. So when you approach customers, what was the typical key value position differentiation, given there so many models out there, right? So, yeah, why do they choose value? Yeah, other than your training data for
sense, essentially, just fine tuning our model on their material. So with House of David, training their characters, their footage, their scenes, into the model, you know, it takes at most, probably 48 hours generally, to train, you know, to that of like, that much footage, essentially. So it's a very short kind of period, but there is a period but there is a period where we have to say, Okay, give us a week. Give us your materials. We train the model, and then that becomes just a deployment that they they get, they have on prem that's like, this is your model for House of David that's been trained with your footage as well. So it knows your characters and your style and those kind of things. So that's what they're paying for on top of the, you know, just like the baseline model, obviously. So when you approach customers, what was the typical key value position differentiation, given there so many models out there, right? So, yeah, why do they choose value? Yeah, other than your training data for
S Speaker 215:32the first piece I would say is they actually don't have that many options. If you're, you know, if you're a brand or a movie studio here, a lot of the video models today, first of all, they're Chinese video models, and so that. And so that ends up being kind of Bucha for a lot of the students to be able to use them. When you take those away, you essentially have maybe six Max kind of feasible models that you can use a lot of these different research labs and stuff. And so you have that. And then of that six, you have several that are consumer facing, like pico labs, Luma. These are models that are much more for consumer audience. They're not for serious, kind of professional high definition footage. Definition footage. And so really, if you're trying to do you know this kind of and do like production grade footage, your options are essentially three. Today. You have runway, you have defined field model, and you have our model. And so compared to those two, the advantages that we have, one is the clean data, obviously. Second piece is our model. Is the first model that's that can generate a native high definition and that ends up being probably a bigger differentiator, even in the clean model, is that VO and runway, they've only been able to train up to 720 p, and so the way that they do is they'll generate a 720 p, and then they'll use what's called an upscaler to basically make it 1020, 1080p, or X ray forte. And all an upscaler is doing is literally resizing, and so it loses all the details. And so you can't actually use outputs from models like view in one way, on TV or much less in movies on the big screen, is that it doesn't have that visual fidelity our model by default. And in a couple of months, we'll be able to do it in 14 native we as well. And so that ends up being a very significant piece as well, and actually being able to use it, and outside of that just pure kind of quality and motion and fidelity, our model spent our tire and is able to deliver that. But goal over time. So that's not it's not something that we differentiate on today, but the goal over the next couple of the next couple of months is the controllability layers that we're building. Our roots really are. We haven't been able to deploy those yet, though, because the focus to date has just been building our foundational model. But over the next few months, now that we have this model, there's a whole range of very deep and precise control adapters that we're building that that are just going to drive the process in a way that you can't do today. So on those differences, controllability, high risk, action.
the first piece I would say is they actually don't have that many options. If you're, you know, if you're a brand or a movie studio here, a lot of the video models today, first of all, they're Chinese video models, and so that. And so that ends up being kind of Bucha for a lot of the students to be able to use them. When you take those away, you essentially have maybe six Max kind of feasible models that you can use a lot of these different research labs and stuff. And so you have that. And then of that six, you have several that are consumer facing, like pico labs, Luma. These are models that are much more for consumer audience. They're not for serious, kind of professional high definition footage. Definition footage. And so really, if you're trying to do you know this kind of and do like production grade footage, your options are essentially three. Today. You have runway, you have defined field model, and you have our model. And so compared to those two, the advantages that we have, one is the clean data, obviously. Second piece is our model. Is the first model that's that can generate a native high definition and that ends up being probably a bigger differentiator, even in the clean model, is that VO and runway, they've only been able to train up to 720 p, and so the way that they do is they'll generate a 720 p, and then they'll use what's called an upscaler to basically make it 1020, 1080p, or X ray forte. And all an upscaler is doing is literally resizing, and so it loses all the details. And so you can't actually use outputs from models like view in one way, on TV or much less in movies on the big screen, is that it doesn't have that visual fidelity our model by default. And in a couple of months, we'll be able to do it in 14 native we as well. And so that ends up being a very significant piece as well, and actually being able to use it, and outside of that just pure kind of quality and motion and fidelity, our model spent our tire and is able to deliver that. But goal over time. So that's not it's not something that we differentiate on today, but the goal over the next couple of the next couple of months is the controllability layers that we're building. Our roots really are. We haven't been able to deploy those yet, though, because the focus to date has just been building our foundational model. But over the next few months, now that we have this model, there's a whole range of very deep and precise control adapters that we're building that that are just going to drive the process in a way that you can't do today. So on those differences, controllability, high risk, action.
the first piece I would say is they actually don't have that many options. If you're, you know, if you're a brand or a movie studio here, a lot of the video models today, first of all, they're Chinese video models, and so that. And so that ends up being kind of Bucha for a lot of the students to be able to use them. When you take those away, you essentially have maybe six Max kind of feasible models that you can use a lot of these different research labs and stuff. And so you have that. And then of that six, you have several that are consumer facing, like pico labs, Luma. These are models that are much more for consumer audience. They're not for serious, kind of professional high definition footage. Definition footage. And so really, if you're trying to do you know this kind of and do like production grade footage, your options are essentially three. Today. You have runway, you have defined field model, and you have our model. And so compared to those two, the advantages that we have, one is the clean data, obviously. Second piece is our model. Is the first model that's that can generate a native high definition and that ends up being probably a bigger differentiator, even in the clean model, is that VO and runway, they've only been able to train up to 720 p, and so the way that they do is they'll generate a 720 p, and then they'll use what's called an upscaler to basically make it 1020, 1080p, or X ray forte. And all an upscaler is doing is literally resizing, and so it loses all the details. And so you can't actually use outputs from models like view in one way, on TV or much less in movies on the big screen, is that it doesn't have that visual fidelity our model by default. And in a couple of months, we'll be able to do it in 14 native we as well. And so that ends up being a very significant piece as well, and actually being able to use it, and outside of that just pure kind of quality and motion and fidelity, our model spent our tire and is able to deliver that. But goal over time. So that's not it's not something that we differentiate on today, but the goal over the next couple of the next couple of months is the controllability layers that we're building. Our roots really are. We haven't been able to deploy those yet, though, because the focus to date has just been building our foundational model. But over the next few months, now that we have this model, there's a whole range of very deep and precise control adapters that we're building that that are just going to drive the process in a way that you can't do today. So on those differences, controllability, high risk, action.
the first piece I would say is they actually don't have that many options. If you're, you know, if you're a brand or a movie studio here, a lot of the video models today, first of all, they're Chinese video models, and so that. And so that ends up being kind of Bucha for a lot of the students to be able to use them. When you take those away, you essentially have maybe six Max kind of feasible models that you can use a lot of these different research labs and stuff. And so you have that. And then of that six, you have several that are consumer facing, like pico labs, Luma. These are models that are much more for consumer audience. They're not for serious, kind of professional high definition footage. Definition footage. And so really, if you're trying to do you know this kind of and do like production grade footage, your options are essentially three. Today. You have runway, you have defined field model, and you have our model. And so compared to those two, the advantages that we have, one is the clean data, obviously. Second piece is our model. Is the first model that's that can generate a native high definition and that ends up being probably a bigger differentiator, even in the clean model, is that VO and runway, they've only been able to train up to 720 p, and so the way that they do is they'll generate a 720 p, and then they'll use what's called an upscaler to basically make it 1020, 1080p, or X ray forte. And all an upscaler is doing is literally resizing, and so it loses all the details. And so you can't actually use outputs from models like view in one way, on TV or much less in movies on the big screen, is that it doesn't have that visual fidelity our model by default. And in a couple of months, we'll be able to do it in 14 native we as well. And so that ends up being a very significant piece as well, and actually being able to use it, and outside of that just pure kind of quality and motion and fidelity, our model spent our tire and is able to deliver that. But goal over time. So that's not it's not something that we differentiate on today, but the goal over the next couple of the next couple of months is the controllability layers that we're building. Our roots really are. We haven't been able to deploy those yet, though, because the focus to date has just been building our foundational model. But over the next few months, now that we have this model, there's a whole range of very deep and precise control adapters that we're building that that are just going to drive the process in a way that you can't do today. So on those differences, controllability, high risk, action.
S Speaker 417:23What? Why do you think the competitors choose not to do those? Or is there any technical barriers? Yeah,
What? Why do you think the competitors choose not to do those? Or is there any technical barriers? Yeah,
What? Why do you think the competitors choose not to do those? Or is there any technical barriers? Yeah,
What? Why do you think the competitors choose not to do those? Or is there any technical barriers? Yeah,
S Speaker 217:29it's not that they choose not to. It's that they just haven't been able to so far. So that's where just the pedigree of the team, you know, kind of comes in. We know pretty quite well. Sort of the challenges of those companies have like Luma labs, head of infrastructure joined us, runways, Head of Research joined us, bridges. So, you know, video models are an interesting one, because they have a very high ceiling but a very low floor. So it's very easy to build video models, because you can take image models, and there's lots of open source ones available out there, and just turn them like, extend temporal layers and make some video models. And so it's quite easy to build a basic video model. However, to do a really, really really good video model is very challenging, and the only way to do that is you need to have knowledge of scale, knowledge in particular, of LLM scale. So it's only really companies that not only monitor video models, but also that have experience building one of the three or four big llms. And so really it's only the Sora folks at open AI who have now more or less disbanded, or the VO folks at DeepMind that know how to do this, and that's why DeepMind models, up until today, have consistently been far ahead of any of the competitors. Is because, so the short answer is, we're the only ones that actually know how to do this technically, to be able to build a model of our size, so it's a 32 billion parameter model that's never been done before, to be able to train on HD data. We're the first ones that have managed to do that while still being able to actually, you know, make it possible with today's compute and a number of other you know, just to give you kind of a benchmark, even our VAE, which is our auto encoder that that our our training data goes through the industry standard, VAE, most people use open AI is VAE is about 4x compressive, basically. So, like, that's how much compresses the training data to use it. We built our own VAE, even, which our competitors didn't, that's almost 32x compressive. So it's three orders of magnitude more compressive than the best in class model today, so that's ultimately what it will adapt to. It's just we have a deep technical supremacy in our in our anywhere else today. But if you look at some of these benchmarks out there, what are the or the strength and weakness of your model compared to the competitors?
it's not that they choose not to. It's that they just haven't been able to so far. So that's where just the pedigree of the team, you know, kind of comes in. We know pretty quite well. Sort of the challenges of those companies have like Luma labs, head of infrastructure joined us, runways, Head of Research joined us, bridges. So, you know, video models are an interesting one, because they have a very high ceiling but a very low floor. So it's very easy to build video models, because you can take image models, and there's lots of open source ones available out there, and just turn them like, extend temporal layers and make some video models. And so it's quite easy to build a basic video model. However, to do a really, really really good video model is very challenging, and the only way to do that is you need to have knowledge of scale, knowledge in particular, of LLM scale. So it's only really companies that not only monitor video models, but also that have experience building one of the three or four big llms. And so really it's only the Sora folks at open AI who have now more or less disbanded, or the VO folks at DeepMind that know how to do this, and that's why DeepMind models, up until today, have consistently been far ahead of any of the competitors. Is because, so the short answer is, we're the only ones that actually know how to do this technically, to be able to build a model of our size, so it's a 32 billion parameter model that's never been done before, to be able to train on HD data. We're the first ones that have managed to do that while still being able to actually, you know, make it possible with today's compute and a number of other you know, just to give you kind of a benchmark, even our VAE, which is our auto encoder that that our our training data goes through the industry standard, VAE, most people use open AI is VAE is about 4x compressive, basically. So, like, that's how much compresses the training data to use it. We built our own VAE, even, which our competitors didn't, that's almost 32x compressive. So it's three orders of magnitude more compressive than the best in class model today, so that's ultimately what it will adapt to. It's just we have a deep technical supremacy in our in our anywhere else today. But if you look at some of these benchmarks out there, what are the or the strength and weakness of your model compared to the competitors?
it's not that they choose not to. It's that they just haven't been able to so far. So that's where just the pedigree of the team, you know, kind of comes in. We know pretty quite well. Sort of the challenges of those companies have like Luma labs, head of infrastructure joined us, runways, Head of Research joined us, bridges. So, you know, video models are an interesting one, because they have a very high ceiling but a very low floor. So it's very easy to build video models, because you can take image models, and there's lots of open source ones available out there, and just turn them like, extend temporal layers and make some video models. And so it's quite easy to build a basic video model. However, to do a really, really really good video model is very challenging, and the only way to do that is you need to have knowledge of scale, knowledge in particular, of LLM scale. So it's only really companies that not only monitor video models, but also that have experience building one of the three or four big llms. And so really it's only the Sora folks at open AI who have now more or less disbanded, or the VO folks at DeepMind that know how to do this, and that's why DeepMind models, up until today, have consistently been far ahead of any of the competitors. Is because, so the short answer is, we're the only ones that actually know how to do this technically, to be able to build a model of our size, so it's a 32 billion parameter model that's never been done before, to be able to train on HD data. We're the first ones that have managed to do that while still being able to actually, you know, make it possible with today's compute and a number of other you know, just to give you kind of a benchmark, even our VAE, which is our auto encoder that that our our training data goes through the industry standard, VAE, most people use open AI is VAE is about 4x compressive, basically. So, like, that's how much compresses the training data to use it. We built our own VAE, even, which our competitors didn't, that's almost 32x compressive. So it's three orders of magnitude more compressive than the best in class model today, so that's ultimately what it will adapt to. It's just we have a deep technical supremacy in our in our anywhere else today. But if you look at some of these benchmarks out there, what are the or the strength and weakness of your model compared to the competitors?
it's not that they choose not to. It's that they just haven't been able to so far. So that's where just the pedigree of the team, you know, kind of comes in. We know pretty quite well. Sort of the challenges of those companies have like Luma labs, head of infrastructure joined us, runways, Head of Research joined us, bridges. So, you know, video models are an interesting one, because they have a very high ceiling but a very low floor. So it's very easy to build video models, because you can take image models, and there's lots of open source ones available out there, and just turn them like, extend temporal layers and make some video models. And so it's quite easy to build a basic video model. However, to do a really, really really good video model is very challenging, and the only way to do that is you need to have knowledge of scale, knowledge in particular, of LLM scale. So it's only really companies that not only monitor video models, but also that have experience building one of the three or four big llms. And so really it's only the Sora folks at open AI who have now more or less disbanded, or the VO folks at DeepMind that know how to do this, and that's why DeepMind models, up until today, have consistently been far ahead of any of the competitors. Is because, so the short answer is, we're the only ones that actually know how to do this technically, to be able to build a model of our size, so it's a 32 billion parameter model that's never been done before, to be able to train on HD data. We're the first ones that have managed to do that while still being able to actually, you know, make it possible with today's compute and a number of other you know, just to give you kind of a benchmark, even our VAE, which is our auto encoder that that our our training data goes through the industry standard, VAE, most people use open AI is VAE is about 4x compressive, basically. So, like, that's how much compresses the training data to use it. We built our own VAE, even, which our competitors didn't, that's almost 32x compressive. So it's three orders of magnitude more compressive than the best in class model today, so that's ultimately what it will adapt to. It's just we have a deep technical supremacy in our in our anywhere else today. But if you look at some of these benchmarks out there, what are the or the strength and weakness of your model compared to the competitors?
S Speaker 219:16first piece is, I would say that benchmarking is definitely still an issue in this industry, broadly is, I think it doesn't have the robustness. And we're working on setting up some standards on that, and we have there. One of the biggest issues for that is because so much of the judgment of models is subjective, and like what is visually interesting and what's not, but broadly, when you look at kind of benchmarking in general, the areas that are objective, which you can generally measure, are essentially prompt adherence. So that's the first and probably the biggest one is, how well does it is actually able to match to your second one, just being kind of visual fidelity, and some people call it cinematic fidelity, right? And so that's just like, you know, does it look like things that you want it to look like? Does it look is the motion good? Is it dynamic motion? Is it, you know, that kind of thing. And in general, what we've seen to date. Now, I will caveat by saying that our model is still training, so we have a version today that's already there, but there's this 1.1 that we're gonna be launching in, like a real competitive one. However, today, on prompt adherence in any of the internal benchmarks that we've run, our model has been at the top by a fair margin. So our model is able to understand both the depth of prompts, but also much longer prompts in a way that other models can. So you can so you can do like, sequential prompts and have, like, a lot of different things happening one after another, which really, really effectively. And the second piece is just in visual fidelity. Now, visual fidelity is kind of interesting, because on one hand, that's one area that our model still needs to train a little bit more because it hasn't done. For instance, we haven't trained our model on animation. It hasn't seen a lot of footage, so our model doesn't want to do animated like the visuals, if you do anything animated doesn't look as strong as our competitors. However, for our model is much higher than other models because of that HD piece. So the reason that that HD is so important is that our model is learning details that other models don't have. And so I'm sure you've seen the famous problem of the fingers in these generative models always being messed up. You get multiple hands, multiple fingers, and the reason for that is because they're training on lower resolution. So they'll take a model, or they'll take a video down for ADP, and then they'll use their auto encoder to compress it into this tiny, tiny thing, and that's what they're learning on. And so the model is actually not able to pick up all those small details, like the fingers, like the individual things. The model is just learning this big blob for a hand, basically by training in HD, our model is actually seeing all of those little details. And so the long and short of it is, our model is state of the art. Across most benchmarks that you look at, there's areas that it still entirely based on coverage, primarily. So we need to show it more cinematic footage. It's seen a lot of like YouTube, you know, handheld type of footage so far. So it needs to learn more cinematic, more animation, more of these, like more vertical, specific things. And that's the training one that we're doing right now. But in start in terms of, just like the technical capabilities of the model benchmarks at the top, across all of the tests that we've been doing internally, we have a third party that we brought in now, and we're gonna start doing benchmarking over the next couple of weeks as well, so that we have that as something external to
first piece is, I would say that benchmarking is definitely still an issue in this industry, broadly is, I think it doesn't have the robustness. And we're working on setting up some standards on that, and we have there. One of the biggest issues for that is because so much of the judgment of models is subjective, and like what is visually interesting and what's not, but broadly, when you look at kind of benchmarking in general, the areas that are objective, which you can generally measure, are essentially prompt adherence. So that's the first and probably the biggest one is, how well does it is actually able to match to your second one, just being kind of visual fidelity, and some people call it cinematic fidelity, right? And so that's just like, you know, does it look like things that you want it to look like? Does it look is the motion good? Is it dynamic motion? Is it, you know, that kind of thing. And in general, what we've seen to date. Now, I will caveat by saying that our model is still training, so we have a version today that's already there, but there's this 1.1 that we're gonna be launching in, like a real competitive one. However, today, on prompt adherence in any of the internal benchmarks that we've run, our model has been at the top by a fair margin. So our model is able to understand both the depth of prompts, but also much longer prompts in a way that other models can. So you can so you can do like, sequential prompts and have, like, a lot of different things happening one after another, which really, really effectively. And the second piece is just in visual fidelity. Now, visual fidelity is kind of interesting, because on one hand, that's one area that our model still needs to train a little bit more because it hasn't done. For instance, we haven't trained our model on animation. It hasn't seen a lot of footage, so our model doesn't want to do animated like the visuals, if you do anything animated doesn't look as strong as our competitors. However, for our model is much higher than other models because of that HD piece. So the reason that that HD is so important is that our model is learning details that other models don't have. And so I'm sure you've seen the famous problem of the fingers in these generative models always being messed up. You get multiple hands, multiple fingers, and the reason for that is because they're training on lower resolution. So they'll take a model, or they'll take a video down for ADP, and then they'll use their auto encoder to compress it into this tiny, tiny thing, and that's what they're learning on. And so the model is actually not able to pick up all those small details, like the fingers, like the individual things. The model is just learning this big blob for a hand, basically by training in HD, our model is actually seeing all of those little details. And so the long and short of it is, our model is state of the art. Across most benchmarks that you look at, there's areas that it still entirely based on coverage, primarily. So we need to show it more cinematic footage. It's seen a lot of like YouTube, you know, handheld type of footage so far. So it needs to learn more cinematic, more animation, more of these, like more vertical, specific things. And that's the training one that we're doing right now. But in start in terms of, just like the technical capabilities of the model benchmarks at the top, across all of the tests that we've been doing internally, we have a third party that we brought in now, and we're gonna start doing benchmarking over the next couple of weeks as well, so that we have that as something external to
first piece is, I would say that benchmarking is definitely still an issue in this industry, broadly is, I think it doesn't have the robustness. And we're working on setting up some standards on that, and we have there. One of the biggest issues for that is because so much of the judgment of models is subjective, and like what is visually interesting and what's not, but broadly, when you look at kind of benchmarking in general, the areas that are objective, which you can generally measure, are essentially prompt adherence. So that's the first and probably the biggest one is, how well does it is actually able to match to your second one, just being kind of visual fidelity, and some people call it cinematic fidelity, right? And so that's just like, you know, does it look like things that you want it to look like? Does it look is the motion good? Is it dynamic motion? Is it, you know, that kind of thing. And in general, what we've seen to date. Now, I will caveat by saying that our model is still training, so we have a version today that's already there, but there's this 1.1 that we're gonna be launching in, like a real competitive one. However, today, on prompt adherence in any of the internal benchmarks that we've run, our model has been at the top by a fair margin. So our model is able to understand both the depth of prompts, but also much longer prompts in a way that other models can. So you can so you can do like, sequential prompts and have, like, a lot of different things happening one after another, which really, really effectively. And the second piece is just in visual fidelity. Now, visual fidelity is kind of interesting, because on one hand, that's one area that our model still needs to train a little bit more because it hasn't done. For instance, we haven't trained our model on animation. It hasn't seen a lot of footage, so our model doesn't want to do animated like the visuals, if you do anything animated doesn't look as strong as our competitors. However, for our model is much higher than other models because of that HD piece. So the reason that that HD is so important is that our model is learning details that other models don't have. And so I'm sure you've seen the famous problem of the fingers in these generative models always being messed up. You get multiple hands, multiple fingers, and the reason for that is because they're training on lower resolution. So they'll take a model, or they'll take a video down for ADP, and then they'll use their auto encoder to compress it into this tiny, tiny thing, and that's what they're learning on. And so the model is actually not able to pick up all those small details, like the fingers, like the individual things. The model is just learning this big blob for a hand, basically by training in HD, our model is actually seeing all of those little details. And so the long and short of it is, our model is state of the art. Across most benchmarks that you look at, there's areas that it still entirely based on coverage, primarily. So we need to show it more cinematic footage. It's seen a lot of like YouTube, you know, handheld type of footage so far. So it needs to learn more cinematic, more animation, more of these, like more vertical, specific things. And that's the training one that we're doing right now. But in start in terms of, just like the technical capabilities of the model benchmarks at the top, across all of the tests that we've been doing internally, we have a third party that we brought in now, and we're gonna start doing benchmarking over the next couple of weeks as well, so that we have that as something external to
first piece is, I would say that benchmarking is definitely still an issue in this industry, broadly is, I think it doesn't have the robustness. And we're working on setting up some standards on that, and we have there. One of the biggest issues for that is because so much of the judgment of models is subjective, and like what is visually interesting and what's not, but broadly, when you look at kind of benchmarking in general, the areas that are objective, which you can generally measure, are essentially prompt adherence. So that's the first and probably the biggest one is, how well does it is actually able to match to your second one, just being kind of visual fidelity, and some people call it cinematic fidelity, right? And so that's just like, you know, does it look like things that you want it to look like? Does it look is the motion good? Is it dynamic motion? Is it, you know, that kind of thing. And in general, what we've seen to date. Now, I will caveat by saying that our model is still training, so we have a version today that's already there, but there's this 1.1 that we're gonna be launching in, like a real competitive one. However, today, on prompt adherence in any of the internal benchmarks that we've run, our model has been at the top by a fair margin. So our model is able to understand both the depth of prompts, but also much longer prompts in a way that other models can. So you can so you can do like, sequential prompts and have, like, a lot of different things happening one after another, which really, really effectively. And the second piece is just in visual fidelity. Now, visual fidelity is kind of interesting, because on one hand, that's one area that our model still needs to train a little bit more because it hasn't done. For instance, we haven't trained our model on animation. It hasn't seen a lot of footage, so our model doesn't want to do animated like the visuals, if you do anything animated doesn't look as strong as our competitors. However, for our model is much higher than other models because of that HD piece. So the reason that that HD is so important is that our model is learning details that other models don't have. And so I'm sure you've seen the famous problem of the fingers in these generative models always being messed up. You get multiple hands, multiple fingers, and the reason for that is because they're training on lower resolution. So they'll take a model, or they'll take a video down for ADP, and then they'll use their auto encoder to compress it into this tiny, tiny thing, and that's what they're learning on. And so the model is actually not able to pick up all those small details, like the fingers, like the individual things. The model is just learning this big blob for a hand, basically by training in HD, our model is actually seeing all of those little details. And so the long and short of it is, our model is state of the art. Across most benchmarks that you look at, there's areas that it still entirely based on coverage, primarily. So we need to show it more cinematic footage. It's seen a lot of like YouTube, you know, handheld type of footage so far. So it needs to learn more cinematic, more animation, more of these, like more vertical, specific things. And that's the training one that we're doing right now. But in start in terms of, just like the technical capabilities of the model benchmarks at the top, across all of the tests that we've been doing internally, we have a third party that we brought in now, and we're gonna start doing benchmarking over the next couple of weeks as well, so that we have that as something external to
S Speaker 521:54us as well, but from our internal tests, thanks. I'm sorry. I'm joined a few minutes late. You talked about the model you have, which is this 32 billion parameter model right for the use cases for studios versus brands, the same model being applied was a different model for ad generation versus like, intuitively, I would think for making a movie would be more complex, especially versus making somebody wearing shoes or carrying bag other things. But I may be wrong.
us as well, but from our internal tests, thanks. I'm sorry. I'm joined a few minutes late. You talked about the model you have, which is this 32 billion parameter model right for the use cases for studios versus brands, the same model being applied was a different model for ad generation versus like, intuitively, I would think for making a movie would be more complex, especially versus making somebody wearing shoes or carrying bag other things. But I may be wrong.
us as well, but from our internal tests, thanks. I'm sorry. I'm joined a few minutes late. You talked about the model you have, which is this 32 billion parameter model right for the use cases for studios versus brands, the same model being applied was a different model for ad generation versus like, intuitively, I would think for making a movie would be more complex, especially versus making somebody wearing shoes or carrying bag other things. But I may be wrong.
us as well, but from our internal tests, thanks. I'm sorry. I'm joined a few minutes late. You talked about the model you have, which is this 32 billion parameter model right for the use cases for studios versus brands, the same model being applied was a different model for ad generation versus like, intuitively, I would think for making a movie would be more complex, especially versus making somebody wearing shoes or carrying bag other things. But I may be wrong.
S Speaker 222:18You are You are right. The complexity is definitely, you know, orders of magnitude higher. Today we're using the same model, definitely in the ads case, it's a little bit of kind of, you know, using a bazooka shoe to fly sort of thing, like you could, you could get away with a much smaller model if you were just focused on on ads. And in fact, we have a 7 billion parameter model, which we have used fairly successfully to do a lot of the same things that we're doing. So the truth of it is, you can, you can't get away with much more model in general, though, the delineation between, let's say, films, ads, another big one is music videos. So that's we're working on a partnership with Sony Music. That's another big category of interest. The specific needs for those are, generally, we're finding are better met with adapters. So you have this kind of core based model, and then you train these specialized adapters for these specific things that you need. So the ads example is a good one. Is one of the biggest challenges. And the reason that ads, you can't use other models today to do ads, is they don't know how to do product preservation. So if you have, you know, this water bottle, and you want to create an ad of this water bottle, you have no way of doing that with today's models. If you use any of our competitors models, you can't actually create an ad that or a video that features this exact water bottle with the right, you know, features and things like that. And so we've been building an adapter for exactly that, for product preservation that's been trained on, like this kind of use case, which is like, given a product, it learns how to preserve the logo. It learns how to be these very specific things. And I think that ultimately those, like vertical adapters are also what set us apart from, especially like the bigger research works like DeepMind is not going to be making a special, specific adapter just for, you know, these specific different, different places, and that enables us to
You are You are right. The complexity is definitely, you know, orders of magnitude higher. Today we're using the same model, definitely in the ads case, it's a little bit of kind of, you know, using a bazooka shoe to fly sort of thing, like you could, you could get away with a much smaller model if you were just focused on on ads. And in fact, we have a 7 billion parameter model, which we have used fairly successfully to do a lot of the same things that we're doing. So the truth of it is, you can, you can't get away with much more model in general, though, the delineation between, let's say, films, ads, another big one is music videos. So that's we're working on a partnership with Sony Music. That's another big category of interest. The specific needs for those are, generally, we're finding are better met with adapters. So you have this kind of core based model, and then you train these specialized adapters for these specific things that you need. So the ads example is a good one. Is one of the biggest challenges. And the reason that ads, you can't use other models today to do ads, is they don't know how to do product preservation. So if you have, you know, this water bottle, and you want to create an ad of this water bottle, you have no way of doing that with today's models. If you use any of our competitors models, you can't actually create an ad that or a video that features this exact water bottle with the right, you know, features and things like that. And so we've been building an adapter for exactly that, for product preservation that's been trained on, like this kind of use case, which is like, given a product, it learns how to preserve the logo. It learns how to be these very specific things. And I think that ultimately those, like vertical adapters are also what set us apart from, especially like the bigger research works like DeepMind is not going to be making a special, specific adapter just for, you know, these specific different, different places, and that enables us to
You are You are right. The complexity is definitely, you know, orders of magnitude higher. Today we're using the same model, definitely in the ads case, it's a little bit of kind of, you know, using a bazooka shoe to fly sort of thing, like you could, you could get away with a much smaller model if you were just focused on on ads. And in fact, we have a 7 billion parameter model, which we have used fairly successfully to do a lot of the same things that we're doing. So the truth of it is, you can, you can't get away with much more model in general, though, the delineation between, let's say, films, ads, another big one is music videos. So that's we're working on a partnership with Sony Music. That's another big category of interest. The specific needs for those are, generally, we're finding are better met with adapters. So you have this kind of core based model, and then you train these specialized adapters for these specific things that you need. So the ads example is a good one. Is one of the biggest challenges. And the reason that ads, you can't use other models today to do ads, is they don't know how to do product preservation. So if you have, you know, this water bottle, and you want to create an ad of this water bottle, you have no way of doing that with today's models. If you use any of our competitors models, you can't actually create an ad that or a video that features this exact water bottle with the right, you know, features and things like that. And so we've been building an adapter for exactly that, for product preservation that's been trained on, like this kind of use case, which is like, given a product, it learns how to preserve the logo. It learns how to be these very specific things. And I think that ultimately those, like vertical adapters are also what set us apart from, especially like the bigger research works like DeepMind is not going to be making a special, specific adapter just for, you know, these specific different, different places, and that enables us to
You are You are right. The complexity is definitely, you know, orders of magnitude higher. Today we're using the same model, definitely in the ads case, it's a little bit of kind of, you know, using a bazooka shoe to fly sort of thing, like you could, you could get away with a much smaller model if you were just focused on on ads. And in fact, we have a 7 billion parameter model, which we have used fairly successfully to do a lot of the same things that we're doing. So the truth of it is, you can, you can't get away with much more model in general, though, the delineation between, let's say, films, ads, another big one is music videos. So that's we're working on a partnership with Sony Music. That's another big category of interest. The specific needs for those are, generally, we're finding are better met with adapters. So you have this kind of core based model, and then you train these specialized adapters for these specific things that you need. So the ads example is a good one. Is one of the biggest challenges. And the reason that ads, you can't use other models today to do ads, is they don't know how to do product preservation. So if you have, you know, this water bottle, and you want to create an ad of this water bottle, you have no way of doing that with today's models. If you use any of our competitors models, you can't actually create an ad that or a video that features this exact water bottle with the right, you know, features and things like that. And so we've been building an adapter for exactly that, for product preservation that's been trained on, like this kind of use case, which is like, given a product, it learns how to preserve the logo. It learns how to be these very specific things. And I think that ultimately those, like vertical adapters are also what set us apart from, especially like the bigger research works like DeepMind is not going to be making a special, specific adapter just for, you know, these specific different, different places, and that enables us to
S Speaker 123:42kind of run appliance. What about any object preservation, you know, like, even for if you want to create humans, then as soon as it pans out and comes back in altogether, for sure, so eventually you'll be
kind of run appliance. What about any object preservation, you know, like, even for if you want to create humans, then as soon as it pans out and comes back in altogether, for sure, so eventually you'll be
kind of run appliance. What about any object preservation, you know, like, even for if you want to create humans, then as soon as it pans out and comes back in altogether, for sure, so eventually you'll be
kind of run appliance. What about any object preservation, you know, like, even for if you want to create humans, then as soon as it pans out and comes back in altogether, for sure, so eventually you'll be
S Speaker 223:51able to models. That's a very interesting point you bring up because that's a good example, where eventually the model will get big enough that you no longer need the adapters, and it, by itself, will be able to condition any to any basically, today that's still a challenge. So you can't, like condition on anything. The way that we approach it is figuring out what are the categories that we want to actually focus on. So there's three areas. There's three conditioning adapters that we're working on. One is for objects broadly. So traditionally, that's products. We can also use it like we've done it with, like a sword, right? Like things that you would use in film. You can also use this adapter for the second one is people. So to your point, character consistency has been one of the biggest problems with existing models. So being able to have the same characters, and as part of the software that we're building outside with the companion software model, one of the things that you have is a character creator, so you can create or upload characters and then bring them into scenes, basically, and reference them as you need to, and the model is going to do the work of keeping them consistent. And then the third one is environmental consistency. So essentially, being able to have sets right, if I want to have multiple scenes, but they're taking place in the same place. I need to be in continuity, and I need to be the kind of the same environment. So we have where I said, Yeah, we're trying to be three different adapters for each of those use cases that have different types of data that obviously have to go into them with people. It's much more about just finding variety. So I have one fakes that show up in all these different scenes, and then with environmental consistency, that's a very different problem. It's like I need to be able to train on very long scenes that take place in the same area. So there's, like, very specialized data sets that we've kind of curated for each of these. All three of those are basically we're building in, I would say the product conditioning is already being deployed and being used, you know, professionally. That's the furthest along the people. Conditioning is going to be ready when we launch in GA in June. The environmental one I'm not sure about. So that one we're hoping we'll be able to do by the time we go into GA, but we just don't know yet. It's still relatively early in
able to models. That's a very interesting point you bring up because that's a good example, where eventually the model will get big enough that you no longer need the adapters, and it, by itself, will be able to condition any to any basically, today that's still a challenge. So you can't, like condition on anything. The way that we approach it is figuring out what are the categories that we want to actually focus on. So there's three areas. There's three conditioning adapters that we're working on. One is for objects broadly. So traditionally, that's products. We can also use it like we've done it with, like a sword, right? Like things that you would use in film. You can also use this adapter for the second one is people. So to your point, character consistency has been one of the biggest problems with existing models. So being able to have the same characters, and as part of the software that we're building outside with the companion software model, one of the things that you have is a character creator, so you can create or upload characters and then bring them into scenes, basically, and reference them as you need to, and the model is going to do the work of keeping them consistent. And then the third one is environmental consistency. So essentially, being able to have sets right, if I want to have multiple scenes, but they're taking place in the same place. I need to be in continuity, and I need to be the kind of the same environment. So we have where I said, Yeah, we're trying to be three different adapters for each of those use cases that have different types of data that obviously have to go into them with people. It's much more about just finding variety. So I have one fakes that show up in all these different scenes, and then with environmental consistency, that's a very different problem. It's like I need to be able to train on very long scenes that take place in the same area. So there's, like, very specialized data sets that we've kind of curated for each of these. All three of those are basically we're building in, I would say the product conditioning is already being deployed and being used, you know, professionally. That's the furthest along the people. Conditioning is going to be ready when we launch in GA in June. The environmental one I'm not sure about. So that one we're hoping we'll be able to do by the time we go into GA, but we just don't know yet. It's still relatively early in
able to models. That's a very interesting point you bring up because that's a good example, where eventually the model will get big enough that you no longer need the adapters, and it, by itself, will be able to condition any to any basically, today that's still a challenge. So you can't, like condition on anything. The way that we approach it is figuring out what are the categories that we want to actually focus on. So there's three areas. There's three conditioning adapters that we're working on. One is for objects broadly. So traditionally, that's products. We can also use it like we've done it with, like a sword, right? Like things that you would use in film. You can also use this adapter for the second one is people. So to your point, character consistency has been one of the biggest problems with existing models. So being able to have the same characters, and as part of the software that we're building outside with the companion software model, one of the things that you have is a character creator, so you can create or upload characters and then bring them into scenes, basically, and reference them as you need to, and the model is going to do the work of keeping them consistent. And then the third one is environmental consistency. So essentially, being able to have sets right, if I want to have multiple scenes, but they're taking place in the same place. I need to be in continuity, and I need to be the kind of the same environment. So we have where I said, Yeah, we're trying to be three different adapters for each of those use cases that have different types of data that obviously have to go into them with people. It's much more about just finding variety. So I have one fakes that show up in all these different scenes, and then with environmental consistency, that's a very different problem. It's like I need to be able to train on very long scenes that take place in the same area. So there's, like, very specialized data sets that we've kind of curated for each of these. All three of those are basically we're building in, I would say the product conditioning is already being deployed and being used, you know, professionally. That's the furthest along the people. Conditioning is going to be ready when we launch in GA in June. The environmental one I'm not sure about. So that one we're hoping we'll be able to do by the time we go into GA, but we just don't know yet. It's still relatively early in
able to models. That's a very interesting point you bring up because that's a good example, where eventually the model will get big enough that you no longer need the adapters, and it, by itself, will be able to condition any to any basically, today that's still a challenge. So you can't, like condition on anything. The way that we approach it is figuring out what are the categories that we want to actually focus on. So there's three areas. There's three conditioning adapters that we're working on. One is for objects broadly. So traditionally, that's products. We can also use it like we've done it with, like a sword, right? Like things that you would use in film. You can also use this adapter for the second one is people. So to your point, character consistency has been one of the biggest problems with existing models. So being able to have the same characters, and as part of the software that we're building outside with the companion software model, one of the things that you have is a character creator, so you can create or upload characters and then bring them into scenes, basically, and reference them as you need to, and the model is going to do the work of keeping them consistent. And then the third one is environmental consistency. So essentially, being able to have sets right, if I want to have multiple scenes, but they're taking place in the same place. I need to be in continuity, and I need to be the kind of the same environment. So we have where I said, Yeah, we're trying to be three different adapters for each of those use cases that have different types of data that obviously have to go into them with people. It's much more about just finding variety. So I have one fakes that show up in all these different scenes, and then with environmental consistency, that's a very different problem. It's like I need to be able to train on very long scenes that take place in the same area. So there's, like, very specialized data sets that we've kind of curated for each of these. All three of those are basically we're building in, I would say the product conditioning is already being deployed and being used, you know, professionally. That's the furthest along the people. Conditioning is going to be ready when we launch in GA in June. The environmental one I'm not sure about. So that one we're hoping we'll be able to do by the time we go into GA, but we just don't know yet. It's still relatively early in
S Speaker 525:24terms of business opportunities in your film studios versus other times, obviously, very big, right? What do you see? Maybe low hanging fruit, if you will. It seems to me, intuitively, to do a film or Apple, so that seems a lot harder, versus just doing ad, right? I'm guessing, to generate some ads and stuff, probably maybe one year time. Opinion could be even larger over time. Yeah,
terms of business opportunities in your film studios versus other times, obviously, very big, right? What do you see? Maybe low hanging fruit, if you will. It seems to me, intuitively, to do a film or Apple, so that seems a lot harder, versus just doing ad, right? I'm guessing, to generate some ads and stuff, probably maybe one year time. Opinion could be even larger over time. Yeah,
terms of business opportunities in your film studios versus other times, obviously, very big, right? What do you see? Maybe low hanging fruit, if you will. It seems to me, intuitively, to do a film or Apple, so that seems a lot harder, versus just doing ad, right? I'm guessing, to generate some ads and stuff, probably maybe one year time. Opinion could be even larger over time. Yeah,
terms of business opportunities in your film studios versus other times, obviously, very big, right? What do you see? Maybe low hanging fruit, if you will. It seems to me, intuitively, to do a film or Apple, so that seems a lot harder, versus just doing ad, right? I'm guessing, to generate some ads and stuff, probably maybe one year time. Opinion could be even larger over time. Yeah,
S Speaker 225:45it's, it's interesting. I think it's interesting that ads seem to be overlooked in the space. So, you know, most video companies are focused on, you know, kind of, either consumer or they're focused on film. And that's generally we're seeing. None of them are really, you know, kind of focused on ads, from our perspective, a to your point, in the short term, there's the advantages. The videos are shorter. It's a much simpler problem to solve, much less compute heavy like, and are, you know, the way that we kind of look at it, and I think that the market size is significantly bigger as well, right? For advertising, yeah, like, we're, you know, we're working on this partnership with WPP. They're spending north of 20 million, they said, on, sorry, they're spending north of $200,000 a week right now, just on DeepMind feel model for a lot of their internal advertising, like projects they started this a month ago, right? One Agency, and obviously it's still gonna expand. So I honestly think that brands are gonna be the biggest place, you know, in the short term, as well as the long term. However, I think what's interesting with film and Hollywood in particular, is that they're the taste makers, and so whatever tooling Hollywood and film ends up using, that's the one that ends up being adopted by everybody else. And so the advantage that we have in Hollywood is because of the studio. We have these deep roots in Hollywood. And so that's why, you know, we had Bob Iger coming to our studio last month. We have JJ Abrams coming and visiting us next week, and he was a federal London office last week. Like, because of these relationships, we're able to become Hollywood's choice, essentially. But 100% is much more complex. I think the models are still pretty far away for being able to do, you know, end to end movies, certainly. But we look at it as, like, that's our Brand Builder. Essentially, everybody hears that we're working with all the top Hollywood Studios. They also want to use our models to do their stuff. And that's how we think about it in the short term. Like even music videos, which we weren't really thinking about, has turned out to, you know, chatting with Spotify a couple of times over the last little bit. And Sony Music, as I said, they want to roll out across all of their artists. It's not, it's not a big, long term thing, at least, like, there's a massive opportunity. And it's a much simpler task to solve these models than big, big film or whatever. Now,
it's, it's interesting. I think it's interesting that ads seem to be overlooked in the space. So, you know, most video companies are focused on, you know, kind of, either consumer or they're focused on film. And that's generally we're seeing. None of them are really, you know, kind of focused on ads, from our perspective, a to your point, in the short term, there's the advantages. The videos are shorter. It's a much simpler problem to solve, much less compute heavy like, and are, you know, the way that we kind of look at it, and I think that the market size is significantly bigger as well, right? For advertising, yeah, like, we're, you know, we're working on this partnership with WPP. They're spending north of 20 million, they said, on, sorry, they're spending north of $200,000 a week right now, just on DeepMind feel model for a lot of their internal advertising, like projects they started this a month ago, right? One Agency, and obviously it's still gonna expand. So I honestly think that brands are gonna be the biggest place, you know, in the short term, as well as the long term. However, I think what's interesting with film and Hollywood in particular, is that they're the taste makers, and so whatever tooling Hollywood and film ends up using, that's the one that ends up being adopted by everybody else. And so the advantage that we have in Hollywood is because of the studio. We have these deep roots in Hollywood. And so that's why, you know, we had Bob Iger coming to our studio last month. We have JJ Abrams coming and visiting us next week, and he was a federal London office last week. Like, because of these relationships, we're able to become Hollywood's choice, essentially. But 100% is much more complex. I think the models are still pretty far away for being able to do, you know, end to end movies, certainly. But we look at it as, like, that's our Brand Builder. Essentially, everybody hears that we're working with all the top Hollywood Studios. They also want to use our models to do their stuff. And that's how we think about it in the short term. Like even music videos, which we weren't really thinking about, has turned out to, you know, chatting with Spotify a couple of times over the last little bit. And Sony Music, as I said, they want to roll out across all of their artists. It's not, it's not a big, long term thing, at least, like, there's a massive opportunity. And it's a much simpler task to solve these models than big, big film or whatever. Now,
it's, it's interesting. I think it's interesting that ads seem to be overlooked in the space. So, you know, most video companies are focused on, you know, kind of, either consumer or they're focused on film. And that's generally we're seeing. None of them are really, you know, kind of focused on ads, from our perspective, a to your point, in the short term, there's the advantages. The videos are shorter. It's a much simpler problem to solve, much less compute heavy like, and are, you know, the way that we kind of look at it, and I think that the market size is significantly bigger as well, right? For advertising, yeah, like, we're, you know, we're working on this partnership with WPP. They're spending north of 20 million, they said, on, sorry, they're spending north of $200,000 a week right now, just on DeepMind feel model for a lot of their internal advertising, like projects they started this a month ago, right? One Agency, and obviously it's still gonna expand. So I honestly think that brands are gonna be the biggest place, you know, in the short term, as well as the long term. However, I think what's interesting with film and Hollywood in particular, is that they're the taste makers, and so whatever tooling Hollywood and film ends up using, that's the one that ends up being adopted by everybody else. And so the advantage that we have in Hollywood is because of the studio. We have these deep roots in Hollywood. And so that's why, you know, we had Bob Iger coming to our studio last month. We have JJ Abrams coming and visiting us next week, and he was a federal London office last week. Like, because of these relationships, we're able to become Hollywood's choice, essentially. But 100% is much more complex. I think the models are still pretty far away for being able to do, you know, end to end movies, certainly. But we look at it as, like, that's our Brand Builder. Essentially, everybody hears that we're working with all the top Hollywood Studios. They also want to use our models to do their stuff. And that's how we think about it in the short term. Like even music videos, which we weren't really thinking about, has turned out to, you know, chatting with Spotify a couple of times over the last little bit. And Sony Music, as I said, they want to roll out across all of their artists. It's not, it's not a big, long term thing, at least, like, there's a massive opportunity. And it's a much simpler task to solve these models than big, big film or whatever. Now,
it's, it's interesting. I think it's interesting that ads seem to be overlooked in the space. So, you know, most video companies are focused on, you know, kind of, either consumer or they're focused on film. And that's generally we're seeing. None of them are really, you know, kind of focused on ads, from our perspective, a to your point, in the short term, there's the advantages. The videos are shorter. It's a much simpler problem to solve, much less compute heavy like, and are, you know, the way that we kind of look at it, and I think that the market size is significantly bigger as well, right? For advertising, yeah, like, we're, you know, we're working on this partnership with WPP. They're spending north of 20 million, they said, on, sorry, they're spending north of $200,000 a week right now, just on DeepMind feel model for a lot of their internal advertising, like projects they started this a month ago, right? One Agency, and obviously it's still gonna expand. So I honestly think that brands are gonna be the biggest place, you know, in the short term, as well as the long term. However, I think what's interesting with film and Hollywood in particular, is that they're the taste makers, and so whatever tooling Hollywood and film ends up using, that's the one that ends up being adopted by everybody else. And so the advantage that we have in Hollywood is because of the studio. We have these deep roots in Hollywood. And so that's why, you know, we had Bob Iger coming to our studio last month. We have JJ Abrams coming and visiting us next week, and he was a federal London office last week. Like, because of these relationships, we're able to become Hollywood's choice, essentially. But 100% is much more complex. I think the models are still pretty far away for being able to do, you know, end to end movies, certainly. But we look at it as, like, that's our Brand Builder. Essentially, everybody hears that we're working with all the top Hollywood Studios. They also want to use our models to do their stuff. And that's how we think about it in the short term. Like even music videos, which we weren't really thinking about, has turned out to, you know, chatting with Spotify a couple of times over the last little bit. And Sony Music, as I said, they want to roll out across all of their artists. It's not, it's not a big, long term thing, at least, like, there's a massive opportunity. And it's a much simpler task to solve these models than big, big film or whatever. Now,
S Speaker 527:32in the big film, what I'm just trying to figure out is it is a goal, eventually, to get to the place where every synthetic, like, what is it augmented scenes, like putting those actors in front of, like, some scenes, or, like, what like animation is like a lot of synthetic in some way, yeah, but in the real world, like you actually shooting the actor, actress, really. But then if you generate this with AI model, right? That is, like some synthetic movie, right? Was, like, it's always thinking, like, along those lines, did they try to reduce costs of production, or is it something else they're trying to get to? So I would say
in the big film, what I'm just trying to figure out is it is a goal, eventually, to get to the place where every synthetic, like, what is it augmented scenes, like putting those actors in front of, like, some scenes, or, like, what like animation is like a lot of synthetic in some way, yeah, but in the real world, like you actually shooting the actor, actress, really. But then if you generate this with AI model, right? That is, like some synthetic movie, right? Was, like, it's always thinking, like, along those lines, did they try to reduce costs of production, or is it something else they're trying to get to? So I would say
in the big film, what I'm just trying to figure out is it is a goal, eventually, to get to the place where every synthetic, like, what is it augmented scenes, like putting those actors in front of, like, some scenes, or, like, what like animation is like a lot of synthetic in some way, yeah, but in the real world, like you actually shooting the actor, actress, really. But then if you generate this with AI model, right? That is, like some synthetic movie, right? Was, like, it's always thinking, like, along those lines, did they try to reduce costs of production, or is it something else they're trying to get to? So I would say
in the big film, what I'm just trying to figure out is it is a goal, eventually, to get to the place where every synthetic, like, what is it augmented scenes, like putting those actors in front of, like, some scenes, or, like, what like animation is like a lot of synthetic in some way, yeah, but in the real world, like you actually shooting the actor, actress, really. But then if you generate this with AI model, right? That is, like some synthetic movie, right? Was, like, it's always thinking, like, along those lines, did they try to reduce costs of production, or is it something else they're trying to get to? So I would say
S Speaker 228:04that in terms of priorities, we see two different priorities in Hollywood, from the studios and the executives. It's essentially all about cost, right? It's like, hey, we can now do a $70 million movie for ten million right? However, the individual creators, so like the producers, directors, those folks, what they're interested in is now they can do more than they could do before. So they're saying, given the $75 million budget, I can now create a movie that the four would take me two $50 million to do. Right? So those are the two different needs that we hear, and what we're trying to figure out is, who's the one that's actually going to drive this buying decision? Is it the advocate of the director? Is it, you know, is it the studio? We're actually seeing both things happen. In some cases we're seeing it's the directors and the artists that are pushing this through. In other cases we're seeing, it's the executives coming in, you know, kind of talking monetarily, but broadly, our bet is that film is going to be hybrid filmmaking like we think that this is kind of next generation realization of CGI. And so it's going to automate large parts of current production. It's going to make things significantly cheaper. However, you don't have to get to the point where it's entirely synthetic like and I'll give you an example is we're working on this. Our model is being deployed on the small show project, thing that Natasha Leon is working on and and what she and her cast are doing is they're actually recording themselves acting in like, Zoom basically, they're just like, recording themselves the way that we are today on an iPhone, and then they're using video to video technology on these models to translate that to them actually being in kind of the environment. And so that's how we think that things are going to evolve this. You'll still have actors, you'll still have a lot of these pieces, but it's going to become, as you said, it's gonna look a lot, start to look a lot more like animation in that way. So if you look at animation, you still have actors, but they're doing voice acting, they're doing motion capture, you know, type stuff. I think that's what you're gonna have. And, you know what? And our studio, and you know, if you're in LA, you know, have an open invite, we'll have to show you, because I think that's where you can really see the future of what's coming. Is we have the studio, and it's a single sound stage, so we just have this, like, single set, basically, and then you go in and they even have these miniaturized sets that they're creating. And so they'll create scenes on this miniaturized set and then use the model to extrapolate it and turn it into something real. And so that is what we think is like a real future model is 100% isn't going to be synthetic, but you know your mouth that you no longer need to fly your entire cast to a remote location. You no longer need these massive production sets. You can bring things and make it a lot more impressive and do a lot more work. And so that's why I think the audience that's really going to be interesting here is independent studios, because I think for the big film studios, obviously they'll be able to cut costs, but now you're going to have this entire new economy open up where independent studios, with four or five people, will now be able to do big budget work that they could have never done before. And that's, I think, where the real market is going to start to expand, and
that in terms of priorities, we see two different priorities in Hollywood, from the studios and the executives. It's essentially all about cost, right? It's like, hey, we can now do a $70 million movie for ten million right? However, the individual creators, so like the producers, directors, those folks, what they're interested in is now they can do more than they could do before. So they're saying, given the $75 million budget, I can now create a movie that the four would take me two $50 million to do. Right? So those are the two different needs that we hear, and what we're trying to figure out is, who's the one that's actually going to drive this buying decision? Is it the advocate of the director? Is it, you know, is it the studio? We're actually seeing both things happen. In some cases we're seeing it's the directors and the artists that are pushing this through. In other cases we're seeing, it's the executives coming in, you know, kind of talking monetarily, but broadly, our bet is that film is going to be hybrid filmmaking like we think that this is kind of next generation realization of CGI. And so it's going to automate large parts of current production. It's going to make things significantly cheaper. However, you don't have to get to the point where it's entirely synthetic like and I'll give you an example is we're working on this. Our model is being deployed on the small show project, thing that Natasha Leon is working on and and what she and her cast are doing is they're actually recording themselves acting in like, Zoom basically, they're just like, recording themselves the way that we are today on an iPhone, and then they're using video to video technology on these models to translate that to them actually being in kind of the environment. And so that's how we think that things are going to evolve this. You'll still have actors, you'll still have a lot of these pieces, but it's going to become, as you said, it's gonna look a lot, start to look a lot more like animation in that way. So if you look at animation, you still have actors, but they're doing voice acting, they're doing motion capture, you know, type stuff. I think that's what you're gonna have. And, you know what? And our studio, and you know, if you're in LA, you know, have an open invite, we'll have to show you, because I think that's where you can really see the future of what's coming. Is we have the studio, and it's a single sound stage, so we just have this, like, single set, basically, and then you go in and they even have these miniaturized sets that they're creating. And so they'll create scenes on this miniaturized set and then use the model to extrapolate it and turn it into something real. And so that is what we think is like a real future model is 100% isn't going to be synthetic, but you know your mouth that you no longer need to fly your entire cast to a remote location. You no longer need these massive production sets. You can bring things and make it a lot more impressive and do a lot more work. And so that's why I think the audience that's really going to be interesting here is independent studios, because I think for the big film studios, obviously they'll be able to cut costs, but now you're going to have this entire new economy open up where independent studios, with four or five people, will now be able to do big budget work that they could have never done before. And that's, I think, where the real market is going to start to expand, and
that in terms of priorities, we see two different priorities in Hollywood, from the studios and the executives. It's essentially all about cost, right? It's like, hey, we can now do a $70 million movie for ten million right? However, the individual creators, so like the producers, directors, those folks, what they're interested in is now they can do more than they could do before. So they're saying, given the $75 million budget, I can now create a movie that the four would take me two $50 million to do. Right? So those are the two different needs that we hear, and what we're trying to figure out is, who's the one that's actually going to drive this buying decision? Is it the advocate of the director? Is it, you know, is it the studio? We're actually seeing both things happen. In some cases we're seeing it's the directors and the artists that are pushing this through. In other cases we're seeing, it's the executives coming in, you know, kind of talking monetarily, but broadly, our bet is that film is going to be hybrid filmmaking like we think that this is kind of next generation realization of CGI. And so it's going to automate large parts of current production. It's going to make things significantly cheaper. However, you don't have to get to the point where it's entirely synthetic like and I'll give you an example is we're working on this. Our model is being deployed on the small show project, thing that Natasha Leon is working on and and what she and her cast are doing is they're actually recording themselves acting in like, Zoom basically, they're just like, recording themselves the way that we are today on an iPhone, and then they're using video to video technology on these models to translate that to them actually being in kind of the environment. And so that's how we think that things are going to evolve this. You'll still have actors, you'll still have a lot of these pieces, but it's going to become, as you said, it's gonna look a lot, start to look a lot more like animation in that way. So if you look at animation, you still have actors, but they're doing voice acting, they're doing motion capture, you know, type stuff. I think that's what you're gonna have. And, you know what? And our studio, and you know, if you're in LA, you know, have an open invite, we'll have to show you, because I think that's where you can really see the future of what's coming. Is we have the studio, and it's a single sound stage, so we just have this, like, single set, basically, and then you go in and they even have these miniaturized sets that they're creating. And so they'll create scenes on this miniaturized set and then use the model to extrapolate it and turn it into something real. And so that is what we think is like a real future model is 100% isn't going to be synthetic, but you know your mouth that you no longer need to fly your entire cast to a remote location. You no longer need these massive production sets. You can bring things and make it a lot more impressive and do a lot more work. And so that's why I think the audience that's really going to be interesting here is independent studios, because I think for the big film studios, obviously they'll be able to cut costs, but now you're going to have this entire new economy open up where independent studios, with four or five people, will now be able to do big budget work that they could have never done before. And that's, I think, where the real market is going to start to expand, and
that in terms of priorities, we see two different priorities in Hollywood, from the studios and the executives. It's essentially all about cost, right? It's like, hey, we can now do a $70 million movie for ten million right? However, the individual creators, so like the producers, directors, those folks, what they're interested in is now they can do more than they could do before. So they're saying, given the $75 million budget, I can now create a movie that the four would take me two $50 million to do. Right? So those are the two different needs that we hear, and what we're trying to figure out is, who's the one that's actually going to drive this buying decision? Is it the advocate of the director? Is it, you know, is it the studio? We're actually seeing both things happen. In some cases we're seeing it's the directors and the artists that are pushing this through. In other cases we're seeing, it's the executives coming in, you know, kind of talking monetarily, but broadly, our bet is that film is going to be hybrid filmmaking like we think that this is kind of next generation realization of CGI. And so it's going to automate large parts of current production. It's going to make things significantly cheaper. However, you don't have to get to the point where it's entirely synthetic like and I'll give you an example is we're working on this. Our model is being deployed on the small show project, thing that Natasha Leon is working on and and what she and her cast are doing is they're actually recording themselves acting in like, Zoom basically, they're just like, recording themselves the way that we are today on an iPhone, and then they're using video to video technology on these models to translate that to them actually being in kind of the environment. And so that's how we think that things are going to evolve this. You'll still have actors, you'll still have a lot of these pieces, but it's going to become, as you said, it's gonna look a lot, start to look a lot more like animation in that way. So if you look at animation, you still have actors, but they're doing voice acting, they're doing motion capture, you know, type stuff. I think that's what you're gonna have. And, you know what? And our studio, and you know, if you're in LA, you know, have an open invite, we'll have to show you, because I think that's where you can really see the future of what's coming. Is we have the studio, and it's a single sound stage, so we just have this, like, single set, basically, and then you go in and they even have these miniaturized sets that they're creating. And so they'll create scenes on this miniaturized set and then use the model to extrapolate it and turn it into something real. And so that is what we think is like a real future model is 100% isn't going to be synthetic, but you know your mouth that you no longer need to fly your entire cast to a remote location. You no longer need these massive production sets. You can bring things and make it a lot more impressive and do a lot more work. And so that's why I think the audience that's really going to be interesting here is independent studios, because I think for the big film studios, obviously they'll be able to cut costs, but now you're going to have this entire new economy open up where independent studios, with four or five people, will now be able to do big budget work that they could have never done before. And that's, I think, where the real market is going to start to expand, and
S Speaker 530:20who the most advanced in terms of, like, film production? Obviously, you have traditional studios, right? Then you have Amazon. All these guys are making all sorts of movie series, etc, etc, right? Yeah, who's the one that's the forefront in terms of adopting tech and stuff? I assume that traditional Hollywood guys, universal whatever, there may be a little less, or maybe they are, I don't know. Yeah,
who the most advanced in terms of, like, film production? Obviously, you have traditional studios, right? Then you have Amazon. All these guys are making all sorts of movie series, etc, etc, right? Yeah, who's the one that's the forefront in terms of adopting tech and stuff? I assume that traditional Hollywood guys, universal whatever, there may be a little less, or maybe they are, I don't know. Yeah,
who the most advanced in terms of, like, film production? Obviously, you have traditional studios, right? Then you have Amazon. All these guys are making all sorts of movie series, etc, etc, right? Yeah, who's the one that's the forefront in terms of adopting tech and stuff? I assume that traditional Hollywood guys, universal whatever, there may be a little less, or maybe they are, I don't know. Yeah,
who the most advanced in terms of, like, film production? Obviously, you have traditional studios, right? Then you have Amazon. All these guys are making all sorts of movie series, etc, etc, right? Yeah, who's the one that's the forefront in terms of adopting tech and stuff? I assume that traditional Hollywood guys, universal whatever, there may be a little less, or maybe they are, I don't know. Yeah,
S Speaker 230:38that's, that's a great question. I think it's sort of been all over the map from what we've seen in Hollywood. Some folks are further behind than we expected. So Disney is, I think, a good example where they have a whole AI team and stuff. But as we spend time with them, they're still very conservative. They're still very cagey about this stuff. The ones who have generally been ahead of the curve. Funny one was Fox entertainment. They're like very I think, because they also just don't care, so that they're just, you know, everybody else, they're the least like conservative in that sense. So they're created these divisions, and they're playing with these tools and toys. But in general, I think Amazon Prime and Netflix have been, have been consistently very out of the curve. I mentioned. We're working on a big licensing deal with them. We had Ted, Sarah strand and the entire Netflix team at our studio last week, actually. So we've just started a partnership discussion with them as well. They have a large AI team. They've been investing very heavily into into generative data. So, yeah. I mean, I guess, not too surprising. But in general, those two have just been, you know, from from our perspective, much more ahead of the curve from everybody else.
that's, that's a great question. I think it's sort of been all over the map from what we've seen in Hollywood. Some folks are further behind than we expected. So Disney is, I think, a good example where they have a whole AI team and stuff. But as we spend time with them, they're still very conservative. They're still very cagey about this stuff. The ones who have generally been ahead of the curve. Funny one was Fox entertainment. They're like very I think, because they also just don't care, so that they're just, you know, everybody else, they're the least like conservative in that sense. So they're created these divisions, and they're playing with these tools and toys. But in general, I think Amazon Prime and Netflix have been, have been consistently very out of the curve. I mentioned. We're working on a big licensing deal with them. We had Ted, Sarah strand and the entire Netflix team at our studio last week, actually. So we've just started a partnership discussion with them as well. They have a large AI team. They've been investing very heavily into into generative data. So, yeah. I mean, I guess, not too surprising. But in general, those two have just been, you know, from from our perspective, much more ahead of the curve from everybody else.
that's, that's a great question. I think it's sort of been all over the map from what we've seen in Hollywood. Some folks are further behind than we expected. So Disney is, I think, a good example where they have a whole AI team and stuff. But as we spend time with them, they're still very conservative. They're still very cagey about this stuff. The ones who have generally been ahead of the curve. Funny one was Fox entertainment. They're like very I think, because they also just don't care, so that they're just, you know, everybody else, they're the least like conservative in that sense. So they're created these divisions, and they're playing with these tools and toys. But in general, I think Amazon Prime and Netflix have been, have been consistently very out of the curve. I mentioned. We're working on a big licensing deal with them. We had Ted, Sarah strand and the entire Netflix team at our studio last week, actually. So we've just started a partnership discussion with them as well. They have a large AI team. They've been investing very heavily into into generative data. So, yeah. I mean, I guess, not too surprising. But in general, those two have just been, you know, from from our perspective, much more ahead of the curve from everybody else.
that's, that's a great question. I think it's sort of been all over the map from what we've seen in Hollywood. Some folks are further behind than we expected. So Disney is, I think, a good example where they have a whole AI team and stuff. But as we spend time with them, they're still very conservative. They're still very cagey about this stuff. The ones who have generally been ahead of the curve. Funny one was Fox entertainment. They're like very I think, because they also just don't care, so that they're just, you know, everybody else, they're the least like conservative in that sense. So they're created these divisions, and they're playing with these tools and toys. But in general, I think Amazon Prime and Netflix have been, have been consistently very out of the curve. I mentioned. We're working on a big licensing deal with them. We had Ted, Sarah strand and the entire Netflix team at our studio last week, actually. So we've just started a partnership discussion with them as well. They have a large AI team. They've been investing very heavily into into generative data. So, yeah. I mean, I guess, not too surprising. But in general, those two have just been, you know, from from our perspective, much more ahead of the curve from everybody else.
S Speaker 531:32Yeah. And when you think first the video production of film production using the model will be in, like in TV or consumers two years from now, a year from now, three years from now,
Yeah. And when you think first the video production of film production using the model will be in, like in TV or consumers two years from now, a year from now, three years from now,
Yeah. And when you think first the video production of film production using the model will be in, like in TV or consumers two years from now, a year from now, three years from now,
Yeah. And when you think first the video production of film production using the model will be in, like in TV or consumers two years from now, a year from now, three years from now,
S Speaker 231:43probably four months from now, House of David, which is what we're doing with Amazon Prime. That season you'll be able to watch starting in October. And so, you know, right now, it's a lot of VFX stuff. So there's, you know, but definitely you'll already be able to see certain scenes and certain things that are being done with AI. And what you can do with AI today is, you know, I wouldn't trust it for your main scenes, necessarily. You can do VFX stuff in main scenes, but, you know, you're going to be shooting traditionally, but what it's doing all of the sort of background and, like, B roll type footage. And that's why, like Casa de big example, is it's this, like, big, you know, historic, epic thing that they have. And so a lot of the footage that that they're using AI to do is, like, you know, large sweeping shots of, like Egyptian pyramids or, you know, these big armies and things like that. So those are the types of scenes that you'll be able to see. And I think also David is gonna be great starting and then wrecking ball is another movie that we're planning to work on with Amazon. That's with Dave Bautista and a few others. That one, I think, is slated to be October, November of this year as well. So yeah, basically, last quarter of this year, we should see the first real, real examples of generative AI hitting the big
probably four months from now, House of David, which is what we're doing with Amazon Prime. That season you'll be able to watch starting in October. And so, you know, right now, it's a lot of VFX stuff. So there's, you know, but definitely you'll already be able to see certain scenes and certain things that are being done with AI. And what you can do with AI today is, you know, I wouldn't trust it for your main scenes, necessarily. You can do VFX stuff in main scenes, but, you know, you're going to be shooting traditionally, but what it's doing all of the sort of background and, like, B roll type footage. And that's why, like Casa de big example, is it's this, like, big, you know, historic, epic thing that they have. And so a lot of the footage that that they're using AI to do is, like, you know, large sweeping shots of, like Egyptian pyramids or, you know, these big armies and things like that. So those are the types of scenes that you'll be able to see. And I think also David is gonna be great starting and then wrecking ball is another movie that we're planning to work on with Amazon. That's with Dave Bautista and a few others. That one, I think, is slated to be October, November of this year as well. So yeah, basically, last quarter of this year, we should see the first real, real examples of generative AI hitting the big
probably four months from now, House of David, which is what we're doing with Amazon Prime. That season you'll be able to watch starting in October. And so, you know, right now, it's a lot of VFX stuff. So there's, you know, but definitely you'll already be able to see certain scenes and certain things that are being done with AI. And what you can do with AI today is, you know, I wouldn't trust it for your main scenes, necessarily. You can do VFX stuff in main scenes, but, you know, you're going to be shooting traditionally, but what it's doing all of the sort of background and, like, B roll type footage. And that's why, like Casa de big example, is it's this, like, big, you know, historic, epic thing that they have. And so a lot of the footage that that they're using AI to do is, like, you know, large sweeping shots of, like Egyptian pyramids or, you know, these big armies and things like that. So those are the types of scenes that you'll be able to see. And I think also David is gonna be great starting and then wrecking ball is another movie that we're planning to work on with Amazon. That's with Dave Bautista and a few others. That one, I think, is slated to be October, November of this year as well. So yeah, basically, last quarter of this year, we should see the first real, real examples of generative AI hitting the big
probably four months from now, House of David, which is what we're doing with Amazon Prime. That season you'll be able to watch starting in October. And so, you know, right now, it's a lot of VFX stuff. So there's, you know, but definitely you'll already be able to see certain scenes and certain things that are being done with AI. And what you can do with AI today is, you know, I wouldn't trust it for your main scenes, necessarily. You can do VFX stuff in main scenes, but, you know, you're going to be shooting traditionally, but what it's doing all of the sort of background and, like, B roll type footage. And that's why, like Casa de big example, is it's this, like, big, you know, historic, epic thing that they have. And so a lot of the footage that that they're using AI to do is, like, you know, large sweeping shots of, like Egyptian pyramids or, you know, these big armies and things like that. So those are the types of scenes that you'll be able to see. And I think also David is gonna be great starting and then wrecking ball is another movie that we're planning to work on with Amazon. That's with Dave Bautista and a few others. That one, I think, is slated to be October, November of this year as well. So yeah, basically, last quarter of this year, we should see the first real, real examples of generative AI hitting the big
32:40screen. Okay, moving. It's definitely
screen. Okay, moving. It's definitely
screen. Okay, moving. It's definitely
screen. Okay, moving. It's definitely
S Speaker 132:44moving. Let's switch gears to the plan for like in terms of financials, which is, what's the revenue plan for this year? It seems to go to market. Is more studios for this year? Are you also thinking about going having consumer angle as well for the creators and also
moving. Let's switch gears to the plan for like in terms of financials, which is, what's the revenue plan for this year? It seems to go to market. Is more studios for this year? Are you also thinking about going having consumer angle as well for the creators and also
moving. Let's switch gears to the plan for like in terms of financials, which is, what's the revenue plan for this year? It seems to go to market. Is more studios for this year? Are you also thinking about going having consumer angle as well for the creators and also
moving. Let's switch gears to the plan for like in terms of financials, which is, what's the revenue plan for this year? It seems to go to market. Is more studios for this year? Are you also thinking about going having consumer angle as well for the creators and also
33:01advertisements. Yeah, yeah. So we're
advertisements. Yeah, yeah. So we're
advertisements. Yeah, yeah. So we're
advertisements. Yeah, yeah. So we're
S Speaker 233:03rolling across three different fronts, basically, so the bulk of our just human effort is on the studio side, and so we're building these relationships. And these are, these are gonna be kind of a longer term play. The second one is the brand side, so that's the one where we've already sold a bunch of deals. So we we have a pretty good idea of what we can do there. We're pretty confident that we should be able to scale that to north of five, 10 million an Arr, by the end of this year, just on kind of our current motion basically, advertising part. So the brands like being able to sell to brands. That's right, that's right. There's opportunity a lot more, but that's going to be those big enterprise potential, 78 figure ones. I think that's just going to take a little bit more time to build up to. But I think in the short term, we're confident we'll be able to do that, and we will be releasing our model into GA and for consumer use. There isn't a whole lot of like focus there in terms of manual, but, you know, there's just so much revenue there right now that in the short term, like we're rolling out on foul.ai is one of the channels that we're releasing our model. And you know, their team was telling us they probably shouldn't have been telling us this, but they were telling us that cling, which is one of the models that's on there, they're generating 20. Remember the exact number? I think it was, I believe it was 5 million a month in revenue. Entirely to that there was no margin that foul was taking, just from use on foul. That's not counting all the other channels that cling is on, right? And so, yeah, the GA Metro expertise are insane. So we expect that the bulk of our revenue in the short term for the this year is going to be ga i think that long term is where we expect that enterprise, those enterprise deals, to start. The goal is that next year we have, let's say, a $5 million deal with Walmart, or a ten million deal with Nike, or a ten million deal with, you know, Netflix, like, that's, that's where we expect that to really start expanding.
rolling across three different fronts, basically, so the bulk of our just human effort is on the studio side, and so we're building these relationships. And these are, these are gonna be kind of a longer term play. The second one is the brand side, so that's the one where we've already sold a bunch of deals. So we we have a pretty good idea of what we can do there. We're pretty confident that we should be able to scale that to north of five, 10 million an Arr, by the end of this year, just on kind of our current motion basically, advertising part. So the brands like being able to sell to brands. That's right, that's right. There's opportunity a lot more, but that's going to be those big enterprise potential, 78 figure ones. I think that's just going to take a little bit more time to build up to. But I think in the short term, we're confident we'll be able to do that, and we will be releasing our model into GA and for consumer use. There isn't a whole lot of like focus there in terms of manual, but, you know, there's just so much revenue there right now that in the short term, like we're rolling out on foul.ai is one of the channels that we're releasing our model. And you know, their team was telling us they probably shouldn't have been telling us this, but they were telling us that cling, which is one of the models that's on there, they're generating 20. Remember the exact number? I think it was, I believe it was 5 million a month in revenue. Entirely to that there was no margin that foul was taking, just from use on foul. That's not counting all the other channels that cling is on, right? And so, yeah, the GA Metro expertise are insane. So we expect that the bulk of our revenue in the short term for the this year is going to be ga i think that long term is where we expect that enterprise, those enterprise deals, to start. The goal is that next year we have, let's say, a $5 million deal with Walmart, or a ten million deal with Nike, or a ten million deal with, you know, Netflix, like, that's, that's where we expect that to really start expanding.
rolling across three different fronts, basically, so the bulk of our just human effort is on the studio side, and so we're building these relationships. And these are, these are gonna be kind of a longer term play. The second one is the brand side, so that's the one where we've already sold a bunch of deals. So we we have a pretty good idea of what we can do there. We're pretty confident that we should be able to scale that to north of five, 10 million an Arr, by the end of this year, just on kind of our current motion basically, advertising part. So the brands like being able to sell to brands. That's right, that's right. There's opportunity a lot more, but that's going to be those big enterprise potential, 78 figure ones. I think that's just going to take a little bit more time to build up to. But I think in the short term, we're confident we'll be able to do that, and we will be releasing our model into GA and for consumer use. There isn't a whole lot of like focus there in terms of manual, but, you know, there's just so much revenue there right now that in the short term, like we're rolling out on foul.ai is one of the channels that we're releasing our model. And you know, their team was telling us they probably shouldn't have been telling us this, but they were telling us that cling, which is one of the models that's on there, they're generating 20. Remember the exact number? I think it was, I believe it was 5 million a month in revenue. Entirely to that there was no margin that foul was taking, just from use on foul. That's not counting all the other channels that cling is on, right? And so, yeah, the GA Metro expertise are insane. So we expect that the bulk of our revenue in the short term for the this year is going to be ga i think that long term is where we expect that enterprise, those enterprise deals, to start. The goal is that next year we have, let's say, a $5 million deal with Walmart, or a ten million deal with Nike, or a ten million deal with, you know, Netflix, like, that's, that's where we expect that to really start expanding.
rolling across three different fronts, basically, so the bulk of our just human effort is on the studio side, and so we're building these relationships. And these are, these are gonna be kind of a longer term play. The second one is the brand side, so that's the one where we've already sold a bunch of deals. So we we have a pretty good idea of what we can do there. We're pretty confident that we should be able to scale that to north of five, 10 million an Arr, by the end of this year, just on kind of our current motion basically, advertising part. So the brands like being able to sell to brands. That's right, that's right. There's opportunity a lot more, but that's going to be those big enterprise potential, 78 figure ones. I think that's just going to take a little bit more time to build up to. But I think in the short term, we're confident we'll be able to do that, and we will be releasing our model into GA and for consumer use. There isn't a whole lot of like focus there in terms of manual, but, you know, there's just so much revenue there right now that in the short term, like we're rolling out on foul.ai is one of the channels that we're releasing our model. And you know, their team was telling us they probably shouldn't have been telling us this, but they were telling us that cling, which is one of the models that's on there, they're generating 20. Remember the exact number? I think it was, I believe it was 5 million a month in revenue. Entirely to that there was no margin that foul was taking, just from use on foul. That's not counting all the other channels that cling is on, right? And so, yeah, the GA Metro expertise are insane. So we expect that the bulk of our revenue in the short term for the this year is going to be ga i think that long term is where we expect that enterprise, those enterprise deals, to start. The goal is that next year we have, let's say, a $5 million deal with Walmart, or a ten million deal with Nike, or a ten million deal with, you know, Netflix, like, that's, that's where we expect that to really start expanding.
S Speaker 134:35Is kind of q1 of next year. And then these deals with Amazon, which, those are my guesses. They already signed right for the two shows for this year. So
Is kind of q1 of next year. And then these deals with Amazon, which, those are my guesses. They already signed right for the two shows for this year. So
Is kind of q1 of next year. And then these deals with Amazon, which, those are my guesses. They already signed right for the two shows for this year. So
Is kind of q1 of next year. And then these deals with Amazon, which, those are my guesses. They already signed right for the two shows for this year. So
S Speaker 234:41the Amazon one is like final stages right now, basically, so we're expecting in the next week based on there's a broader partnership that's also involved on the Amazon one. So what we're discussing with Amazon Prime is not just that we would get the show, but that we would become basically their official AI partner. So with Amazon Prime, essentially what the deal, or what the positioning is, is we get right of first refusal on every any show or movie, that one on Amazon Prime that wants to use AI, we would get first kick of the bucket to work with them, basically. So we look at that as probably being by far our biggest channel in the long term. The other one, though, I think, which is important to call out, is we have NBC, Universal income cast coming into our round, and they want to do an official content partnership. And I actually didn't know this until recently, but NBC has by far the biggest budgets in content of anybody. I didn't realize, bigger than $40 billion a year on content. And so they want to do for them, it's obviously going to be an exclusive partnership, because they're investing so I think on the film side, we expect Amazon Prime and as a distribution channel and NBC to be the bulk of what we're going to do in the next
the Amazon one is like final stages right now, basically, so we're expecting in the next week based on there's a broader partnership that's also involved on the Amazon one. So what we're discussing with Amazon Prime is not just that we would get the show, but that we would become basically their official AI partner. So with Amazon Prime, essentially what the deal, or what the positioning is, is we get right of first refusal on every any show or movie, that one on Amazon Prime that wants to use AI, we would get first kick of the bucket to work with them, basically. So we look at that as probably being by far our biggest channel in the long term. The other one, though, I think, which is important to call out, is we have NBC, Universal income cast coming into our round, and they want to do an official content partnership. And I actually didn't know this until recently, but NBC has by far the biggest budgets in content of anybody. I didn't realize, bigger than $40 billion a year on content. And so they want to do for them, it's obviously going to be an exclusive partnership, because they're investing so I think on the film side, we expect Amazon Prime and as a distribution channel and NBC to be the bulk of what we're going to do in the next
the Amazon one is like final stages right now, basically, so we're expecting in the next week based on there's a broader partnership that's also involved on the Amazon one. So what we're discussing with Amazon Prime is not just that we would get the show, but that we would become basically their official AI partner. So with Amazon Prime, essentially what the deal, or what the positioning is, is we get right of first refusal on every any show or movie, that one on Amazon Prime that wants to use AI, we would get first kick of the bucket to work with them, basically. So we look at that as probably being by far our biggest channel in the long term. The other one, though, I think, which is important to call out, is we have NBC, Universal income cast coming into our round, and they want to do an official content partnership. And I actually didn't know this until recently, but NBC has by far the biggest budgets in content of anybody. I didn't realize, bigger than $40 billion a year on content. And so they want to do for them, it's obviously going to be an exclusive partnership, because they're investing so I think on the film side, we expect Amazon Prime and as a distribution channel and NBC to be the bulk of what we're going to do in the next
the Amazon one is like final stages right now, basically, so we're expecting in the next week based on there's a broader partnership that's also involved on the Amazon one. So what we're discussing with Amazon Prime is not just that we would get the show, but that we would become basically their official AI partner. So with Amazon Prime, essentially what the deal, or what the positioning is, is we get right of first refusal on every any show or movie, that one on Amazon Prime that wants to use AI, we would get first kick of the bucket to work with them, basically. So we look at that as probably being by far our biggest channel in the long term. The other one, though, I think, which is important to call out, is we have NBC, Universal income cast coming into our round, and they want to do an official content partnership. And I actually didn't know this until recently, but NBC has by far the biggest budgets in content of anybody. I didn't realize, bigger than $40 billion a year on content. And so they want to do for them, it's obviously going to be an exclusive partnership, because they're investing so I think on the film side, we expect Amazon Prime and as a distribution channel and NBC to be the bulk of what we're going to do in the next
S Speaker 135:39alarm and these deals. What do you expect the size for, like, Amazon, size deals, I think you mentioned you already have a 250 K POC.
alarm and these deals. What do you expect the size for, like, Amazon, size deals, I think you mentioned you already have a 250 K POC.
alarm and these deals. What do you expect the size for, like, Amazon, size deals, I think you mentioned you already have a 250 K POC.
alarm and these deals. What do you expect the size for, like, Amazon, size deals, I think you mentioned you already have a 250 K POC.
S Speaker 235:46So David deal, which is probably the closest to, like, the pattern that we're matching, is $1 million for one season. So we estimate that production budget is around 15 million of that one season, and we're getting about a billion. So I think that that's probably what we would expect moving forward, is like, yeah, you have $100 million kind of production. And let's say $4 million goes into
So David deal, which is probably the closest to, like, the pattern that we're matching, is $1 million for one season. So we estimate that production budget is around 15 million of that one season, and we're getting about a billion. So I think that that's probably what we would expect moving forward, is like, yeah, you have $100 million kind of production. And let's say $4 million goes into
So David deal, which is probably the closest to, like, the pattern that we're matching, is $1 million for one season. So we estimate that production budget is around 15 million of that one season, and we're getting about a billion. So I think that that's probably what we would expect moving forward, is like, yeah, you have $100 million kind of production. And let's say $4 million goes into
So David deal, which is probably the closest to, like, the pattern that we're matching, is $1 million for one season. So we estimate that production budget is around 15 million of that one season, and we're getting about a billion. So I think that that's probably what we would expect moving forward, is like, yeah, you have $100 million kind of production. And let's say $4 million goes into
S Speaker 336:06and the million is largely for still the effects part of it like there's room to expand as you start
and the million is largely for still the effects part of it like there's room to expand as you start
and the million is largely for still the effects part of it like there's room to expand as you start
and the million is largely for still the effects part of it like there's room to expand as you start
S Speaker 536:26lot of learning that we have to do. Consumer stuff already wrote down on that yet, you know? So we're
lot of learning that we have to do. Consumer stuff already wrote down on that yet, you know? So we're
lot of learning that we have to do. Consumer stuff already wrote down on that yet, you know? So we're
lot of learning that we have to do. Consumer stuff already wrote down on that yet, you know? So we're
S Speaker 236:29rolling it out. So we're doing an alpha program right now. We have a number of alpha filmmakers. We have to do red teaming and, you know, some safety stuff before we can really roll it out. So June is when we're rolling that out to a consumer, and that will be hosted on something like a foul and that's right, by words, that's right. So foul replicas, we're doing a partnership with the comfy UI team, so we're going to do, they're deploying this new API thing. We'll be there and probably on Amazon bedrock as well, although I don't you know, we don't expect that to be as big as a channel, but bedrock, you don't expect that to be a big I think just because the just the audience that's there, I think that we'll probably get interesting leads to go into either the brand or the studio side. But in terms of just pure, like, you know, consumer revenue coming in from there, I don't think it's gonna be at that level. You know, as an example, they have stable diffusion models up. I think those are the only image models on bedrock, and they their stable diffusability is generating, I believe, 2 million or 3 million a year from bedrock. So, you know, it's a good amount, but it's a good amount, but it's not, it's not that obviously, nothing compared to whereas with foul, what they told us is that, I think, over six years, 70% of the revenue now is coming from video, and they're expecting that number to do 90% doesn't take any cut. They don't know. So they take, they do take, they have, like, implementation fees, and then they charge their users online quotas and different subscriptions, but the base deployment that users pay for, they don't take any cuts there. You know that. So that that 5 million a month that cling is getting is basically 99% margin to claim, which is, I don't know how sustainable
rolling it out. So we're doing an alpha program right now. We have a number of alpha filmmakers. We have to do red teaming and, you know, some safety stuff before we can really roll it out. So June is when we're rolling that out to a consumer, and that will be hosted on something like a foul and that's right, by words, that's right. So foul replicas, we're doing a partnership with the comfy UI team, so we're going to do, they're deploying this new API thing. We'll be there and probably on Amazon bedrock as well, although I don't you know, we don't expect that to be as big as a channel, but bedrock, you don't expect that to be a big I think just because the just the audience that's there, I think that we'll probably get interesting leads to go into either the brand or the studio side. But in terms of just pure, like, you know, consumer revenue coming in from there, I don't think it's gonna be at that level. You know, as an example, they have stable diffusion models up. I think those are the only image models on bedrock, and they their stable diffusability is generating, I believe, 2 million or 3 million a year from bedrock. So, you know, it's a good amount, but it's a good amount, but it's not, it's not that obviously, nothing compared to whereas with foul, what they told us is that, I think, over six years, 70% of the revenue now is coming from video, and they're expecting that number to do 90% doesn't take any cut. They don't know. So they take, they do take, they have, like, implementation fees, and then they charge their users online quotas and different subscriptions, but the base deployment that users pay for, they don't take any cuts there. You know that. So that that 5 million a month that cling is getting is basically 99% margin to claim, which is, I don't know how sustainable
rolling it out. So we're doing an alpha program right now. We have a number of alpha filmmakers. We have to do red teaming and, you know, some safety stuff before we can really roll it out. So June is when we're rolling that out to a consumer, and that will be hosted on something like a foul and that's right, by words, that's right. So foul replicas, we're doing a partnership with the comfy UI team, so we're going to do, they're deploying this new API thing. We'll be there and probably on Amazon bedrock as well, although I don't you know, we don't expect that to be as big as a channel, but bedrock, you don't expect that to be a big I think just because the just the audience that's there, I think that we'll probably get interesting leads to go into either the brand or the studio side. But in terms of just pure, like, you know, consumer revenue coming in from there, I don't think it's gonna be at that level. You know, as an example, they have stable diffusion models up. I think those are the only image models on bedrock, and they their stable diffusability is generating, I believe, 2 million or 3 million a year from bedrock. So, you know, it's a good amount, but it's a good amount, but it's not, it's not that obviously, nothing compared to whereas with foul, what they told us is that, I think, over six years, 70% of the revenue now is coming from video, and they're expecting that number to do 90% doesn't take any cut. They don't know. So they take, they do take, they have, like, implementation fees, and then they charge their users online quotas and different subscriptions, but the base deployment that users pay for, they don't take any cuts there. You know that. So that that 5 million a month that cling is getting is basically 99% margin to claim, which is, I don't know how sustainable
rolling it out. So we're doing an alpha program right now. We have a number of alpha filmmakers. We have to do red teaming and, you know, some safety stuff before we can really roll it out. So June is when we're rolling that out to a consumer, and that will be hosted on something like a foul and that's right, by words, that's right. So foul replicas, we're doing a partnership with the comfy UI team, so we're going to do, they're deploying this new API thing. We'll be there and probably on Amazon bedrock as well, although I don't you know, we don't expect that to be as big as a channel, but bedrock, you don't expect that to be a big I think just because the just the audience that's there, I think that we'll probably get interesting leads to go into either the brand or the studio side. But in terms of just pure, like, you know, consumer revenue coming in from there, I don't think it's gonna be at that level. You know, as an example, they have stable diffusion models up. I think those are the only image models on bedrock, and they their stable diffusability is generating, I believe, 2 million or 3 million a year from bedrock. So, you know, it's a good amount, but it's a good amount, but it's not, it's not that obviously, nothing compared to whereas with foul, what they told us is that, I think, over six years, 70% of the revenue now is coming from video, and they're expecting that number to do 90% doesn't take any cut. They don't know. So they take, they do take, they have, like, implementation fees, and then they charge their users online quotas and different subscriptions, but the base deployment that users pay for, they don't take any cuts there. You know that. So that that 5 million a month that cling is getting is basically 99% margin to claim, which is, I don't know how sustainable
S Speaker 137:51is charging whatever GPU hours cost, yeah, the usage of whatever cling is something, and then they're just passing that on to click Exactly,
is charging whatever GPU hours cost, yeah, the usage of whatever cling is something, and then they're just passing that on to click Exactly,
is charging whatever GPU hours cost, yeah, the usage of whatever cling is something, and then they're just passing that on to click Exactly,
is charging whatever GPU hours cost, yeah, the usage of whatever cling is something, and then they're just passing that on to click Exactly,
S Speaker 237:59yeah, they're just being the inference provider, like they're, they're so they're giving us h2 100 for Andrew. What was it? One nine, right? $1.90 like, it's an insane price, you know, and it's, I don't know, sustainable it is. But, you know, even if it's more expensive, it's still obviously what works out for us, but we can squeeze out extra margin right now. Okay,
yeah, they're just being the inference provider, like they're, they're so they're giving us h2 100 for Andrew. What was it? One nine, right? $1.90 like, it's an insane price, you know, and it's, I don't know, sustainable it is. But, you know, even if it's more expensive, it's still obviously what works out for us, but we can squeeze out extra margin right now. Okay,
yeah, they're just being the inference provider, like they're, they're so they're giving us h2 100 for Andrew. What was it? One nine, right? $1.90 like, it's an insane price, you know, and it's, I don't know, sustainable it is. But, you know, even if it's more expensive, it's still obviously what works out for us, but we can squeeze out extra margin right now. Okay,
yeah, they're just being the inference provider, like they're, they're so they're giving us h2 100 for Andrew. What was it? One nine, right? $1.90 like, it's an insane price, you know, and it's, I don't know, sustainable it is. But, you know, even if it's more expensive, it's still obviously what works out for us, but we can squeeze out extra margin right now. Okay,
S Speaker 139:30post. And then the lead for this day was
post. And then the lead for this day was
post. And then the lead for this day was
post. And then the lead for this day was
S Speaker 239:34General catalyst. So Hemanta, GC, is our, is our main partner. And he led, he so the seed was co led with GC and KB, and then this one is just led by KV is participating with smaller Jack,
General catalyst. So Hemanta, GC, is our, is our main partner. And he led, he so the seed was co led with GC and KB, and then this one is just led by KV is participating with smaller Jack,
General catalyst. So Hemanta, GC, is our, is our main partner. And he led, he so the seed was co led with GC and KB, and then this one is just led by KV is participating with smaller Jack,
General catalyst. So Hemanta, GC, is our, is our main partner. And he led, he so the seed was co led with GC and KB, and then this one is just led by KV is participating with smaller Jack,
S Speaker 539:5175 million signed. So when is the money or you plan to is it going to be rolling close?
75 million signed. So when is the money or you plan to is it going to be rolling close?
75 million signed. So when is the money or you plan to is it going to be rolling close?
75 million signed. So when is the money or you plan to is it going to be rolling close?
S Speaker 240:00You closer to rolling close? Yeah. Let's the 75 has already been closed, mostly wired, and the remainder, yeah, we're keeping open for now, but yeah, the board wants us to get it done. Get it done, yeah, by the end of this month
You closer to rolling close? Yeah. Let's the 75 has already been closed, mostly wired, and the remainder, yeah, we're keeping open for now, but yeah, the board wants us to get it done. Get it done, yeah, by the end of this month
You closer to rolling close? Yeah. Let's the 75 has already been closed, mostly wired, and the remainder, yeah, we're keeping open for now, but yeah, the board wants us to get it done. Get it done, yeah, by the end of this month
You closer to rolling close? Yeah. Let's the 75 has already been closed, mostly wired, and the remainder, yeah, we're keeping open for now, but yeah, the board wants us to get it done. Get it done, yeah, by the end of this month
S Speaker 540:16and the 100 million, that says you raise a full amount. How long was that? Long Way to take you.
and the 100 million, that says you raise a full amount. How long was that? Long Way to take you.
and the 100 million, that says you raise a full amount. How long was that? Long Way to take you.
and the 100 million, that says you raise a full amount. How long was that? Long Way to take you.
S Speaker 240:20So without being too conservative, it'll comfortably take us to the end of next year. So that's assuming, you know, we're able to continue investing in model development and scaling the team, etc. If we needed to, we could, like, we don't have to train our next model, let's say, with this funding and internally, that's how we're planning is we don't want any of these funds to go towards the next version of our model. We want to do that with our Series A and so if you don't kind of account for next model training, then we're looking at over three years of runway, basically from from today. Or if you don't use the money for the next one, for the next model, the next foundational, yeah, 2026, includes model training. Yeah, that's right. Okay.
So without being too conservative, it'll comfortably take us to the end of next year. So that's assuming, you know, we're able to continue investing in model development and scaling the team, etc. If we needed to, we could, like, we don't have to train our next model, let's say, with this funding and internally, that's how we're planning is we don't want any of these funds to go towards the next version of our model. We want to do that with our Series A and so if you don't kind of account for next model training, then we're looking at over three years of runway, basically from from today. Or if you don't use the money for the next one, for the next model, the next foundational, yeah, 2026, includes model training. Yeah, that's right. Okay.
So without being too conservative, it'll comfortably take us to the end of next year. So that's assuming, you know, we're able to continue investing in model development and scaling the team, etc. If we needed to, we could, like, we don't have to train our next model, let's say, with this funding and internally, that's how we're planning is we don't want any of these funds to go towards the next version of our model. We want to do that with our Series A and so if you don't kind of account for next model training, then we're looking at over three years of runway, basically from from today. Or if you don't use the money for the next one, for the next model, the next foundational, yeah, 2026, includes model training. Yeah, that's right. Okay.
So without being too conservative, it'll comfortably take us to the end of next year. So that's assuming, you know, we're able to continue investing in model development and scaling the team, etc. If we needed to, we could, like, we don't have to train our next model, let's say, with this funding and internally, that's how we're planning is we don't want any of these funds to go towards the next version of our model. We want to do that with our Series A and so if you don't kind of account for next model training, then we're looking at over three years of runway, basically from from today. Or if you don't use the money for the next one, for the next model, the next foundational, yeah, 2026, includes model training. Yeah, that's right. Okay.
40:58And how much of the hundreds is kind
And how much of the hundreds is kind
And how much of the hundreds is kind
And how much of the hundreds is kind
41:00of allocated from other training roughly a third or more, roughly,
of allocated from other training roughly a third or more, roughly,
of allocated from other training roughly a third or more, roughly,
of allocated from other training roughly a third or more, roughly,
S Speaker 241:02yeah, roughly a third. So for reference, our first model essentially cost us $15 million in Compute and then about 10 million in data. But the data is licensed over a number of years, so we won't need to repurchase it for the next round. And in terms of just pure compute, we expect the next one to be in that kind of 20 to $30 million ballpark of compute for and we, you know, we're obviously, won't know until the machines and stuff, but we would expect that to be probably an 80 to 90 billion parameter model at its at its peak. But no plans right now. Our model is, it still has a lot of potential, so we don't expect that we'll need another model until next year, at least. We'll be quite comfortable with this one for
yeah, roughly a third. So for reference, our first model essentially cost us $15 million in Compute and then about 10 million in data. But the data is licensed over a number of years, so we won't need to repurchase it for the next round. And in terms of just pure compute, we expect the next one to be in that kind of 20 to $30 million ballpark of compute for and we, you know, we're obviously, won't know until the machines and stuff, but we would expect that to be probably an 80 to 90 billion parameter model at its at its peak. But no plans right now. Our model is, it still has a lot of potential, so we don't expect that we'll need another model until next year, at least. We'll be quite comfortable with this one for
yeah, roughly a third. So for reference, our first model essentially cost us $15 million in Compute and then about 10 million in data. But the data is licensed over a number of years, so we won't need to repurchase it for the next round. And in terms of just pure compute, we expect the next one to be in that kind of 20 to $30 million ballpark of compute for and we, you know, we're obviously, won't know until the machines and stuff, but we would expect that to be probably an 80 to 90 billion parameter model at its at its peak. But no plans right now. Our model is, it still has a lot of potential, so we don't expect that we'll need another model until next year, at least. We'll be quite comfortable with this one for
yeah, roughly a third. So for reference, our first model essentially cost us $15 million in Compute and then about 10 million in data. But the data is licensed over a number of years, so we won't need to repurchase it for the next round. And in terms of just pure compute, we expect the next one to be in that kind of 20 to $30 million ballpark of compute for and we, you know, we're obviously, won't know until the machines and stuff, but we would expect that to be probably an 80 to 90 billion parameter model at its at its peak. But no plans right now. Our model is, it still has a lot of potential, so we don't expect that we'll need another model until next year, at least. We'll be quite comfortable with this one for
S Speaker 541:36a while. Okay, so the next model potentially be almost three times big, right? Or maybe two and a half times,
a while. Okay, so the next model potentially be almost three times big, right? Or maybe two and a half times,
a while. Okay, so the next model potentially be almost three times big, right? Or maybe two and a half times,
a while. Okay, so the next model potentially be almost three times big, right? Or maybe two and a half times,
45:13with. Thank you very much.
with. Thank you very much.
with. Thank you very much.
with. Thank you very much.
S Speaker 145:17Yeah, so thanks. Name Andrew. Really enjoyed the discussion again. So thank you. We'll discuss internal we obviously have. We talked about it last time we had aspirations to we have aspirations to work with the you on both compute, which is our aipcs and mobile, and then we have some data center products as well, which would be exciting. So
Yeah, so thanks. Name Andrew. Really enjoyed the discussion again. So thank you. We'll discuss internal we obviously have. We talked about it last time we had aspirations to we have aspirations to work with the you on both compute, which is our aipcs and mobile, and then we have some data center products as well, which would be exciting. So
Yeah, so thanks. Name Andrew. Really enjoyed the discussion again. So thank you. We'll discuss internal we obviously have. We talked about it last time we had aspirations to we have aspirations to work with the you on both compute, which is our aipcs and mobile, and then we have some data center products as well, which would be exciting. So
Yeah, so thanks. Name Andrew. Really enjoyed the discussion again. So thank you. We'll discuss internal we obviously have. We talked about it last time we had aspirations to we have aspirations to work with the you on both compute, which is our aipcs and mobile, and then we have some data center products as well, which would be exciting. So
S Speaker 545:50and these, thank you very much for your time. Appreciate meeting you.
and these, thank you very much for your time. Appreciate meeting you.
and these, thank you very much for your time. Appreciate meeting you.
and these, thank you very much for your time. Appreciate meeting you.
45:54Thank you, everybody. Great to meet you.
Thank you, everybody. Great to meet you.
Thank you, everybody. Great to meet you.
Thank you, everybody. Great to meet you.
45:58Thanks. Bye, bye, yeah,
Thanks. Bye, bye, yeah,
Thanks. Bye, bye, yeah,
Thanks. Bye, bye, yeah,