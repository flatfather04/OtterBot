Meeting: Traverse - Team meeting
Fri, Jul 12, 2024
9:01 AM
30 min
Priyesh P
AI and virtual training simulatio
URL: https://otter.ai/u/fNW0qKI9Q8ENLEFMvtxfzIHR3uU
Downloaded: 2025-12-22T15:07:05.527031
Method: text_extraction
============================================================

S Speaker 10:00So I thought, hey, I'll just I'll tell you right off the bat. But But I noticed we're kind of besides like the accumulation that comes with that we are now actually we are moving pretty rapidly with some of the opportunities that we've had on the defense side. So that's exciting as well that we now have the ability to meet. We don't need to step on the gas on fundraising just yet. But we have Demo Day coming up in October and at that point, we are going to be in the same position and we're going to be fundraising and we want to start shaping that now that that goes into conversations that we're having with you today. And we, as a team have kind of talked about it. Want to get your take on a few things as well. So very excited. Why don't we do this?
So I thought, hey, I'll just I'll tell you right off the bat. But But I noticed we're kind of besides like the accumulation that comes with that we are now actually we are moving pretty rapidly with some of the opportunities that we've had on the defense side. So that's exciting as well that we now have the ability to meet. We don't need to step on the gas on fundraising just yet. But we have Demo Day coming up in October and at that point, we are going to be in the same position and we're going to be fundraising and we want to start shaping that now that that goes into conversations that we're having with you today. And we, as a team have kind of talked about it. Want to get your take on a few things as well. So very excited. Why don't we do this?
So I thought, hey, I'll just I'll tell you right off the bat. But But I noticed we're kind of besides like the accumulation that comes with that we are now actually we are moving pretty rapidly with some of the opportunities that we've had on the defense side. So that's exciting as well that we now have the ability to meet. We don't need to step on the gas on fundraising just yet. But we have Demo Day coming up in October and at that point, we are going to be in the same position and we're going to be fundraising and we want to start shaping that now that that goes into conversations that we're having with you today. And we, as a team have kind of talked about it. Want to get your take on a few things as well. So very excited. Why don't we do this?
So I thought, hey, I'll just I'll tell you right off the bat. But But I noticed we're kind of besides like the accumulation that comes with that we are now actually we are moving pretty rapidly with some of the opportunities that we've had on the defense side. So that's exciting as well that we now have the ability to meet. We don't need to step on the gas on fundraising just yet. But we have Demo Day coming up in October and at that point, we are going to be in the same position and we're going to be fundraising and we want to start shaping that now that that goes into conversations that we're having with you today. And we, as a team have kind of talked about it. Want to get your take on a few things as well. So very excited. Why don't we do this?
S Speaker 20:52For the purpose of the meeting today, since we have Albert's joined, Carlos and
For the purpose of the meeting today, since we have Albert's joined, Carlos and
For the purpose of the meeting today, since we have Albert's joined, Carlos and
For the purpose of the meeting today, since we have Albert's joined, Carlos and
S Speaker 21:02And then why don't we quickly sync on this after the call
And then why don't we quickly sync on this after the call
And then why don't we quickly sync on this after the call
And then why don't we quickly sync on this after the call
S Speaker 11:15And let me know what everybody's here we'll do introductions. Their co founder community and
And let me know what everybody's here we'll do introductions. Their co founder community and
And let me know what everybody's here we'll do introductions. Their co founder community and
And let me know what everybody's here we'll do introductions. Their co founder community and
1:42Priyesh Good to see you as well. Good to see you. James. Congrats on the speedrun Thank you
Priyesh Good to see you as well. Good to see you. James. Congrats on the speedrun Thank you
Priyesh Good to see you as well. Good to see you. James. Congrats on the speedrun Thank you
Priyesh Good to see you as well. Good to see you. James. Congrats on the speedrun Thank you
S Speaker 21:56think I think Carlos as well we can probably get started. Yeah, I just joined Mmm What was your we'll be off camera. Okay.
think I think Carlos as well we can probably get started. Yeah, I just joined Mmm What was your we'll be off camera. Okay.
think I think Carlos as well we can probably get started. Yeah, I just joined Mmm What was your we'll be off camera. Okay.
think I think Carlos as well we can probably get started. Yeah, I just joined Mmm What was your we'll be off camera. Okay.
S Speaker 22:05Alright. Thanks, Robert as well. Well, I know you have the read.
Alright. Thanks, Robert as well. Well, I know you have the read.
Alright. Thanks, Robert as well. Well, I know you have the read.
Alright. Thanks, Robert as well. Well, I know you have the read.
S Speaker 12:21You want me to record it and I can send it to you backwards. Or I can send the read as well.
You want me to record it and I can send it to you backwards. Or I can send the read as well.
You want me to record it and I can send it to you backwards. Or I can send the read as well.
You want me to record it and I can send it to you backwards. Or I can send the read as well.
2:26Have it read? Does it record the meeting as well?
S Speaker 12:29It does. So we can either do like a regular recording and I can send that out after the call. We don't have to read or I can be read and send that out as well and make you guys all viewers of that file and you can see the full video as well.
It does. So we can either do like a regular recording and I can send that out after the call. We don't have to read or I can be read and send that out as well and make you guys all viewers of that file and you can see the full video as well.
It does. So we can either do like a regular recording and I can send that out after the call. We don't have to read or I can be read and send that out as well and make you guys all viewers of that file and you can see the full video as well.
It does. So we can either do like a regular recording and I can send that out after the call. We don't have to read or I can be read and send that out as well and make you guys all viewers of that file and you can see the full video as well.
S Speaker 22:41Okay, so let's do that. Yeah, I think it sends it to me. Anyway, last time it's entered. It was pretty cool. Yeah, okay. So I think we can get started so I'll do the introductions that are and real quick. So Carlos runs North America and Latin America ventures. And then Albert is also on the US investment team. He's done a lot of tech investments as well so familiar with this space. So thanks, Carlos and Albert. Traverse Jameson Quinn. And I guess we I don't think we've met before but I've heard a lot to
Okay, so let's do that. Yeah, I think it sends it to me. Anyway, last time it's entered. It was pretty cool. Yeah, okay. So I think we can get started so I'll do the introductions that are and real quick. So Carlos runs North America and Latin America ventures. And then Albert is also on the US investment team. He's done a lot of tech investments as well so familiar with this space. So thanks, Carlos and Albert. Traverse Jameson Quinn. And I guess we I don't think we've met before but I've heard a lot to
Okay, so let's do that. Yeah, I think it sends it to me. Anyway, last time it's entered. It was pretty cool. Yeah, okay. So I think we can get started so I'll do the introductions that are and real quick. So Carlos runs North America and Latin America ventures. And then Albert is also on the US investment team. He's done a lot of tech investments as well so familiar with this space. So thanks, Carlos and Albert. Traverse Jameson Quinn. And I guess we I don't think we've met before but I've heard a lot to
Okay, so let's do that. Yeah, I think it sends it to me. Anyway, last time it's entered. It was pretty cool. Yeah, okay. So I think we can get started so I'll do the introductions that are and real quick. So Carlos runs North America and Latin America ventures. And then Albert is also on the US investment team. He's done a lot of tech investments as well so familiar with this space. So thanks, Carlos and Albert. Traverse Jameson Quinn. And I guess we I don't think we've met before but I've heard a lot to
3:24me as well. I've heard great things from you as well.
me as well. I've heard great things from you as well.
me as well. I've heard great things from you as well.
me as well. I've heard great things from you as well.
S Speaker 23:28Yeah. So I'll hand it over to you James. And then maybe go through introductions. I'd love to actually.
Yeah. So I'll hand it over to you James. And then maybe go through introductions. I'd love to actually.
Yeah. So I'll hand it over to you James. And then maybe go through introductions. I'd love to actually.
Yeah. So I'll hand it over to you James. And then maybe go through introductions. I'd love to actually.
S Speaker 13:37I think if you go through the pitch, that'd be great. Yeah, absolutely. We're looking forward to this. Also, just one thing before we get into it, I think we are you. Did you are you going to add it? Read AI? I don't think so. Okay, it says I am not the host so I can't record and I think that'd be a I actually I did wish it whoever the host is I'm just trying to record to make sure that we get this out.
I think if you go through the pitch, that'd be great. Yeah, absolutely. We're looking forward to this. Also, just one thing before we get into it, I think we are you. Did you are you going to add it? Read AI? I don't think so. Okay, it says I am not the host so I can't record and I think that'd be a I actually I did wish it whoever the host is I'm just trying to record to make sure that we get this out.
I think if you go through the pitch, that'd be great. Yeah, absolutely. We're looking forward to this. Also, just one thing before we get into it, I think we are you. Did you are you going to add it? Read AI? I don't think so. Okay, it says I am not the host so I can't record and I think that'd be a I actually I did wish it whoever the host is I'm just trying to record to make sure that we get this out.
I think if you go through the pitch, that'd be great. Yeah, absolutely. We're looking forward to this. Also, just one thing before we get into it, I think we are you. Did you are you going to add it? Read AI? I don't think so. Okay, it says I am not the host so I can't record and I think that'd be a I actually I did wish it whoever the host is I'm just trying to record to make sure that we get this out.
S Speaker 24:04I am the host that's not me. Like it was sent through RDA. So
I am the host that's not me. Like it was sent through RDA. So
I am the host that's not me. Like it was sent through RDA. So
I am the host that's not me. Like it was sent through RDA. So
S Speaker 14:13record? Yeah, it just dropped off. But that's okay. I think that we might have to go pen and paper for this one. But if everyone's looking at that I say we push. Yeah, that's fine. Let's do that. Yeah. All right. Start with introductions of myself. First, just for those who don't know. I'm James Brown, CEO of a diverse background in mechanical and Robotics Engineering. And then I was in the Marine Corps for a number of years as an infantry officer on the ground and then as an Information Operations Officer and a technical staff role, then came to Stanford I got my MBA and degree MS in symbolic systems. also worked in the Virtual Human Interaction Lab. That's where I met we I've done twin since undergrad, and we started traverse about a year ago, really in earnest. To get after ai, 3d or virtual training and simulation. I'll kick it over to WWE. For his background and then we'll do
record? Yeah, it just dropped off. But that's okay. I think that we might have to go pen and paper for this one. But if everyone's looking at that I say we push. Yeah, that's fine. Let's do that. Yeah. All right. Start with introductions of myself. First, just for those who don't know. I'm James Brown, CEO of a diverse background in mechanical and Robotics Engineering. And then I was in the Marine Corps for a number of years as an infantry officer on the ground and then as an Information Operations Officer and a technical staff role, then came to Stanford I got my MBA and degree MS in symbolic systems. also worked in the Virtual Human Interaction Lab. That's where I met we I've done twin since undergrad, and we started traverse about a year ago, really in earnest. To get after ai, 3d or virtual training and simulation. I'll kick it over to WWE. For his background and then we'll do
record? Yeah, it just dropped off. But that's okay. I think that we might have to go pen and paper for this one. But if everyone's looking at that I say we push. Yeah, that's fine. Let's do that. Yeah. All right. Start with introductions of myself. First, just for those who don't know. I'm James Brown, CEO of a diverse background in mechanical and Robotics Engineering. And then I was in the Marine Corps for a number of years as an infantry officer on the ground and then as an Information Operations Officer and a technical staff role, then came to Stanford I got my MBA and degree MS in symbolic systems. also worked in the Virtual Human Interaction Lab. That's where I met we I've done twin since undergrad, and we started traverse about a year ago, really in earnest. To get after ai, 3d or virtual training and simulation. I'll kick it over to WWE. For his background and then we'll do
record? Yeah, it just dropped off. But that's okay. I think that we might have to go pen and paper for this one. But if everyone's looking at that I say we push. Yeah, that's fine. Let's do that. Yeah. All right. Start with introductions of myself. First, just for those who don't know. I'm James Brown, CEO of a diverse background in mechanical and Robotics Engineering. And then I was in the Marine Corps for a number of years as an infantry officer on the ground and then as an Information Operations Officer and a technical staff role, then came to Stanford I got my MBA and degree MS in symbolic systems. also worked in the Virtual Human Interaction Lab. That's where I met we I've done twin since undergrad, and we started traverse about a year ago, really in earnest. To get after ai, 3d or virtual training and simulation. I'll kick it over to WWE. For his background and then we'll do
S Speaker 35:10that it's really great to meet you. We see to traverse basically been building VR apps since high school, just graduated my undergrad Master's at Stanford. My other represents block systems my master's was in computer science on the graphics track, done a bunch of research and software engineering roles. Particularly I was a co author in three computational neuroscience papers during my time at Stanford and also worked at the virtual Interaction Lab like Gibson said. Some when I run operations and business development for the team, my background I was on Capitol Hill for a number of years, both in the House and Senate as a policy advisor. Also for a Committee on Homeland Security for time. shifted to business school. After that I went to undergrad School of Management at Arizona State University. And for the last 40 plus years I've been doing enterprise account sales at a company called Velocity global. Global HR is platform company. That was one of the first record should be first enterprise account manager. They're focused on our largest accounts. And now I'm here with the team. So thanks. Nice to meet you and Albert and Carlos and good to see you.
that it's really great to meet you. We see to traverse basically been building VR apps since high school, just graduated my undergrad Master's at Stanford. My other represents block systems my master's was in computer science on the graphics track, done a bunch of research and software engineering roles. Particularly I was a co author in three computational neuroscience papers during my time at Stanford and also worked at the virtual Interaction Lab like Gibson said. Some when I run operations and business development for the team, my background I was on Capitol Hill for a number of years, both in the House and Senate as a policy advisor. Also for a Committee on Homeland Security for time. shifted to business school. After that I went to undergrad School of Management at Arizona State University. And for the last 40 plus years I've been doing enterprise account sales at a company called Velocity global. Global HR is platform company. That was one of the first record should be first enterprise account manager. They're focused on our largest accounts. And now I'm here with the team. So thanks. Nice to meet you and Albert and Carlos and good to see you.
that it's really great to meet you. We see to traverse basically been building VR apps since high school, just graduated my undergrad Master's at Stanford. My other represents block systems my master's was in computer science on the graphics track, done a bunch of research and software engineering roles. Particularly I was a co author in three computational neuroscience papers during my time at Stanford and also worked at the virtual Interaction Lab like Gibson said. Some when I run operations and business development for the team, my background I was on Capitol Hill for a number of years, both in the House and Senate as a policy advisor. Also for a Committee on Homeland Security for time. shifted to business school. After that I went to undergrad School of Management at Arizona State University. And for the last 40 plus years I've been doing enterprise account sales at a company called Velocity global. Global HR is platform company. That was one of the first record should be first enterprise account manager. They're focused on our largest accounts. And now I'm here with the team. So thanks. Nice to meet you and Albert and Carlos and good to see you.
that it's really great to meet you. We see to traverse basically been building VR apps since high school, just graduated my undergrad Master's at Stanford. My other represents block systems my master's was in computer science on the graphics track, done a bunch of research and software engineering roles. Particularly I was a co author in three computational neuroscience papers during my time at Stanford and also worked at the virtual Interaction Lab like Gibson said. Some when I run operations and business development for the team, my background I was on Capitol Hill for a number of years, both in the House and Senate as a policy advisor. Also for a Committee on Homeland Security for time. shifted to business school. After that I went to undergrad School of Management at Arizona State University. And for the last 40 plus years I've been doing enterprise account sales at a company called Velocity global. Global HR is platform company. That was one of the first record should be first enterprise account manager. They're focused on our largest accounts. And now I'm here with the team. So thanks. Nice to meet you and Albert and Carlos and good to see you.
S Speaker 46:51I always always a fan of another coin and that's it. There's not many of us where I know Oh, really? We're see we're even rarer. The single and cleanse
I always always a fan of another coin and that's it. There's not many of us where I know Oh, really? We're see we're even rarer. The single and cleanse
I always always a fan of another coin and that's it. There's not many of us where I know Oh, really? We're see we're even rarer. The single and cleanse
I always always a fan of another coin and that's it. There's not many of us where I know Oh, really? We're see we're even rarer. The single and cleanse
S Speaker 17:22No, no worries at all when I'm James CEO, we is CTO Mike Quinn is CEO of diverse. We also have two people that are not my fault. They are just full stack engineers. And then we're probably actually going to make a hire here pretty soon on the front end side to go after a few projects. That really does round out the team. I think now I could probably get into the pitch. There's a lot that I'll go through but if anyone has any questions at any time, please just stop me and throw it out. Feel free to interrupt. The everybody. Correct? Yes. Perfect. So I want to talk a little bit about mission mission critical training and simulation and this is something that is very near and dear to me. It's something that I did a lot of due to the picture there on the left side that's That's actually me with my fire support team in the Marine Corps. And in aggregate, we we trained for a combined 12 years across very highly specialized courses in order to do our jobs to do one deployment across the Middle East. And that's that's very typical. That's exactly what the military and a lot of people in mission critical industries do. If you are in a very high risk, dangerous job, you have to get training on these high risk dangerous jobs before you actually go to that event. And that includes individual training like driving maintenance, but it also includes kind of group training or cognitive training where you're doing planning and simulation, large walkthroughs of missions before you go out there, etc. And there's a spectrum of training. The top end there is actual real life training, but it's extremely expensive. It's hard to do. You only get to do it maybe once a month at max probably even once a quarter. At the low end. There's what everyone is very familiar with, to go through the educational system or what you might think of when you think of training, where it's classroom training where it's not very effective, but it is very cheap. It's very easy to do and it's pretty much the bare minimum but you have to do to transfer knowledge from one person to another and in the middle. There is this huge, very effective gap where they have some type of augmented simulation that is presenting some type of virtual environment or 3d asset to the individual, but it's really really difficult to make. I spent a lot of time actually in the simulator that you see below. That's actually fire simulator. So we would get up on top of hills and we would drop ordnance on a variety of things and I spent hundreds of hours just dropping fake bombs on fake tanks in that simulator right there. Now, the problem with that middle gap, even though it is probably the most effective is that it is extremely hard to make it's a $400 billion industry, virtual training and simulation. But right now it's really only in the realm of the large scale pilot simulators or special operations simulators. Which the one that I showed you really falls under state tax and facts. That because it's really difficult to create these virtual environments, it's very difficult to create individual assets. It takes about a week, and it's very difficult to create full scale environments as well. It takes months. And if you even want to do something like reality capture, then it takes multiple hours. It's very low quality. And typically you don't have a lot that you can do with that data. So the state of the market even as effective as it is and as large as it is, it's still operating off 10 year old tools. And defense is a huge piece of that as well. So we are technically dual use, but defense is I would say our beachhead and there's a huge opportunity here to really disrupt a lot of the large incumbents that have already made like strong revenue streams in the defense space. But like I said, they focus on the bespoke expensive simulators that are usually millions of dollars that only get a fraction of the 900,000 people that need to get trained every single year on one of 10,000 different military occupational specialties. I will send this out after as well. And we do have some analysis that you can click into here just like a row of all the contracts that have come out over the past few years and their prices what who's bidding on that who's winning them etc. And so this team is actually probably perfect for the opportunity. I started to see some of the needs for this when I was in service. When I came to Stanford. I was working in the Virtual Human Interaction Lab, we and we were building virtual environments. And we started to see this kind of Renaissance happening in AI enabled computer graphics. So these are things like nerf that's Gaussian splatting that are enabling super fast, high quality rendering at a fraction of the cost that you've had previously. And you can couple that with some of the latest advancements with multimodal models. To create these really rich interactive experiences, and really generate these 3d applications extremely quickly. And then there's nobody else that I knew in government that has more experience with the sales and operations that quit. So we formed this team last year to really see how we could go after this opportunity. And we've come a long way since then, and done a lot of really awesome pilots. And I think we've validated that need, and we validated that the the solution for this is really the ability to this platform to enable anybody to rapidly create these 3d environments in minutes. And then not only create them, but query them, understand them, and then in the future actually be able to do much more with them than they have been previously in the virtual training and simulation world. Go ahead, go ahead.
No, no worries at all when I'm James CEO, we is CTO Mike Quinn is CEO of diverse. We also have two people that are not my fault. They are just full stack engineers. And then we're probably actually going to make a hire here pretty soon on the front end side to go after a few projects. That really does round out the team. I think now I could probably get into the pitch. There's a lot that I'll go through but if anyone has any questions at any time, please just stop me and throw it out. Feel free to interrupt. The everybody. Correct? Yes. Perfect. So I want to talk a little bit about mission mission critical training and simulation and this is something that is very near and dear to me. It's something that I did a lot of due to the picture there on the left side that's That's actually me with my fire support team in the Marine Corps. And in aggregate, we we trained for a combined 12 years across very highly specialized courses in order to do our jobs to do one deployment across the Middle East. And that's that's very typical. That's exactly what the military and a lot of people in mission critical industries do. If you are in a very high risk, dangerous job, you have to get training on these high risk dangerous jobs before you actually go to that event. And that includes individual training like driving maintenance, but it also includes kind of group training or cognitive training where you're doing planning and simulation, large walkthroughs of missions before you go out there, etc. And there's a spectrum of training. The top end there is actual real life training, but it's extremely expensive. It's hard to do. You only get to do it maybe once a month at max probably even once a quarter. At the low end. There's what everyone is very familiar with, to go through the educational system or what you might think of when you think of training, where it's classroom training where it's not very effective, but it is very cheap. It's very easy to do and it's pretty much the bare minimum but you have to do to transfer knowledge from one person to another and in the middle. There is this huge, very effective gap where they have some type of augmented simulation that is presenting some type of virtual environment or 3d asset to the individual, but it's really really difficult to make. I spent a lot of time actually in the simulator that you see below. That's actually fire simulator. So we would get up on top of hills and we would drop ordnance on a variety of things and I spent hundreds of hours just dropping fake bombs on fake tanks in that simulator right there. Now, the problem with that middle gap, even though it is probably the most effective is that it is extremely hard to make it's a $400 billion industry, virtual training and simulation. But right now it's really only in the realm of the large scale pilot simulators or special operations simulators. Which the one that I showed you really falls under state tax and facts. That because it's really difficult to create these virtual environments, it's very difficult to create individual assets. It takes about a week, and it's very difficult to create full scale environments as well. It takes months. And if you even want to do something like reality capture, then it takes multiple hours. It's very low quality. And typically you don't have a lot that you can do with that data. So the state of the market even as effective as it is and as large as it is, it's still operating off 10 year old tools. And defense is a huge piece of that as well. So we are technically dual use, but defense is I would say our beachhead and there's a huge opportunity here to really disrupt a lot of the large incumbents that have already made like strong revenue streams in the defense space. But like I said, they focus on the bespoke expensive simulators that are usually millions of dollars that only get a fraction of the 900,000 people that need to get trained every single year on one of 10,000 different military occupational specialties. I will send this out after as well. And we do have some analysis that you can click into here just like a row of all the contracts that have come out over the past few years and their prices what who's bidding on that who's winning them etc. And so this team is actually probably perfect for the opportunity. I started to see some of the needs for this when I was in service. When I came to Stanford. I was working in the Virtual Human Interaction Lab, we and we were building virtual environments. And we started to see this kind of Renaissance happening in AI enabled computer graphics. So these are things like nerf that's Gaussian splatting that are enabling super fast, high quality rendering at a fraction of the cost that you've had previously. And you can couple that with some of the latest advancements with multimodal models. To create these really rich interactive experiences, and really generate these 3d applications extremely quickly. And then there's nobody else that I knew in government that has more experience with the sales and operations that quit. So we formed this team last year to really see how we could go after this opportunity. And we've come a long way since then, and done a lot of really awesome pilots. And I think we've validated that need, and we validated that the the solution for this is really the ability to this platform to enable anybody to rapidly create these 3d environments in minutes. And then not only create them, but query them, understand them, and then in the future actually be able to do much more with them than they have been previously in the virtual training and simulation world. Go ahead, go ahead.
No, no worries at all when I'm James CEO, we is CTO Mike Quinn is CEO of diverse. We also have two people that are not my fault. They are just full stack engineers. And then we're probably actually going to make a hire here pretty soon on the front end side to go after a few projects. That really does round out the team. I think now I could probably get into the pitch. There's a lot that I'll go through but if anyone has any questions at any time, please just stop me and throw it out. Feel free to interrupt. The everybody. Correct? Yes. Perfect. So I want to talk a little bit about mission mission critical training and simulation and this is something that is very near and dear to me. It's something that I did a lot of due to the picture there on the left side that's That's actually me with my fire support team in the Marine Corps. And in aggregate, we we trained for a combined 12 years across very highly specialized courses in order to do our jobs to do one deployment across the Middle East. And that's that's very typical. That's exactly what the military and a lot of people in mission critical industries do. If you are in a very high risk, dangerous job, you have to get training on these high risk dangerous jobs before you actually go to that event. And that includes individual training like driving maintenance, but it also includes kind of group training or cognitive training where you're doing planning and simulation, large walkthroughs of missions before you go out there, etc. And there's a spectrum of training. The top end there is actual real life training, but it's extremely expensive. It's hard to do. You only get to do it maybe once a month at max probably even once a quarter. At the low end. There's what everyone is very familiar with, to go through the educational system or what you might think of when you think of training, where it's classroom training where it's not very effective, but it is very cheap. It's very easy to do and it's pretty much the bare minimum but you have to do to transfer knowledge from one person to another and in the middle. There is this huge, very effective gap where they have some type of augmented simulation that is presenting some type of virtual environment or 3d asset to the individual, but it's really really difficult to make. I spent a lot of time actually in the simulator that you see below. That's actually fire simulator. So we would get up on top of hills and we would drop ordnance on a variety of things and I spent hundreds of hours just dropping fake bombs on fake tanks in that simulator right there. Now, the problem with that middle gap, even though it is probably the most effective is that it is extremely hard to make it's a $400 billion industry, virtual training and simulation. But right now it's really only in the realm of the large scale pilot simulators or special operations simulators. Which the one that I showed you really falls under state tax and facts. That because it's really difficult to create these virtual environments, it's very difficult to create individual assets. It takes about a week, and it's very difficult to create full scale environments as well. It takes months. And if you even want to do something like reality capture, then it takes multiple hours. It's very low quality. And typically you don't have a lot that you can do with that data. So the state of the market even as effective as it is and as large as it is, it's still operating off 10 year old tools. And defense is a huge piece of that as well. So we are technically dual use, but defense is I would say our beachhead and there's a huge opportunity here to really disrupt a lot of the large incumbents that have already made like strong revenue streams in the defense space. But like I said, they focus on the bespoke expensive simulators that are usually millions of dollars that only get a fraction of the 900,000 people that need to get trained every single year on one of 10,000 different military occupational specialties. I will send this out after as well. And we do have some analysis that you can click into here just like a row of all the contracts that have come out over the past few years and their prices what who's bidding on that who's winning them etc. And so this team is actually probably perfect for the opportunity. I started to see some of the needs for this when I was in service. When I came to Stanford. I was working in the Virtual Human Interaction Lab, we and we were building virtual environments. And we started to see this kind of Renaissance happening in AI enabled computer graphics. So these are things like nerf that's Gaussian splatting that are enabling super fast, high quality rendering at a fraction of the cost that you've had previously. And you can couple that with some of the latest advancements with multimodal models. To create these really rich interactive experiences, and really generate these 3d applications extremely quickly. And then there's nobody else that I knew in government that has more experience with the sales and operations that quit. So we formed this team last year to really see how we could go after this opportunity. And we've come a long way since then, and done a lot of really awesome pilots. And I think we've validated that need, and we validated that the the solution for this is really the ability to this platform to enable anybody to rapidly create these 3d environments in minutes. And then not only create them, but query them, understand them, and then in the future actually be able to do much more with them than they have been previously in the virtual training and simulation world. Go ahead, go ahead.
No, no worries at all when I'm James CEO, we is CTO Mike Quinn is CEO of diverse. We also have two people that are not my fault. They are just full stack engineers. And then we're probably actually going to make a hire here pretty soon on the front end side to go after a few projects. That really does round out the team. I think now I could probably get into the pitch. There's a lot that I'll go through but if anyone has any questions at any time, please just stop me and throw it out. Feel free to interrupt. The everybody. Correct? Yes. Perfect. So I want to talk a little bit about mission mission critical training and simulation and this is something that is very near and dear to me. It's something that I did a lot of due to the picture there on the left side that's That's actually me with my fire support team in the Marine Corps. And in aggregate, we we trained for a combined 12 years across very highly specialized courses in order to do our jobs to do one deployment across the Middle East. And that's that's very typical. That's exactly what the military and a lot of people in mission critical industries do. If you are in a very high risk, dangerous job, you have to get training on these high risk dangerous jobs before you actually go to that event. And that includes individual training like driving maintenance, but it also includes kind of group training or cognitive training where you're doing planning and simulation, large walkthroughs of missions before you go out there, etc. And there's a spectrum of training. The top end there is actual real life training, but it's extremely expensive. It's hard to do. You only get to do it maybe once a month at max probably even once a quarter. At the low end. There's what everyone is very familiar with, to go through the educational system or what you might think of when you think of training, where it's classroom training where it's not very effective, but it is very cheap. It's very easy to do and it's pretty much the bare minimum but you have to do to transfer knowledge from one person to another and in the middle. There is this huge, very effective gap where they have some type of augmented simulation that is presenting some type of virtual environment or 3d asset to the individual, but it's really really difficult to make. I spent a lot of time actually in the simulator that you see below. That's actually fire simulator. So we would get up on top of hills and we would drop ordnance on a variety of things and I spent hundreds of hours just dropping fake bombs on fake tanks in that simulator right there. Now, the problem with that middle gap, even though it is probably the most effective is that it is extremely hard to make it's a $400 billion industry, virtual training and simulation. But right now it's really only in the realm of the large scale pilot simulators or special operations simulators. Which the one that I showed you really falls under state tax and facts. That because it's really difficult to create these virtual environments, it's very difficult to create individual assets. It takes about a week, and it's very difficult to create full scale environments as well. It takes months. And if you even want to do something like reality capture, then it takes multiple hours. It's very low quality. And typically you don't have a lot that you can do with that data. So the state of the market even as effective as it is and as large as it is, it's still operating off 10 year old tools. And defense is a huge piece of that as well. So we are technically dual use, but defense is I would say our beachhead and there's a huge opportunity here to really disrupt a lot of the large incumbents that have already made like strong revenue streams in the defense space. But like I said, they focus on the bespoke expensive simulators that are usually millions of dollars that only get a fraction of the 900,000 people that need to get trained every single year on one of 10,000 different military occupational specialties. I will send this out after as well. And we do have some analysis that you can click into here just like a row of all the contracts that have come out over the past few years and their prices what who's bidding on that who's winning them etc. And so this team is actually probably perfect for the opportunity. I started to see some of the needs for this when I was in service. When I came to Stanford. I was working in the Virtual Human Interaction Lab, we and we were building virtual environments. And we started to see this kind of Renaissance happening in AI enabled computer graphics. So these are things like nerf that's Gaussian splatting that are enabling super fast, high quality rendering at a fraction of the cost that you've had previously. And you can couple that with some of the latest advancements with multimodal models. To create these really rich interactive experiences, and really generate these 3d applications extremely quickly. And then there's nobody else that I knew in government that has more experience with the sales and operations that quit. So we formed this team last year to really see how we could go after this opportunity. And we've come a long way since then, and done a lot of really awesome pilots. And I think we've validated that need, and we validated that the the solution for this is really the ability to this platform to enable anybody to rapidly create these 3d environments in minutes. And then not only create them, but query them, understand them, and then in the future actually be able to do much more with them than they have been previously in the virtual training and simulation world. Go ahead, go ahead.
S Speaker 513:21So in order to create this 3d model
13:28what are the inputs you'd have to get and then
S Speaker 513:34and how, how automated or how bad you can generate some goodwill or invest in a company with no matter for the basically goes to how to take videos
and how, how automated or how bad you can generate some goodwill or invest in a company with no matter for the basically goes to how to take videos
and how, how automated or how bad you can generate some goodwill or invest in a company with no matter for the basically goes to how to take videos
and how, how automated or how bad you can generate some goodwill or invest in a company with no matter for the basically goes to how to take videos
S Speaker 514:02So in your scenario, by assumption is similar input you will get maybe different
So in your scenario, by assumption is similar input you will get maybe different
So in your scenario, by assumption is similar input you will get maybe different
So in your scenario, by assumption is similar input you will get maybe different
S Speaker 114:10understand that? No, totally. So right now we just use RGB video. So you still have to go through a proper structure for motion pipeline and ensure that you're capturing enough information with with the video I'm getting it from a proper amount of angles, so you don't get floaters or artifacts. But it really is just any camera that you would want actually doesn't really correlate too well to the quality of the camera resolution. You can call it correlates more to the number of images that you get. It's actually perfectly suited for somebody with a smartphone on the ground. And I'll show you a demo of that here in a second, or a drone in the air as long as they can get proper coverage of the of the scene. And then in terms of time, it's actually extremely fast, probably about 20 to 30 minutes for a large scale scene and less or smaller scene. And on top of that there's a lot of research that's come out that supports the ability to do this, well under five minutes, even even 90 seconds, and I can show that research here at the end as well if you're still interested, and then go ahead.
understand that? No, totally. So right now we just use RGB video. So you still have to go through a proper structure for motion pipeline and ensure that you're capturing enough information with with the video I'm getting it from a proper amount of angles, so you don't get floaters or artifacts. But it really is just any camera that you would want actually doesn't really correlate too well to the quality of the camera resolution. You can call it correlates more to the number of images that you get. It's actually perfectly suited for somebody with a smartphone on the ground. And I'll show you a demo of that here in a second, or a drone in the air as long as they can get proper coverage of the of the scene. And then in terms of time, it's actually extremely fast, probably about 20 to 30 minutes for a large scale scene and less or smaller scene. And on top of that there's a lot of research that's come out that supports the ability to do this, well under five minutes, even even 90 seconds, and I can show that research here at the end as well if you're still interested, and then go ahead.
understand that? No, totally. So right now we just use RGB video. So you still have to go through a proper structure for motion pipeline and ensure that you're capturing enough information with with the video I'm getting it from a proper amount of angles, so you don't get floaters or artifacts. But it really is just any camera that you would want actually doesn't really correlate too well to the quality of the camera resolution. You can call it correlates more to the number of images that you get. It's actually perfectly suited for somebody with a smartphone on the ground. And I'll show you a demo of that here in a second, or a drone in the air as long as they can get proper coverage of the of the scene. And then in terms of time, it's actually extremely fast, probably about 20 to 30 minutes for a large scale scene and less or smaller scene. And on top of that there's a lot of research that's come out that supports the ability to do this, well under five minutes, even even 90 seconds, and I can show that research here at the end as well if you're still interested, and then go ahead.
understand that? No, totally. So right now we just use RGB video. So you still have to go through a proper structure for motion pipeline and ensure that you're capturing enough information with with the video I'm getting it from a proper amount of angles, so you don't get floaters or artifacts. But it really is just any camera that you would want actually doesn't really correlate too well to the quality of the camera resolution. You can call it correlates more to the number of images that you get. It's actually perfectly suited for somebody with a smartphone on the ground. And I'll show you a demo of that here in a second, or a drone in the air as long as they can get proper coverage of the of the scene. And then in terms of time, it's actually extremely fast, probably about 20 to 30 minutes for a large scale scene and less or smaller scene. And on top of that there's a lot of research that's come out that supports the ability to do this, well under five minutes, even even 90 seconds, and I can show that research here at the end as well if you're still interested, and then go ahead.
S Speaker 215:16if you explain the difference from a technology perspective between photogrammetry traditional ways of doing Yeah Could be worse is worse is gotcha and splatting so like one level, deeper, not like super deep at one level deeper. In technology would
if you explain the difference from a technology perspective between photogrammetry traditional ways of doing Yeah Could be worse is worse is gotcha and splatting so like one level, deeper, not like super deep at one level deeper. In technology would
if you explain the difference from a technology perspective between photogrammetry traditional ways of doing Yeah Could be worse is worse is gotcha and splatting so like one level, deeper, not like super deep at one level deeper. In technology would
if you explain the difference from a technology perspective between photogrammetry traditional ways of doing Yeah Could be worse is worse is gotcha and splatting so like one level, deeper, not like super deep at one level deeper. In technology would
S Speaker 115:35be absolutely this is my favorite thing. I'll go one level deeper. And if we need to get to level deeper or more trouble, we'll bring it for support. But I know it's a great question and it really it kind of underpins why this is why this is such a renaissance as I said before, so in traditional photogrammetry pipelines you are you are it's a very procedural operation where you are taking the pixels in this image and you're finding lines and edges and you're taking the pixels in this image, you're getting the depth and then from those two, those three points really that you've extrapolated, you can now try and build a 3d scene out of that using something like if you're familiar with like a truncated signed distance function, but you need that triangle between those points and you need proper overlap between those images to do that properly. You still do need that's the structure for motion pipeline or that like if that's a piece of it that you have with this. You still need to get those viewpoints, you still need to ensure that there's key features that align between both of those images. But in this instance, both for nerf and Gaussian splatting you're actually running it through an neural net. And then from that you're actually trying to almost back solve and have those of your training images and you're trying to get it to reproduce that 3d asset that matches those training images. But it also extends to novel views. And that's why you hear a lot about novel view synthesis is what you're what you're doing with these technologies. Nerf and Gosselins dividing are pretty radically different in their approaches, but they each have different advantages. nerf is actually you're looking really at only a 2d image. And every time you change the location of the camera, you are running inference on the model and it's presenting a new 2d image to you from that neural net and Gaussians planning. It's a little bit different they're actually they take these Gaussians like people ovals, similar to a point cloud, and they use the training images to optimize the location and color of those Gaussians to reproduce that 3d image and so they're splitting Gaussians they are changing the color Gaussians in the configuration and I'll actually point that out in one of the demos that we have until they get to this beautiful it's similar to a point file, but it's not quite at one file called a splat. That is a full reconstruction of that 3d scene, just based off of images alone. So in summary, one of the biggest reasons why this is such an upgrade is because we're now using all the high powered compute that we have been using for AI, your graphics with GPUs, and we're directly applying it to the photogrammetry process in a way that it wasn't being applied before if that makes sense.
be absolutely this is my favorite thing. I'll go one level deeper. And if we need to get to level deeper or more trouble, we'll bring it for support. But I know it's a great question and it really it kind of underpins why this is why this is such a renaissance as I said before, so in traditional photogrammetry pipelines you are you are it's a very procedural operation where you are taking the pixels in this image and you're finding lines and edges and you're taking the pixels in this image, you're getting the depth and then from those two, those three points really that you've extrapolated, you can now try and build a 3d scene out of that using something like if you're familiar with like a truncated signed distance function, but you need that triangle between those points and you need proper overlap between those images to do that properly. You still do need that's the structure for motion pipeline or that like if that's a piece of it that you have with this. You still need to get those viewpoints, you still need to ensure that there's key features that align between both of those images. But in this instance, both for nerf and Gaussian splatting you're actually running it through an neural net. And then from that you're actually trying to almost back solve and have those of your training images and you're trying to get it to reproduce that 3d asset that matches those training images. But it also extends to novel views. And that's why you hear a lot about novel view synthesis is what you're what you're doing with these technologies. Nerf and Gosselins dividing are pretty radically different in their approaches, but they each have different advantages. nerf is actually you're looking really at only a 2d image. And every time you change the location of the camera, you are running inference on the model and it's presenting a new 2d image to you from that neural net and Gaussians planning. It's a little bit different they're actually they take these Gaussians like people ovals, similar to a point cloud, and they use the training images to optimize the location and color of those Gaussians to reproduce that 3d image and so they're splitting Gaussians they are changing the color Gaussians in the configuration and I'll actually point that out in one of the demos that we have until they get to this beautiful it's similar to a point file, but it's not quite at one file called a splat. That is a full reconstruction of that 3d scene, just based off of images alone. So in summary, one of the biggest reasons why this is such an upgrade is because we're now using all the high powered compute that we have been using for AI, your graphics with GPUs, and we're directly applying it to the photogrammetry process in a way that it wasn't being applied before if that makes sense.
be absolutely this is my favorite thing. I'll go one level deeper. And if we need to get to level deeper or more trouble, we'll bring it for support. But I know it's a great question and it really it kind of underpins why this is why this is such a renaissance as I said before, so in traditional photogrammetry pipelines you are you are it's a very procedural operation where you are taking the pixels in this image and you're finding lines and edges and you're taking the pixels in this image, you're getting the depth and then from those two, those three points really that you've extrapolated, you can now try and build a 3d scene out of that using something like if you're familiar with like a truncated signed distance function, but you need that triangle between those points and you need proper overlap between those images to do that properly. You still do need that's the structure for motion pipeline or that like if that's a piece of it that you have with this. You still need to get those viewpoints, you still need to ensure that there's key features that align between both of those images. But in this instance, both for nerf and Gaussian splatting you're actually running it through an neural net. And then from that you're actually trying to almost back solve and have those of your training images and you're trying to get it to reproduce that 3d asset that matches those training images. But it also extends to novel views. And that's why you hear a lot about novel view synthesis is what you're what you're doing with these technologies. Nerf and Gosselins dividing are pretty radically different in their approaches, but they each have different advantages. nerf is actually you're looking really at only a 2d image. And every time you change the location of the camera, you are running inference on the model and it's presenting a new 2d image to you from that neural net and Gaussians planning. It's a little bit different they're actually they take these Gaussians like people ovals, similar to a point cloud, and they use the training images to optimize the location and color of those Gaussians to reproduce that 3d image and so they're splitting Gaussians they are changing the color Gaussians in the configuration and I'll actually point that out in one of the demos that we have until they get to this beautiful it's similar to a point file, but it's not quite at one file called a splat. That is a full reconstruction of that 3d scene, just based off of images alone. So in summary, one of the biggest reasons why this is such an upgrade is because we're now using all the high powered compute that we have been using for AI, your graphics with GPUs, and we're directly applying it to the photogrammetry process in a way that it wasn't being applied before if that makes sense.
be absolutely this is my favorite thing. I'll go one level deeper. And if we need to get to level deeper or more trouble, we'll bring it for support. But I know it's a great question and it really it kind of underpins why this is why this is such a renaissance as I said before, so in traditional photogrammetry pipelines you are you are it's a very procedural operation where you are taking the pixels in this image and you're finding lines and edges and you're taking the pixels in this image, you're getting the depth and then from those two, those three points really that you've extrapolated, you can now try and build a 3d scene out of that using something like if you're familiar with like a truncated signed distance function, but you need that triangle between those points and you need proper overlap between those images to do that properly. You still do need that's the structure for motion pipeline or that like if that's a piece of it that you have with this. You still need to get those viewpoints, you still need to ensure that there's key features that align between both of those images. But in this instance, both for nerf and Gaussian splatting you're actually running it through an neural net. And then from that you're actually trying to almost back solve and have those of your training images and you're trying to get it to reproduce that 3d asset that matches those training images. But it also extends to novel views. And that's why you hear a lot about novel view synthesis is what you're what you're doing with these technologies. Nerf and Gosselins dividing are pretty radically different in their approaches, but they each have different advantages. nerf is actually you're looking really at only a 2d image. And every time you change the location of the camera, you are running inference on the model and it's presenting a new 2d image to you from that neural net and Gaussians planning. It's a little bit different they're actually they take these Gaussians like people ovals, similar to a point cloud, and they use the training images to optimize the location and color of those Gaussians to reproduce that 3d image and so they're splitting Gaussians they are changing the color Gaussians in the configuration and I'll actually point that out in one of the demos that we have until they get to this beautiful it's similar to a point file, but it's not quite at one file called a splat. That is a full reconstruction of that 3d scene, just based off of images alone. So in summary, one of the biggest reasons why this is such an upgrade is because we're now using all the high powered compute that we have been using for AI, your graphics with GPUs, and we're directly applying it to the photogrammetry process in a way that it wasn't being applied before if that makes sense.
S Speaker 518:23me ask you this I'm not familiar with but
S Speaker 520:04and there's open source models, right. So you have companies like metaphor could leverage that same day, if you will, government providers, do similar things. Begin to understand what's unique about what you're doing, leveraging government, other company models.
and there's open source models, right. So you have companies like metaphor could leverage that same day, if you will, government providers, do similar things. Begin to understand what's unique about what you're doing, leveraging government, other company models.
and there's open source models, right. So you have companies like metaphor could leverage that same day, if you will, government providers, do similar things. Begin to understand what's unique about what you're doing, leveraging government, other company models.
and there's open source models, right. So you have companies like metaphor could leverage that same day, if you will, government providers, do similar things. Begin to understand what's unique about what you're doing, leveraging government, other company models.
S Speaker 529:35like in one of the example like Dallas Fort Worth, yeah.
like in one of the example like Dallas Fort Worth, yeah.
like in one of the example like Dallas Fort Worth, yeah.
like in one of the example like Dallas Fort Worth, yeah.