Meeting: Kindo #2 + Deepak
Tue, Aug 12
9:59 AM
1 h 21 min
Priyesh P
Reconnecting and Introductions
2:08
Conne
URL: https://otter.ai/u/Vg4ciTgYuUYqOZR2LRtA6gXETpc
Downloaded: 2025-12-21T21:21:41.126500
Method: text_extraction
============================================================

2:08I Hey, Priyesh, how you doing? Hey,
I Hey, Priyesh, how you doing? Hey,
I Hey, Priyesh, how you doing? Hey,
I Hey, Priyesh, how you doing? Hey,
S Speaker 13:13Hi Ron. Good to meet you again. How's it going? Good, good.
Hi Ron. Good to meet you again. How's it going? Good, good.
Hi Ron. Good to meet you again. How's it going? Good, good.
Hi Ron. Good to meet you again. How's it going? Good, good.
3:20You guys having a good day? Yeah,
You guys having a good day? Yeah,
You guys having a good day? Yeah,
You guys having a good day? Yeah,
S Speaker 13:21it's been good so far. I think it's almost been a month since we last spoke. So good to reconnect, Ron, I know Deepak would be joining any moment. Let me just give him a quick ping. All
it's been good so far. I think it's almost been a month since we last spoke. So good to reconnect, Ron, I know Deepak would be joining any moment. Let me just give him a quick ping. All
it's been good so far. I think it's almost been a month since we last spoke. So good to reconnect, Ron, I know Deepak would be joining any moment. Let me just give him a quick ping. All
it's been good so far. I think it's almost been a month since we last spoke. So good to reconnect, Ron, I know Deepak would be joining any moment. Let me just give him a quick ping. All
3:34right. Yeah, sounds good. No problem.
right. Yeah, sounds good. No problem.
right. Yeah, sounds good. No problem.
right. Yeah, sounds good. No problem.
S Speaker 13:36And was Deepak able to meet you, Ron, or just the CTO? I um,
And was Deepak able to meet you, Ron, or just the CTO? I um,
And was Deepak able to meet you, Ron, or just the CTO? I um,
And was Deepak able to meet you, Ron, or just the CTO? I um,
3:44I think just, yeah, I think I was off
I think just, yeah, I think I was off
I think just, yeah, I think I was off
I think just, yeah, I think I was off
S Speaker 23:49in another meeting. Okay, okay, but when I caught up with Brian or CTA, they'll be on just a minute. The he
in another meeting. Okay, okay, but when I caught up with Brian or CTA, they'll be on just a minute. The he
in another meeting. Okay, okay, but when I caught up with Brian or CTA, they'll be on just a minute. The he
in another meeting. Okay, okay, but when I caught up with Brian or CTA, they'll be on just a minute. The he
4:01mentioned someone from Paul kind of dropped by.
mentioned someone from Paul kind of dropped by.
mentioned someone from Paul kind of dropped by.
mentioned someone from Paul kind of dropped by.
S Speaker 24:04So that'd be great to let you know we made a lot of interest. It was really good black
So that'd be great to let you know we made a lot of interest. It was really good black
So that'd be great to let you know we made a lot of interest. It was really good black
So that'd be great to let you know we made a lot of interest. It was really good black
4:10hat for us. Yeah, I can imagine walk away
hat for us. Yeah, I can imagine walk away
hat for us. Yeah, I can imagine walk away
hat for us. Yeah, I can imagine walk away
4:13with almost 600 leads and
with almost 600 leads and
with almost 600 leads and
with almost 600 leads and
S Speaker 24:18nine follow up meetings, things like that too. So, oh, good feedback from the from the crowd and even some of the vendors dropping by. So it was good,
nine follow up meetings, things like that too. So, oh, good feedback from the from the crowd and even some of the vendors dropping by. So it was good,
nine follow up meetings, things like that too. So, oh, good feedback from the from the crowd and even some of the vendors dropping by. So it was good,
nine follow up meetings, things like that too. So, oh, good feedback from the from the crowd and even some of the vendors dropping by. So it was good,
S Speaker 14:30very, very interesting one that's, that's good to know. Yeah, Deepak and I, we have been sinking on the space, trying to sort of work. And yeah, Deepak has joined as well. So he, came back with some good impressions of the team as well.
very, very interesting one that's, that's good to know. Yeah, Deepak and I, we have been sinking on the space, trying to sort of work. And yeah, Deepak has joined as well. So he, came back with some good impressions of the team as well.
very, very interesting one that's, that's good to know. Yeah, Deepak and I, we have been sinking on the space, trying to sort of work. And yeah, Deepak has joined as well. So he, came back with some good impressions of the team as well.
very, very interesting one that's, that's good to know. Yeah, Deepak and I, we have been sinking on the space, trying to sort of work. And yeah, Deepak has joined as well. So he, came back with some good impressions of the team as well.
4:46Yeah, they did. Hey guys, can you hear
Yeah, they did. Hey guys, can you hear
Yeah, they did. Hey guys, can you hear
Yeah, they did. Hey guys, can you hear
4:52Just give me a minute.
Just give me a minute.
Just give me a minute.
Just give me a minute.
4:55I'm having some connectivity issues. Just give me a minute.
I'm having some connectivity issues. Just give me a minute.
I'm having some connectivity issues. Just give me a minute.
I'm having some connectivity issues. Just give me a minute.
S Speaker 34:57Yeah, no problem. I
S Speaker 15:12So Ron Brian, just a quick intro. Deepak is a director at Qualcomm ventures, and he leads our DevOps investment thesis. I'll let him introduce himself, but that's a quick primer. I am a Senior Associate. I'm working with Deepak on some of our DevOps investments right now. Hey,
So Ron Brian, just a quick intro. Deepak is a director at Qualcomm ventures, and he leads our DevOps investment thesis. I'll let him introduce himself, but that's a quick primer. I am a Senior Associate. I'm working with Deepak on some of our DevOps investments right now. Hey,
So Ron Brian, just a quick intro. Deepak is a director at Qualcomm ventures, and he leads our DevOps investment thesis. I'll let him introduce himself, but that's a quick primer. I am a Senior Associate. I'm working with Deepak on some of our DevOps investments right now. Hey,
So Ron Brian, just a quick intro. Deepak is a director at Qualcomm ventures, and he leads our DevOps investment thesis. I'll let him introduce himself, but that's a quick primer. I am a Senior Associate. I'm working with Deepak on some of our DevOps investments right now. Hey,
5:29Brian, good to meet you. Hey, nice to meet you.
Brian, good to meet you. Hey, nice to meet you.
Brian, good to meet you. Hey, nice to meet you.
Brian, good to meet you. Hey, nice to meet you.
S Speaker 45:36I think Deepak I met at the conference, right?
I think Deepak I met at the conference, right?
I think Deepak I met at the conference, right?
I think Deepak I met at the conference, right?
S Speaker 15:41Yeah. Deepak is having some connectivity issues. They'll be joining in a minute, just rethinking. Yes, no problem.
Yeah. Deepak is having some connectivity issues. They'll be joining in a minute, just rethinking. Yes, no problem.
Yeah. Deepak is having some connectivity issues. They'll be joining in a minute, just rethinking. Yes, no problem.
Yeah. Deepak is having some connectivity issues. They'll be joining in a minute, just rethinking. Yes, no problem.
S Speaker 25:49I was at a startup as employee. We were trying to fix this problem. It's a hard problem to fix.
I was at a startup as employee. We were trying to fix this problem. It's a hard problem to fix.
I was at a startup as employee. We were trying to fix this problem. It's a hard problem to fix.
I was at a startup as employee. We were trying to fix this problem. It's a hard problem to fix.
S Speaker 15:55Yeah, it's so unexpected. Just happens every time, and then we use Microsoft teams internally, but we've been very, very to not use teams for external coming external meetings. But seems like zoom is also having issues now.
Yeah, it's so unexpected. Just happens every time, and then we use Microsoft teams internally, but we've been very, very to not use teams for external coming external meetings. But seems like zoom is also having issues now.
Yeah, it's so unexpected. Just happens every time, and then we use Microsoft teams internally, but we've been very, very to not use teams for external coming external meetings. But seems like zoom is also having issues now.
Yeah, it's so unexpected. Just happens every time, and then we use Microsoft teams internally, but we've been very, very to not use teams for external coming external meetings. But seems like zoom is also having issues now.
S Speaker 26:10Yeah, yeah. I was I had to change devices yesterday to get on that Zoom call. It was frustrating.
Yeah, yeah. I was I had to change devices yesterday to get on that Zoom call. It was frustrating.
Yeah, yeah. I was I had to change devices yesterday to get on that Zoom call. It was frustrating.
Yeah, yeah. I was I had to change devices yesterday to get on that Zoom call. It was frustrating.
6:19And Brian, Ron, where are you guys dining from?
And Brian, Ron, where are you guys dining from?
And Brian, Ron, where are you guys dining from?
And Brian, Ron, where are you guys dining from?
S Speaker 26:23We're in Venice, California, next to Santa Monica. Oh, right, yes, we have an office here. Most people work out the office. We've got a few sales people kind of different places around the US, but almost all of us are here in Venice.
We're in Venice, California, next to Santa Monica. Oh, right, yes, we have an office here. Most people work out the office. We've got a few sales people kind of different places around the US, but almost all of us are here in Venice.
We're in Venice, California, next to Santa Monica. Oh, right, yes, we have an office here. Most people work out the office. We've got a few sales people kind of different places around the US, but almost all of us are here in Venice.
We're in Venice, California, next to Santa Monica. Oh, right, yes, we have an office here. Most people work out the office. We've got a few sales people kind of different places around the US, but almost all of us are here in Venice.
6:39Are you guys out of San Diego, or where you guys have
Are you guys out of San Diego, or where you guys have
Are you guys out of San Diego, or where you guys have
Are you guys out of San Diego, or where you guys have
S Speaker 16:41so our team is split between Bay Area and San Diego. Both Deepak and I, we are in the Bay Area, roughly about eight members Team Six here in the Bay Area and couple in San Diego.
so our team is split between Bay Area and San Diego. Both Deepak and I, we are in the Bay Area, roughly about eight members Team Six here in the Bay Area and couple in San Diego.
so our team is split between Bay Area and San Diego. Both Deepak and I, we are in the Bay Area, roughly about eight members Team Six here in the Bay Area and couple in San Diego.
so our team is split between Bay Area and San Diego. Both Deepak and I, we are in the Bay Area, roughly about eight members Team Six here in the Bay Area and couple in San Diego.
7:08Well said we didn't really get DEF CON also
Well said we didn't really get DEF CON also
Well said we didn't really get DEF CON also
Well said we didn't really get DEF CON also
S Speaker 27:13DEF CON over the convention center. It's a much better location for it. It's not as crowded. You can move through all the different villages and things. It's, it's, there's some great corporate sponsorships going on. So some really cool villages now, yeah, somebody brought a satellite in to hack on in the aerospace one. Hey guys, can you?
DEF CON over the convention center. It's a much better location for it. It's not as crowded. You can move through all the different villages and things. It's, it's, there's some great corporate sponsorships going on. So some really cool villages now, yeah, somebody brought a satellite in to hack on in the aerospace one. Hey guys, can you?
DEF CON over the convention center. It's a much better location for it. It's not as crowded. You can move through all the different villages and things. It's, it's, there's some great corporate sponsorships going on. So some really cool villages now, yeah, somebody brought a satellite in to hack on in the aerospace one. Hey guys, can you?
DEF CON over the convention center. It's a much better location for it. It's not as crowded. You can move through all the different villages and things. It's, it's, there's some great corporate sponsorships going on. So some really cool villages now, yeah, somebody brought a satellite in to hack on in the aerospace one. Hey guys, can you?
7:38Can you guys hear me now? Yeah,
Can you guys hear me now? Yeah,
Can you guys hear me now? Yeah,
Can you guys hear me now? Yeah,
S Speaker 57:42yeah. So while it gets why my desktop gets resolved for zoom, like Priyesh mentioned, we don't usually use Zoom as often. I don't know why it rarely happens, but when it happens, it's just very stubborn. But I'm glad we could connect. What we can do is maybe get started with the discussion, and I'll join on video, and maybe in a few minutes, hopefully. But I met you guys at Black Hat, and I think Priyesh was also communicating with you on the side, and I think I kind of liked what I saw. And, you know, we went through some of the case studies and some use cases that, you know, you guys tailor to. Maybe what will be good is, you know, getting started with a quick introduction, and then talking about the genesis of the company and where you guys are in terms of traction. And maybe let's take a couple of customers that are currently using Kindle and how they're leveraging it in production today. So I think maybe that's a good agenda, but happy to take it in whichever direction you guys want.
yeah. So while it gets why my desktop gets resolved for zoom, like Priyesh mentioned, we don't usually use Zoom as often. I don't know why it rarely happens, but when it happens, it's just very stubborn. But I'm glad we could connect. What we can do is maybe get started with the discussion, and I'll join on video, and maybe in a few minutes, hopefully. But I met you guys at Black Hat, and I think Priyesh was also communicating with you on the side, and I think I kind of liked what I saw. And, you know, we went through some of the case studies and some use cases that, you know, you guys tailor to. Maybe what will be good is, you know, getting started with a quick introduction, and then talking about the genesis of the company and where you guys are in terms of traction. And maybe let's take a couple of customers that are currently using Kindle and how they're leveraging it in production today. So I think maybe that's a good agenda, but happy to take it in whichever direction you guys want.
yeah. So while it gets why my desktop gets resolved for zoom, like Priyesh mentioned, we don't usually use Zoom as often. I don't know why it rarely happens, but when it happens, it's just very stubborn. But I'm glad we could connect. What we can do is maybe get started with the discussion, and I'll join on video, and maybe in a few minutes, hopefully. But I met you guys at Black Hat, and I think Priyesh was also communicating with you on the side, and I think I kind of liked what I saw. And, you know, we went through some of the case studies and some use cases that, you know, you guys tailor to. Maybe what will be good is, you know, getting started with a quick introduction, and then talking about the genesis of the company and where you guys are in terms of traction. And maybe let's take a couple of customers that are currently using Kindle and how they're leveraging it in production today. So I think maybe that's a good agenda, but happy to take it in whichever direction you guys want.
yeah. So while it gets why my desktop gets resolved for zoom, like Priyesh mentioned, we don't usually use Zoom as often. I don't know why it rarely happens, but when it happens, it's just very stubborn. But I'm glad we could connect. What we can do is maybe get started with the discussion, and I'll join on video, and maybe in a few minutes, hopefully. But I met you guys at Black Hat, and I think Priyesh was also communicating with you on the side, and I think I kind of liked what I saw. And, you know, we went through some of the case studies and some use cases that, you know, you guys tailor to. Maybe what will be good is, you know, getting started with a quick introduction, and then talking about the genesis of the company and where you guys are in terms of traction. And maybe let's take a couple of customers that are currently using Kindle and how they're leveraging it in production today. So I think maybe that's a good agenda, but happy to take it in whichever direction you guys want.
8:53Yeah, yeah. Happy to do
Yeah, yeah. Happy to do
Yeah, yeah. Happy to do
Yeah, yeah. Happy to do
S Speaker 28:59it the Brian. Do you have any particular questions about particular questions about
it the Brian. Do you have any particular questions about particular questions about
it the Brian. Do you have any particular questions about particular questions about
it the Brian. Do you have any particular questions about particular questions about
9:06welcome in terms of, like, welcome ventures?
welcome in terms of, like, welcome ventures?
welcome in terms of, like, welcome ventures?
welcome in terms of, like, welcome ventures?
S Speaker 49:11I mean, yeah, I'm just gonna, I guess maybe the thing I'd be curious is, just like, like others, particular types of companies that you like to invest in, particularly stage, what do you what do you really look for?
I mean, yeah, I'm just gonna, I guess maybe the thing I'd be curious is, just like, like others, particular types of companies that you like to invest in, particularly stage, what do you what do you really look for?
I mean, yeah, I'm just gonna, I guess maybe the thing I'd be curious is, just like, like others, particular types of companies that you like to invest in, particularly stage, what do you what do you really look for?
I mean, yeah, I'm just gonna, I guess maybe the thing I'd be curious is, just like, like others, particular types of companies that you like to invest in, particularly stage, what do you what do you really look for?
S Speaker 212:29from there. Okay, great. Any questions? Brian,
from there. Okay, great. Any questions? Brian,
from there. Okay, great. Any questions? Brian,
from there. Okay, great. Any questions? Brian,
13:06but we go through that with piyysh
but we go through that with piyysh
but we go through that with piyysh
but we go through that with piyysh
13:09And yeah, however you guys wanna take it
And yeah, however you guys wanna take it
And yeah, however you guys wanna take it
And yeah, however you guys wanna take it
S Speaker 431:39of like, the take that I got, because I got a lot of, like, demos from, from a bunch of them during the, I could
of like, the take that I got, because I got a lot of, like, demos from, from a bunch of them during the, I could
of like, the take that I got, because I got a lot of, like, demos from, from a bunch of them during the, I could
of like, the take that I got, because I got a lot of, like, demos from, from a bunch of them during the, I could
31:45imagine, right? Yeah. I mean, we kind of
imagine, right? Yeah. I mean, we kind of
imagine, right? Yeah. I mean, we kind of
imagine, right? Yeah. I mean, we kind of
S Speaker 431:47did a lot of recon when we were at Black Hat, just kind of see where the competition is. And I did the same thing RSA, yeah, you know, tying splint ops work. They really hadn't changed much since, since we got the demos at RSA, right? You know, really, I mean, as you've probably seen in their demos, like very heavy like UI based workflow solutions, kind of focusing on soar, you know, incident, incident response type of and so especially for dink, blink ops and torque. But names as well, they're trying to be more generic, kind of like Zapier, like Ron mentioned. But, you know, one of the things that that I really was trying to get at from them is like, Do you have a true agentic capability within this like, is it all about these static workflows, or is there a way for the agent to determine its path on its own, given, like, the user prompt, right? They have a way to do it, but the way that you have to construct it in their environment is, frankly, a lot of work. Like you have to, like, you can't call arbitrary tools like you would be able to in our system. So in our system, they've got a fleet of MCP servers. Each of those MCP servers exposes a number of tools. Each of those tools is like, a specific action you would take, like, list out my JIRA tickets, or, you know, pull the file from GitHub, or, you know, query this metric in Datadog. In their systems, they kind of come from this static store soar, perspective and workflow focus. So when they incorporated these agentic capabilities into it, they effectively become like one step in these broader like static workflows, those agents, the tools that are given access to, are essentially a set of the predefined workflows that they have elsewhere in the products, which should either be like templates or you build yourself. But the problem is that, like, those things all have to be either handcrafted by the end user to, like, run these specific workflows, or, if you go from their template ones, they tend to be these really complicated workflows that are like 30 different steps, like, you know, if then statements and loops and all that. So, you know, it's just a much more complicated product. The other thing that they don't have is any way to drive it from chat like a terminal. And I think, I think where we believe the world is going is certainly like these automations that do happen in the background. Yeah, without having this sort of way for a human to kind of drive these AI agents dynamically, it's it. You gotta, gotta meet the user where they're at. Like, there's, there's a trust that has to be built up in, like, what are you gonna allow this agent to do? So we definitely believe in a human in the loop. So when we're talking about, like, driving these, like, tool calling from chat, humans, like, proving, like, individual actions, like, yes, you're allowed to do this one, just this one time, or is, and as the human builds out that that trust and like, yes, this, this sequence that I just ran, this is like a playbook that I want to institutionalize into an agent background. I'm now comfortable with it taking these actions. That's where you would like translate that into an agent that runs in the background. But there's no way to kind of do that ephemeral, dynamic aspect from it, from their products, you kind of have to go straight to these agents, which is part of the reason why they wanted to put so many guardrails on it. So it just makes it for a much more you know, I think painstaking process, because they come from that static approach, whereas we've kind of taken it, you know, we're from the grassroots, AI native from the get go, so we're kind of taking a different approach, which is like, less focus on the static workflow at build time, and more on what the agent can do at runtime. Tools, put some policy guardrails around it, but then let it go, and then you'll see, as a human like, what it's able to do. Well, maybe you need to tweak your prompt, maybe give guidance. And so what we what we eventually find as like really high quality agents that run reliably, is you could put like a playbook, as long as you're explicit in what you want to get done, and you can refine how explicit you need to be, but once you define that runbook and you put it in an agent, now this thing can actually accomplish these these tasks with pretty high reliability, and they're pretty resilient to failures,
did a lot of recon when we were at Black Hat, just kind of see where the competition is. And I did the same thing RSA, yeah, you know, tying splint ops work. They really hadn't changed much since, since we got the demos at RSA, right? You know, really, I mean, as you've probably seen in their demos, like very heavy like UI based workflow solutions, kind of focusing on soar, you know, incident, incident response type of and so especially for dink, blink ops and torque. But names as well, they're trying to be more generic, kind of like Zapier, like Ron mentioned. But, you know, one of the things that that I really was trying to get at from them is like, Do you have a true agentic capability within this like, is it all about these static workflows, or is there a way for the agent to determine its path on its own, given, like, the user prompt, right? They have a way to do it, but the way that you have to construct it in their environment is, frankly, a lot of work. Like you have to, like, you can't call arbitrary tools like you would be able to in our system. So in our system, they've got a fleet of MCP servers. Each of those MCP servers exposes a number of tools. Each of those tools is like, a specific action you would take, like, list out my JIRA tickets, or, you know, pull the file from GitHub, or, you know, query this metric in Datadog. In their systems, they kind of come from this static store soar, perspective and workflow focus. So when they incorporated these agentic capabilities into it, they effectively become like one step in these broader like static workflows, those agents, the tools that are given access to, are essentially a set of the predefined workflows that they have elsewhere in the products, which should either be like templates or you build yourself. But the problem is that, like, those things all have to be either handcrafted by the end user to, like, run these specific workflows, or, if you go from their template ones, they tend to be these really complicated workflows that are like 30 different steps, like, you know, if then statements and loops and all that. So, you know, it's just a much more complicated product. The other thing that they don't have is any way to drive it from chat like a terminal. And I think, I think where we believe the world is going is certainly like these automations that do happen in the background. Yeah, without having this sort of way for a human to kind of drive these AI agents dynamically, it's it. You gotta, gotta meet the user where they're at. Like, there's, there's a trust that has to be built up in, like, what are you gonna allow this agent to do? So we definitely believe in a human in the loop. So when we're talking about, like, driving these, like, tool calling from chat, humans, like, proving, like, individual actions, like, yes, you're allowed to do this one, just this one time, or is, and as the human builds out that that trust and like, yes, this, this sequence that I just ran, this is like a playbook that I want to institutionalize into an agent background. I'm now comfortable with it taking these actions. That's where you would like translate that into an agent that runs in the background. But there's no way to kind of do that ephemeral, dynamic aspect from it, from their products, you kind of have to go straight to these agents, which is part of the reason why they wanted to put so many guardrails on it. So it just makes it for a much more you know, I think painstaking process, because they come from that static approach, whereas we've kind of taken it, you know, we're from the grassroots, AI native from the get go, so we're kind of taking a different approach, which is like, less focus on the static workflow at build time, and more on what the agent can do at runtime. Tools, put some policy guardrails around it, but then let it go, and then you'll see, as a human like, what it's able to do. Well, maybe you need to tweak your prompt, maybe give guidance. And so what we what we eventually find as like really high quality agents that run reliably, is you could put like a playbook, as long as you're explicit in what you want to get done, and you can refine how explicit you need to be, but once you define that runbook and you put it in an agent, now this thing can actually accomplish these these tasks with pretty high reliability, and they're pretty resilient to failures,
did a lot of recon when we were at Black Hat, just kind of see where the competition is. And I did the same thing RSA, yeah, you know, tying splint ops work. They really hadn't changed much since, since we got the demos at RSA, right? You know, really, I mean, as you've probably seen in their demos, like very heavy like UI based workflow solutions, kind of focusing on soar, you know, incident, incident response type of and so especially for dink, blink ops and torque. But names as well, they're trying to be more generic, kind of like Zapier, like Ron mentioned. But, you know, one of the things that that I really was trying to get at from them is like, Do you have a true agentic capability within this like, is it all about these static workflows, or is there a way for the agent to determine its path on its own, given, like, the user prompt, right? They have a way to do it, but the way that you have to construct it in their environment is, frankly, a lot of work. Like you have to, like, you can't call arbitrary tools like you would be able to in our system. So in our system, they've got a fleet of MCP servers. Each of those MCP servers exposes a number of tools. Each of those tools is like, a specific action you would take, like, list out my JIRA tickets, or, you know, pull the file from GitHub, or, you know, query this metric in Datadog. In their systems, they kind of come from this static store soar, perspective and workflow focus. So when they incorporated these agentic capabilities into it, they effectively become like one step in these broader like static workflows, those agents, the tools that are given access to, are essentially a set of the predefined workflows that they have elsewhere in the products, which should either be like templates or you build yourself. But the problem is that, like, those things all have to be either handcrafted by the end user to, like, run these specific workflows, or, if you go from their template ones, they tend to be these really complicated workflows that are like 30 different steps, like, you know, if then statements and loops and all that. So, you know, it's just a much more complicated product. The other thing that they don't have is any way to drive it from chat like a terminal. And I think, I think where we believe the world is going is certainly like these automations that do happen in the background. Yeah, without having this sort of way for a human to kind of drive these AI agents dynamically, it's it. You gotta, gotta meet the user where they're at. Like, there's, there's a trust that has to be built up in, like, what are you gonna allow this agent to do? So we definitely believe in a human in the loop. So when we're talking about, like, driving these, like, tool calling from chat, humans, like, proving, like, individual actions, like, yes, you're allowed to do this one, just this one time, or is, and as the human builds out that that trust and like, yes, this, this sequence that I just ran, this is like a playbook that I want to institutionalize into an agent background. I'm now comfortable with it taking these actions. That's where you would like translate that into an agent that runs in the background. But there's no way to kind of do that ephemeral, dynamic aspect from it, from their products, you kind of have to go straight to these agents, which is part of the reason why they wanted to put so many guardrails on it. So it just makes it for a much more you know, I think painstaking process, because they come from that static approach, whereas we've kind of taken it, you know, we're from the grassroots, AI native from the get go, so we're kind of taking a different approach, which is like, less focus on the static workflow at build time, and more on what the agent can do at runtime. Tools, put some policy guardrails around it, but then let it go, and then you'll see, as a human like, what it's able to do. Well, maybe you need to tweak your prompt, maybe give guidance. And so what we what we eventually find as like really high quality agents that run reliably, is you could put like a playbook, as long as you're explicit in what you want to get done, and you can refine how explicit you need to be, but once you define that runbook and you put it in an agent, now this thing can actually accomplish these these tasks with pretty high reliability, and they're pretty resilient to failures,
did a lot of recon when we were at Black Hat, just kind of see where the competition is. And I did the same thing RSA, yeah, you know, tying splint ops work. They really hadn't changed much since, since we got the demos at RSA, right? You know, really, I mean, as you've probably seen in their demos, like very heavy like UI based workflow solutions, kind of focusing on soar, you know, incident, incident response type of and so especially for dink, blink ops and torque. But names as well, they're trying to be more generic, kind of like Zapier, like Ron mentioned. But, you know, one of the things that that I really was trying to get at from them is like, Do you have a true agentic capability within this like, is it all about these static workflows, or is there a way for the agent to determine its path on its own, given, like, the user prompt, right? They have a way to do it, but the way that you have to construct it in their environment is, frankly, a lot of work. Like you have to, like, you can't call arbitrary tools like you would be able to in our system. So in our system, they've got a fleet of MCP servers. Each of those MCP servers exposes a number of tools. Each of those tools is like, a specific action you would take, like, list out my JIRA tickets, or, you know, pull the file from GitHub, or, you know, query this metric in Datadog. In their systems, they kind of come from this static store soar, perspective and workflow focus. So when they incorporated these agentic capabilities into it, they effectively become like one step in these broader like static workflows, those agents, the tools that are given access to, are essentially a set of the predefined workflows that they have elsewhere in the products, which should either be like templates or you build yourself. But the problem is that, like, those things all have to be either handcrafted by the end user to, like, run these specific workflows, or, if you go from their template ones, they tend to be these really complicated workflows that are like 30 different steps, like, you know, if then statements and loops and all that. So, you know, it's just a much more complicated product. The other thing that they don't have is any way to drive it from chat like a terminal. And I think, I think where we believe the world is going is certainly like these automations that do happen in the background. Yeah, without having this sort of way for a human to kind of drive these AI agents dynamically, it's it. You gotta, gotta meet the user where they're at. Like, there's, there's a trust that has to be built up in, like, what are you gonna allow this agent to do? So we definitely believe in a human in the loop. So when we're talking about, like, driving these, like, tool calling from chat, humans, like, proving, like, individual actions, like, yes, you're allowed to do this one, just this one time, or is, and as the human builds out that that trust and like, yes, this, this sequence that I just ran, this is like a playbook that I want to institutionalize into an agent background. I'm now comfortable with it taking these actions. That's where you would like translate that into an agent that runs in the background. But there's no way to kind of do that ephemeral, dynamic aspect from it, from their products, you kind of have to go straight to these agents, which is part of the reason why they wanted to put so many guardrails on it. So it just makes it for a much more you know, I think painstaking process, because they come from that static approach, whereas we've kind of taken it, you know, we're from the grassroots, AI native from the get go, so we're kind of taking a different approach, which is like, less focus on the static workflow at build time, and more on what the agent can do at runtime. Tools, put some policy guardrails around it, but then let it go, and then you'll see, as a human like, what it's able to do. Well, maybe you need to tweak your prompt, maybe give guidance. And so what we what we eventually find as like really high quality agents that run reliably, is you could put like a playbook, as long as you're explicit in what you want to get done, and you can refine how explicit you need to be, but once you define that runbook and you put it in an agent, now this thing can actually accomplish these these tasks with pretty high reliability, and they're pretty resilient to failures,
36:13like in one of the demos I had that I showed Deepak,
like in one of the demos I had that I showed Deepak,
like in one of the demos I had that I showed Deepak,
like in one of the demos I had that I showed Deepak,
S Speaker 436:17it'll try to, like, create a git branch, but it prints some name For that git branch. It hits a failure, and that failure is because that branch name is already used. Okay, great. Well, let me just use a different branch, but that's the type of, like, backtracking and, yeah, flying again, that you're not going to get from a static workflow. So I think that that's key differentiator. So they're trying to catch up, I think, with, like, the true agentic, but it's bolt on. You know, they're trying to do it after that, and they have so much they're so steeped in this static workflow that it's, it's, they have innovators dilemma. It's hard for them to just like, let go of that and go truly agentic. Yeah, you know,
it'll try to, like, create a git branch, but it prints some name For that git branch. It hits a failure, and that failure is because that branch name is already used. Okay, great. Well, let me just use a different branch, but that's the type of, like, backtracking and, yeah, flying again, that you're not going to get from a static workflow. So I think that that's key differentiator. So they're trying to catch up, I think, with, like, the true agentic, but it's bolt on. You know, they're trying to do it after that, and they have so much they're so steeped in this static workflow that it's, it's, they have innovators dilemma. It's hard for them to just like, let go of that and go truly agentic. Yeah, you know,
it'll try to, like, create a git branch, but it prints some name For that git branch. It hits a failure, and that failure is because that branch name is already used. Okay, great. Well, let me just use a different branch, but that's the type of, like, backtracking and, yeah, flying again, that you're not going to get from a static workflow. So I think that that's key differentiator. So they're trying to catch up, I think, with, like, the true agentic, but it's bolt on. You know, they're trying to do it after that, and they have so much they're so steeped in this static workflow that it's, it's, they have innovators dilemma. It's hard for them to just like, let go of that and go truly agentic. Yeah, you know,
it'll try to, like, create a git branch, but it prints some name For that git branch. It hits a failure, and that failure is because that branch name is already used. Okay, great. Well, let me just use a different branch, but that's the type of, like, backtracking and, yeah, flying again, that you're not going to get from a static workflow. So I think that that's key differentiator. So they're trying to catch up, I think, with, like, the true agentic, but it's bolt on. You know, they're trying to do it after that, and they have so much they're so steeped in this static workflow that it's, it's, they have innovators dilemma. It's hard for them to just like, let go of that and go truly agentic. Yeah, you know,
S Speaker 236:52I say the two other things that are more kind of more macro to think about, those tools are all predicated on the idea that all the SaaS apps are still going to be around, right? Like, like, like, they built connectors to Salesforce, or they built connectors to whatever. And their whole, their whole thing is, hey, we make it easy for you to code all those SaaS apps and get them all working together, pass data between them, and do all that kind of stuff. And that's very valuable today, right? And it's been very valuable in the area of SaaS, but I don't know what they're going to do. And all those SaaS apps are a single pane of glass that just has
I say the two other things that are more kind of more macro to think about, those tools are all predicated on the idea that all the SaaS apps are still going to be around, right? Like, like, like, they built connectors to Salesforce, or they built connectors to whatever. And their whole, their whole thing is, hey, we make it easy for you to code all those SaaS apps and get them all working together, pass data between them, and do all that kind of stuff. And that's very valuable today, right? And it's been very valuable in the area of SaaS, but I don't know what they're going to do. And all those SaaS apps are a single pane of glass that just has
I say the two other things that are more kind of more macro to think about, those tools are all predicated on the idea that all the SaaS apps are still going to be around, right? Like, like, like, they built connectors to Salesforce, or they built connectors to whatever. And their whole, their whole thing is, hey, we make it easy for you to code all those SaaS apps and get them all working together, pass data between them, and do all that kind of stuff. And that's very valuable today, right? And it's been very valuable in the area of SaaS, but I don't know what they're going to do. And all those SaaS apps are a single pane of glass that just has
I say the two other things that are more kind of more macro to think about, those tools are all predicated on the idea that all the SaaS apps are still going to be around, right? Like, like, like, they built connectors to Salesforce, or they built connectors to whatever. And their whole, their whole thing is, hey, we make it easy for you to code all those SaaS apps and get them all working together, pass data between them, and do all that kind of stuff. And that's very valuable today, right? And it's been very valuable in the area of SaaS, but I don't know what they're going to do. And all those SaaS apps are a single pane of glass that just has
37:27an LLM talking to the databases,
an LLM talking to the databases,
an LLM talking to the databases,
an LLM talking to the databases,
S Speaker 237:30and so all of that's like, it's just the wrong path lying down right? Yeah. And then the other macro thing is, and mostly what Brian was talking to you, as the models get better, you just don't need any deterministic code, and you don't want it actually, like you don't want to but like with kinder, you're basically hiring alien interns to work for you. With those guys, you're building these complicated little factories that have all these touch points that have to all work properly along the way, and someone has to go and maintain them. You're basically just using another coding language and what the LLM has allowed them to do is, is hopefully the LLM will run that coding language so the human doesn't have to, but it's still running that kind of brittle coding language. And so that's the big difference.
and so all of that's like, it's just the wrong path lying down right? Yeah. And then the other macro thing is, and mostly what Brian was talking to you, as the models get better, you just don't need any deterministic code, and you don't want it actually, like you don't want to but like with kinder, you're basically hiring alien interns to work for you. With those guys, you're building these complicated little factories that have all these touch points that have to all work properly along the way, and someone has to go and maintain them. You're basically just using another coding language and what the LLM has allowed them to do is, is hopefully the LLM will run that coding language so the human doesn't have to, but it's still running that kind of brittle coding language. And so that's the big difference.
and so all of that's like, it's just the wrong path lying down right? Yeah. And then the other macro thing is, and mostly what Brian was talking to you, as the models get better, you just don't need any deterministic code, and you don't want it actually, like you don't want to but like with kinder, you're basically hiring alien interns to work for you. With those guys, you're building these complicated little factories that have all these touch points that have to all work properly along the way, and someone has to go and maintain them. You're basically just using another coding language and what the LLM has allowed them to do is, is hopefully the LLM will run that coding language so the human doesn't have to, but it's still running that kind of brittle coding language. And so that's the big difference.
and so all of that's like, it's just the wrong path lying down right? Yeah. And then the other macro thing is, and mostly what Brian was talking to you, as the models get better, you just don't need any deterministic code, and you don't want it actually, like you don't want to but like with kinder, you're basically hiring alien interns to work for you. With those guys, you're building these complicated little factories that have all these touch points that have to all work properly along the way, and someone has to go and maintain them. You're basically just using another coding language and what the LLM has allowed them to do is, is hopefully the LLM will run that coding language so the human doesn't have to, but it's still running that kind of brittle coding language. And so that's the big difference.
38:15I'll say, we just
S Speaker 238:17say, I need it from the beginning. You know, one way I think about it. And if I was an investor today, if they were born, probably before August or July
say, I need it from the beginning. You know, one way I think about it. And if I was an investor today, if they were born, probably before August or July
say, I need it from the beginning. You know, one way I think about it. And if I was an investor today, if they were born, probably before August or July
say, I need it from the beginning. You know, one way I think about it. And if I was an investor today, if they were born, probably before August or July
38:31they just missed the general
they just missed the general
they just missed the general
they just missed the general
S Speaker 538:34I understand. And you know, this is music to my ears. Honestly, Ron and Brian, I
I understand. And you know, this is music to my ears. Honestly, Ron and Brian, I
I understand. And you know, this is music to my ears. Honestly, Ron and Brian, I
I understand. And you know, this is music to my ears. Honestly, Ron and Brian, I
38:40am enjoying this discussion. You can't see it on my face,
am enjoying this discussion. You can't see it on my face,
am enjoying this discussion. You can't see it on my face,
am enjoying this discussion. You can't see it on my face,
S Speaker 538:44but Brian knows this. I was, I spent a lot, quite a bit of time at the booth. Probably was the first, first few of the day, that day. So one thing I want to.
but Brian knows this. I was, I spent a lot, quite a bit of time at the booth. Probably was the first, first few of the day, that day. So one thing I want to.
but Brian knows this. I was, I spent a lot, quite a bit of time at the booth. Probably was the first, first few of the day, that day. So one thing I want to.
but Brian knows this. I was, I spent a lot, quite a bit of time at the booth. Probably was the first, first few of the day, that day. So one thing I want to.
S Speaker 539:34Are customers seeing that. So the question I have is, as a customer's buying, am I today buying for future proofing, my technology and future roadmap in mind? Or am I buying for hey, I need this solved today. I understand you guys are technically, you know, maybe two steps ahead from an architecture point of view, but I need it to be solved, right? I don't really care. I probably care for scale. I care for reliability. I care for accuracy. I don't care if you're using an open source model and a closest what I'm trying to get to is, has the market matured yet? And my question, I'm still dabbling, because there, there are these bolt on companies that are trying to go and go out and sell, hey, we were doing this before. Now we have agent to capability. Is it? Is it clear to like, it's clear to me what you just explained, but is the customer as aware, as indicated, as, let's say, in some of the investors would be right. So I do run into that challenge in explaining why you guys stand out.
Are customers seeing that. So the question I have is, as a customer's buying, am I today buying for future proofing, my technology and future roadmap in mind? Or am I buying for hey, I need this solved today. I understand you guys are technically, you know, maybe two steps ahead from an architecture point of view, but I need it to be solved, right? I don't really care. I probably care for scale. I care for reliability. I care for accuracy. I don't care if you're using an open source model and a closest what I'm trying to get to is, has the market matured yet? And my question, I'm still dabbling, because there, there are these bolt on companies that are trying to go and go out and sell, hey, we were doing this before. Now we have agent to capability. Is it? Is it clear to like, it's clear to me what you just explained, but is the customer as aware, as indicated, as, let's say, in some of the investors would be right. So I do run into that challenge in explaining why you guys stand out.
Are customers seeing that. So the question I have is, as a customer's buying, am I today buying for future proofing, my technology and future roadmap in mind? Or am I buying for hey, I need this solved today. I understand you guys are technically, you know, maybe two steps ahead from an architecture point of view, but I need it to be solved, right? I don't really care. I probably care for scale. I care for reliability. I care for accuracy. I don't care if you're using an open source model and a closest what I'm trying to get to is, has the market matured yet? And my question, I'm still dabbling, because there, there are these bolt on companies that are trying to go and go out and sell, hey, we were doing this before. Now we have agent to capability. Is it? Is it clear to like, it's clear to me what you just explained, but is the customer as aware, as indicated, as, let's say, in some of the investors would be right. So I do run into that challenge in explaining why you guys stand out.
Are customers seeing that. So the question I have is, as a customer's buying, am I today buying for future proofing, my technology and future roadmap in mind? Or am I buying for hey, I need this solved today. I understand you guys are technically, you know, maybe two steps ahead from an architecture point of view, but I need it to be solved, right? I don't really care. I probably care for scale. I care for reliability. I care for accuracy. I don't care if you're using an open source model and a closest what I'm trying to get to is, has the market matured yet? And my question, I'm still dabbling, because there, there are these bolt on companies that are trying to go and go out and sell, hey, we were doing this before. Now we have agent to capability. Is it? Is it clear to like, it's clear to me what you just explained, but is the customer as aware, as indicated, as, let's say, in some of the investors would be right. So I do run into that challenge in explaining why you guys stand out.
S Speaker 240:40Yeah, we do, for sure, like they have a sense the market is still more immature, the it's not everybody, and it's rapidly shifting, and it's kind of on us from a sales job. Usually, when you see a demo of Kindle, when you see a demo of mineral times, or whatever, you totally get it. You're like, Okay, wait, this is, and this is why we saw it. Black Hat, wait, you know, we had a couple 100 people come up and see these demos, and they were like, Wait, this is, like, totally different, right? The way, you guys, I really liked it, right? And so, so, so we went, I think in a head to head there, we actually don't run into those guys too much right now, though, which is interesting, and in part because we are a lot of our customers are in the self managed on prem world, and most of those tools still require you a component in the cloud to run like when they say self managed, what they mean is, well, maybe you can run The Model self managed, or you can run whatever, but you still have to trust us to see all the stuff you know or whatever. Your kinder. You don't have to trust us at all. Right? We'll show you an enterprise license. You install it, and then you're on your own. We're happy to support you or whatever, but, but we don't see your data. We don't have to you don't have to trust us. You don't have to trust if you, if you'd like the open source models and there's great open source models that can do almost all these cases that you might have in your enterprise. Then you could deploy those, and you don't have to expose any of your data to anybody like that's how we actually think everybody should be building AI today, because it's going to really matter down the road, which AI has seen your data, I think, right? So, you know, Sam Altman is probably going to want to be an insurance company one day, not just have company one day, not just have insurance companies as customers, for examples, right? And so the so, so, and they're smart on prem customers that are thinking through that. And there's more and more coming, kind of every day. So more in the on prem world, we're pretty blue ocean, like Microsoft can do almost everything we can do if you trust Microsoft for sure. And a lot of you know, some of these older school on prem security conscious companies do, or, you know, self managed ones, but it's a nightmare to wire up a lot of it doesn't work super well because it's thrilled with Microsoft's still a huge company trying to get all the stuff to work, and they've been pretty good at marketing it, but there's the analog, and it's expensive, like a lot of prompts and so so, so, you know, I think that is, you know, we just don't run into much. I think we run into more, like everybody's marketing more, and they only have big boots this year and all that stuff. And I think head to head, just, we're going to win on the simplicity and
Yeah, we do, for sure, like they have a sense the market is still more immature, the it's not everybody, and it's rapidly shifting, and it's kind of on us from a sales job. Usually, when you see a demo of Kindle, when you see a demo of mineral times, or whatever, you totally get it. You're like, Okay, wait, this is, and this is why we saw it. Black Hat, wait, you know, we had a couple 100 people come up and see these demos, and they were like, Wait, this is, like, totally different, right? The way, you guys, I really liked it, right? And so, so, so we went, I think in a head to head there, we actually don't run into those guys too much right now, though, which is interesting, and in part because we are a lot of our customers are in the self managed on prem world, and most of those tools still require you a component in the cloud to run like when they say self managed, what they mean is, well, maybe you can run The Model self managed, or you can run whatever, but you still have to trust us to see all the stuff you know or whatever. Your kinder. You don't have to trust us at all. Right? We'll show you an enterprise license. You install it, and then you're on your own. We're happy to support you or whatever, but, but we don't see your data. We don't have to you don't have to trust us. You don't have to trust if you, if you'd like the open source models and there's great open source models that can do almost all these cases that you might have in your enterprise. Then you could deploy those, and you don't have to expose any of your data to anybody like that's how we actually think everybody should be building AI today, because it's going to really matter down the road, which AI has seen your data, I think, right? So, you know, Sam Altman is probably going to want to be an insurance company one day, not just have company one day, not just have insurance companies as customers, for examples, right? And so the so, so, and they're smart on prem customers that are thinking through that. And there's more and more coming, kind of every day. So more in the on prem world, we're pretty blue ocean, like Microsoft can do almost everything we can do if you trust Microsoft for sure. And a lot of you know, some of these older school on prem security conscious companies do, or, you know, self managed ones, but it's a nightmare to wire up a lot of it doesn't work super well because it's thrilled with Microsoft's still a huge company trying to get all the stuff to work, and they've been pretty good at marketing it, but there's the analog, and it's expensive, like a lot of prompts and so so, so, you know, I think that is, you know, we just don't run into much. I think we run into more, like everybody's marketing more, and they only have big boots this year and all that stuff. And I think head to head, just, we're going to win on the simplicity and
Yeah, we do, for sure, like they have a sense the market is still more immature, the it's not everybody, and it's rapidly shifting, and it's kind of on us from a sales job. Usually, when you see a demo of Kindle, when you see a demo of mineral times, or whatever, you totally get it. You're like, Okay, wait, this is, and this is why we saw it. Black Hat, wait, you know, we had a couple 100 people come up and see these demos, and they were like, Wait, this is, like, totally different, right? The way, you guys, I really liked it, right? And so, so, so we went, I think in a head to head there, we actually don't run into those guys too much right now, though, which is interesting, and in part because we are a lot of our customers are in the self managed on prem world, and most of those tools still require you a component in the cloud to run like when they say self managed, what they mean is, well, maybe you can run The Model self managed, or you can run whatever, but you still have to trust us to see all the stuff you know or whatever. Your kinder. You don't have to trust us at all. Right? We'll show you an enterprise license. You install it, and then you're on your own. We're happy to support you or whatever, but, but we don't see your data. We don't have to you don't have to trust us. You don't have to trust if you, if you'd like the open source models and there's great open source models that can do almost all these cases that you might have in your enterprise. Then you could deploy those, and you don't have to expose any of your data to anybody like that's how we actually think everybody should be building AI today, because it's going to really matter down the road, which AI has seen your data, I think, right? So, you know, Sam Altman is probably going to want to be an insurance company one day, not just have company one day, not just have insurance companies as customers, for examples, right? And so the so, so, and they're smart on prem customers that are thinking through that. And there's more and more coming, kind of every day. So more in the on prem world, we're pretty blue ocean, like Microsoft can do almost everything we can do if you trust Microsoft for sure. And a lot of you know, some of these older school on prem security conscious companies do, or, you know, self managed ones, but it's a nightmare to wire up a lot of it doesn't work super well because it's thrilled with Microsoft's still a huge company trying to get all the stuff to work, and they've been pretty good at marketing it, but there's the analog, and it's expensive, like a lot of prompts and so so, so, you know, I think that is, you know, we just don't run into much. I think we run into more, like everybody's marketing more, and they only have big boots this year and all that stuff. And I think head to head, just, we're going to win on the simplicity and
Yeah, we do, for sure, like they have a sense the market is still more immature, the it's not everybody, and it's rapidly shifting, and it's kind of on us from a sales job. Usually, when you see a demo of Kindle, when you see a demo of mineral times, or whatever, you totally get it. You're like, Okay, wait, this is, and this is why we saw it. Black Hat, wait, you know, we had a couple 100 people come up and see these demos, and they were like, Wait, this is, like, totally different, right? The way, you guys, I really liked it, right? And so, so, so we went, I think in a head to head there, we actually don't run into those guys too much right now, though, which is interesting, and in part because we are a lot of our customers are in the self managed on prem world, and most of those tools still require you a component in the cloud to run like when they say self managed, what they mean is, well, maybe you can run The Model self managed, or you can run whatever, but you still have to trust us to see all the stuff you know or whatever. Your kinder. You don't have to trust us at all. Right? We'll show you an enterprise license. You install it, and then you're on your own. We're happy to support you or whatever, but, but we don't see your data. We don't have to you don't have to trust us. You don't have to trust if you, if you'd like the open source models and there's great open source models that can do almost all these cases that you might have in your enterprise. Then you could deploy those, and you don't have to expose any of your data to anybody like that's how we actually think everybody should be building AI today, because it's going to really matter down the road, which AI has seen your data, I think, right? So, you know, Sam Altman is probably going to want to be an insurance company one day, not just have company one day, not just have insurance companies as customers, for examples, right? And so the so, so, and they're smart on prem customers that are thinking through that. And there's more and more coming, kind of every day. So more in the on prem world, we're pretty blue ocean, like Microsoft can do almost everything we can do if you trust Microsoft for sure. And a lot of you know, some of these older school on prem security conscious companies do, or, you know, self managed ones, but it's a nightmare to wire up a lot of it doesn't work super well because it's thrilled with Microsoft's still a huge company trying to get all the stuff to work, and they've been pretty good at marketing it, but there's the analog, and it's expensive, like a lot of prompts and so so, so, you know, I think that is, you know, we just don't run into much. I think we run into more, like everybody's marketing more, and they only have big boots this year and all that stuff. And I think head to head, just, we're going to win on the simplicity and
S Speaker 543:05see more than Ron, because you mentioned you don't see the kind of, you know, the typical players you're,
see more than Ron, because you mentioned you don't see the kind of, you know, the typical players you're,
see more than Ron, because you mentioned you don't see the kind of, you know, the typical players you're,
see more than Ron, because you mentioned you don't see the kind of, you know, the typical players you're,
43:11yeah, it's mostly Microsoft.
yeah, it's mostly Microsoft.
yeah, it's mostly Microsoft.
yeah, it's mostly Microsoft.
43:15Is probably the key, the key
Is probably the key, the key
Is probably the key, the key
Is probably the key, the key
S Speaker 243:17thing that pops up, we'll get questions about maybe a UI path or something that somebody's using because, you know, they're really old school kind of thing.
thing that pops up, we'll get questions about maybe a UI path or something that somebody's using because, you know, they're really old school kind of thing.
thing that pops up, we'll get questions about maybe a UI path or something that somebody's using because, you know, they're really old school kind of thing.
thing that pops up, we'll get questions about maybe a UI path or something that somebody's using because, you know, they're really old school kind of thing.
43:27And then, and then, some of, most, all
And then, and then, some of, most, all
And then, and then, some of, most, all
And then, and then, some of, most, all
S Speaker 243:30of our customers, have bolt on AI with our SaaS apps. So, so we usually have to talk through that too, where it's like, Wait, well, marketing has Qualtrics and it has an AI thing. Yeah,
of our customers, have bolt on AI with our SaaS apps. So, so we usually have to talk through that too, where it's like, Wait, well, marketing has Qualtrics and it has an AI thing. Yeah,
of our customers, have bolt on AI with our SaaS apps. So, so we usually have to talk through that too, where it's like, Wait, well, marketing has Qualtrics and it has an AI thing. Yeah,
of our customers, have bolt on AI with our SaaS apps. So, so we usually have to talk through that too, where it's like, Wait, well, marketing has Qualtrics and it has an AI thing. Yeah,
43:41right, yeah, yeah.
S Speaker 243:43And then, and then, you know? Why? Why would marketing want to use your tablet? I'm like, Well, let me ask you this, how do you keep marketing from downloading a spreadsheet out of Salesforce with all your CRM contacts and uploading it to that Qualtrics Chatbot? Do you want Qualtrics to have all your CRM data? And they're like, Well, no, we only want I'm like, okay, so how do you manage that? And that's how you kind of start winning. That's how you want Sonesta, right as a like the 11th largest hotel chain, right? And so the but, and now, by the way, black hat, there's a whole nother set of things to sell on. You know, they're all these hacks at Black Hat, of, you know, letting, letting these third party apps, so that, you know, they hack self sells for us. They have some Microsoft stuff. If you let one of these SAS apps with a spoke on AI talk outside your organization, they're super vulnerable. The prompt
And then, and then, you know? Why? Why would marketing want to use your tablet? I'm like, Well, let me ask you this, how do you keep marketing from downloading a spreadsheet out of Salesforce with all your CRM contacts and uploading it to that Qualtrics Chatbot? Do you want Qualtrics to have all your CRM data? And they're like, Well, no, we only want I'm like, okay, so how do you manage that? And that's how you kind of start winning. That's how you want Sonesta, right as a like the 11th largest hotel chain, right? And so the but, and now, by the way, black hat, there's a whole nother set of things to sell on. You know, they're all these hacks at Black Hat, of, you know, letting, letting these third party apps, so that, you know, they hack self sells for us. They have some Microsoft stuff. If you let one of these SAS apps with a spoke on AI talk outside your organization, they're super vulnerable. The prompt
And then, and then, you know? Why? Why would marketing want to use your tablet? I'm like, Well, let me ask you this, how do you keep marketing from downloading a spreadsheet out of Salesforce with all your CRM contacts and uploading it to that Qualtrics Chatbot? Do you want Qualtrics to have all your CRM data? And they're like, Well, no, we only want I'm like, okay, so how do you manage that? And that's how you kind of start winning. That's how you want Sonesta, right as a like the 11th largest hotel chain, right? And so the but, and now, by the way, black hat, there's a whole nother set of things to sell on. You know, they're all these hacks at Black Hat, of, you know, letting, letting these third party apps, so that, you know, they hack self sells for us. They have some Microsoft stuff. If you let one of these SAS apps with a spoke on AI talk outside your organization, they're super vulnerable. The prompt
And then, and then, you know? Why? Why would marketing want to use your tablet? I'm like, Well, let me ask you this, how do you keep marketing from downloading a spreadsheet out of Salesforce with all your CRM contacts and uploading it to that Qualtrics Chatbot? Do you want Qualtrics to have all your CRM data? And they're like, Well, no, we only want I'm like, okay, so how do you manage that? And that's how you kind of start winning. That's how you want Sonesta, right as a like the 11th largest hotel chain, right? And so the but, and now, by the way, black hat, there's a whole nother set of things to sell on. You know, they're all these hacks at Black Hat, of, you know, letting, letting these third party apps, so that, you know, they hack self sells for us. They have some Microsoft stuff. If you let one of these SAS apps with a spoke on AI talk outside your organization, they're super vulnerable. The prompt
44:34injection, Oh, right.
injection, Oh, right.
injection, Oh, right.
injection, Oh, right.
S Speaker 544:35Oh, I read about the GitHub private repo, right? Yeah, that was called MCP. It was like, you know, it had the keys, but guess what? It could access the private repo too. So do it to your point. You know, yes, the market is not there yet, but it's maybe faster evolving. It is, it is fast evolving. Yeah, that's, that's super interesting, too. And I don't know, I don't I'm actually okay to stay and I am enjoying this discussion. How are you guys on time? Because I know I want to be respectful of your time. If we can go maybe 1015, minutes more.
Oh, I read about the GitHub private repo, right? Yeah, that was called MCP. It was like, you know, it had the keys, but guess what? It could access the private repo too. So do it to your point. You know, yes, the market is not there yet, but it's maybe faster evolving. It is, it is fast evolving. Yeah, that's, that's super interesting, too. And I don't know, I don't I'm actually okay to stay and I am enjoying this discussion. How are you guys on time? Because I know I want to be respectful of your time. If we can go maybe 1015, minutes more.
Oh, I read about the GitHub private repo, right? Yeah, that was called MCP. It was like, you know, it had the keys, but guess what? It could access the private repo too. So do it to your point. You know, yes, the market is not there yet, but it's maybe faster evolving. It is, it is fast evolving. Yeah, that's, that's super interesting, too. And I don't know, I don't I'm actually okay to stay and I am enjoying this discussion. How are you guys on time? Because I know I want to be respectful of your time. If we can go maybe 1015, minutes more.
Oh, I read about the GitHub private repo, right? Yeah, that was called MCP. It was like, you know, it had the keys, but guess what? It could access the private repo too. So do it to your point. You know, yes, the market is not there yet, but it's maybe faster evolving. It is, it is fast evolving. Yeah, that's, that's super interesting, too. And I don't know, I don't I'm actually okay to stay and I am enjoying this discussion. How are you guys on time? Because I know I want to be respectful of your time. If we can go maybe 1015, minutes more.
45:10Yeah, I will carry you. Okay, Brian,
Yeah, I will carry you. Okay, Brian,
Yeah, I will carry you. Okay, Brian,
Yeah, I will carry you. Okay, Brian,
S Speaker 545:14yeah. So I was thinking of maybe doing two things. One is unpacking the architecture. How have you built? Because it's something which is very differentiated, and I don't know how many people outside appreciate what you've built now. So I wanted to unpack that a little bit. How have you built the MCP layer? How have you built the agent? You know, sandbox, lifecycle management. Piece, all of these things are, you know, big differentiators as you grow. That's number one. Number two is, maybe, if you can take one or two customers and talk about how they expanded, right? So maybe Was it easy to identify the initial use case? The reason I bring this up Ron and Brian, like, when you when you talk to Qualcomm, and I love to get Qualcomm as a customer at some point, so I'll definitely be in touch. But I would need a clear, specific use case and a clear buyer, right? I can't if I, if I go to Qualcomm, I would be like, Okay, who, which division, which problem statement, which, you know, budget is this going to come from, right? So, so that's why I want to discuss some, some customer case study with a proper expansion story that you have. So can we do two things? One is maybe break down the architecture a little bit, and the second is, take a customer case study, talk about the initial use case and how you expand it. From there, it'll give me some ideas of how to tackle Qualcomm as a customer.
yeah. So I was thinking of maybe doing two things. One is unpacking the architecture. How have you built? Because it's something which is very differentiated, and I don't know how many people outside appreciate what you've built now. So I wanted to unpack that a little bit. How have you built the MCP layer? How have you built the agent? You know, sandbox, lifecycle management. Piece, all of these things are, you know, big differentiators as you grow. That's number one. Number two is, maybe, if you can take one or two customers and talk about how they expanded, right? So maybe Was it easy to identify the initial use case? The reason I bring this up Ron and Brian, like, when you when you talk to Qualcomm, and I love to get Qualcomm as a customer at some point, so I'll definitely be in touch. But I would need a clear, specific use case and a clear buyer, right? I can't if I, if I go to Qualcomm, I would be like, Okay, who, which division, which problem statement, which, you know, budget is this going to come from, right? So, so that's why I want to discuss some, some customer case study with a proper expansion story that you have. So can we do two things? One is maybe break down the architecture a little bit, and the second is, take a customer case study, talk about the initial use case and how you expand it. From there, it'll give me some ideas of how to tackle Qualcomm as a customer.
yeah. So I was thinking of maybe doing two things. One is unpacking the architecture. How have you built? Because it's something which is very differentiated, and I don't know how many people outside appreciate what you've built now. So I wanted to unpack that a little bit. How have you built the MCP layer? How have you built the agent? You know, sandbox, lifecycle management. Piece, all of these things are, you know, big differentiators as you grow. That's number one. Number two is, maybe, if you can take one or two customers and talk about how they expanded, right? So maybe Was it easy to identify the initial use case? The reason I bring this up Ron and Brian, like, when you when you talk to Qualcomm, and I love to get Qualcomm as a customer at some point, so I'll definitely be in touch. But I would need a clear, specific use case and a clear buyer, right? I can't if I, if I go to Qualcomm, I would be like, Okay, who, which division, which problem statement, which, you know, budget is this going to come from, right? So, so that's why I want to discuss some, some customer case study with a proper expansion story that you have. So can we do two things? One is maybe break down the architecture a little bit, and the second is, take a customer case study, talk about the initial use case and how you expand it. From there, it'll give me some ideas of how to tackle Qualcomm as a customer.
yeah. So I was thinking of maybe doing two things. One is unpacking the architecture. How have you built? Because it's something which is very differentiated, and I don't know how many people outside appreciate what you've built now. So I wanted to unpack that a little bit. How have you built the MCP layer? How have you built the agent? You know, sandbox, lifecycle management. Piece, all of these things are, you know, big differentiators as you grow. That's number one. Number two is, maybe, if you can take one or two customers and talk about how they expanded, right? So maybe Was it easy to identify the initial use case? The reason I bring this up Ron and Brian, like, when you when you talk to Qualcomm, and I love to get Qualcomm as a customer at some point, so I'll definitely be in touch. But I would need a clear, specific use case and a clear buyer, right? I can't if I, if I go to Qualcomm, I would be like, Okay, who, which division, which problem statement, which, you know, budget is this going to come from, right? So, so that's why I want to discuss some, some customer case study with a proper expansion story that you have. So can we do two things? One is maybe break down the architecture a little bit, and the second is, take a customer case study, talk about the initial use case and how you expand it. From there, it'll give me some ideas of how to tackle Qualcomm as a customer.
S Speaker 246:41Okay, wait, just let Brian ask the first one. I think they've dropped off in just like five or 10 minutes, and I'll be right back. And then we'll talk the two customer use cases.
Okay, wait, just let Brian ask the first one. I think they've dropped off in just like five or 10 minutes, and I'll be right back. And then we'll talk the two customer use cases.
Okay, wait, just let Brian ask the first one. I think they've dropped off in just like five or 10 minutes, and I'll be right back. And then we'll talk the two customer use cases.
Okay, wait, just let Brian ask the first one. I think they've dropped off in just like five or 10 minutes, and I'll be right back. And then we'll talk the two customer use cases.
46:53Thanks. Thanks. Sure. All
Thanks. Thanks. Sure. All
Thanks. Thanks. Sure. All
Thanks. Thanks. Sure. All
S Speaker 446:56right, let me give you an overview of the architecture. So I mean, basically the architecture, so the number of pieces, so I guess let's cover the MCP. Since that's kind of like instrumental to calling we've currently built out all of our MCP servers ourselves. We have aI automation that can essentially write them almost completely by through automation. So it probably takes us about a half a day to write a new MCP server process, effectively is, you know, we have AI agents that will go out and research the REST API documentation out there, we have several example golden MCP servers that have previously been built. So the combination of the research the API specification, prior examples of like what good MCP servers look like, all get fed into an AI model that essentially generates the MCP server. They tend to be like a few 1000 lines of code each.
right, let me give you an overview of the architecture. So I mean, basically the architecture, so the number of pieces, so I guess let's cover the MCP. Since that's kind of like instrumental to calling we've currently built out all of our MCP servers ourselves. We have aI automation that can essentially write them almost completely by through automation. So it probably takes us about a half a day to write a new MCP server process, effectively is, you know, we have AI agents that will go out and research the REST API documentation out there, we have several example golden MCP servers that have previously been built. So the combination of the research the API specification, prior examples of like what good MCP servers look like, all get fed into an AI model that essentially generates the MCP server. They tend to be like a few 1000 lines of code each.
right, let me give you an overview of the architecture. So I mean, basically the architecture, so the number of pieces, so I guess let's cover the MCP. Since that's kind of like instrumental to calling we've currently built out all of our MCP servers ourselves. We have aI automation that can essentially write them almost completely by through automation. So it probably takes us about a half a day to write a new MCP server process, effectively is, you know, we have AI agents that will go out and research the REST API documentation out there, we have several example golden MCP servers that have previously been built. So the combination of the research the API specification, prior examples of like what good MCP servers look like, all get fed into an AI model that essentially generates the MCP server. They tend to be like a few 1000 lines of code each.
right, let me give you an overview of the architecture. So I mean, basically the architecture, so the number of pieces, so I guess let's cover the MCP. Since that's kind of like instrumental to calling we've currently built out all of our MCP servers ourselves. We have aI automation that can essentially write them almost completely by through automation. So it probably takes us about a half a day to write a new MCP server process, effectively is, you know, we have AI agents that will go out and research the REST API documentation out there, we have several example golden MCP servers that have previously been built. So the combination of the research the API specification, prior examples of like what good MCP servers look like, all get fed into an AI model that essentially generates the MCP server. They tend to be like a few 1000 lines of code each.
S Speaker 547:56And right now, one question there, and is it okay if we keep this open ended, like open conversation? So one of the things that around MCP, right even the documentation, the way, I think about the documentation, it is written for humans today, right? It'll be, there will be a newer format, or maybe there's already a format that will be agent readable, or LLM readable, right? So you might not, you know, you might not need a rest API documentation written for a human, but your agent will figure out this is the template for these APIs. And building an MCP server would be not a half a day effort, but maybe a 30 minute or five minute effort, right? Do you see that happening? Is there a standard coming where, you know, companies are making it easier to build MCP servers?
And right now, one question there, and is it okay if we keep this open ended, like open conversation? So one of the things that around MCP, right even the documentation, the way, I think about the documentation, it is written for humans today, right? It'll be, there will be a newer format, or maybe there's already a format that will be agent readable, or LLM readable, right? So you might not, you know, you might not need a rest API documentation written for a human, but your agent will figure out this is the template for these APIs. And building an MCP server would be not a half a day effort, but maybe a 30 minute or five minute effort, right? Do you see that happening? Is there a standard coming where, you know, companies are making it easier to build MCP servers?
And right now, one question there, and is it okay if we keep this open ended, like open conversation? So one of the things that around MCP, right even the documentation, the way, I think about the documentation, it is written for humans today, right? It'll be, there will be a newer format, or maybe there's already a format that will be agent readable, or LLM readable, right? So you might not, you know, you might not need a rest API documentation written for a human, but your agent will figure out this is the template for these APIs. And building an MCP server would be not a half a day effort, but maybe a 30 minute or five minute effort, right? Do you see that happening? Is there a standard coming where, you know, companies are making it easier to build MCP servers?
And right now, one question there, and is it okay if we keep this open ended, like open conversation? So one of the things that around MCP, right even the documentation, the way, I think about the documentation, it is written for humans today, right? It'll be, there will be a newer format, or maybe there's already a format that will be agent readable, or LLM readable, right? So you might not, you know, you might not need a rest API documentation written for a human, but your agent will figure out this is the template for these APIs. And building an MCP server would be not a half a day effort, but maybe a 30 minute or five minute effort, right? Do you see that happening? Is there a standard coming where, you know, companies are making it easier to build MCP servers?
S Speaker 448:48Yeah. And when I say half day, I'm really kind of talking end to end, including whatever testing we do like that. It probably includes setting an account with that system, loading up synthetic data, like making sure that it works end to end. Yeah, I see, I see, we'll obviously get better that over time. And you could, you could fully rely on, like, synthetic tests against their that REST API. But if you really want to be more confident of it, then you have to do more of that sort of testing, and probably more automation can happen there. But right now, it's a little tedious to go actually set up an account on Splunk and, yeah, some data. So anyway, that's actually a lot of the load. It's actually not like human time. It's the system will maybe crunch for an hour or more on the whole process understood, and human will take a look at it, get it checked in, like, run the tests and all that. But yeah, I think that the whole process will get faster, because as the AI models get better, yeah, yeah, better ways to automate so. And the other thing is that, like, we, we built our own, mostly because the industry hadn't fully caught up yet. Like, there's a really strong reason why we have to roll our own. So, like, for any provider that's exposing their own MCP server, we could just integrate with that. Just that there were various reasons, like authorization, for example, yeah, some of the cases, they wanted to handle authorization differently than what would be most convenient for an external third party service like us, that is authenticating the user for that service, like for example, like with linear you'd have to do a separate authentication through an OAuth flow to get access to their MCP servers, from what you'd have to do to get web hooks. So if we're going to use their MCP servers, we effectively have to have our users go through two separate OAuth flows in order to, like, integrate, which
Yeah. And when I say half day, I'm really kind of talking end to end, including whatever testing we do like that. It probably includes setting an account with that system, loading up synthetic data, like making sure that it works end to end. Yeah, I see, I see, we'll obviously get better that over time. And you could, you could fully rely on, like, synthetic tests against their that REST API. But if you really want to be more confident of it, then you have to do more of that sort of testing, and probably more automation can happen there. But right now, it's a little tedious to go actually set up an account on Splunk and, yeah, some data. So anyway, that's actually a lot of the load. It's actually not like human time. It's the system will maybe crunch for an hour or more on the whole process understood, and human will take a look at it, get it checked in, like, run the tests and all that. But yeah, I think that the whole process will get faster, because as the AI models get better, yeah, yeah, better ways to automate so. And the other thing is that, like, we, we built our own, mostly because the industry hadn't fully caught up yet. Like, there's a really strong reason why we have to roll our own. So, like, for any provider that's exposing their own MCP server, we could just integrate with that. Just that there were various reasons, like authorization, for example, yeah, some of the cases, they wanted to handle authorization differently than what would be most convenient for an external third party service like us, that is authenticating the user for that service, like for example, like with linear you'd have to do a separate authentication through an OAuth flow to get access to their MCP servers, from what you'd have to do to get web hooks. So if we're going to use their MCP servers, we effectively have to have our users go through two separate OAuth flows in order to, like, integrate, which
Yeah. And when I say half day, I'm really kind of talking end to end, including whatever testing we do like that. It probably includes setting an account with that system, loading up synthetic data, like making sure that it works end to end. Yeah, I see, I see, we'll obviously get better that over time. And you could, you could fully rely on, like, synthetic tests against their that REST API. But if you really want to be more confident of it, then you have to do more of that sort of testing, and probably more automation can happen there. But right now, it's a little tedious to go actually set up an account on Splunk and, yeah, some data. So anyway, that's actually a lot of the load. It's actually not like human time. It's the system will maybe crunch for an hour or more on the whole process understood, and human will take a look at it, get it checked in, like, run the tests and all that. But yeah, I think that the whole process will get faster, because as the AI models get better, yeah, yeah, better ways to automate so. And the other thing is that, like, we, we built our own, mostly because the industry hadn't fully caught up yet. Like, there's a really strong reason why we have to roll our own. So, like, for any provider that's exposing their own MCP server, we could just integrate with that. Just that there were various reasons, like authorization, for example, yeah, some of the cases, they wanted to handle authorization differently than what would be most convenient for an external third party service like us, that is authenticating the user for that service, like for example, like with linear you'd have to do a separate authentication through an OAuth flow to get access to their MCP servers, from what you'd have to do to get web hooks. So if we're going to use their MCP servers, we effectively have to have our users go through two separate OAuth flows in order to, like, integrate, which
Yeah. And when I say half day, I'm really kind of talking end to end, including whatever testing we do like that. It probably includes setting an account with that system, loading up synthetic data, like making sure that it works end to end. Yeah, I see, I see, we'll obviously get better that over time. And you could, you could fully rely on, like, synthetic tests against their that REST API. But if you really want to be more confident of it, then you have to do more of that sort of testing, and probably more automation can happen there. But right now, it's a little tedious to go actually set up an account on Splunk and, yeah, some data. So anyway, that's actually a lot of the load. It's actually not like human time. It's the system will maybe crunch for an hour or more on the whole process understood, and human will take a look at it, get it checked in, like, run the tests and all that. But yeah, I think that the whole process will get faster, because as the AI models get better, yeah, yeah, better ways to automate so. And the other thing is that, like, we, we built our own, mostly because the industry hadn't fully caught up yet. Like, there's a really strong reason why we have to roll our own. So, like, for any provider that's exposing their own MCP server, we could just integrate with that. Just that there were various reasons, like authorization, for example, yeah, some of the cases, they wanted to handle authorization differently than what would be most convenient for an external third party service like us, that is authenticating the user for that service, like for example, like with linear you'd have to do a separate authentication through an OAuth flow to get access to their MCP servers, from what you'd have to do to get web hooks. So if we're going to use their MCP servers, we effectively have to have our users go through two separate OAuth flows in order to, like, integrate, which
S Speaker 550:39is not the best optimal, yeah, which is not what you want to do. So, so that's on the MCP side. The reason I'm also asking is, let's say, if we make a bet on Kindle, right, I'm also making a bet that integrations will be easier. We'll get to the size and scale of, let's say, times, or, you know, a typical saw, right? Legacy, sore, very way faster. And we will be there in terms of number of connectors sooner than later, right? So that's number one. So I understood the MCP layer part. What are the other pieces of the stack that we can talk about? Yeah,
is not the best optimal, yeah, which is not what you want to do. So, so that's on the MCP side. The reason I'm also asking is, let's say, if we make a bet on Kindle, right, I'm also making a bet that integrations will be easier. We'll get to the size and scale of, let's say, times, or, you know, a typical saw, right? Legacy, sore, very way faster. And we will be there in terms of number of connectors sooner than later, right? So that's number one. So I understood the MCP layer part. What are the other pieces of the stack that we can talk about? Yeah,
is not the best optimal, yeah, which is not what you want to do. So, so that's on the MCP side. The reason I'm also asking is, let's say, if we make a bet on Kindle, right, I'm also making a bet that integrations will be easier. We'll get to the size and scale of, let's say, times, or, you know, a typical saw, right? Legacy, sore, very way faster. And we will be there in terms of number of connectors sooner than later, right? So that's number one. So I understood the MCP layer part. What are the other pieces of the stack that we can talk about? Yeah,
is not the best optimal, yeah, which is not what you want to do. So, so that's on the MCP side. The reason I'm also asking is, let's say, if we make a bet on Kindle, right, I'm also making a bet that integrations will be easier. We'll get to the size and scale of, let's say, times, or, you know, a typical saw, right? Legacy, sore, very way faster. And we will be there in terms of number of connectors sooner than later, right? So that's number one. So I understood the MCP layer part. What are the other pieces of the stack that we can talk about? Yeah,
S Speaker 451:14so the other big tool piece, so I'll kind of like, go buy tools first. That's fine. That's funny. Essential brain. So the other big key tool part is the sandbox we're currently using Daytona. We might switch this micro sandbox, but effectively, it's just a a way to have these isolated sandbox containers that you can just go run arbitrary bash or script commands, code, whatever. And the whole point is that, like they just need, we just need a way to have them isolated in a protected manner, be able to save the state so that, like the AI agent, can go back to that exact same sandbox, because it'll iteratively do a set of commands. So it might say, pull the GitHub repo. Now walk the file hierarchy, so it's doing a set of actions in that sandbox. So it's effectively like it's SSH into a virtual machine somewhere, and now iteratively do some work, kind of like a cloud code or cursor local machine. We're doing a cloud environment, and that gives a lot of power, because the one the version of Linux that we're running in that sandbox now is Kali Linux. Back to the whole security focus versus general platform right now, since our sales plays are more focused on security, poly Linux, which already is, comes loaded with all the like, tools and things that have pretty engineered one so that's like, that's a really key foundation. That's also a scratch pad that they can use for downloading files. And if it wants to, you know, certain files can be, like, really large. Or if it's trying to navigate a code base right using the sandbox, it can just use standard commands to say, get me just these set of give me this, these rows in the file so I can or these lines in the file. So it gives it a lot more power than just grabbing the whole file, and it doesn't blow up the context as much, is
so the other big tool piece, so I'll kind of like, go buy tools first. That's fine. That's funny. Essential brain. So the other big key tool part is the sandbox we're currently using Daytona. We might switch this micro sandbox, but effectively, it's just a a way to have these isolated sandbox containers that you can just go run arbitrary bash or script commands, code, whatever. And the whole point is that, like they just need, we just need a way to have them isolated in a protected manner, be able to save the state so that, like the AI agent, can go back to that exact same sandbox, because it'll iteratively do a set of commands. So it might say, pull the GitHub repo. Now walk the file hierarchy, so it's doing a set of actions in that sandbox. So it's effectively like it's SSH into a virtual machine somewhere, and now iteratively do some work, kind of like a cloud code or cursor local machine. We're doing a cloud environment, and that gives a lot of power, because the one the version of Linux that we're running in that sandbox now is Kali Linux. Back to the whole security focus versus general platform right now, since our sales plays are more focused on security, poly Linux, which already is, comes loaded with all the like, tools and things that have pretty engineered one so that's like, that's a really key foundation. That's also a scratch pad that they can use for downloading files. And if it wants to, you know, certain files can be, like, really large. Or if it's trying to navigate a code base right using the sandbox, it can just use standard commands to say, get me just these set of give me this, these rows in the file so I can or these lines in the file. So it gives it a lot more power than just grabbing the whole file, and it doesn't blow up the context as much, is
so the other big tool piece, so I'll kind of like, go buy tools first. That's fine. That's funny. Essential brain. So the other big key tool part is the sandbox we're currently using Daytona. We might switch this micro sandbox, but effectively, it's just a a way to have these isolated sandbox containers that you can just go run arbitrary bash or script commands, code, whatever. And the whole point is that, like they just need, we just need a way to have them isolated in a protected manner, be able to save the state so that, like the AI agent, can go back to that exact same sandbox, because it'll iteratively do a set of commands. So it might say, pull the GitHub repo. Now walk the file hierarchy, so it's doing a set of actions in that sandbox. So it's effectively like it's SSH into a virtual machine somewhere, and now iteratively do some work, kind of like a cloud code or cursor local machine. We're doing a cloud environment, and that gives a lot of power, because the one the version of Linux that we're running in that sandbox now is Kali Linux. Back to the whole security focus versus general platform right now, since our sales plays are more focused on security, poly Linux, which already is, comes loaded with all the like, tools and things that have pretty engineered one so that's like, that's a really key foundation. That's also a scratch pad that they can use for downloading files. And if it wants to, you know, certain files can be, like, really large. Or if it's trying to navigate a code base right using the sandbox, it can just use standard commands to say, get me just these set of give me this, these rows in the file so I can or these lines in the file. So it gives it a lot more power than just grabbing the whole file, and it doesn't blow up the context as much, is
so the other big tool piece, so I'll kind of like, go buy tools first. That's fine. That's funny. Essential brain. So the other big key tool part is the sandbox we're currently using Daytona. We might switch this micro sandbox, but effectively, it's just a a way to have these isolated sandbox containers that you can just go run arbitrary bash or script commands, code, whatever. And the whole point is that, like they just need, we just need a way to have them isolated in a protected manner, be able to save the state so that, like the AI agent, can go back to that exact same sandbox, because it'll iteratively do a set of commands. So it might say, pull the GitHub repo. Now walk the file hierarchy, so it's doing a set of actions in that sandbox. So it's effectively like it's SSH into a virtual machine somewhere, and now iteratively do some work, kind of like a cloud code or cursor local machine. We're doing a cloud environment, and that gives a lot of power, because the one the version of Linux that we're running in that sandbox now is Kali Linux. Back to the whole security focus versus general platform right now, since our sales plays are more focused on security, poly Linux, which already is, comes loaded with all the like, tools and things that have pretty engineered one so that's like, that's a really key foundation. That's also a scratch pad that they can use for downloading files. And if it wants to, you know, certain files can be, like, really large. Or if it's trying to navigate a code base right using the sandbox, it can just use standard commands to say, get me just these set of give me this, these rows in the file so I can or these lines in the file. So it gives it a lot more power than just grabbing the whole file, and it doesn't blow up the context as much, is
S Speaker 553:09that like a foundation layer of most of like, let's say, if I think about the next gen or agent first, you know, platform, will that be a core foundation layer of most of the agents today?
that like a foundation layer of most of like, let's say, if I think about the next gen or agent first, you know, platform, will that be a core foundation layer of most of the agents today?
that like a foundation layer of most of like, let's say, if I think about the next gen or agent first, you know, platform, will that be a core foundation layer of most of the agents today?
that like a foundation layer of most of like, let's say, if I think about the next gen or agent first, you know, platform, will that be a core foundation layer of most of the agents today?
S Speaker 453:25I think so. I mean, it depends on what they're certainly for engineering, DevOps, SecOps, like, if you're, if you're the type of engineer that would work in the terminal, obviously, to automate their job, automate the work that they're doing, yeah, like, able to play in that, like, terminal esque environment. Now, talking about, like an HR professional, then probably not as much. However, it may still be useful, because that might be just the scratch pad for it to write code, put the code down, like, if it's going to have trend like, let's say, gave an example in, like, with finance space. Let's say you're just trying to do some analysis of, like, a large spreadsheet. Yeah. Might be the most effective way is to write this, the large CSV that's bigger than it can fit into the context, write a Python script, also put that into the sandbox, and then run the Python script, which will do the analysis. Maybe the AI agent reads some of the header rows, yeah, but read the whole context into memory all at once, so that that can be another way that it's as a tool for the AI agent to do a bunch of like work, and I mean, certainly behind the scenes, that's what behind the scenes? Yeah, they're
I think so. I mean, it depends on what they're certainly for engineering, DevOps, SecOps, like, if you're, if you're the type of engineer that would work in the terminal, obviously, to automate their job, automate the work that they're doing, yeah, like, able to play in that, like, terminal esque environment. Now, talking about, like an HR professional, then probably not as much. However, it may still be useful, because that might be just the scratch pad for it to write code, put the code down, like, if it's going to have trend like, let's say, gave an example in, like, with finance space. Let's say you're just trying to do some analysis of, like, a large spreadsheet. Yeah. Might be the most effective way is to write this, the large CSV that's bigger than it can fit into the context, write a Python script, also put that into the sandbox, and then run the Python script, which will do the analysis. Maybe the AI agent reads some of the header rows, yeah, but read the whole context into memory all at once, so that that can be another way that it's as a tool for the AI agent to do a bunch of like work, and I mean, certainly behind the scenes, that's what behind the scenes? Yeah, they're
I think so. I mean, it depends on what they're certainly for engineering, DevOps, SecOps, like, if you're, if you're the type of engineer that would work in the terminal, obviously, to automate their job, automate the work that they're doing, yeah, like, able to play in that, like, terminal esque environment. Now, talking about, like an HR professional, then probably not as much. However, it may still be useful, because that might be just the scratch pad for it to write code, put the code down, like, if it's going to have trend like, let's say, gave an example in, like, with finance space. Let's say you're just trying to do some analysis of, like, a large spreadsheet. Yeah. Might be the most effective way is to write this, the large CSV that's bigger than it can fit into the context, write a Python script, also put that into the sandbox, and then run the Python script, which will do the analysis. Maybe the AI agent reads some of the header rows, yeah, but read the whole context into memory all at once, so that that can be another way that it's as a tool for the AI agent to do a bunch of like work, and I mean, certainly behind the scenes, that's what behind the scenes? Yeah, they're
I think so. I mean, it depends on what they're certainly for engineering, DevOps, SecOps, like, if you're, if you're the type of engineer that would work in the terminal, obviously, to automate their job, automate the work that they're doing, yeah, like, able to play in that, like, terminal esque environment. Now, talking about, like an HR professional, then probably not as much. However, it may still be useful, because that might be just the scratch pad for it to write code, put the code down, like, if it's going to have trend like, let's say, gave an example in, like, with finance space. Let's say you're just trying to do some analysis of, like, a large spreadsheet. Yeah. Might be the most effective way is to write this, the large CSV that's bigger than it can fit into the context, write a Python script, also put that into the sandbox, and then run the Python script, which will do the analysis. Maybe the AI agent reads some of the header rows, yeah, but read the whole context into memory all at once, so that that can be another way that it's as a tool for the AI agent to do a bunch of like work, and I mean, certainly behind the scenes, that's what behind the scenes? Yeah, they're
54:32all doing that, spinning up, spinning up, spinning off
all doing that, spinning up, spinning up, spinning off
all doing that, spinning up, spinning up, spinning off
all doing that, spinning up, spinning up, spinning off
S Speaker 554:35these virtual sandboxes by itself. So let's say you talked about your agent will figure out what tool to call what what fields or what documents to access, right? It'll provision a sandbox by itself. Is that what the what the understanding is?
these virtual sandboxes by itself. So let's say you talked about your agent will figure out what tool to call what what fields or what documents to access, right? It'll provision a sandbox by itself. Is that what the what the understanding is?
these virtual sandboxes by itself. So let's say you talked about your agent will figure out what tool to call what what fields or what documents to access, right? It'll provision a sandbox by itself. Is that what the what the understanding is?
these virtual sandboxes by itself. So let's say you talked about your agent will figure out what tool to call what what fields or what documents to access, right? It'll provision a sandbox by itself. Is that what the what the understanding is?
S Speaker 454:50Yeah. So we have a system that we use, like, so we one of the architectural, and I should probably said this at the beginning, like, one of the architectural paired principles that we follow is that our entire stack, and I think I told you this at Black Hat, our entire stack, can be run in a customer's environment. Like, yes, even, like an air gapped
Yeah. So we have a system that we use, like, so we one of the architectural, and I should probably said this at the beginning, like, one of the architectural paired principles that we follow is that our entire stack, and I think I told you this at Black Hat, our entire stack, can be run in a customer's environment. Like, yes, even, like an air gapped
Yeah. So we have a system that we use, like, so we one of the architectural, and I should probably said this at the beginning, like, one of the architectural paired principles that we follow is that our entire stack, and I think I told you this at Black Hat, our entire stack, can be run in a customer's environment. Like, yes, even, like an air gapped
Yeah. So we have a system that we use, like, so we one of the architectural, and I should probably said this at the beginning, like, one of the architectural paired principles that we follow is that our entire stack, and I think I told you this at Black Hat, our entire stack, can be run in a customer's environment. Like, yes, even, like an air gapped
S Speaker 555:07Oh, yes, yes. I remember that because we talked about E to B as well. And you mentioned about HashiCorp dependency, right? Customers don't like that. And I remember we also mentioned that the self managed option with E to B is not that wasn't that I'm trying to look at my look through my notes,
Oh, yes, yes. I remember that because we talked about E to B as well. And you mentioned about HashiCorp dependency, right? Customers don't like that. And I remember we also mentioned that the self managed option with E to B is not that wasn't that I'm trying to look at my look through my notes,
Oh, yes, yes. I remember that because we talked about E to B as well. And you mentioned about HashiCorp dependency, right? Customers don't like that. And I remember we also mentioned that the self managed option with E to B is not that wasn't that I'm trying to look at my look through my notes,
Oh, yes, yes. I remember that because we talked about E to B as well. And you mentioned about HashiCorp dependency, right? Customers don't like that. And I remember we also mentioned that the self managed option with E to B is not that wasn't that I'm trying to look at my look through my notes,
S Speaker 455:23yeah. I mean, each of you might be okay, like you have to run nomad. Nomad has a license. There it is, open. It's not officially, like OSI blessed, but they're the term. Their license is probably permissive enough that that we could run it in our SAS. Our customers could run it without it, because it's not a competitive offering, but it's not most a desirable license. So that was, like one of the drawbacks. But we, you know, there's a lot of options out there, like, we don't have to invent that technology. There's a lot of options that are open source, that we can run, like micro sandbox, Daytona. Daytona actually is an open source for their so we already
yeah. I mean, each of you might be okay, like you have to run nomad. Nomad has a license. There it is, open. It's not officially, like OSI blessed, but they're the term. Their license is probably permissive enough that that we could run it in our SAS. Our customers could run it without it, because it's not a competitive offering, but it's not most a desirable license. So that was, like one of the drawbacks. But we, you know, there's a lot of options out there, like, we don't have to invent that technology. There's a lot of options that are open source, that we can run, like micro sandbox, Daytona. Daytona actually is an open source for their so we already
yeah. I mean, each of you might be okay, like you have to run nomad. Nomad has a license. There it is, open. It's not officially, like OSI blessed, but they're the term. Their license is probably permissive enough that that we could run it in our SAS. Our customers could run it without it, because it's not a competitive offering, but it's not most a desirable license. So that was, like one of the drawbacks. But we, you know, there's a lot of options out there, like, we don't have to invent that technology. There's a lot of options that are open source, that we can run, like micro sandbox, Daytona. Daytona actually is an open source for their so we already
yeah. I mean, each of you might be okay, like you have to run nomad. Nomad has a license. There it is, open. It's not officially, like OSI blessed, but they're the term. Their license is probably permissive enough that that we could run it in our SAS. Our customers could run it without it, because it's not a competitive offering, but it's not most a desirable license. So that was, like one of the drawbacks. But we, you know, there's a lot of options out there, like, we don't have to invent that technology. There's a lot of options that are open source, that we can run, like micro sandbox, Daytona. Daytona actually is an open source for their so we already
S Speaker 556:04have an enterprise they have an enterprise license, they have an open source community, but most of the features are an enterprise license, right? Exactly.
have an enterprise they have an enterprise license, they have an open source community, but most of the features are an enterprise license, right? Exactly.
have an enterprise they have an enterprise license, they have an open source community, but most of the features are an enterprise license, right? Exactly.
have an enterprise they have an enterprise license, they have an open source community, but most of the features are an enterprise license, right? Exactly.
S Speaker 456:10So, so we may switch to like, there's a lot of options, like, there's judge zero. So we may switch to like, micro sandbox. But point for us is that like, it just, it's a capability. We don't need to go invent like, the VM like orchestration part of it. We just need a system there that we can run in a self managed environment that has that a bit and where we can establish which images we want to run. So like Kali Linux, or pre installed packages, whatever we want to have on that. And then so like beyond that, like architectural, architecture wise, we are, you know, we have the central AI agent engine, which is the thing that effectively is running this agentic loop that's got a set of prompts, of course, that like system prompts that guide it, and that's where it's aware of like, what it's supposed to be doing. It gets the user prompts, it's given all the list of tools, and effectively, it runs in a loop until it considers itself done. We're all like, very common standards here, like, we're using the vercel ai SDK, which kind of has, like, pretty standardized ways of representing these conversational histories, yeah, but when you run it from like chat, then effectively, it just kicks that off, runs in the background. We have, we use a number of systems for durability, so, yeah, once you kick it off, you know something on the back end will then capture that and then make sure that that continues to handle reliably. If you if your browser crashes, you restart, you resume. There's a resumable streaming component that will then stream back the latest date, back to your browser. So that's like, if you do a refresh, it'll like all come back in, you know, but like that, that back end, part of it is kind of intrinsically tied in with our overall platform. So that's kind of like our core back end, and that's also what handles just general chat conversations, you know, our workflow architecture. So we have chat and we also have workflow agents, yeah. So the workflow agents, you can either build them as like, static steps, right? Or you could, you could call dynamically any API, so any REST API can just be called. So even if we don't have an MCP server available, you could statically define that call and just say, like, all right, I'm going to call this API, and I'm going to call this API, and you could put into your credentials and everything, and so any REST API can be called. And then you could kind of put a set of those steps, and then you could put one of these ejecting steps in there, which now allows you to call out to, you know, kind of run the exact same that I showed you in the demo, where it makes basically call out to any of these set of tools. So those kind of like the core pieces. Then, of course, behind all this, you know, we we have all of our compliance and sort of administrative policy engine that runs though we have, we use light LM as a layer underneath all of our models. So when we call out to an AI model, it actually goes through this proxy layer that's leveraging light LLM, which is another open source framework, and that has folks that allow us to apply policy information for the administrator, such as like, which models are allowed. It also has like, that's where we hook in, DLP, that's where we hook in, like, auto logging. So there's that. So that's kind of like our hook point for all of all the sort of policy information in a single checkpoint. So essentially, it's like we have our front end, we have our back end, which is kind of like the core back end that powers most of the product, product capabilities lay LM as the proxy layer to go access any LLM because we are agnostic to llms. Maybe support, say, a couple dozen out of box, but any customer can bring their own bottles. If they want to configure within our platform. They can just put in their credentials and but there's a JSON structure that I have to add, but once you've incorporated that, then any, any model can be incorporated, but that all sits below this proxy layer. So all the policy information kind of goes through that. And
So, so we may switch to like, there's a lot of options, like, there's judge zero. So we may switch to like, micro sandbox. But point for us is that like, it just, it's a capability. We don't need to go invent like, the VM like orchestration part of it. We just need a system there that we can run in a self managed environment that has that a bit and where we can establish which images we want to run. So like Kali Linux, or pre installed packages, whatever we want to have on that. And then so like beyond that, like architectural, architecture wise, we are, you know, we have the central AI agent engine, which is the thing that effectively is running this agentic loop that's got a set of prompts, of course, that like system prompts that guide it, and that's where it's aware of like, what it's supposed to be doing. It gets the user prompts, it's given all the list of tools, and effectively, it runs in a loop until it considers itself done. We're all like, very common standards here, like, we're using the vercel ai SDK, which kind of has, like, pretty standardized ways of representing these conversational histories, yeah, but when you run it from like chat, then effectively, it just kicks that off, runs in the background. We have, we use a number of systems for durability, so, yeah, once you kick it off, you know something on the back end will then capture that and then make sure that that continues to handle reliably. If you if your browser crashes, you restart, you resume. There's a resumable streaming component that will then stream back the latest date, back to your browser. So that's like, if you do a refresh, it'll like all come back in, you know, but like that, that back end, part of it is kind of intrinsically tied in with our overall platform. So that's kind of like our core back end, and that's also what handles just general chat conversations, you know, our workflow architecture. So we have chat and we also have workflow agents, yeah. So the workflow agents, you can either build them as like, static steps, right? Or you could, you could call dynamically any API, so any REST API can just be called. So even if we don't have an MCP server available, you could statically define that call and just say, like, all right, I'm going to call this API, and I'm going to call this API, and you could put into your credentials and everything, and so any REST API can be called. And then you could kind of put a set of those steps, and then you could put one of these ejecting steps in there, which now allows you to call out to, you know, kind of run the exact same that I showed you in the demo, where it makes basically call out to any of these set of tools. So those kind of like the core pieces. Then, of course, behind all this, you know, we we have all of our compliance and sort of administrative policy engine that runs though we have, we use light LM as a layer underneath all of our models. So when we call out to an AI model, it actually goes through this proxy layer that's leveraging light LLM, which is another open source framework, and that has folks that allow us to apply policy information for the administrator, such as like, which models are allowed. It also has like, that's where we hook in, DLP, that's where we hook in, like, auto logging. So there's that. So that's kind of like our hook point for all of all the sort of policy information in a single checkpoint. So essentially, it's like we have our front end, we have our back end, which is kind of like the core back end that powers most of the product, product capabilities lay LM as the proxy layer to go access any LLM because we are agnostic to llms. Maybe support, say, a couple dozen out of box, but any customer can bring their own bottles. If they want to configure within our platform. They can just put in their credentials and but there's a JSON structure that I have to add, but once you've incorporated that, then any, any model can be incorporated, but that all sits below this proxy layer. So all the policy information kind of goes through that. And
So, so we may switch to like, there's a lot of options, like, there's judge zero. So we may switch to like, micro sandbox. But point for us is that like, it just, it's a capability. We don't need to go invent like, the VM like orchestration part of it. We just need a system there that we can run in a self managed environment that has that a bit and where we can establish which images we want to run. So like Kali Linux, or pre installed packages, whatever we want to have on that. And then so like beyond that, like architectural, architecture wise, we are, you know, we have the central AI agent engine, which is the thing that effectively is running this agentic loop that's got a set of prompts, of course, that like system prompts that guide it, and that's where it's aware of like, what it's supposed to be doing. It gets the user prompts, it's given all the list of tools, and effectively, it runs in a loop until it considers itself done. We're all like, very common standards here, like, we're using the vercel ai SDK, which kind of has, like, pretty standardized ways of representing these conversational histories, yeah, but when you run it from like chat, then effectively, it just kicks that off, runs in the background. We have, we use a number of systems for durability, so, yeah, once you kick it off, you know something on the back end will then capture that and then make sure that that continues to handle reliably. If you if your browser crashes, you restart, you resume. There's a resumable streaming component that will then stream back the latest date, back to your browser. So that's like, if you do a refresh, it'll like all come back in, you know, but like that, that back end, part of it is kind of intrinsically tied in with our overall platform. So that's kind of like our core back end, and that's also what handles just general chat conversations, you know, our workflow architecture. So we have chat and we also have workflow agents, yeah. So the workflow agents, you can either build them as like, static steps, right? Or you could, you could call dynamically any API, so any REST API can just be called. So even if we don't have an MCP server available, you could statically define that call and just say, like, all right, I'm going to call this API, and I'm going to call this API, and you could put into your credentials and everything, and so any REST API can be called. And then you could kind of put a set of those steps, and then you could put one of these ejecting steps in there, which now allows you to call out to, you know, kind of run the exact same that I showed you in the demo, where it makes basically call out to any of these set of tools. So those kind of like the core pieces. Then, of course, behind all this, you know, we we have all of our compliance and sort of administrative policy engine that runs though we have, we use light LM as a layer underneath all of our models. So when we call out to an AI model, it actually goes through this proxy layer that's leveraging light LLM, which is another open source framework, and that has folks that allow us to apply policy information for the administrator, such as like, which models are allowed. It also has like, that's where we hook in, DLP, that's where we hook in, like, auto logging. So there's that. So that's kind of like our hook point for all of all the sort of policy information in a single checkpoint. So essentially, it's like we have our front end, we have our back end, which is kind of like the core back end that powers most of the product, product capabilities lay LM as the proxy layer to go access any LLM because we are agnostic to llms. Maybe support, say, a couple dozen out of box, but any customer can bring their own bottles. If they want to configure within our platform. They can just put in their credentials and but there's a JSON structure that I have to add, but once you've incorporated that, then any, any model can be incorporated, but that all sits below this proxy layer. So all the policy information kind of goes through that. And
So, so we may switch to like, there's a lot of options, like, there's judge zero. So we may switch to like, micro sandbox. But point for us is that like, it just, it's a capability. We don't need to go invent like, the VM like orchestration part of it. We just need a system there that we can run in a self managed environment that has that a bit and where we can establish which images we want to run. So like Kali Linux, or pre installed packages, whatever we want to have on that. And then so like beyond that, like architectural, architecture wise, we are, you know, we have the central AI agent engine, which is the thing that effectively is running this agentic loop that's got a set of prompts, of course, that like system prompts that guide it, and that's where it's aware of like, what it's supposed to be doing. It gets the user prompts, it's given all the list of tools, and effectively, it runs in a loop until it considers itself done. We're all like, very common standards here, like, we're using the vercel ai SDK, which kind of has, like, pretty standardized ways of representing these conversational histories, yeah, but when you run it from like chat, then effectively, it just kicks that off, runs in the background. We have, we use a number of systems for durability, so, yeah, once you kick it off, you know something on the back end will then capture that and then make sure that that continues to handle reliably. If you if your browser crashes, you restart, you resume. There's a resumable streaming component that will then stream back the latest date, back to your browser. So that's like, if you do a refresh, it'll like all come back in, you know, but like that, that back end, part of it is kind of intrinsically tied in with our overall platform. So that's kind of like our core back end, and that's also what handles just general chat conversations, you know, our workflow architecture. So we have chat and we also have workflow agents, yeah. So the workflow agents, you can either build them as like, static steps, right? Or you could, you could call dynamically any API, so any REST API can just be called. So even if we don't have an MCP server available, you could statically define that call and just say, like, all right, I'm going to call this API, and I'm going to call this API, and you could put into your credentials and everything, and so any REST API can be called. And then you could kind of put a set of those steps, and then you could put one of these ejecting steps in there, which now allows you to call out to, you know, kind of run the exact same that I showed you in the demo, where it makes basically call out to any of these set of tools. So those kind of like the core pieces. Then, of course, behind all this, you know, we we have all of our compliance and sort of administrative policy engine that runs though we have, we use light LM as a layer underneath all of our models. So when we call out to an AI model, it actually goes through this proxy layer that's leveraging light LLM, which is another open source framework, and that has folks that allow us to apply policy information for the administrator, such as like, which models are allowed. It also has like, that's where we hook in, DLP, that's where we hook in, like, auto logging. So there's that. So that's kind of like our hook point for all of all the sort of policy information in a single checkpoint. So essentially, it's like we have our front end, we have our back end, which is kind of like the core back end that powers most of the product, product capabilities lay LM as the proxy layer to go access any LLM because we are agnostic to llms. Maybe support, say, a couple dozen out of box, but any customer can bring their own bottles. If they want to configure within our platform. They can just put in their credentials and but there's a JSON structure that I have to add, but once you've incorporated that, then any, any model can be incorporated, but that all sits below this proxy layer. So all the policy information kind of goes through that. And
S Speaker 51:00:22anything on the audit, audit, auditability, or, you know, accuracy, or like a post. So one perspective I've had since talking to so many agent startups is that it is hard to always be certain of their outcome, right? And some bit of it is prevented by deterministic tool calling, because you know that the tool will give you a specific output, right? But depending on the persona, depending on the context, depending on you know how what flow it's it's actually a part of the agent might behave differently. So do you do any auditability today to ensure that your agents are always doing what they're supposed to do and are highly reliable. And do you run into that chart? Because I'm talking to startups that are solving that exact pain point, and these are and I'm talking autonomous agents, Brian, I'm not talking about your rule based or, you know, heavy guardrail, the you know agents. So where are you in that journey, right? Are you fully autonomous, or are you still like we actually can tightly control what it can do, what it can't do, so we are very certain of its reliability. How would you position Kendall today?
anything on the audit, audit, auditability, or, you know, accuracy, or like a post. So one perspective I've had since talking to so many agent startups is that it is hard to always be certain of their outcome, right? And some bit of it is prevented by deterministic tool calling, because you know that the tool will give you a specific output, right? But depending on the persona, depending on the context, depending on you know how what flow it's it's actually a part of the agent might behave differently. So do you do any auditability today to ensure that your agents are always doing what they're supposed to do and are highly reliable. And do you run into that chart? Because I'm talking to startups that are solving that exact pain point, and these are and I'm talking autonomous agents, Brian, I'm not talking about your rule based or, you know, heavy guardrail, the you know agents. So where are you in that journey, right? Are you fully autonomous, or are you still like we actually can tightly control what it can do, what it can't do, so we are very certain of its reliability. How would you position Kendall today?
anything on the audit, audit, auditability, or, you know, accuracy, or like a post. So one perspective I've had since talking to so many agent startups is that it is hard to always be certain of their outcome, right? And some bit of it is prevented by deterministic tool calling, because you know that the tool will give you a specific output, right? But depending on the persona, depending on the context, depending on you know how what flow it's it's actually a part of the agent might behave differently. So do you do any auditability today to ensure that your agents are always doing what they're supposed to do and are highly reliable. And do you run into that chart? Because I'm talking to startups that are solving that exact pain point, and these are and I'm talking autonomous agents, Brian, I'm not talking about your rule based or, you know, heavy guardrail, the you know agents. So where are you in that journey, right? Are you fully autonomous, or are you still like we actually can tightly control what it can do, what it can't do, so we are very certain of its reliability. How would you position Kendall today?
anything on the audit, audit, auditability, or, you know, accuracy, or like a post. So one perspective I've had since talking to so many agent startups is that it is hard to always be certain of their outcome, right? And some bit of it is prevented by deterministic tool calling, because you know that the tool will give you a specific output, right? But depending on the persona, depending on the context, depending on you know how what flow it's it's actually a part of the agent might behave differently. So do you do any auditability today to ensure that your agents are always doing what they're supposed to do and are highly reliable. And do you run into that chart? Because I'm talking to startups that are solving that exact pain point, and these are and I'm talking autonomous agents, Brian, I'm not talking about your rule based or, you know, heavy guardrail, the you know agents. So where are you in that journey, right? Are you fully autonomous, or are you still like we actually can tightly control what it can do, what it can't do, so we are very certain of its reliability. How would you position Kendall today?
S Speaker 41:01:40We don't make a stance on that as like our product. I mean, I mean we are we make sure that, we make sure MCP servers are tested, that we we at least have like that, that API contract handled. But really the reliance is on like the human to establish their tool policies in the way that that that. So one of the things that's coming out as part of the GA is the ability for the human to right now you can basically say, like, this tool is available. This tool is not available, but if we want to make it, but if they want to really, where we want to get to with that is this policy is something that can run any like this is an auto proven policy. Like this tool can be called because maybe it's a read only action, and I don't need to prove it as a human. Southern set of tool calls like, you know, making some sort of destructed right? I always need to prove it as a human. So that's, that's, that's all kind of the package of how we allow humans to control. And this control can happen both at the end user level as well as the administrative level. So the administrator may say these are the set of tools that I'm even allowing my organization at all. But then user can kind of decide, like, well, what things do I want to manually approve, versus like, yeah, I've already blessed this type of operation.
We don't make a stance on that as like our product. I mean, I mean we are we make sure that, we make sure MCP servers are tested, that we we at least have like that, that API contract handled. But really the reliance is on like the human to establish their tool policies in the way that that that. So one of the things that's coming out as part of the GA is the ability for the human to right now you can basically say, like, this tool is available. This tool is not available, but if we want to make it, but if they want to really, where we want to get to with that is this policy is something that can run any like this is an auto proven policy. Like this tool can be called because maybe it's a read only action, and I don't need to prove it as a human. Southern set of tool calls like, you know, making some sort of destructed right? I always need to prove it as a human. So that's, that's, that's all kind of the package of how we allow humans to control. And this control can happen both at the end user level as well as the administrative level. So the administrator may say these are the set of tools that I'm even allowing my organization at all. But then user can kind of decide, like, well, what things do I want to manually approve, versus like, yeah, I've already blessed this type of operation.
We don't make a stance on that as like our product. I mean, I mean we are we make sure that, we make sure MCP servers are tested, that we we at least have like that, that API contract handled. But really the reliance is on like the human to establish their tool policies in the way that that that. So one of the things that's coming out as part of the GA is the ability for the human to right now you can basically say, like, this tool is available. This tool is not available, but if we want to make it, but if they want to really, where we want to get to with that is this policy is something that can run any like this is an auto proven policy. Like this tool can be called because maybe it's a read only action, and I don't need to prove it as a human. Southern set of tool calls like, you know, making some sort of destructed right? I always need to prove it as a human. So that's, that's, that's all kind of the package of how we allow humans to control. And this control can happen both at the end user level as well as the administrative level. So the administrator may say these are the set of tools that I'm even allowing my organization at all. But then user can kind of decide, like, well, what things do I want to manually approve, versus like, yeah, I've already blessed this type of operation.
We don't make a stance on that as like our product. I mean, I mean we are we make sure that, we make sure MCP servers are tested, that we we at least have like that, that API contract handled. But really the reliance is on like the human to establish their tool policies in the way that that that. So one of the things that's coming out as part of the GA is the ability for the human to right now you can basically say, like, this tool is available. This tool is not available, but if we want to make it, but if they want to really, where we want to get to with that is this policy is something that can run any like this is an auto proven policy. Like this tool can be called because maybe it's a read only action, and I don't need to prove it as a human. Southern set of tool calls like, you know, making some sort of destructed right? I always need to prove it as a human. So that's, that's, that's all kind of the package of how we allow humans to control. And this control can happen both at the end user level as well as the administrative level. So the administrator may say these are the set of tools that I'm even allowing my organization at all. But then user can kind of decide, like, well, what things do I want to manually approve, versus like, yeah, I've already blessed this type of operation.
S Speaker 51:03:00Understood, understood. Okay, that makes that makes sense, yeah?
Understood, understood. Okay, that makes that makes sense, yeah?
Understood, understood. Okay, that makes that makes sense, yeah?
Understood, understood. Okay, that makes that makes sense, yeah?
S Speaker 41:03:04But I mean, like, you know that said it doesn't mean that, like, there's that's kind of about, like, is this going to do something bad? But there's also still failures, like, maybe, maybe the AI gets the API call wrong, or, yeah. So those are the types of things that we're working on. We have, we have a quality team that's now hill climbing on this, and that's actually two, two sides of it. So like Ron mentioned, we have our own proprietary model. So we actually are building out fine tuning data sets based on learning from the product like, hey, this this tool call failed, so let's incorporate a tweak to how we build out our supervised fine tuning data set to call those tools in the future. But then the the other side of it is just the agentic quality team, so they're the ones more focused on the architectural, dark agent architecture around it. So you have this AI agent brain in the middle, but really it's all about context engineering around that, right? So it's by the tools, too many tools all at once. Like, there's all these techniques that we're building out of an internal benchmark for like, here's the set of use cases that it should be get really good at, and how we can and scoring them for now we have, like, rudimentary things such as, like, you know, how often does it we can we can still tell whether it's getting it right or wrong. Like, did it fail or not? That is a baseline metric. But we're trying to build a more comprehensive benchmark so we can kind of hill climb against more and more difficult tasks, and that allowed us to, like, optimize the way that this agentic framework and how we do context engineering around it, make sure we passing just the right information for tool calling, deal with some of the other problems with like, like, one of the problems that I think everybody hits in this is that you run out of context, you know, You call an API, you get back too much context. So there's a lot of that type of quality work that we have, quality team that will that's that's continuing to optimize over
But I mean, like, you know that said it doesn't mean that, like, there's that's kind of about, like, is this going to do something bad? But there's also still failures, like, maybe, maybe the AI gets the API call wrong, or, yeah. So those are the types of things that we're working on. We have, we have a quality team that's now hill climbing on this, and that's actually two, two sides of it. So like Ron mentioned, we have our own proprietary model. So we actually are building out fine tuning data sets based on learning from the product like, hey, this this tool call failed, so let's incorporate a tweak to how we build out our supervised fine tuning data set to call those tools in the future. But then the the other side of it is just the agentic quality team, so they're the ones more focused on the architectural, dark agent architecture around it. So you have this AI agent brain in the middle, but really it's all about context engineering around that, right? So it's by the tools, too many tools all at once. Like, there's all these techniques that we're building out of an internal benchmark for like, here's the set of use cases that it should be get really good at, and how we can and scoring them for now we have, like, rudimentary things such as, like, you know, how often does it we can we can still tell whether it's getting it right or wrong. Like, did it fail or not? That is a baseline metric. But we're trying to build a more comprehensive benchmark so we can kind of hill climb against more and more difficult tasks, and that allowed us to, like, optimize the way that this agentic framework and how we do context engineering around it, make sure we passing just the right information for tool calling, deal with some of the other problems with like, like, one of the problems that I think everybody hits in this is that you run out of context, you know, You call an API, you get back too much context. So there's a lot of that type of quality work that we have, quality team that will that's that's continuing to optimize over
But I mean, like, you know that said it doesn't mean that, like, there's that's kind of about, like, is this going to do something bad? But there's also still failures, like, maybe, maybe the AI gets the API call wrong, or, yeah. So those are the types of things that we're working on. We have, we have a quality team that's now hill climbing on this, and that's actually two, two sides of it. So like Ron mentioned, we have our own proprietary model. So we actually are building out fine tuning data sets based on learning from the product like, hey, this this tool call failed, so let's incorporate a tweak to how we build out our supervised fine tuning data set to call those tools in the future. But then the the other side of it is just the agentic quality team, so they're the ones more focused on the architectural, dark agent architecture around it. So you have this AI agent brain in the middle, but really it's all about context engineering around that, right? So it's by the tools, too many tools all at once. Like, there's all these techniques that we're building out of an internal benchmark for like, here's the set of use cases that it should be get really good at, and how we can and scoring them for now we have, like, rudimentary things such as, like, you know, how often does it we can we can still tell whether it's getting it right or wrong. Like, did it fail or not? That is a baseline metric. But we're trying to build a more comprehensive benchmark so we can kind of hill climb against more and more difficult tasks, and that allowed us to, like, optimize the way that this agentic framework and how we do context engineering around it, make sure we passing just the right information for tool calling, deal with some of the other problems with like, like, one of the problems that I think everybody hits in this is that you run out of context, you know, You call an API, you get back too much context. So there's a lot of that type of quality work that we have, quality team that will that's that's continuing to optimize over
But I mean, like, you know that said it doesn't mean that, like, there's that's kind of about, like, is this going to do something bad? But there's also still failures, like, maybe, maybe the AI gets the API call wrong, or, yeah. So those are the types of things that we're working on. We have, we have a quality team that's now hill climbing on this, and that's actually two, two sides of it. So like Ron mentioned, we have our own proprietary model. So we actually are building out fine tuning data sets based on learning from the product like, hey, this this tool call failed, so let's incorporate a tweak to how we build out our supervised fine tuning data set to call those tools in the future. But then the the other side of it is just the agentic quality team, so they're the ones more focused on the architectural, dark agent architecture around it. So you have this AI agent brain in the middle, but really it's all about context engineering around that, right? So it's by the tools, too many tools all at once. Like, there's all these techniques that we're building out of an internal benchmark for like, here's the set of use cases that it should be get really good at, and how we can and scoring them for now we have, like, rudimentary things such as, like, you know, how often does it we can we can still tell whether it's getting it right or wrong. Like, did it fail or not? That is a baseline metric. But we're trying to build a more comprehensive benchmark so we can kind of hill climb against more and more difficult tasks, and that allowed us to, like, optimize the way that this agentic framework and how we do context engineering around it, make sure we passing just the right information for tool calling, deal with some of the other problems with like, like, one of the problems that I think everybody hits in this is that you run out of context, you know, You call an API, you get back too much context. So there's a lot of that type of quality work that we have, quality team that will that's that's continuing to optimize over
1:04:58time. Does that? Does that
time. Does that? Does that
time. Does that? Does that
time. Does that? Does that