Meeting: Recursal Ai
Mon, Jul 29, 2024
3:00 PM
38 min
Priyesh P
AI language model development and potential f
URL: https://otter.ai/u/zN3mIWEgebdYKn59OZfu9fDFvzQ
Downloaded: 2025-12-22T15:03:15.030536
Method: text_extraction
============================================================

S Speaker 10:00For our foundation model on the list of supported models a lot from slides, and they were like we did that
For our foundation model on the list of supported models a lot from slides, and they were like we did that
For our foundation model on the list of supported models a lot from slides, and they were like we did that
For our foundation model on the list of supported models a lot from slides, and they were like we did that
S Speaker 10:36Very basic platform for this week. And last week, I'm actually in Singapore mostly because here for some AI events, right. So we had a solid AI gathering, which is, which is the gathering of LPs and stakeholders regarding Southeast Asia positioning in AI. There's only two polishable to the Support Station and to just one of it includes the one that got flown over
Very basic platform for this week. And last week, I'm actually in Singapore mostly because here for some AI events, right. So we had a solid AI gathering, which is, which is the gathering of LPs and stakeholders regarding Southeast Asia positioning in AI. There's only two polishable to the Support Station and to just one of it includes the one that got flown over
Very basic platform for this week. And last week, I'm actually in Singapore mostly because here for some AI events, right. So we had a solid AI gathering, which is, which is the gathering of LPs and stakeholders regarding Southeast Asia positioning in AI. There's only two polishable to the Support Station and to just one of it includes the one that got flown over
Very basic platform for this week. And last week, I'm actually in Singapore mostly because here for some AI events, right. So we had a solid AI gathering, which is, which is the gathering of LPs and stakeholders regarding Southeast Asia positioning in AI. There's only two polishable to the Support Station and to just one of it includes the one that got flown over
S Speaker 11:08Are in Singapore. The team is the team is actually scattered. So Australia, Singapore, Malaysia, New York, UK. We're fairly distributed, but I'm primarily based in SF because of direct connection to investors and potential customers, either.
Are in Singapore. The team is the team is actually scattered. So Australia, Singapore, Malaysia, New York, UK. We're fairly distributed, but I'm primarily based in SF because of direct connection to investors and potential customers, either.
Are in Singapore. The team is the team is actually scattered. So Australia, Singapore, Malaysia, New York, UK. We're fairly distributed, but I'm primarily based in SF because of direct connection to investors and potential customers, either.
Are in Singapore. The team is the team is actually scattered. So Australia, Singapore, Malaysia, New York, UK. We're fairly distributed, but I'm primarily based in SF because of direct connection to investors and potential customers, either.
S Speaker 31:37Your accent Are you from Singapore. You
S Speaker 11:41see this happens all the time. This is one sentence, okay. It's on the recorders. I just happened to live in San Francisco. Yeah, people don't really like there's there's like a meta game of sport the Singapore minutes in a certain amount of words. Or sentence that the average is more like three to four sentences.
see this happens all the time. This is one sentence, okay. It's on the recorders. I just happened to live in San Francisco. Yeah, people don't really like there's there's like a meta game of sport the Singapore minutes in a certain amount of words. Or sentence that the average is more like three to four sentences.
see this happens all the time. This is one sentence, okay. It's on the recorders. I just happened to live in San Francisco. Yeah, people don't really like there's there's like a meta game of sport the Singapore minutes in a certain amount of words. Or sentence that the average is more like three to four sentences.
see this happens all the time. This is one sentence, okay. It's on the recorders. I just happened to live in San Francisco. Yeah, people don't really like there's there's like a meta game of sport the Singapore minutes in a certain amount of words. Or sentence that the average is more like three to four sentences.
S Speaker 12:13People find out yeah, they are highly Singaporean.
S Speaker 32:22Yeah, you know, in recent times, I have spent more time with Singaporeans it's more the Singapore funds. You might be familiar with Temasek and and PIF. And I've spent some time those folks so so so now I can identify Singaporean accent very well.
Yeah, you know, in recent times, I have spent more time with Singaporeans it's more the Singapore funds. You might be familiar with Temasek and and PIF. And I've spent some time those folks so so so now I can identify Singaporean accent very well.
Yeah, you know, in recent times, I have spent more time with Singaporeans it's more the Singapore funds. You might be familiar with Temasek and and PIF. And I've spent some time those folks so so so now I can identify Singaporean accent very well.
Yeah, you know, in recent times, I have spent more time with Singaporeans it's more the Singapore funds. You might be familiar with Temasek and and PIF. And I've spent some time those folks so so so now I can identify Singaporean accent very well.
S Speaker 32:57It seems Priya spoke with you earlier. as well. Is that right?
It seems Priya spoke with you earlier. as well. Is that right?
It seems Priya spoke with you earlier. as well. Is that right?
It seems Priya spoke with you earlier. as well. Is that right?
3:13I actually appreciate I think Sebastian mentioned.
S Speaker 13:28Yeah, I believe I met the alpha squared people. Okay, so they fix everything.
Yeah, I believe I met the alpha squared people. Okay, so they fix everything.
Yeah, I believe I met the alpha squared people. Okay, so they fix everything.
Yeah, I believe I met the alpha squared people. Okay, so they fix everything.
S Speaker 33:34they mentioned about the you and that's how we find out found out. We tell you about us a little bit, and then I can tell you about Qualcomm ventures as well. We'd love to learn more about you and what you're building. We you know, my background is in engineering and product at Qualcomm and over the last seven years have been investing in AI and deep tech. Qualcomm Ventures has been around for 24 years so we invest in areas that are strategically relevant to Qualcomm in there lots of different things. You might have been a little familiar with us with Qualcomm in general. We do chips for mobile and a lot of other things. Mobile server laptops, IoT, automotive, and networking and a bunch of other things. And then so all of these areas are strategically relevant all the way from software to hardware. So we invest one $50 million over one year typically, and series agnostic, we can lead rounds we can follow participate with others, as well as two to $10 million tends to be our sweet tends to be a check sizes depending on the round side.
they mentioned about the you and that's how we find out found out. We tell you about us a little bit, and then I can tell you about Qualcomm ventures as well. We'd love to learn more about you and what you're building. We you know, my background is in engineering and product at Qualcomm and over the last seven years have been investing in AI and deep tech. Qualcomm Ventures has been around for 24 years so we invest in areas that are strategically relevant to Qualcomm in there lots of different things. You might have been a little familiar with us with Qualcomm in general. We do chips for mobile and a lot of other things. Mobile server laptops, IoT, automotive, and networking and a bunch of other things. And then so all of these areas are strategically relevant all the way from software to hardware. So we invest one $50 million over one year typically, and series agnostic, we can lead rounds we can follow participate with others, as well as two to $10 million tends to be our sweet tends to be a check sizes depending on the round side.
they mentioned about the you and that's how we find out found out. We tell you about us a little bit, and then I can tell you about Qualcomm ventures as well. We'd love to learn more about you and what you're building. We you know, my background is in engineering and product at Qualcomm and over the last seven years have been investing in AI and deep tech. Qualcomm Ventures has been around for 24 years so we invest in areas that are strategically relevant to Qualcomm in there lots of different things. You might have been a little familiar with us with Qualcomm in general. We do chips for mobile and a lot of other things. Mobile server laptops, IoT, automotive, and networking and a bunch of other things. And then so all of these areas are strategically relevant all the way from software to hardware. So we invest one $50 million over one year typically, and series agnostic, we can lead rounds we can follow participate with others, as well as two to $10 million tends to be our sweet tends to be a check sizes depending on the round side.
they mentioned about the you and that's how we find out found out. We tell you about us a little bit, and then I can tell you about Qualcomm ventures as well. We'd love to learn more about you and what you're building. We you know, my background is in engineering and product at Qualcomm and over the last seven years have been investing in AI and deep tech. Qualcomm Ventures has been around for 24 years so we invest in areas that are strategically relevant to Qualcomm in there lots of different things. You might have been a little familiar with us with Qualcomm in general. We do chips for mobile and a lot of other things. Mobile server laptops, IoT, automotive, and networking and a bunch of other things. And then so all of these areas are strategically relevant all the way from software to hardware. So we invest one $50 million over one year typically, and series agnostic, we can lead rounds we can follow participate with others, as well as two to $10 million tends to be our sweet tends to be a check sizes depending on the round side.
S Speaker 35:05yeah, that's a high level overview of Qualcomm ventures and Qualcomm. I looked up prior to this call, I was looking@featherless.ai.
yeah, that's a high level overview of Qualcomm ventures and Qualcomm. I looked up prior to this call, I was looking@featherless.ai.
yeah, that's a high level overview of Qualcomm ventures and Qualcomm. I looked up prior to this call, I was looking@featherless.ai.
yeah, that's a high level overview of Qualcomm ventures and Qualcomm. I looked up prior to this call, I was looking@featherless.ai.
5:20And so I'm curious very familiar with this space.
S Speaker 35:24We I mean, in print as a service space in general, very familiar. We, I mean, I know about several companies in this space. Together. Is is doing really well. So very familiar. But actually I'm I'm on that team for a while as well. And we do have a portfolio company in this space that who pivoted to this that's October a if
We I mean, in print as a service space in general, very familiar. We, I mean, I know about several companies in this space. Together. Is is doing really well. So very familiar. But actually I'm I'm on that team for a while as well. And we do have a portfolio company in this space that who pivoted to this that's October a if
We I mean, in print as a service space in general, very familiar. We, I mean, I know about several companies in this space. Together. Is is doing really well. So very familiar. But actually I'm I'm on that team for a while as well. And we do have a portfolio company in this space that who pivoted to this that's October a if
We I mean, in print as a service space in general, very familiar. We, I mean, I know about several companies in this space. Together. Is is doing really well. So very familiar. But actually I'm I'm on that team for a while as well. And we do have a portfolio company in this space that who pivoted to this that's October a if
S Speaker 35:58Yeah. And then there's others as well. So I'm very curious to learn more about how you're thinking about this space and how you're thinking about building a company. Yeah.
Yeah. And then there's others as well. So I'm very curious to learn more about how you're thinking about this space and how you're thinking about building a company. Yeah.
Yeah. And then there's others as well. So I'm very curious to learn more about how you're thinking about this space and how you're thinking about building a company. Yeah.
Yeah. And then there's others as well. So I'm very curious to learn more about how you're thinking about this space and how you're thinking about building a company. Yeah.
S Speaker 16:13Yeah. So I think I think before I jump in, the more usual apply about the company as a whole right to tell your background that I think y'all will be familiar with is that we are the team. We are the same team. We are the BTB. All of us on the same core contributor behind the Arabic TV, we exclusion of our some of our China and Russia, open source contributors because we couldn't when we formalize the entity we realized due to current Situ geopolitical situations we could exclude and we couldn't include certain members. But we still collaborate with them in the open source space. And as I was actually relationship
Yeah. So I think I think before I jump in, the more usual apply about the company as a whole right to tell your background that I think y'all will be familiar with is that we are the team. We are the same team. We are the BTB. All of us on the same core contributor behind the Arabic TV, we exclusion of our some of our China and Russia, open source contributors because we couldn't when we formalize the entity we realized due to current Situ geopolitical situations we could exclude and we couldn't include certain members. But we still collaborate with them in the open source space. And as I was actually relationship
Yeah. So I think I think before I jump in, the more usual apply about the company as a whole right to tell your background that I think y'all will be familiar with is that we are the team. We are the same team. We are the BTB. All of us on the same core contributor behind the Arabic TV, we exclusion of our some of our China and Russia, open source contributors because we couldn't when we formalize the entity we realized due to current Situ geopolitical situations we could exclude and we couldn't include certain members. But we still collaborate with them in the open source space. And as I was actually relationship
Yeah. So I think I think before I jump in, the more usual apply about the company as a whole right to tell your background that I think y'all will be familiar with is that we are the team. We are the same team. We are the BTB. All of us on the same core contributor behind the Arabic TV, we exclusion of our some of our China and Russia, open source contributors because we couldn't when we formalize the entity we realized due to current Situ geopolitical situations we could exclude and we couldn't include certain members. But we still collaborate with them in the open source space. And as I was actually relationship
S Speaker 38:25maybe from my knowledge and you know, so spent some time understanding some quadratic models and a few companies building some quadratic, but it's more so you had to maybe educate me, which is you know, just bedtimes with companies that are building sub quadratic models like state space models are kW. We is that Ardabil KV is that similar to six state space, or it's a new architecture
maybe from my knowledge and you know, so spent some time understanding some quadratic models and a few companies building some quadratic, but it's more so you had to maybe educate me, which is you know, just bedtimes with companies that are building sub quadratic models like state space models are kW. We is that Ardabil KV is that similar to six state space, or it's a new architecture
maybe from my knowledge and you know, so spent some time understanding some quadratic models and a few companies building some quadratic, but it's more so you had to maybe educate me, which is you know, just bedtimes with companies that are building sub quadratic models like state space models are kW. We is that Ardabil KV is that similar to six state space, or it's a new architecture
maybe from my knowledge and you know, so spent some time understanding some quadratic models and a few companies building some quadratic, but it's more so you had to maybe educate me, which is you know, just bedtimes with companies that are building sub quadratic models like state space models are kW. We is that Ardabil KV is that similar to six state space, or it's a new architecture
S Speaker 18:59it's a new architecture is different, but it actually predates database version.
it's a new architecture is different, but it actually predates database version.
it's a new architecture is different, but it actually predates database version.
it's a new architecture is different, but it actually predates database version.
S Speaker 39:06Okay, yeah. Initially, I guess even state space has been around for a long time. But so so it's a different architecture and then how are the two different
Okay, yeah. Initially, I guess even state space has been around for a long time. But so so it's a different architecture and then how are the two different
Okay, yeah. Initially, I guess even state space has been around for a long time. But so so it's a different architecture and then how are the two different
Okay, yeah. Initially, I guess even state space has been around for a long time. But so so it's a different architecture and then how are the two different
S Speaker 19:16So without going into too much technical detail, right is each sub project ignored or tends to generate a compressed state instead of expending state to explain the difference between state space and other activities we have different designs in all the states generated but from a technical standpoint regarding compute efficiency, right, we are on our pants okay. So it may affect the quality of the output. And this is something that is an active research area, and if anything, actually is stick space, in other b2b, we kind of like learn from each other continuously. And
So without going into too much technical detail, right is each sub project ignored or tends to generate a compressed state instead of expending state to explain the difference between state space and other activities we have different designs in all the states generated but from a technical standpoint regarding compute efficiency, right, we are on our pants okay. So it may affect the quality of the output. And this is something that is an active research area, and if anything, actually is stick space, in other b2b, we kind of like learn from each other continuously. And
So without going into too much technical detail, right is each sub project ignored or tends to generate a compressed state instead of expending state to explain the difference between state space and other activities we have different designs in all the states generated but from a technical standpoint regarding compute efficiency, right, we are on our pants okay. So it may affect the quality of the output. And this is something that is an active research area, and if anything, actually is stick space, in other b2b, we kind of like learn from each other continuously. And
So without going into too much technical detail, right is each sub project ignored or tends to generate a compressed state instead of expending state to explain the difference between state space and other activities we have different designs in all the states generated but from a technical standpoint regarding compute efficiency, right, we are on our pants okay. So it may affect the quality of the output. And this is something that is an active research area, and if anything, actually is stick space, in other b2b, we kind of like learn from each other continuously. And
S Speaker 310:05from my knowledge, which is state space models, the good news is of course state space models, you know, unlimited context land, they've started showing some good applications of state space. Text to Speech is has become really, really good. It's much cheaper than transformer based approaches. The quality is also good enough. cortesia came out with the first versions of the text to speech models. And then, from what I know, there's a bunch of teams that are training these dates model based models to to now output text tokens or generate text and other modalities as well as similar quality as transformer based models. And, and it's TBD whether you know at the same time, you've got open AI and llama matter and Gemini teams using these fusion based approaches for the next multimodal model in Transformers versus also, entropic is still working on latent fusion based approaches. But these companies are all going after larger transformer based models.
from my knowledge, which is state space models, the good news is of course state space models, you know, unlimited context land, they've started showing some good applications of state space. Text to Speech is has become really, really good. It's much cheaper than transformer based approaches. The quality is also good enough. cortesia came out with the first versions of the text to speech models. And then, from what I know, there's a bunch of teams that are training these dates model based models to to now output text tokens or generate text and other modalities as well as similar quality as transformer based models. And, and it's TBD whether you know at the same time, you've got open AI and llama matter and Gemini teams using these fusion based approaches for the next multimodal model in Transformers versus also, entropic is still working on latent fusion based approaches. But these companies are all going after larger transformer based models.
from my knowledge, which is state space models, the good news is of course state space models, you know, unlimited context land, they've started showing some good applications of state space. Text to Speech is has become really, really good. It's much cheaper than transformer based approaches. The quality is also good enough. cortesia came out with the first versions of the text to speech models. And then, from what I know, there's a bunch of teams that are training these dates model based models to to now output text tokens or generate text and other modalities as well as similar quality as transformer based models. And, and it's TBD whether you know at the same time, you've got open AI and llama matter and Gemini teams using these fusion based approaches for the next multimodal model in Transformers versus also, entropic is still working on latent fusion based approaches. But these companies are all going after larger transformer based models.
from my knowledge, which is state space models, the good news is of course state space models, you know, unlimited context land, they've started showing some good applications of state space. Text to Speech is has become really, really good. It's much cheaper than transformer based approaches. The quality is also good enough. cortesia came out with the first versions of the text to speech models. And then, from what I know, there's a bunch of teams that are training these dates model based models to to now output text tokens or generate text and other modalities as well as similar quality as transformer based models. And, and it's TBD whether you know at the same time, you've got open AI and llama matter and Gemini teams using these fusion based approaches for the next multimodal model in Transformers versus also, entropic is still working on latent fusion based approaches. But these companies are all going after larger transformer based models.
S Speaker 311:24our W KV fit in this landscape which is what would be the first application of an RW KV based approach where it can challenge the status quo off which is you know what first cost reasons, quality reasons it does not make sense to use transformers for this particular application.
our W KV fit in this landscape which is what would be the first application of an RW KV based approach where it can challenge the status quo off which is you know what first cost reasons, quality reasons it does not make sense to use transformers for this particular application.
our W KV fit in this landscape which is what would be the first application of an RW KV based approach where it can challenge the status quo off which is you know what first cost reasons, quality reasons it does not make sense to use transformers for this particular application.
our W KV fit in this landscape which is what would be the first application of an RW KV based approach where it can challenge the status quo off which is you know what first cost reasons, quality reasons it does not make sense to use transformers for this particular application.
S Speaker 111:50Alright, specifically as it is right now, right? Our seven e and l 14 B right, is currently best performing and it does not have a control for compute is currently the best performance Southeast Asia model in the sense that we support over 100 languages including the vast majority of Southeast Asian languages. And
Alright, specifically as it is right now, right? Our seven e and l 14 B right, is currently best performing and it does not have a control for compute is currently the best performance Southeast Asia model in the sense that we support over 100 languages including the vast majority of Southeast Asian languages. And
Alright, specifically as it is right now, right? Our seven e and l 14 B right, is currently best performing and it does not have a control for compute is currently the best performance Southeast Asia model in the sense that we support over 100 languages including the vast majority of Southeast Asian languages. And
Alright, specifically as it is right now, right? Our seven e and l 14 B right, is currently best performing and it does not have a control for compute is currently the best performance Southeast Asia model in the sense that we support over 100 languages including the vast majority of Southeast Asian languages. And
S Speaker 112:17because we are able to do competitive llama to tree performance when it comes to the the English performance itself. So, so far, we are not going to claim that we have the best English performance because that's where you have the opportunity to train models we have, we have some of the best performance when it comes to multilingual. And for us, it's actually just more about training in skilling architecture. The way I do it, right? I actually do that, at least for the company, right? We don't intend to go into the training parameter fight. And I'll explain that subsequently. We build a small like the release of the database where we will be in the sub 200 p per meter arena, if that makes sense.
because we are able to do competitive llama to tree performance when it comes to the the English performance itself. So, so far, we are not going to claim that we have the best English performance because that's where you have the opportunity to train models we have, we have some of the best performance when it comes to multilingual. And for us, it's actually just more about training in skilling architecture. The way I do it, right? I actually do that, at least for the company, right? We don't intend to go into the training parameter fight. And I'll explain that subsequently. We build a small like the release of the database where we will be in the sub 200 p per meter arena, if that makes sense.
because we are able to do competitive llama to tree performance when it comes to the the English performance itself. So, so far, we are not going to claim that we have the best English performance because that's where you have the opportunity to train models we have, we have some of the best performance when it comes to multilingual. And for us, it's actually just more about training in skilling architecture. The way I do it, right? I actually do that, at least for the company, right? We don't intend to go into the training parameter fight. And I'll explain that subsequently. We build a small like the release of the database where we will be in the sub 200 p per meter arena, if that makes sense.
because we are able to do competitive llama to tree performance when it comes to the the English performance itself. So, so far, we are not going to claim that we have the best English performance because that's where you have the opportunity to train models we have, we have some of the best performance when it comes to multilingual. And for us, it's actually just more about training in skilling architecture. The way I do it, right? I actually do that, at least for the company, right? We don't intend to go into the training parameter fight. And I'll explain that subsequently. We build a small like the release of the database where we will be in the sub 200 p per meter arena, if that makes sense.
S Speaker 313:06Okay, good. So and you are going in the direction of training both large, I mean, essentially, you're going in the direction of training, a RW KV based foundation model, and at the same time also building a featherless study. I like our training as a hindrance as a service company.
Okay, good. So and you are going in the direction of training both large, I mean, essentially, you're going in the direction of training, a RW KV based foundation model, and at the same time also building a featherless study. I like our training as a hindrance as a service company.
Okay, good. So and you are going in the direction of training both large, I mean, essentially, you're going in the direction of training, a RW KV based foundation model, and at the same time also building a featherless study. I like our training as a hindrance as a service company.
Okay, good. So and you are going in the direction of training both large, I mean, essentially, you're going in the direction of training, a RW KV based foundation model, and at the same time also building a featherless study. I like our training as a hindrance as a service company.
S Speaker 120:19we have over right now we have 1000 Sign up over over? It went past
we have over right now we have 1000 Sign up over over? It went past
we have over right now we have 1000 Sign up over over? It went past
we have over right now we have 1000 Sign up over over? It went past
S Speaker 120:34we suppose MPP and developers you're fighting okay. 70,000,000,400
we suppose MPP and developers you're fighting okay. 70,000,000,400
we suppose MPP and developers you're fighting okay. 70,000,000,400
we suppose MPP and developers you're fighting okay. 70,000,000,400
S Speaker 320:38which is yeah, the llama 3.1. So, I'm guessing, you know,
which is yeah, the llama 3.1. So, I'm guessing, you know,
which is yeah, the llama 3.1. So, I'm guessing, you know,
which is yeah, the llama 3.1. So, I'm guessing, you know,
S Speaker 121:02Yeah. Intention, optimize our entire infrastructure stack right to make use of lower end GPUs, even even the 4g or 5g or is running on on a fleet of United's and coordinate essentially. But
Yeah. Intention, optimize our entire infrastructure stack right to make use of lower end GPUs, even even the 4g or 5g or is running on on a fleet of United's and coordinate essentially. But
Yeah. Intention, optimize our entire infrastructure stack right to make use of lower end GPUs, even even the 4g or 5g or is running on on a fleet of United's and coordinate essentially. But
Yeah. Intention, optimize our entire infrastructure stack right to make use of lower end GPUs, even even the 4g or 5g or is running on on a fleet of United's and coordinate essentially. But
S Speaker 321:18I mean, essentially, your performance in terms of latency is similar.
I mean, essentially, your performance in terms of latency is similar.
I mean, essentially, your performance in terms of latency is similar.
I mean, essentially, your performance in terms of latency is similar.
S Speaker 121:24So yeah, it's similar, similar to some of the providers are, to be clear, not similar to grok, for example, but it's similar to some of the more traditional insurance provider and, and the reason behind that is because we optimized for throughput and cost not absolute tokens per second. And most customers are actually okay with that because our unique selling point is the variety and the cost efficiency and and the reason why we optimizing normally, we are actually using an army of discarded discarded GPUs from specific we did from the typical days.
So yeah, it's similar, similar to some of the providers are, to be clear, not similar to grok, for example, but it's similar to some of the more traditional insurance provider and, and the reason behind that is because we optimized for throughput and cost not absolute tokens per second. And most customers are actually okay with that because our unique selling point is the variety and the cost efficiency and and the reason why we optimizing normally, we are actually using an army of discarded discarded GPUs from specific we did from the typical days.
So yeah, it's similar, similar to some of the providers are, to be clear, not similar to grok, for example, but it's similar to some of the more traditional insurance provider and, and the reason behind that is because we optimized for throughput and cost not absolute tokens per second. And most customers are actually okay with that because our unique selling point is the variety and the cost efficiency and and the reason why we optimizing normally, we are actually using an army of discarded discarded GPUs from specific we did from the typical days.
So yeah, it's similar, similar to some of the providers are, to be clear, not similar to grok, for example, but it's similar to some of the more traditional insurance provider and, and the reason behind that is because we optimized for throughput and cost not absolute tokens per second. And most customers are actually okay with that because our unique selling point is the variety and the cost efficiency and and the reason why we optimizing normally, we are actually using an army of discarded discarded GPUs from specific we did from the typical days.
S Speaker 122:09so typically they have a 5090 Wow. So basically, and the reason why we can make use of this right is that this ties into why we use Adobe TV, most of these GPUs fired up by considered by the industry right? Too slow to be used reasonably. But because we use out of activity encoder, we are able to speed up these GPUs by let's say four or five times essentially
so typically they have a 5090 Wow. So basically, and the reason why we can make use of this right is that this ties into why we use Adobe TV, most of these GPUs fired up by considered by the industry right? Too slow to be used reasonably. But because we use out of activity encoder, we are able to speed up these GPUs by let's say four or five times essentially
so typically they have a 5090 Wow. So basically, and the reason why we can make use of this right is that this ties into why we use Adobe TV, most of these GPUs fired up by considered by the industry right? Too slow to be used reasonably. But because we use out of activity encoder, we are able to speed up these GPUs by let's say four or five times essentially
so typically they have a 5090 Wow. So basically, and the reason why we can make use of this right is that this ties into why we use Adobe TV, most of these GPUs fired up by considered by the industry right? Too slow to be used reasonably. But because we use out of activity encoder, we are able to speed up these GPUs by let's say four or five times essentially
S Speaker 322:55and explain it to me in easy what I can grasp. I think that's the key. It sounds like.
and explain it to me in easy what I can grasp. I think that's the key. It sounds like.
and explain it to me in easy what I can grasp. I think that's the key. It sounds like.
and explain it to me in easy what I can grasp. I think that's the key. It sounds like.
S Speaker 123:03Yeah, so the thing is on this is already a known established technique. And it's actually been done with other trends or models is that you can use another foundation model as a speculative decoder. So especially decoder is the process where use a smaller model to create a draft of the answer then the larger module will check the output in ventures. This is because of the way GPUs work right mathematically speaking, and I don't want to go too deep into it is cheaper to check the weather tokens right, of course right there to generate one token at a time. Does that make sense?
Yeah, so the thing is on this is already a known established technique. And it's actually been done with other trends or models is that you can use another foundation model as a speculative decoder. So especially decoder is the process where use a smaller model to create a draft of the answer then the larger module will check the output in ventures. This is because of the way GPUs work right mathematically speaking, and I don't want to go too deep into it is cheaper to check the weather tokens right, of course right there to generate one token at a time. Does that make sense?
Yeah, so the thing is on this is already a known established technique. And it's actually been done with other trends or models is that you can use another foundation model as a speculative decoder. So especially decoder is the process where use a smaller model to create a draft of the answer then the larger module will check the output in ventures. This is because of the way GPUs work right mathematically speaking, and I don't want to go too deep into it is cheaper to check the weather tokens right, of course right there to generate one token at a time. Does that make sense?
Yeah, so the thing is on this is already a known established technique. And it's actually been done with other trends or models is that you can use another foundation model as a speculative decoder. So especially decoder is the process where use a smaller model to create a draft of the answer then the larger module will check the output in ventures. This is because of the way GPUs work right mathematically speaking, and I don't want to go too deep into it is cheaper to check the weather tokens right, of course right there to generate one token at a time. Does that make sense?
S Speaker 123:45yeah, because computing things in batches is more efficient than generating one two at a time. What you can do is you can use a small model to drop an answer and then let the big model validate it in batches. The benefit of this right is that if the small model is accurate enough, you can think of it like like you know, your iPhone autocomplete by
yeah, because computing things in batches is more efficient than generating one two at a time. What you can do is you can use a small model to drop an answer and then let the big model validate it in batches. The benefit of this right is that if the small model is accurate enough, you can think of it like like you know, your iPhone autocomplete by
yeah, because computing things in batches is more efficient than generating one two at a time. What you can do is you can use a small model to drop an answer and then let the big model validate it in batches. The benefit of this right is that if the small model is accurate enough, you can think of it like like you know, your iPhone autocomplete by
yeah, because computing things in batches is more efficient than generating one two at a time. What you can do is you can use a small model to drop an answer and then let the big model validate it in batches. The benefit of this right is that if the small model is accurate enough, you can think of it like like you know, your iPhone autocomplete by
S Speaker 324:10To generate tokens. You're running the big model to mostly validate the smaller models output. And in cases where it's invalid, you use the bigger model to generate output. Is that correct?
To generate tokens. You're running the big model to mostly validate the smaller models output. And in cases where it's invalid, you use the bigger model to generate output. Is that correct?
To generate tokens. You're running the big model to mostly validate the smaller models output. And in cases where it's invalid, you use the bigger model to generate output. Is that correct?
To generate tokens. You're running the big model to mostly validate the smaller models output. And in cases where it's invalid, you use the bigger model to generate output. Is that correct?
S Speaker 124:23Correct. So the trick of this approach, is that essentially, right, it matters. So the biggest consideration is how cheap is your model? And how accurate is your small model? Because if your model makes a mistake, and there's a certain percentage error like 10 20%, you are paying top of both the small model and big model compute, but if your model gets it right, you get the substantial itself.
Correct. So the trick of this approach, is that essentially, right, it matters. So the biggest consideration is how cheap is your model? And how accurate is your small model? Because if your model makes a mistake, and there's a certain percentage error like 10 20%, you are paying top of both the small model and big model compute, but if your model gets it right, you get the substantial itself.
Correct. So the trick of this approach, is that essentially, right, it matters. So the biggest consideration is how cheap is your model? And how accurate is your small model? Because if your model makes a mistake, and there's a certain percentage error like 10 20%, you are paying top of both the small model and big model compute, but if your model gets it right, you get the substantial itself.
Correct. So the trick of this approach, is that essentially, right, it matters. So the biggest consideration is how cheap is your model? And how accurate is your small model? Because if your model makes a mistake, and there's a certain percentage error like 10 20%, you are paying top of both the small model and big model compute, but if your model gets it right, you get the substantial itself.
S Speaker 324:53That's pretty. That's very smart. Okay,
S Speaker 124:57yeah. And the key, the one of the key things is that you need it to be cheaper and accurate enough. And one of the, one of the it's not like this is a new technique. This is already been done with Trump's own models, but the thing is that when you use a control model to speculate decode a transformer, know you're compensated is not as dramatic as on TV. Because for other TV and it's very linked back to the foundation model is that is that our models are perfectly suited for this our seven V model given the same as a 100 GPU is able to run a lot of tokens per second. Will existing trends always model without quantization without coding without without a lot of current optimization runs around either 28 tokens per second and this is this is the current like fundamental order magnitude shift, right? That allows that allows our models to actually be extremely competitive when it comes to like, because the throughput high volume use cases, like I mentioned earlier, the downside is we haven't scaled this model to predict treatment parameters. We have a skilled nurse to send to be so so these models are not cheap in our class. They're not the people in my class. And and that's actually what our horseback adoption are right now. But that is a that is right. It has its users. And that's why we are looking at our first step is to actually use it to integrate into inference to actually speed up all the difference when we scale this model respectively.
yeah. And the key, the one of the key things is that you need it to be cheaper and accurate enough. And one of the, one of the it's not like this is a new technique. This is already been done with Trump's own models, but the thing is that when you use a control model to speculate decode a transformer, know you're compensated is not as dramatic as on TV. Because for other TV and it's very linked back to the foundation model is that is that our models are perfectly suited for this our seven V model given the same as a 100 GPU is able to run a lot of tokens per second. Will existing trends always model without quantization without coding without without a lot of current optimization runs around either 28 tokens per second and this is this is the current like fundamental order magnitude shift, right? That allows that allows our models to actually be extremely competitive when it comes to like, because the throughput high volume use cases, like I mentioned earlier, the downside is we haven't scaled this model to predict treatment parameters. We have a skilled nurse to send to be so so these models are not cheap in our class. They're not the people in my class. And and that's actually what our horseback adoption are right now. But that is a that is right. It has its users. And that's why we are looking at our first step is to actually use it to integrate into inference to actually speed up all the difference when we scale this model respectively.
yeah. And the key, the one of the key things is that you need it to be cheaper and accurate enough. And one of the, one of the it's not like this is a new technique. This is already been done with Trump's own models, but the thing is that when you use a control model to speculate decode a transformer, know you're compensated is not as dramatic as on TV. Because for other TV and it's very linked back to the foundation model is that is that our models are perfectly suited for this our seven V model given the same as a 100 GPU is able to run a lot of tokens per second. Will existing trends always model without quantization without coding without without a lot of current optimization runs around either 28 tokens per second and this is this is the current like fundamental order magnitude shift, right? That allows that allows our models to actually be extremely competitive when it comes to like, because the throughput high volume use cases, like I mentioned earlier, the downside is we haven't scaled this model to predict treatment parameters. We have a skilled nurse to send to be so so these models are not cheap in our class. They're not the people in my class. And and that's actually what our horseback adoption are right now. But that is a that is right. It has its users. And that's why we are looking at our first step is to actually use it to integrate into inference to actually speed up all the difference when we scale this model respectively.
yeah. And the key, the one of the key things is that you need it to be cheaper and accurate enough. And one of the, one of the it's not like this is a new technique. This is already been done with Trump's own models, but the thing is that when you use a control model to speculate decode a transformer, know you're compensated is not as dramatic as on TV. Because for other TV and it's very linked back to the foundation model is that is that our models are perfectly suited for this our seven V model given the same as a 100 GPU is able to run a lot of tokens per second. Will existing trends always model without quantization without coding without without a lot of current optimization runs around either 28 tokens per second and this is this is the current like fundamental order magnitude shift, right? That allows that allows our models to actually be extremely competitive when it comes to like, because the throughput high volume use cases, like I mentioned earlier, the downside is we haven't scaled this model to predict treatment parameters. We have a skilled nurse to send to be so so these models are not cheap in our class. They're not the people in my class. And and that's actually what our horseback adoption are right now. But that is a that is right. It has its users. And that's why we are looking at our first step is to actually use it to integrate into inference to actually speed up all the difference when we scale this model respectively.
S Speaker 326:34And how do you compare the output, which is like for example, the output of the small model which is getting validated by the bigger model? How do you know like that the bigger model is doing a good job of validating the output of the smaller model.
And how do you compare the output, which is like for example, the output of the small model which is getting validated by the bigger model? How do you know like that the bigger model is doing a good job of validating the output of the smaller model.
And how do you compare the output, which is like for example, the output of the small model which is getting validated by the bigger model? How do you know like that the bigger model is doing a good job of validating the output of the smaller model.
And how do you compare the output, which is like for example, the output of the small model which is getting validated by the bigger model? How do you know like that the bigger model is doing a good job of validating the output of the smaller model.
S Speaker 126:58So this is back to the specularity encoding technique, right. To be clear, it's not like you drop an answer and then you ask the big one is this right or wrong is actually more more integrated into the system. So So during the during the token generation step, right? It actually takes in the small, small, boldest output to actually process everything at the same time. It does a direct compare of what is the output it wouldn't have done. So by by literally on a macro level. It's the same quality of output as the big model is just using the small models to speed it. Up. It's like you know, how CPUs right where like, you know, what, you can speculate through a direct path ended just in time like compute ahead. That's essentially what is happening here. The small model is computing a hit and then it gets it right right. Is that this contract? In front if it gets it wrong, he ended up paying for all so so
So this is back to the specularity encoding technique, right. To be clear, it's not like you drop an answer and then you ask the big one is this right or wrong is actually more more integrated into the system. So So during the during the token generation step, right? It actually takes in the small, small, boldest output to actually process everything at the same time. It does a direct compare of what is the output it wouldn't have done. So by by literally on a macro level. It's the same quality of output as the big model is just using the small models to speed it. Up. It's like you know, how CPUs right where like, you know, what, you can speculate through a direct path ended just in time like compute ahead. That's essentially what is happening here. The small model is computing a hit and then it gets it right right. Is that this contract? In front if it gets it wrong, he ended up paying for all so so
So this is back to the specularity encoding technique, right. To be clear, it's not like you drop an answer and then you ask the big one is this right or wrong is actually more more integrated into the system. So So during the during the token generation step, right? It actually takes in the small, small, boldest output to actually process everything at the same time. It does a direct compare of what is the output it wouldn't have done. So by by literally on a macro level. It's the same quality of output as the big model is just using the small models to speed it. Up. It's like you know, how CPUs right where like, you know, what, you can speculate through a direct path ended just in time like compute ahead. That's essentially what is happening here. The small model is computing a hit and then it gets it right right. Is that this contract? In front if it gets it wrong, he ended up paying for all so so
So this is back to the specularity encoding technique, right. To be clear, it's not like you drop an answer and then you ask the big one is this right or wrong is actually more more integrated into the system. So So during the during the token generation step, right? It actually takes in the small, small, boldest output to actually process everything at the same time. It does a direct compare of what is the output it wouldn't have done. So by by literally on a macro level. It's the same quality of output as the big model is just using the small models to speed it. Up. It's like you know, how CPUs right where like, you know, what, you can speculate through a direct path ended just in time like compute ahead. That's essentially what is happening here. The small model is computing a hit and then it gets it right right. Is that this contract? In front if it gets it wrong, he ended up paying for all so so
S Speaker 328:07That's an EEG for like now Oh, at what gross margin. Are you able to operate this today? I mean, I know it's definitely a month since you've launched. But at what gross margin are you able to operate today? Because it sounds like you're since you're not even using a 100 you're using something even older.
That's an EEG for like now Oh, at what gross margin. Are you able to operate this today? I mean, I know it's definitely a month since you've launched. But at what gross margin are you able to operate today? Because it sounds like you're since you're not even using a 100 you're using something even older.
That's an EEG for like now Oh, at what gross margin. Are you able to operate this today? I mean, I know it's definitely a month since you've launched. But at what gross margin are you able to operate today? Because it sounds like you're since you're not even using a 100 you're using something even older.
That's an EEG for like now Oh, at what gross margin. Are you able to operate this today? I mean, I know it's definitely a month since you've launched. But at what gross margin are you able to operate today? Because it sounds like you're since you're not even using a 100 you're using something even older.
S Speaker 128:30Currently, so currently, we just crossed the recommended monthly employees even on the hardware, in terms of like the monthly operating costs compared to the amount we receive. And the reason behind that is,
Currently, so currently, we just crossed the recommended monthly employees even on the hardware, in terms of like the monthly operating costs compared to the amount we receive. And the reason behind that is,
Currently, so currently, we just crossed the recommended monthly employees even on the hardware, in terms of like the monthly operating costs compared to the amount we receive. And the reason behind that is,
Currently, so currently, we just crossed the recommended monthly employees even on the hardware, in terms of like the monthly operating costs compared to the amount we receive. And the reason behind that is,
S Speaker 128:54our target gross margin 60% Because as we scale up the infrastructure, we don't need as much
our target gross margin 60% Because as we scale up the infrastructure, we don't need as much
our target gross margin 60% Because as we scale up the infrastructure, we don't need as much
our target gross margin 60% Because as we scale up the infrastructure, we don't need as much
S Speaker 129:1390% cogs and algos will keep 60% cogs and the reason why 90% is their tsp 1/3 of our info Bureau is just NVMe storage.
90% cogs and algos will keep 60% cogs and the reason why 90% is their tsp 1/3 of our info Bureau is just NVMe storage.
90% cogs and algos will keep 60% cogs and the reason why 90% is their tsp 1/3 of our info Bureau is just NVMe storage.
90% cogs and algos will keep 60% cogs and the reason why 90% is their tsp 1/3 of our info Bureau is just NVMe storage.
29:27Because of the amount of models we are posting
S Speaker 129:30this this storage class is not going to scale proportionally to the amount of GPUs. So as we scale our workload more I mean we will actually be able to keep the 60% Cox is
this this storage class is not going to scale proportionally to the amount of GPUs. So as we scale our workload more I mean we will actually be able to keep the 60% Cox is
this this storage class is not going to scale proportionally to the amount of GPUs. So as we scale our workload more I mean we will actually be able to keep the 60% Cox is
this this storage class is not going to scale proportionally to the amount of GPUs. So as we scale our workload more I mean we will actually be able to keep the 60% Cox is
S Speaker 129:48I mean, if our models aren't efficient enough to run on mobile devices, there are ways to use it to speed things up. Okay, so like paying into the software, right, right, right, is that like so like our goal? So that's how we look at the platform play, but on the financial model player is that we intend to be the next generation, although that's fundamentally different because we are able to do that much higher throughput as shown by the cost savings, we provide inference essentially, and it for us and the reason why we can do that is that is that the problem that all of us quadratic is trying to solve is transformers are computationally expensive, their cost scales quadratically over time and you may have experienced this when he was checking yourself that things tend to slow down as your text gets longer and longer. And as right models skill, Ian, and I'm sure as investors you know the difference between subcontracted Skilling and Linnaeus I mean, quadratic scaling and linear scaling. One just keeps going up and up and up. And the other just private stays at a more manageable constant. It's great when it's about making money, it starts when it's about paying the bills. And and this is what leads to the gibberish of digital scarcity and the challenges in running it at low
I mean, if our models aren't efficient enough to run on mobile devices, there are ways to use it to speed things up. Okay, so like paying into the software, right, right, right, is that like so like our goal? So that's how we look at the platform play, but on the financial model player is that we intend to be the next generation, although that's fundamentally different because we are able to do that much higher throughput as shown by the cost savings, we provide inference essentially, and it for us and the reason why we can do that is that is that the problem that all of us quadratic is trying to solve is transformers are computationally expensive, their cost scales quadratically over time and you may have experienced this when he was checking yourself that things tend to slow down as your text gets longer and longer. And as right models skill, Ian, and I'm sure as investors you know the difference between subcontracted Skilling and Linnaeus I mean, quadratic scaling and linear scaling. One just keeps going up and up and up. And the other just private stays at a more manageable constant. It's great when it's about making money, it starts when it's about paying the bills. And and this is what leads to the gibberish of digital scarcity and the challenges in running it at low
I mean, if our models aren't efficient enough to run on mobile devices, there are ways to use it to speed things up. Okay, so like paying into the software, right, right, right, is that like so like our goal? So that's how we look at the platform play, but on the financial model player is that we intend to be the next generation, although that's fundamentally different because we are able to do that much higher throughput as shown by the cost savings, we provide inference essentially, and it for us and the reason why we can do that is that is that the problem that all of us quadratic is trying to solve is transformers are computationally expensive, their cost scales quadratically over time and you may have experienced this when he was checking yourself that things tend to slow down as your text gets longer and longer. And as right models skill, Ian, and I'm sure as investors you know the difference between subcontracted Skilling and Linnaeus I mean, quadratic scaling and linear scaling. One just keeps going up and up and up. And the other just private stays at a more manageable constant. It's great when it's about making money, it starts when it's about paying the bills. And and this is what leads to the gibberish of digital scarcity and the challenges in running it at low
I mean, if our models aren't efficient enough to run on mobile devices, there are ways to use it to speed things up. Okay, so like paying into the software, right, right, right, is that like so like our goal? So that's how we look at the platform play, but on the financial model player is that we intend to be the next generation, although that's fundamentally different because we are able to do that much higher throughput as shown by the cost savings, we provide inference essentially, and it for us and the reason why we can do that is that is that the problem that all of us quadratic is trying to solve is transformers are computationally expensive, their cost scales quadratically over time and you may have experienced this when he was checking yourself that things tend to slow down as your text gets longer and longer. And as right models skill, Ian, and I'm sure as investors you know the difference between subcontracted Skilling and Linnaeus I mean, quadratic scaling and linear scaling. One just keeps going up and up and up. And the other just private stays at a more manageable constant. It's great when it's about making money, it starts when it's about paying the bills. And and this is what leads to the gibberish of digital scarcity and the challenges in running it at low
S Speaker 331:15But I guess I mean, at some it's the chart, the problem is also memory again, it the memory is not scaling at the same pace at the GPUs. But like you said, like the third of the class today is NVMe. Right? So, so it's not exactly going to be linear curve, right. But it won't be as quadratic as, as a transformed curve from in terms of cost, right.
But I guess I mean, at some it's the chart, the problem is also memory again, it the memory is not scaling at the same pace at the GPUs. But like you said, like the third of the class today is NVMe. Right? So, so it's not exactly going to be linear curve, right. But it won't be as quadratic as, as a transformed curve from in terms of cost, right.
But I guess I mean, at some it's the chart, the problem is also memory again, it the memory is not scaling at the same pace at the GPUs. But like you said, like the third of the class today is NVMe. Right? So, so it's not exactly going to be linear curve, right. But it won't be as quadratic as, as a transformed curve from in terms of cost, right.
But I guess I mean, at some it's the chart, the problem is also memory again, it the memory is not scaling at the same pace at the GPUs. But like you said, like the third of the class today is NVMe. Right? So, so it's not exactly going to be linear curve, right. But it won't be as quadratic as, as a transformed curve from in terms of cost, right.
S Speaker 131:49There's a separate topic, but in terms of the model itself, so
There's a separate topic, but in terms of the model itself, so
There's a separate topic, but in terms of the model itself, so
There's a separate topic, but in terms of the model itself, so
S Speaker 331:55I think it trends quadratic models will always came like that and then sub quadratic so that
I think it trends quadratic models will always came like that and then sub quadratic so that
I think it trends quadratic models will always came like that and then sub quadratic so that
I think it trends quadratic models will always came like that and then sub quadratic so that
S Speaker 132:04yeah, so this is more about the model itself, in terms of like the platform completely. So for the the reason why rbtv The model is able to achieve these are is that we combine the best of both earnings and transformer RNN was historically known to be extremely efficient and movement capacity. One of the giants in AI has a dedicated article of unreasonable effectiveness RNNs, but due to its design flaw was nearly impossible to scale in training on site transformers was the first architecture to scale to billions of parameters trillions of token and is what I discovered with AI with hyper hyper when you combine both architectures right? You are able to actually get the same cloud inference efficiency as RNNs, but at the same time, by using the same building block by sprint, almost, you're able to get the same training efficiencies as transformers essentially allowing you to actually have an architecture that provides the best of both worlds essentially. And we think and we continue to essentially like the transformers is effectively the first steam engine that unlocked this industrial revolution. And we are actually bringing in the next generation where it's much more efficient to train and to do inference. And what we mean by this is that if we look at open AI for example, right, they generate 100 billion words but they
yeah, so this is more about the model itself, in terms of like the platform completely. So for the the reason why rbtv The model is able to achieve these are is that we combine the best of both earnings and transformer RNN was historically known to be extremely efficient and movement capacity. One of the giants in AI has a dedicated article of unreasonable effectiveness RNNs, but due to its design flaw was nearly impossible to scale in training on site transformers was the first architecture to scale to billions of parameters trillions of token and is what I discovered with AI with hyper hyper when you combine both architectures right? You are able to actually get the same cloud inference efficiency as RNNs, but at the same time, by using the same building block by sprint, almost, you're able to get the same training efficiencies as transformers essentially allowing you to actually have an architecture that provides the best of both worlds essentially. And we think and we continue to essentially like the transformers is effectively the first steam engine that unlocked this industrial revolution. And we are actually bringing in the next generation where it's much more efficient to train and to do inference. And what we mean by this is that if we look at open AI for example, right, they generate 100 billion words but they
yeah, so this is more about the model itself, in terms of like the platform completely. So for the the reason why rbtv The model is able to achieve these are is that we combine the best of both earnings and transformer RNN was historically known to be extremely efficient and movement capacity. One of the giants in AI has a dedicated article of unreasonable effectiveness RNNs, but due to its design flaw was nearly impossible to scale in training on site transformers was the first architecture to scale to billions of parameters trillions of token and is what I discovered with AI with hyper hyper when you combine both architectures right? You are able to actually get the same cloud inference efficiency as RNNs, but at the same time, by using the same building block by sprint, almost, you're able to get the same training efficiencies as transformers essentially allowing you to actually have an architecture that provides the best of both worlds essentially. And we think and we continue to essentially like the transformers is effectively the first steam engine that unlocked this industrial revolution. And we are actually bringing in the next generation where it's much more efficient to train and to do inference. And what we mean by this is that if we look at open AI for example, right, they generate 100 billion words but they
yeah, so this is more about the model itself, in terms of like the platform completely. So for the the reason why rbtv The model is able to achieve these are is that we combine the best of both earnings and transformer RNN was historically known to be extremely efficient and movement capacity. One of the giants in AI has a dedicated article of unreasonable effectiveness RNNs, but due to its design flaw was nearly impossible to scale in training on site transformers was the first architecture to scale to billions of parameters trillions of token and is what I discovered with AI with hyper hyper when you combine both architectures right? You are able to actually get the same cloud inference efficiency as RNNs, but at the same time, by using the same building block by sprint, almost, you're able to get the same training efficiencies as transformers essentially allowing you to actually have an architecture that provides the best of both worlds essentially. And we think and we continue to essentially like the transformers is effectively the first steam engine that unlocked this industrial revolution. And we are actually bringing in the next generation where it's much more efficient to train and to do inference. And what we mean by this is that if we look at open AI for example, right, they generate 100 billion words but they
S Speaker 333:36are using this math I can understand with with the recursive will be lot less. Maybe do things with gene which is I know since we only blocked 30 When my actual next meeting started, but then I wanted to cover a few more things with you and then we can we can schedule another meeting, which is how big is your team? And is the team all located?
are using this math I can understand with with the recursive will be lot less. Maybe do things with gene which is I know since we only blocked 30 When my actual next meeting started, but then I wanted to cover a few more things with you and then we can we can schedule another meeting, which is how big is your team? And is the team all located?
are using this math I can understand with with the recursive will be lot less. Maybe do things with gene which is I know since we only blocked 30 When my actual next meeting started, but then I wanted to cover a few more things with you and then we can we can schedule another meeting, which is how big is your team? And is the team all located?
are using this math I can understand with with the recursive will be lot less. Maybe do things with gene which is I know since we only blocked 30 When my actual next meeting started, but then I wanted to cover a few more things with you and then we can we can schedule another meeting, which is how big is your team? And is the team all located?
S Speaker 134:02So I'm located in the Bay Area. That team is a remote team that is distributed in nature. And we are currently around 10 people. Okay,
So I'm located in the Bay Area. That team is a remote team that is distributed in nature. And we are currently around 10 people. Okay,
So I'm located in the Bay Area. That team is a remote team that is distributed in nature. And we are currently around 10 people. Okay,
So I'm located in the Bay Area. That team is a remote team that is distributed in nature. And we are currently around 10 people. Okay,
S Speaker 334:1210 people, and then when did you start the company?
10 people, and then when did you start the company?
10 people, and then when did you start the company?
10 people, and then when did you start the company?
S Speaker 134:15We started around six, seven months ago. Okay.
S Speaker 134:24before? So we raised 2.5 million as part of our initial pre seed round to scale the model to launch the platform to do all the initial initial on ramp, and then right. Right now like since last week, we started our race right or five new to accelerate our new platform. Because the thing is federalism is taking off and we just wanted to capitalize on it. So we want to accelerate these 2 million to two or even 3 million arr. Then we will do our next round, which is the scale of our nation model at 35. And because our main thesis you said if we can get a GPT for class, right, there's literally at least 100,000x Cheaper inference, right? They can run on premise, they can run private cloud, maybe even run on HD visors. GPD pocket IPS enterprises is going to purchase the hell out of it. And current foundation models are ignoring the enterprise space especially the private cloud space, because the one of you to run on your server so that you can gather your data. And that is our actually our end goal to to really corner this segment of the enterprise market. But
before? So we raised 2.5 million as part of our initial pre seed round to scale the model to launch the platform to do all the initial initial on ramp, and then right. Right now like since last week, we started our race right or five new to accelerate our new platform. Because the thing is federalism is taking off and we just wanted to capitalize on it. So we want to accelerate these 2 million to two or even 3 million arr. Then we will do our next round, which is the scale of our nation model at 35. And because our main thesis you said if we can get a GPT for class, right, there's literally at least 100,000x Cheaper inference, right? They can run on premise, they can run private cloud, maybe even run on HD visors. GPD pocket IPS enterprises is going to purchase the hell out of it. And current foundation models are ignoring the enterprise space especially the private cloud space, because the one of you to run on your server so that you can gather your data. And that is our actually our end goal to to really corner this segment of the enterprise market. But
before? So we raised 2.5 million as part of our initial pre seed round to scale the model to launch the platform to do all the initial initial on ramp, and then right. Right now like since last week, we started our race right or five new to accelerate our new platform. Because the thing is federalism is taking off and we just wanted to capitalize on it. So we want to accelerate these 2 million to two or even 3 million arr. Then we will do our next round, which is the scale of our nation model at 35. And because our main thesis you said if we can get a GPT for class, right, there's literally at least 100,000x Cheaper inference, right? They can run on premise, they can run private cloud, maybe even run on HD visors. GPD pocket IPS enterprises is going to purchase the hell out of it. And current foundation models are ignoring the enterprise space especially the private cloud space, because the one of you to run on your server so that you can gather your data. And that is our actually our end goal to to really corner this segment of the enterprise market. But
before? So we raised 2.5 million as part of our initial pre seed round to scale the model to launch the platform to do all the initial initial on ramp, and then right. Right now like since last week, we started our race right or five new to accelerate our new platform. Because the thing is federalism is taking off and we just wanted to capitalize on it. So we want to accelerate these 2 million to two or even 3 million arr. Then we will do our next round, which is the scale of our nation model at 35. And because our main thesis you said if we can get a GPT for class, right, there's literally at least 100,000x Cheaper inference, right? They can run on premise, they can run private cloud, maybe even run on HD visors. GPD pocket IPS enterprises is going to purchase the hell out of it. And current foundation models are ignoring the enterprise space especially the private cloud space, because the one of you to run on your server so that you can gather your data. And that is our actually our end goal to to really corner this segment of the enterprise market. But
S Speaker 135:43Are these find new? We already have one view and safe. We are we are and we as we just started last week, so a few others to close on. But
Are these find new? We already have one view and safe. We are we are and we as we just started last week, so a few others to close on. But
Are these find new? We already have one view and safe. We are we are and we as we just started last week, so a few others to close on. But
Are these find new? We already have one view and safe. We are we are and we as we just started last week, so a few others to close on. But
S Speaker 335:59and then we can follow up and have for further discussion. This is definitely very, very exciting. I have to do some homework. I have to actually ping some some of our guys who work on this end and put it on this slide as well. I'm sure we might be able to find those and then we can follow up. He and I have to go for my next meeting. But thank you this is my very exciting. I had not entertain this idea.
and then we can follow up and have for further discussion. This is definitely very, very exciting. I have to do some homework. I have to actually ping some some of our guys who work on this end and put it on this slide as well. I'm sure we might be able to find those and then we can follow up. He and I have to go for my next meeting. But thank you this is my very exciting. I had not entertain this idea.
and then we can follow up and have for further discussion. This is definitely very, very exciting. I have to do some homework. I have to actually ping some some of our guys who work on this end and put it on this slide as well. I'm sure we might be able to find those and then we can follow up. He and I have to go for my next meeting. But thank you this is my very exciting. I had not entertain this idea.
and then we can follow up and have for further discussion. This is definitely very, very exciting. I have to do some homework. I have to actually ping some some of our guys who work on this end and put it on this slide as well. I'm sure we might be able to find those and then we can follow up. He and I have to go for my next meeting. But thank you this is my very exciting. I had not entertain this idea.
36:52Okay, thanks, Eugene. Thanks a lot for your time. And have a good day. Out. soon. Bye.
Okay, thanks, Eugene. Thanks a lot for your time. And have a good day. Out. soon. Bye.
Okay, thanks, Eugene. Thanks a lot for your time. And have a good day. Out. soon. Bye.
Okay, thanks, Eugene. Thanks a lot for your time. And have a good day. Out. soon. Bye.