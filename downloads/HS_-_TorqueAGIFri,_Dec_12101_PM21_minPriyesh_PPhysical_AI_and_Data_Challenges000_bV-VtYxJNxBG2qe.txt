Meeting: HS - TorqueAGI
Fri, Dec 12
1:01 PM
21 min
Priyesh P
Physical AI and Data Challenges
0:00
Robot Appli
URL: https://otter.ai/u/bV-VtYxJNxBG2qe9eoyyafau1_I
Downloaded: 2025-12-21T18:59:00.108020
Method: text_extraction
============================================================

S Speaker 10:00Part of linear regression. You can start with your network in select
Part of linear regression. You can start with your network in select
Part of linear regression. You can start with your network in select
Part of linear regression. You can start with your network in select
S Speaker 20:07Filter integration model. But after listening to lot of today, I think the clock P my clock is that when you're building physical AI, and then, you know, it's the world, data is the problem, but that is not the solution. XOR TPI is the previous model for for lot of different types of robots, and then we put them into production. We have put together a great team of engineers who are award winning researchers from Stanford, from Google, from Microsoft, paired with stellar production engineers from companies like SpaceX, Apple executed, and what we are solving is basically how to make physical AI smarter. So we will talk about what is physical AI doing, what it can do. We will talk about how to address the data problem. Then we are going to talk about how we did models to work on real robots. To give you an idea on what kind of robots we work with, these are some of our partners. So we are very bullish on enterprise applications of the robots, which range from dexterous manufacturing to precision tasks, to handling materials in warehouses. It turns out that the reason we are choosing these applications is just because we are seeing a lot of front end demand from our end customers. So these customers, think of them as the late stage manufacturing and assembly, which involves getting operation. It involves shipping materials from one place to another in boxes. It involves hitting, packing and so on and so forth. So lot of these tasks can be done by humanoids, and we also into verticals such as construction, energy and couple of other places. And physical AI is really making its way into making these robots smarter for me, right? And that's a very, very exciting part. The challenge is, none of these applications have data. They don't have trillions of dollars and 510, years to wait. They want these solutions today, right? And that is, that is that is one of the biggest challenge, because once you're solving your problems, once you're putting these robots into production, that's when the value so I'll show you a video on how we partner with one of the humanoid tools. So here you're going to see a robot, and a robot is going to do a couple of things. It has to find out where it can go. It has to identify the task, which is at first is just the vision task. You have to understand the structure of what you're looking at, understand the task. In this case, it's pulling up that dot box followed by a sequence of action items. In this case, it would have been very hard to grab the box in the first row, so it has to slide in first and then go and grab the box right. So now, when we are building the foundation model for robots, that foundation model has to understand this robot kinematic structure. It has to understand the task. It has to then give robot instructions after understanding the scene on what tasks to complete in what order. So there are three key elements that a AI, or a foundation model, has to is this 2000 there are couple of things which has to do. It has to perceive. So think of it. Has seen your eyes. Second is reason. Before you can act on it, you have to reason about what we are going to do. For example, when you drove here, you may not have just started walking towards you. Plan, okay, I'm going to use Google Maps, I'm going to walk, I'm going to do this. And that's for planning and reasoning. Lastly, you are actually acting, perceived reason. Act. This is the foundation model that it needs to do. So we had the same problem that we wanted to solve three problems about 10 years back a little bit back in time. So we wrote the first paper, best paper award from CBDR. Awarded again, systems and academia. But we started with is, let's start with the internet and SPL data. So there are a lot of recipes, lot of images. One
Filter integration model. But after listening to lot of today, I think the clock P my clock is that when you're building physical AI, and then, you know, it's the world, data is the problem, but that is not the solution. XOR TPI is the previous model for for lot of different types of robots, and then we put them into production. We have put together a great team of engineers who are award winning researchers from Stanford, from Google, from Microsoft, paired with stellar production engineers from companies like SpaceX, Apple executed, and what we are solving is basically how to make physical AI smarter. So we will talk about what is physical AI doing, what it can do. We will talk about how to address the data problem. Then we are going to talk about how we did models to work on real robots. To give you an idea on what kind of robots we work with, these are some of our partners. So we are very bullish on enterprise applications of the robots, which range from dexterous manufacturing to precision tasks, to handling materials in warehouses. It turns out that the reason we are choosing these applications is just because we are seeing a lot of front end demand from our end customers. So these customers, think of them as the late stage manufacturing and assembly, which involves getting operation. It involves shipping materials from one place to another in boxes. It involves hitting, packing and so on and so forth. So lot of these tasks can be done by humanoids, and we also into verticals such as construction, energy and couple of other places. And physical AI is really making its way into making these robots smarter for me, right? And that's a very, very exciting part. The challenge is, none of these applications have data. They don't have trillions of dollars and 510, years to wait. They want these solutions today, right? And that is, that is that is one of the biggest challenge, because once you're solving your problems, once you're putting these robots into production, that's when the value so I'll show you a video on how we partner with one of the humanoid tools. So here you're going to see a robot, and a robot is going to do a couple of things. It has to find out where it can go. It has to identify the task, which is at first is just the vision task. You have to understand the structure of what you're looking at, understand the task. In this case, it's pulling up that dot box followed by a sequence of action items. In this case, it would have been very hard to grab the box in the first row, so it has to slide in first and then go and grab the box right. So now, when we are building the foundation model for robots, that foundation model has to understand this robot kinematic structure. It has to understand the task. It has to then give robot instructions after understanding the scene on what tasks to complete in what order. So there are three key elements that a AI, or a foundation model, has to is this 2000 there are couple of things which has to do. It has to perceive. So think of it. Has seen your eyes. Second is reason. Before you can act on it, you have to reason about what we are going to do. For example, when you drove here, you may not have just started walking towards you. Plan, okay, I'm going to use Google Maps, I'm going to walk, I'm going to do this. And that's for planning and reasoning. Lastly, you are actually acting, perceived reason. Act. This is the foundation model that it needs to do. So we had the same problem that we wanted to solve three problems about 10 years back a little bit back in time. So we wrote the first paper, best paper award from CBDR. Awarded again, systems and academia. But we started with is, let's start with the internet and SPL data. So there are a lot of recipes, lot of images. One
Filter integration model. But after listening to lot of today, I think the clock P my clock is that when you're building physical AI, and then, you know, it's the world, data is the problem, but that is not the solution. XOR TPI is the previous model for for lot of different types of robots, and then we put them into production. We have put together a great team of engineers who are award winning researchers from Stanford, from Google, from Microsoft, paired with stellar production engineers from companies like SpaceX, Apple executed, and what we are solving is basically how to make physical AI smarter. So we will talk about what is physical AI doing, what it can do. We will talk about how to address the data problem. Then we are going to talk about how we did models to work on real robots. To give you an idea on what kind of robots we work with, these are some of our partners. So we are very bullish on enterprise applications of the robots, which range from dexterous manufacturing to precision tasks, to handling materials in warehouses. It turns out that the reason we are choosing these applications is just because we are seeing a lot of front end demand from our end customers. So these customers, think of them as the late stage manufacturing and assembly, which involves getting operation. It involves shipping materials from one place to another in boxes. It involves hitting, packing and so on and so forth. So lot of these tasks can be done by humanoids, and we also into verticals such as construction, energy and couple of other places. And physical AI is really making its way into making these robots smarter for me, right? And that's a very, very exciting part. The challenge is, none of these applications have data. They don't have trillions of dollars and 510, years to wait. They want these solutions today, right? And that is, that is that is one of the biggest challenge, because once you're solving your problems, once you're putting these robots into production, that's when the value so I'll show you a video on how we partner with one of the humanoid tools. So here you're going to see a robot, and a robot is going to do a couple of things. It has to find out where it can go. It has to identify the task, which is at first is just the vision task. You have to understand the structure of what you're looking at, understand the task. In this case, it's pulling up that dot box followed by a sequence of action items. In this case, it would have been very hard to grab the box in the first row, so it has to slide in first and then go and grab the box right. So now, when we are building the foundation model for robots, that foundation model has to understand this robot kinematic structure. It has to understand the task. It has to then give robot instructions after understanding the scene on what tasks to complete in what order. So there are three key elements that a AI, or a foundation model, has to is this 2000 there are couple of things which has to do. It has to perceive. So think of it. Has seen your eyes. Second is reason. Before you can act on it, you have to reason about what we are going to do. For example, when you drove here, you may not have just started walking towards you. Plan, okay, I'm going to use Google Maps, I'm going to walk, I'm going to do this. And that's for planning and reasoning. Lastly, you are actually acting, perceived reason. Act. This is the foundation model that it needs to do. So we had the same problem that we wanted to solve three problems about 10 years back a little bit back in time. So we wrote the first paper, best paper award from CBDR. Awarded again, systems and academia. But we started with is, let's start with the internet and SPL data. So there are a lot of recipes, lot of images. One
Filter integration model. But after listening to lot of today, I think the clock P my clock is that when you're building physical AI, and then, you know, it's the world, data is the problem, but that is not the solution. XOR TPI is the previous model for for lot of different types of robots, and then we put them into production. We have put together a great team of engineers who are award winning researchers from Stanford, from Google, from Microsoft, paired with stellar production engineers from companies like SpaceX, Apple executed, and what we are solving is basically how to make physical AI smarter. So we will talk about what is physical AI doing, what it can do. We will talk about how to address the data problem. Then we are going to talk about how we did models to work on real robots. To give you an idea on what kind of robots we work with, these are some of our partners. So we are very bullish on enterprise applications of the robots, which range from dexterous manufacturing to precision tasks, to handling materials in warehouses. It turns out that the reason we are choosing these applications is just because we are seeing a lot of front end demand from our end customers. So these customers, think of them as the late stage manufacturing and assembly, which involves getting operation. It involves shipping materials from one place to another in boxes. It involves hitting, packing and so on and so forth. So lot of these tasks can be done by humanoids, and we also into verticals such as construction, energy and couple of other places. And physical AI is really making its way into making these robots smarter for me, right? And that's a very, very exciting part. The challenge is, none of these applications have data. They don't have trillions of dollars and 510, years to wait. They want these solutions today, right? And that is, that is that is one of the biggest challenge, because once you're solving your problems, once you're putting these robots into production, that's when the value so I'll show you a video on how we partner with one of the humanoid tools. So here you're going to see a robot, and a robot is going to do a couple of things. It has to find out where it can go. It has to identify the task, which is at first is just the vision task. You have to understand the structure of what you're looking at, understand the task. In this case, it's pulling up that dot box followed by a sequence of action items. In this case, it would have been very hard to grab the box in the first row, so it has to slide in first and then go and grab the box right. So now, when we are building the foundation model for robots, that foundation model has to understand this robot kinematic structure. It has to understand the task. It has to then give robot instructions after understanding the scene on what tasks to complete in what order. So there are three key elements that a AI, or a foundation model, has to is this 2000 there are couple of things which has to do. It has to perceive. So think of it. Has seen your eyes. Second is reason. Before you can act on it, you have to reason about what we are going to do. For example, when you drove here, you may not have just started walking towards you. Plan, okay, I'm going to use Google Maps, I'm going to walk, I'm going to do this. And that's for planning and reasoning. Lastly, you are actually acting, perceived reason. Act. This is the foundation model that it needs to do. So we had the same problem that we wanted to solve three problems about 10 years back a little bit back in time. So we wrote the first paper, best paper award from CBDR. Awarded again, systems and academia. But we started with is, let's start with the internet and SPL data. So there are a lot of recipes, lot of images. One
S Speaker 24:37of the most surprising things, which is, obviously now it now, if
of the most surprising things, which is, obviously now it now, if
of the most surprising things, which is, obviously now it now, if
of the most surprising things, which is, obviously now it now, if
S Speaker 24:45you took all that writing it over here, you're seeing an image. You are being completely structured, and you're seeing the action on the meaning behind it. All three things are actually in the same space of that box. Now that's very far, completely different things, such as vision, language, reading, are all in the same table. So this is, this is one way to think about it, is that our own brain doesn't force things in different places, right? It keeps them into one thing. Then we said, Okay, we have trained this model that apply different problems. Let's say let's give it an
you took all that writing it over here, you're seeing an image. You are being completely structured, and you're seeing the action on the meaning behind it. All three things are actually in the same space of that box. Now that's very far, completely different things, such as vision, language, reading, are all in the same table. So this is, this is one way to think about it, is that our own brain doesn't force things in different places, right? It keeps them into one thing. Then we said, Okay, we have trained this model that apply different problems. Let's say let's give it an
you took all that writing it over here, you're seeing an image. You are being completely structured, and you're seeing the action on the meaning behind it. All three things are actually in the same space of that box. Now that's very far, completely different things, such as vision, language, reading, are all in the same table. So this is, this is one way to think about it, is that our own brain doesn't force things in different places, right? It keeps them into one thing. Then we said, Okay, we have trained this model that apply different problems. Let's say let's give it an
you took all that writing it over here, you're seeing an image. You are being completely structured, and you're seeing the action on the meaning behind it. All three things are actually in the same space of that box. Now that's very far, completely different things, such as vision, language, reading, are all in the same table. So this is, this is one way to think about it, is that our own brain doesn't force things in different places, right? It keeps them into one thing. Then we said, Okay, we have trained this model that apply different problems. Let's say let's give it an
5:25example, AI, how to do this task of pouring milk
example, AI, how to do this task of pouring milk
example, AI, how to do this task of pouring milk
example, AI, how to do this task of pouring milk
S Speaker 25:29on something, and they are sort of what is the pain? And then we validating problems like this
on something, and they are sort of what is the pain? And then we validating problems like this
on something, and they are sort of what is the pain? And then we validating problems like this
on something, and they are sort of what is the pain? And then we validating problems like this
S Speaker 25:39example, and it's not more than it's not even real environmental that environment of data collection. So now, for example, robotics, hundreds of millions of dollars, having 10s and
example, and it's not more than it's not even real environmental that environment of data collection. So now, for example, robotics, hundreds of millions of dollars, having 10s and
example, and it's not more than it's not even real environmental that environment of data collection. So now, for example, robotics, hundreds of millions of dollars, having 10s and
example, and it's not more than it's not even real environmental that environment of data collection. So now, for example, robotics, hundreds of millions of dollars, having 10s and
S Speaker 26:01dozens of robots. Robots to basically each robot, which includes opening office products. So that
dozens of robots. Robots to basically each robot, which includes opening office products. So that
dozens of robots. Robots to basically each robot, which includes opening office products. So that
dozens of robots. Robots to basically each robot, which includes opening office products. So that
6:14show that by showing the robot million
show that by showing the robot million
show that by showing the robot million
show that by showing the robot million
11:33the CTO said, Maria, oh, well,
the CTO said, Maria, oh, well,
the CTO said, Maria, oh, well,
the CTO said, Maria, oh, well,
11:38how much data point do you need? We said, 100. They didn't
how much data point do you need? We said, 100. They didn't
how much data point do you need? We said, 100. They didn't
how much data point do you need? We said, 100. They didn't
S Speaker 211:42believe us. They had worse within two weeks, including business communication, we had a model. Another two weeks. It was in production. Today, this robot is painting these nice whether, if you look at these walls on the back, look at that, know how to learn. They're very hard to do. If ever the construction before, it's not easy to make it flat. It is very difficult. And this is what can really smooth and do precision control on the wall. And this is one of the first Vlas that we put fully in production for this application. That's the second topic is, people also talk about generalist models. Should we take generalized models? Should we make specialized models? This question is answered by the LLM community already. What does open AI or chatgpt do? It has one model, big model, and topic has one big model, but you create agents to do specialist things. So we should just learn from what is working right. So what we have done is that we have said, okay, the thick generalist model and specialist. The nice thing is that if you have one generalist model that will carry the knowledge, the core common knowledge of different types of robots. Now, manuals may be different from boxes, and boxes may be different from button. It turns out there is a lot of similarity between them. I mean, manuals are objects. Small boxes are objects as well, right? And they have certain companies on how you sell them, labs and how you think. And so this design is very important for our community, because if you any point in time, even if you're making your own robot, you change something in the robot. In mind, I want to take out the legs, replace it the wheels. I want to make this bigger and bigger up. You don't want to go back and start so the right way to do is to have build generalist, general model and then fork out the spatial agents that become specific to the robot and the task, and they can always get their learning back to your master model. And that is the right way to build a consistent, ongoing value. Another thing about is that for those of you are familiar with the kind of the website design, right, there's a website back end, to the front end. So think of model as the back end, 10 billion, 100 million model. It could be in the back that's the model. The way you use the model is not just take the model and use you can use it. Use the model in many ways. These models are not just detector signal. Model, very, very powerful models, again, check with the example. You can use it to do make video games, write essays. Check your spreadsheets, right? So what you want to create in your robots are special agents that harness the model, and those agents you can have sufficient product control over you know, like you may want some better performance from one agent, you may want to control it from some operational parameters for another agent, and so on and so forth. And that gives the commonality of the model, the knowledge of the model, but also gives a huge amount of modularity and intervariability. It turns out, we have done some special sauce in how to inject physics and make it work in real time, and that is why, when we worked with one of the top three agricultural companies in the world, we basically with 1000 times less data, generalized to all The robot compactors. This report from the head of machinery saying that we resolve completely separate tasks, different robot factors.
believe us. They had worse within two weeks, including business communication, we had a model. Another two weeks. It was in production. Today, this robot is painting these nice whether, if you look at these walls on the back, look at that, know how to learn. They're very hard to do. If ever the construction before, it's not easy to make it flat. It is very difficult. And this is what can really smooth and do precision control on the wall. And this is one of the first Vlas that we put fully in production for this application. That's the second topic is, people also talk about generalist models. Should we take generalized models? Should we make specialized models? This question is answered by the LLM community already. What does open AI or chatgpt do? It has one model, big model, and topic has one big model, but you create agents to do specialist things. So we should just learn from what is working right. So what we have done is that we have said, okay, the thick generalist model and specialist. The nice thing is that if you have one generalist model that will carry the knowledge, the core common knowledge of different types of robots. Now, manuals may be different from boxes, and boxes may be different from button. It turns out there is a lot of similarity between them. I mean, manuals are objects. Small boxes are objects as well, right? And they have certain companies on how you sell them, labs and how you think. And so this design is very important for our community, because if you any point in time, even if you're making your own robot, you change something in the robot. In mind, I want to take out the legs, replace it the wheels. I want to make this bigger and bigger up. You don't want to go back and start so the right way to do is to have build generalist, general model and then fork out the spatial agents that become specific to the robot and the task, and they can always get their learning back to your master model. And that is the right way to build a consistent, ongoing value. Another thing about is that for those of you are familiar with the kind of the website design, right, there's a website back end, to the front end. So think of model as the back end, 10 billion, 100 million model. It could be in the back that's the model. The way you use the model is not just take the model and use you can use it. Use the model in many ways. These models are not just detector signal. Model, very, very powerful models, again, check with the example. You can use it to do make video games, write essays. Check your spreadsheets, right? So what you want to create in your robots are special agents that harness the model, and those agents you can have sufficient product control over you know, like you may want some better performance from one agent, you may want to control it from some operational parameters for another agent, and so on and so forth. And that gives the commonality of the model, the knowledge of the model, but also gives a huge amount of modularity and intervariability. It turns out, we have done some special sauce in how to inject physics and make it work in real time, and that is why, when we worked with one of the top three agricultural companies in the world, we basically with 1000 times less data, generalized to all The robot compactors. This report from the head of machinery saying that we resolve completely separate tasks, different robot factors.
believe us. They had worse within two weeks, including business communication, we had a model. Another two weeks. It was in production. Today, this robot is painting these nice whether, if you look at these walls on the back, look at that, know how to learn. They're very hard to do. If ever the construction before, it's not easy to make it flat. It is very difficult. And this is what can really smooth and do precision control on the wall. And this is one of the first Vlas that we put fully in production for this application. That's the second topic is, people also talk about generalist models. Should we take generalized models? Should we make specialized models? This question is answered by the LLM community already. What does open AI or chatgpt do? It has one model, big model, and topic has one big model, but you create agents to do specialist things. So we should just learn from what is working right. So what we have done is that we have said, okay, the thick generalist model and specialist. The nice thing is that if you have one generalist model that will carry the knowledge, the core common knowledge of different types of robots. Now, manuals may be different from boxes, and boxes may be different from button. It turns out there is a lot of similarity between them. I mean, manuals are objects. Small boxes are objects as well, right? And they have certain companies on how you sell them, labs and how you think. And so this design is very important for our community, because if you any point in time, even if you're making your own robot, you change something in the robot. In mind, I want to take out the legs, replace it the wheels. I want to make this bigger and bigger up. You don't want to go back and start so the right way to do is to have build generalist, general model and then fork out the spatial agents that become specific to the robot and the task, and they can always get their learning back to your master model. And that is the right way to build a consistent, ongoing value. Another thing about is that for those of you are familiar with the kind of the website design, right, there's a website back end, to the front end. So think of model as the back end, 10 billion, 100 million model. It could be in the back that's the model. The way you use the model is not just take the model and use you can use it. Use the model in many ways. These models are not just detector signal. Model, very, very powerful models, again, check with the example. You can use it to do make video games, write essays. Check your spreadsheets, right? So what you want to create in your robots are special agents that harness the model, and those agents you can have sufficient product control over you know, like you may want some better performance from one agent, you may want to control it from some operational parameters for another agent, and so on and so forth. And that gives the commonality of the model, the knowledge of the model, but also gives a huge amount of modularity and intervariability. It turns out, we have done some special sauce in how to inject physics and make it work in real time, and that is why, when we worked with one of the top three agricultural companies in the world, we basically with 1000 times less data, generalized to all The robot compactors. This report from the head of machinery saying that we resolve completely separate tasks, different robot factors.
believe us. They had worse within two weeks, including business communication, we had a model. Another two weeks. It was in production. Today, this robot is painting these nice whether, if you look at these walls on the back, look at that, know how to learn. They're very hard to do. If ever the construction before, it's not easy to make it flat. It is very difficult. And this is what can really smooth and do precision control on the wall. And this is one of the first Vlas that we put fully in production for this application. That's the second topic is, people also talk about generalist models. Should we take generalized models? Should we make specialized models? This question is answered by the LLM community already. What does open AI or chatgpt do? It has one model, big model, and topic has one big model, but you create agents to do specialist things. So we should just learn from what is working right. So what we have done is that we have said, okay, the thick generalist model and specialist. The nice thing is that if you have one generalist model that will carry the knowledge, the core common knowledge of different types of robots. Now, manuals may be different from boxes, and boxes may be different from button. It turns out there is a lot of similarity between them. I mean, manuals are objects. Small boxes are objects as well, right? And they have certain companies on how you sell them, labs and how you think. And so this design is very important for our community, because if you any point in time, even if you're making your own robot, you change something in the robot. In mind, I want to take out the legs, replace it the wheels. I want to make this bigger and bigger up. You don't want to go back and start so the right way to do is to have build generalist, general model and then fork out the spatial agents that become specific to the robot and the task, and they can always get their learning back to your master model. And that is the right way to build a consistent, ongoing value. Another thing about is that for those of you are familiar with the kind of the website design, right, there's a website back end, to the front end. So think of model as the back end, 10 billion, 100 million model. It could be in the back that's the model. The way you use the model is not just take the model and use you can use it. Use the model in many ways. These models are not just detector signal. Model, very, very powerful models, again, check with the example. You can use it to do make video games, write essays. Check your spreadsheets, right? So what you want to create in your robots are special agents that harness the model, and those agents you can have sufficient product control over you know, like you may want some better performance from one agent, you may want to control it from some operational parameters for another agent, and so on and so forth. And that gives the commonality of the model, the knowledge of the model, but also gives a huge amount of modularity and intervariability. It turns out, we have done some special sauce in how to inject physics and make it work in real time, and that is why, when we worked with one of the top three agricultural companies in the world, we basically with 1000 times less data, generalized to all The robot compactors. This report from the head of machinery saying that we resolve completely separate tasks, different robot factors.
15:30It's something that traditional models
It's something that traditional models
It's something that traditional models
It's something that traditional models
S Speaker 215:35cannot be. The last bit, this is good to show that we can meet the form the form the requirements of our variety of customers, from humanoids to construction robotics factors, but to put them in production, the last thing that is really needed, if you have to get the models done in real time, if you don't, then they are addressed some research demo, right? And how do we do that? Well, there's a little bit of a hindering that is now necessary to you, where you are putting your model in a small embedded GPU, and you don't need to choose anymore. You can do something called decimation and so on and so forth. And you can use GPU module time. And once you do that, your model is running at 30 times. So now, when we went ahead and said, Okay, well, let's go to a place where real time is the problem, right? This, this vertical has so soon, so passionate about throughput. If throughput drops by 1% the head of business unit, half a million dollar in supply chain is literally broken up. It's like you're losing money, right? You have to do things in a very fast things like the online retailer, right? So in this particular case, we partnered with the robotics company, and we were a great model that was able to do things such as handle and back boxes at a very fast speed in trucks and fill it and do things and very complex analysis. Now we are now we are growing. We are very looking forward to doing a lot of things right. So we are partnering with a variety of companies. Different companies have different form factors, different needs. We are handling numerous industrial supply chain and manufacturing use cases. And the reason, to summarize, we are able to do it effectively is two key points. Number one, I think in some sense it's obvious from the online world, have one model, your own model, some other companies, model, KJ, model, who cares? But one model is the key, because it can learn from different robot different experiences. It adds up right at the same time, you can create interpretable, specialized agents that learn using a task, and they get to that 99.99% or whatever the requirement is, and that is the robot AI that works right now, we have some secret sauce, inspired by physics that allows us to do this on 100 data points. 500 data points, we are 1,000x less more data efficiently required 1000 excess data because through physics reasoning, but with all that, we are able to put our customers in production, because it's something that works in real time. I think it's an exciting time to unlock so many robotics opportunities.
cannot be. The last bit, this is good to show that we can meet the form the form the requirements of our variety of customers, from humanoids to construction robotics factors, but to put them in production, the last thing that is really needed, if you have to get the models done in real time, if you don't, then they are addressed some research demo, right? And how do we do that? Well, there's a little bit of a hindering that is now necessary to you, where you are putting your model in a small embedded GPU, and you don't need to choose anymore. You can do something called decimation and so on and so forth. And you can use GPU module time. And once you do that, your model is running at 30 times. So now, when we went ahead and said, Okay, well, let's go to a place where real time is the problem, right? This, this vertical has so soon, so passionate about throughput. If throughput drops by 1% the head of business unit, half a million dollar in supply chain is literally broken up. It's like you're losing money, right? You have to do things in a very fast things like the online retailer, right? So in this particular case, we partnered with the robotics company, and we were a great model that was able to do things such as handle and back boxes at a very fast speed in trucks and fill it and do things and very complex analysis. Now we are now we are growing. We are very looking forward to doing a lot of things right. So we are partnering with a variety of companies. Different companies have different form factors, different needs. We are handling numerous industrial supply chain and manufacturing use cases. And the reason, to summarize, we are able to do it effectively is two key points. Number one, I think in some sense it's obvious from the online world, have one model, your own model, some other companies, model, KJ, model, who cares? But one model is the key, because it can learn from different robot different experiences. It adds up right at the same time, you can create interpretable, specialized agents that learn using a task, and they get to that 99.99% or whatever the requirement is, and that is the robot AI that works right now, we have some secret sauce, inspired by physics that allows us to do this on 100 data points. 500 data points, we are 1,000x less more data efficiently required 1000 excess data because through physics reasoning, but with all that, we are able to put our customers in production, because it's something that works in real time. I think it's an exciting time to unlock so many robotics opportunities.
cannot be. The last bit, this is good to show that we can meet the form the form the requirements of our variety of customers, from humanoids to construction robotics factors, but to put them in production, the last thing that is really needed, if you have to get the models done in real time, if you don't, then they are addressed some research demo, right? And how do we do that? Well, there's a little bit of a hindering that is now necessary to you, where you are putting your model in a small embedded GPU, and you don't need to choose anymore. You can do something called decimation and so on and so forth. And you can use GPU module time. And once you do that, your model is running at 30 times. So now, when we went ahead and said, Okay, well, let's go to a place where real time is the problem, right? This, this vertical has so soon, so passionate about throughput. If throughput drops by 1% the head of business unit, half a million dollar in supply chain is literally broken up. It's like you're losing money, right? You have to do things in a very fast things like the online retailer, right? So in this particular case, we partnered with the robotics company, and we were a great model that was able to do things such as handle and back boxes at a very fast speed in trucks and fill it and do things and very complex analysis. Now we are now we are growing. We are very looking forward to doing a lot of things right. So we are partnering with a variety of companies. Different companies have different form factors, different needs. We are handling numerous industrial supply chain and manufacturing use cases. And the reason, to summarize, we are able to do it effectively is two key points. Number one, I think in some sense it's obvious from the online world, have one model, your own model, some other companies, model, KJ, model, who cares? But one model is the key, because it can learn from different robot different experiences. It adds up right at the same time, you can create interpretable, specialized agents that learn using a task, and they get to that 99.99% or whatever the requirement is, and that is the robot AI that works right now, we have some secret sauce, inspired by physics that allows us to do this on 100 data points. 500 data points, we are 1,000x less more data efficiently required 1000 excess data because through physics reasoning, but with all that, we are able to put our customers in production, because it's something that works in real time. I think it's an exciting time to unlock so many robotics opportunities.
cannot be. The last bit, this is good to show that we can meet the form the form the requirements of our variety of customers, from humanoids to construction robotics factors, but to put them in production, the last thing that is really needed, if you have to get the models done in real time, if you don't, then they are addressed some research demo, right? And how do we do that? Well, there's a little bit of a hindering that is now necessary to you, where you are putting your model in a small embedded GPU, and you don't need to choose anymore. You can do something called decimation and so on and so forth. And you can use GPU module time. And once you do that, your model is running at 30 times. So now, when we went ahead and said, Okay, well, let's go to a place where real time is the problem, right? This, this vertical has so soon, so passionate about throughput. If throughput drops by 1% the head of business unit, half a million dollar in supply chain is literally broken up. It's like you're losing money, right? You have to do things in a very fast things like the online retailer, right? So in this particular case, we partnered with the robotics company, and we were a great model that was able to do things such as handle and back boxes at a very fast speed in trucks and fill it and do things and very complex analysis. Now we are now we are growing. We are very looking forward to doing a lot of things right. So we are partnering with a variety of companies. Different companies have different form factors, different needs. We are handling numerous industrial supply chain and manufacturing use cases. And the reason, to summarize, we are able to do it effectively is two key points. Number one, I think in some sense it's obvious from the online world, have one model, your own model, some other companies, model, KJ, model, who cares? But one model is the key, because it can learn from different robot different experiences. It adds up right at the same time, you can create interpretable, specialized agents that learn using a task, and they get to that 99.99% or whatever the requirement is, and that is the robot AI that works right now, we have some secret sauce, inspired by physics that allows us to do this on 100 data points. 500 data points, we are 1,000x less more data efficiently required 1000 excess data because through physics reasoning, but with all that, we are able to put our customers in production, because it's something that works in real time. I think it's an exciting time to unlock so many robotics opportunities.
18:33Thank you so much ash and
Thank you so much ash and
Thank you so much ash and
Thank you so much ash and
18:42they can reach out to you for questions. Thank you so
they can reach out to you for questions. Thank you so
they can reach out to you for questions. Thank you so
they can reach out to you for questions. Thank you so
S Speaker 319:02much. You big concern.
much. You big concern.
much. You big concern.
much. You big concern.
19:40Know, but Things
S Speaker 320:02You our How
S Speaker 321:02to do. Ready to.