Meeting: Embodi - Hatef
Wed, Jun 4
9:10 AM
35 min
Priyesh P
Introduction and Team Background
0:16
Overview of
URL: https://otter.ai/u/kh_-e4pFEus3_mMrW0m6Zsd3OOQ
Downloaded: 2025-12-22T10:02:26.654266
Method: text_extraction
============================================================

S Speaker 10:16Hey, hi, Hatha, can you hear me? Hi Priyesh, how's it going? It's going great. Am I saying your name, right? Hatif,
Hey, hi, Hatha, can you hear me? Hi Priyesh, how's it going? It's going great. Am I saying your name, right? Hatif,
Hey, hi, Hatha, can you hear me? Hi Priyesh, how's it going? It's going great. Am I saying your name, right? Hatif,
Hey, hi, Hatha, can you hear me? Hi Priyesh, how's it going? It's going great. Am I saying your name, right? Hatif,
0:22yeah, it's hot. F, perfect, perfect.
yeah, it's hot. F, perfect, perfect.
yeah, it's hot. F, perfect, perfect.
yeah, it's hot. F, perfect, perfect.
0:25Where you based Atif,
Where you based Atif,
Where you based Atif,
Where you based Atif,
S Speaker 20:26I'm based in currently, Vancouver, Canada, right?
I'm based in currently, Vancouver, Canada, right?
I'm based in currently, Vancouver, Canada, right?
I'm based in currently, Vancouver, Canada, right?
S Speaker 10:30And where's the company based? Then companies
And where's the company based? Then companies
And where's the company based? Then companies
And where's the company based? Then companies
S Speaker 20:33incorporated in the States than the Delaware company. But for the time being, I'm in Vancouver. Yeah, operations are in Vancouver.
incorporated in the States than the Delaware company. But for the time being, I'm in Vancouver. Yeah, operations are in Vancouver.
incorporated in the States than the Delaware company. But for the time being, I'm in Vancouver. Yeah, operations are in Vancouver.
incorporated in the States than the Delaware company. But for the time being, I'm in Vancouver. Yeah, operations are in Vancouver.
0:44And how many members are in the team today?
And how many members are in the team today?
And how many members are in the team today?
And how many members are in the team today?
S Speaker 20:48Three members. It's myself, my co founder, Holly, and we, I, we have a part time engineer that's that's working on the tech with me, Nils, both world class people. I've worked with Holly back in Sanctuary. I can give you a little intro about that, and she's one of the first hires at sanctuary. She departed sanctuary back in 2020. Went out of her entrepreneurial journey, was involved in a few startups, and she's actually a co founder at startup called bytes. She's gonna based on her old one visa. She's going to transition to embody pretty soon. And Mills, I hired Mills back in Sanctuary, and one of the world's world class engineers on robotic side. And he departed sanctuary as well after I left and and I hired him here as a part time engineer. Yeah,
Three members. It's myself, my co founder, Holly, and we, I, we have a part time engineer that's that's working on the tech with me, Nils, both world class people. I've worked with Holly back in Sanctuary. I can give you a little intro about that, and she's one of the first hires at sanctuary. She departed sanctuary back in 2020. Went out of her entrepreneurial journey, was involved in a few startups, and she's actually a co founder at startup called bytes. She's gonna based on her old one visa. She's going to transition to embody pretty soon. And Mills, I hired Mills back in Sanctuary, and one of the world's world class engineers on robotic side. And he departed sanctuary as well after I left and and I hired him here as a part time engineer. Yeah,
Three members. It's myself, my co founder, Holly, and we, I, we have a part time engineer that's that's working on the tech with me, Nils, both world class people. I've worked with Holly back in Sanctuary. I can give you a little intro about that, and she's one of the first hires at sanctuary. She departed sanctuary back in 2020. Went out of her entrepreneurial journey, was involved in a few startups, and she's actually a co founder at startup called bytes. She's gonna based on her old one visa. She's going to transition to embody pretty soon. And Mills, I hired Mills back in Sanctuary, and one of the world's world class engineers on robotic side. And he departed sanctuary as well after I left and and I hired him here as a part time engineer. Yeah,
Three members. It's myself, my co founder, Holly, and we, I, we have a part time engineer that's that's working on the tech with me, Nils, both world class people. I've worked with Holly back in Sanctuary. I can give you a little intro about that, and she's one of the first hires at sanctuary. She departed sanctuary back in 2020. Went out of her entrepreneurial journey, was involved in a few startups, and she's actually a co founder at startup called bytes. She's gonna based on her old one visa. She's going to transition to embody pretty soon. And Mills, I hired Mills back in Sanctuary, and one of the world's world class engineers on robotic side. And he departed sanctuary as well after I left and and I hired him here as a part time engineer. Yeah,
S Speaker 11:51well, that's, that's good to know you. You the team members go deep with you. So that's, that's always good, yeah,
well, that's, that's good to know you. You the team members go deep with you. So that's, that's always good, yeah,
well, that's, that's good to know you. You the team members go deep with you. So that's, that's always good, yeah,
well, that's, that's good to know you. You the team members go deep with you. So that's, that's always good, yeah,
1:57it's like getting married,
it's like getting married,
it's like getting married,
it's like getting married,
S Speaker 12:00right? So how does I'll give you a brief background about Qualcomm ventures and why, and how are we interested in this space in general, and also about how we operate, how we invest in areas. And then would love to get an overview from your end, and get to know what, how he started to do this crazy thing. So I'm an investor at Qualcomm ventures, been with the Fund for about a year and a half now, prior to that, I had a small stint at Menlo Ventures, and also spent some time in LA with Morpheus ventures. So I'm in the investment team. I work with Tushar, who is our Managing Director, and we take care of AI investments, particularly Tushar Gupta, okay, yeah. He runs us investments team. I worked, actually with him on our AI investment strategy, and particularly myself about a couple of weeks now, I've been exploring the area of physical AI as Qualcomm. It's been it's quite strategically relevant for us, given that a lot of robots would have a lot of in device compute, and that's something we believe Qualcomm can power, given our much better power efficiency metrics, much lower cost than comparables in the market. So that's a hypothesis today, but we want to expand quite aggressively in the in that area. So we talking to a lot of big players in the entire space. And hence the physical AI space becomes very interesting for us. We're talking to also the foundation model companies in this space. So spreading quite a wide gamut, and that's where I got to know about embody from Geek ventures. Was very excited to learn more about you. I understand the data problem is very big in this space, so would love to see how you are approaching it, how it's different from other paradigms of simulation, let's say video, learning things like that, and more about the fund itself. We've been around for about 25 years. We actually complete our 25 years this year, and it's the corporate venture arm of Qualcomm, so we invest out of the balance sheet. It's we typically aim to deploy about 150 $280 million a year in US geography. But it's also a global fund. So we have offices in Latin America, China, Israel, India, Europe, to name a few regions. And then check sizes would be anywhere from two to $15 million we are typically a Series A plus investor, so we prefer to see some revenue and some product market fit before we come in, but at times we have also done seeds, especially in the entire AI environment, so broadly around that region. Don't need the company to be very highly strategically relevant for us. But we do want to see if, if the company we invest in could either down the line, be a partner with Qualcomm, either in engineering go to market, or maybe even have Qualcomm as one of the customers. So, so we explore all of that different angles.
right? So how does I'll give you a brief background about Qualcomm ventures and why, and how are we interested in this space in general, and also about how we operate, how we invest in areas. And then would love to get an overview from your end, and get to know what, how he started to do this crazy thing. So I'm an investor at Qualcomm ventures, been with the Fund for about a year and a half now, prior to that, I had a small stint at Menlo Ventures, and also spent some time in LA with Morpheus ventures. So I'm in the investment team. I work with Tushar, who is our Managing Director, and we take care of AI investments, particularly Tushar Gupta, okay, yeah. He runs us investments team. I worked, actually with him on our AI investment strategy, and particularly myself about a couple of weeks now, I've been exploring the area of physical AI as Qualcomm. It's been it's quite strategically relevant for us, given that a lot of robots would have a lot of in device compute, and that's something we believe Qualcomm can power, given our much better power efficiency metrics, much lower cost than comparables in the market. So that's a hypothesis today, but we want to expand quite aggressively in the in that area. So we talking to a lot of big players in the entire space. And hence the physical AI space becomes very interesting for us. We're talking to also the foundation model companies in this space. So spreading quite a wide gamut, and that's where I got to know about embody from Geek ventures. Was very excited to learn more about you. I understand the data problem is very big in this space, so would love to see how you are approaching it, how it's different from other paradigms of simulation, let's say video, learning things like that, and more about the fund itself. We've been around for about 25 years. We actually complete our 25 years this year, and it's the corporate venture arm of Qualcomm, so we invest out of the balance sheet. It's we typically aim to deploy about 150 $280 million a year in US geography. But it's also a global fund. So we have offices in Latin America, China, Israel, India, Europe, to name a few regions. And then check sizes would be anywhere from two to $15 million we are typically a Series A plus investor, so we prefer to see some revenue and some product market fit before we come in, but at times we have also done seeds, especially in the entire AI environment, so broadly around that region. Don't need the company to be very highly strategically relevant for us. But we do want to see if, if the company we invest in could either down the line, be a partner with Qualcomm, either in engineering go to market, or maybe even have Qualcomm as one of the customers. So, so we explore all of that different angles.
right? So how does I'll give you a brief background about Qualcomm ventures and why, and how are we interested in this space in general, and also about how we operate, how we invest in areas. And then would love to get an overview from your end, and get to know what, how he started to do this crazy thing. So I'm an investor at Qualcomm ventures, been with the Fund for about a year and a half now, prior to that, I had a small stint at Menlo Ventures, and also spent some time in LA with Morpheus ventures. So I'm in the investment team. I work with Tushar, who is our Managing Director, and we take care of AI investments, particularly Tushar Gupta, okay, yeah. He runs us investments team. I worked, actually with him on our AI investment strategy, and particularly myself about a couple of weeks now, I've been exploring the area of physical AI as Qualcomm. It's been it's quite strategically relevant for us, given that a lot of robots would have a lot of in device compute, and that's something we believe Qualcomm can power, given our much better power efficiency metrics, much lower cost than comparables in the market. So that's a hypothesis today, but we want to expand quite aggressively in the in that area. So we talking to a lot of big players in the entire space. And hence the physical AI space becomes very interesting for us. We're talking to also the foundation model companies in this space. So spreading quite a wide gamut, and that's where I got to know about embody from Geek ventures. Was very excited to learn more about you. I understand the data problem is very big in this space, so would love to see how you are approaching it, how it's different from other paradigms of simulation, let's say video, learning things like that, and more about the fund itself. We've been around for about 25 years. We actually complete our 25 years this year, and it's the corporate venture arm of Qualcomm, so we invest out of the balance sheet. It's we typically aim to deploy about 150 $280 million a year in US geography. But it's also a global fund. So we have offices in Latin America, China, Israel, India, Europe, to name a few regions. And then check sizes would be anywhere from two to $15 million we are typically a Series A plus investor, so we prefer to see some revenue and some product market fit before we come in, but at times we have also done seeds, especially in the entire AI environment, so broadly around that region. Don't need the company to be very highly strategically relevant for us. But we do want to see if, if the company we invest in could either down the line, be a partner with Qualcomm, either in engineering go to market, or maybe even have Qualcomm as one of the customers. So, so we explore all of that different angles.
right? So how does I'll give you a brief background about Qualcomm ventures and why, and how are we interested in this space in general, and also about how we operate, how we invest in areas. And then would love to get an overview from your end, and get to know what, how he started to do this crazy thing. So I'm an investor at Qualcomm ventures, been with the Fund for about a year and a half now, prior to that, I had a small stint at Menlo Ventures, and also spent some time in LA with Morpheus ventures. So I'm in the investment team. I work with Tushar, who is our Managing Director, and we take care of AI investments, particularly Tushar Gupta, okay, yeah. He runs us investments team. I worked, actually with him on our AI investment strategy, and particularly myself about a couple of weeks now, I've been exploring the area of physical AI as Qualcomm. It's been it's quite strategically relevant for us, given that a lot of robots would have a lot of in device compute, and that's something we believe Qualcomm can power, given our much better power efficiency metrics, much lower cost than comparables in the market. So that's a hypothesis today, but we want to expand quite aggressively in the in that area. So we talking to a lot of big players in the entire space. And hence the physical AI space becomes very interesting for us. We're talking to also the foundation model companies in this space. So spreading quite a wide gamut, and that's where I got to know about embody from Geek ventures. Was very excited to learn more about you. I understand the data problem is very big in this space, so would love to see how you are approaching it, how it's different from other paradigms of simulation, let's say video, learning things like that, and more about the fund itself. We've been around for about 25 years. We actually complete our 25 years this year, and it's the corporate venture arm of Qualcomm, so we invest out of the balance sheet. It's we typically aim to deploy about 150 $280 million a year in US geography. But it's also a global fund. So we have offices in Latin America, China, Israel, India, Europe, to name a few regions. And then check sizes would be anywhere from two to $15 million we are typically a Series A plus investor, so we prefer to see some revenue and some product market fit before we come in, but at times we have also done seeds, especially in the entire AI environment, so broadly around that region. Don't need the company to be very highly strategically relevant for us. But we do want to see if, if the company we invest in could either down the line, be a partner with Qualcomm, either in engineering go to market, or maybe even have Qualcomm as one of the customers. So, so we explore all of that different angles.
S Speaker 25:10That's awesome. So I got a couple of questions that's great information. Got a couple of questions for the seed round that that you mentioned you invested in a few companies. What is the check size for the seats.
That's awesome. So I got a couple of questions that's great information. Got a couple of questions for the seed round that that you mentioned you invested in a few companies. What is the check size for the seats.
That's awesome. So I got a couple of questions that's great information. Got a couple of questions for the seed round that that you mentioned you invested in a few companies. What is the check size for the seats.
That's awesome. So I got a couple of questions that's great information. Got a couple of questions for the seed round that that you mentioned you invested in a few companies. What is the check size for the seats.
S Speaker 15:21So I think we can go as low as 1.52 but, but that's the lowest size of check we would write. Below that, it does not really make sense to get the entire corporate moving the legal compliance processes. So
So I think we can go as low as 1.52 but, but that's the lowest size of check we would write. Below that, it does not really make sense to get the entire corporate moving the legal compliance processes. So
So I think we can go as low as 1.52 but, but that's the lowest size of check we would write. Below that, it does not really make sense to get the entire corporate moving the legal compliance processes. So
So I think we can go as low as 1.52 but, but that's the lowest size of check we would write. Below that, it does not really make sense to get the entire corporate moving the legal compliance processes. So
S Speaker 25:34yeah, that that'll make sense. That's awesome. The other thing is, I wanted to ask about your the process of investment. Is it like consensus based? Or how do you handle that?
yeah, that that'll make sense. That's awesome. The other thing is, I wanted to ask about your the process of investment. Is it like consensus based? Or how do you handle that?
yeah, that that'll make sense. That's awesome. The other thing is, I wanted to ask about your the process of investment. Is it like consensus based? Or how do you handle that?
yeah, that that'll make sense. That's awesome. The other thing is, I wanted to ask about your the process of investment. Is it like consensus based? Or how do you handle that?
S Speaker 15:52So, yeah, it is consensus based. But typically in one I am sort of leads the entire process themselves. So we have discussions within the team every Monday, which is our investment managers meeting, and there we on a consensus, sort of decide if we want to make the if we want to take the process forward. So post that, we have one more process when we where we include geo leads, so investors from different regions. And it's also a voting process there. But I would say if, if, if an investment manager has a very high conviction, usually the consensus thing is just sort of getting everyone on board. It's not really, I would say, like everyone decides together kind
So, yeah, it is consensus based. But typically in one I am sort of leads the entire process themselves. So we have discussions within the team every Monday, which is our investment managers meeting, and there we on a consensus, sort of decide if we want to make the if we want to take the process forward. So post that, we have one more process when we where we include geo leads, so investors from different regions. And it's also a voting process there. But I would say if, if, if an investment manager has a very high conviction, usually the consensus thing is just sort of getting everyone on board. It's not really, I would say, like everyone decides together kind
So, yeah, it is consensus based. But typically in one I am sort of leads the entire process themselves. So we have discussions within the team every Monday, which is our investment managers meeting, and there we on a consensus, sort of decide if we want to make the if we want to take the process forward. So post that, we have one more process when we where we include geo leads, so investors from different regions. And it's also a voting process there. But I would say if, if, if an investment manager has a very high conviction, usually the consensus thing is just sort of getting everyone on board. It's not really, I would say, like everyone decides together kind
So, yeah, it is consensus based. But typically in one I am sort of leads the entire process themselves. So we have discussions within the team every Monday, which is our investment managers meeting, and there we on a consensus, sort of decide if we want to make the if we want to take the process forward. So post that, we have one more process when we where we include geo leads, so investors from different regions. And it's also a voting process there. But I would say if, if, if an investment manager has a very high conviction, usually the consensus thing is just sort of getting everyone on board. It's not really, I would say, like everyone decides together kind
S Speaker 26:35of thing. And what kind of like timelines are you
of thing. And what kind of like timelines are you
of thing. And what kind of like timelines are you
of thing. And what kind of like timelines are you
S Speaker 16:40Yeah, typically, typically, the process, entire process for us, from the first meeting to sort of wiring the money, takes about three to four weeks, so roughly, roughly around that we can sort of try to shorten it by a week or so, but it would take a little bit more time than the standard Financial venture for us. Then
Yeah, typically, typically, the process, entire process for us, from the first meeting to sort of wiring the money, takes about three to four weeks, so roughly, roughly around that we can sort of try to shorten it by a week or so, but it would take a little bit more time than the standard Financial venture for us. Then
Yeah, typically, typically, the process, entire process for us, from the first meeting to sort of wiring the money, takes about three to four weeks, so roughly, roughly around that we can sort of try to shorten it by a week or so, but it would take a little bit more time than the standard Financial venture for us. Then
Yeah, typically, typically, the process, entire process for us, from the first meeting to sort of wiring the money, takes about three to four weeks, so roughly, roughly around that we can sort of try to shorten it by a week or so, but it would take a little bit more time than the standard Financial venture for us. Then
S Speaker 27:01that's awesome. That's no that's that's all good. I I think those are all the questions that I have for the time being, and I'll probably have more later on. But yeah, little bit, a little bit of background about about myself. I've been in robotics and AI for more than 10 years now, and one of the first gigs I started was with sanctuary. Ai, I don't know if you, if you're familiar with sanctuary? Ai, yeah. So I hired, I was hired there as one of the first founding engineers. Obviously, early startup days were a lot of hats, lot of technical work, and later on, transitioned into management, so I founded and led a Department, Department of controls engineering. There grew that team to be more than 15 people, and, yeah, heavily involved in critical initiatives like locomotion, like the manipulation projects were basically the API between the hardware and the and the brain. So I'm quite familiar with both sides of the sides of the spectrum, how to deal with hardware, and also like how to deal with the kind of like brain we're heavily involved with deployment. So we deployed a robot a few times. Most of my team was, was heavily involved. There pretty familiar with with data collection challenges and
that's awesome. That's no that's that's all good. I I think those are all the questions that I have for the time being, and I'll probably have more later on. But yeah, little bit, a little bit of background about about myself. I've been in robotics and AI for more than 10 years now, and one of the first gigs I started was with sanctuary. Ai, I don't know if you, if you're familiar with sanctuary? Ai, yeah. So I hired, I was hired there as one of the first founding engineers. Obviously, early startup days were a lot of hats, lot of technical work, and later on, transitioned into management, so I founded and led a Department, Department of controls engineering. There grew that team to be more than 15 people, and, yeah, heavily involved in critical initiatives like locomotion, like the manipulation projects were basically the API between the hardware and the and the brain. So I'm quite familiar with both sides of the sides of the spectrum, how to deal with hardware, and also like how to deal with the kind of like brain we're heavily involved with deployment. So we deployed a robot a few times. Most of my team was, was heavily involved. There pretty familiar with with data collection challenges and
that's awesome. That's no that's that's all good. I I think those are all the questions that I have for the time being, and I'll probably have more later on. But yeah, little bit, a little bit of background about about myself. I've been in robotics and AI for more than 10 years now, and one of the first gigs I started was with sanctuary. Ai, I don't know if you, if you're familiar with sanctuary? Ai, yeah. So I hired, I was hired there as one of the first founding engineers. Obviously, early startup days were a lot of hats, lot of technical work, and later on, transitioned into management, so I founded and led a Department, Department of controls engineering. There grew that team to be more than 15 people, and, yeah, heavily involved in critical initiatives like locomotion, like the manipulation projects were basically the API between the hardware and the and the brain. So I'm quite familiar with both sides of the sides of the spectrum, how to deal with hardware, and also like how to deal with the kind of like brain we're heavily involved with deployment. So we deployed a robot a few times. Most of my team was, was heavily involved. There pretty familiar with with data collection challenges and
that's awesome. That's no that's that's all good. I I think those are all the questions that I have for the time being, and I'll probably have more later on. But yeah, little bit, a little bit of background about about myself. I've been in robotics and AI for more than 10 years now, and one of the first gigs I started was with sanctuary. Ai, I don't know if you, if you're familiar with sanctuary? Ai, yeah. So I hired, I was hired there as one of the first founding engineers. Obviously, early startup days were a lot of hats, lot of technical work, and later on, transitioned into management, so I founded and led a Department, Department of controls engineering. There grew that team to be more than 15 people, and, yeah, heavily involved in critical initiatives like locomotion, like the manipulation projects were basically the API between the hardware and the and the brain. So I'm quite familiar with both sides of the sides of the spectrum, how to deal with hardware, and also like how to deal with the kind of like brain we're heavily involved with deployment. So we deployed a robot a few times. Most of my team was, was heavily involved. There pretty familiar with with data collection challenges and
8:33and deployment, commercialization and deployment.
and deployment, commercialization and deployment.
and deployment, commercialization and deployment.
and deployment, commercialization and deployment.
S Speaker 28:37And, yeah, I was leading that team for for for a few years. Last year, April, I decided to part ways from sanctuary to basically think of my own business. So, like, I rested a little bit and then embody came to mind, like the data collection. We always knew that data, data was a problem in robotics, and I'm sure I don't need to convince you on that, just like llms, you know, llms require a lot of data to be trained, same with robotic brains foundation models, but this data is not readily available, like the internet corpus, so companies resort to teleoperation, right? So, like, that's remote controlling the robot to capture data. But if you think of it, tele operation is not scalable, right? So like, if you look at figure, for instance, figure AI, they partner with BMW, what they've done is they've actually replicated BMW production cells in house to tell operate the robot there and collect data there. It's extremely expensive. Imagine trying to do that for every single task that you want to automate. And these robots are going to automate everything. So that's that's the whole point of humanoid robots, right? So that's not scalable. That's not something that you can do, especially on specialized tasks like manufacturing and all of that specialized tasks. It's not feasible. So the idea of embody is like, okay, instead of three entities being in the loop and three entities being the robot that the pilot who's piloting the robot and the worker who's been doing the work there. So you got the pilot needs to learn that task, right. Instead of three entities, we reduce it to one entity. So just just that one person who's doing the work has been doing that work for years. So we collect directly from them via some wearables. So like these are gloves and goggles. And there's a reason for this. A lot of companies you see out there are collecting data from video alone, right? So video alone, there's a lot of information lost in video, right? So how, hard are you pressing on the objects? If your hands are occluded, what's going to happen? Anyways? So some some hardware. We're going to process that data and convert that's human space. We're going to process that data, convert that to a robot space, as if the robot is doing that task, and basically give it to customers. So the market potential for this is massive, right? So like all these robotics companies who are data hungry, thirsty for data, they would need something like this to scale like without something like this, it's going to be really difficult to scale. I mean, there are other ways as well, but this is one of the ways that we're kind of like tackling the problem. Obviously, you mentioned simulation. Yes, simulation is viable, but there's always that centriole gap. So you see a lot of, you know, good work coming out of simulation for bulk movements of the robot, right? So the bulk movements are, you know, you see unitary robots dance, you know, Tesla robots walk like, just like humans. So that, that's our lha, that's basically reinforcement learning with human feedback,
And, yeah, I was leading that team for for for a few years. Last year, April, I decided to part ways from sanctuary to basically think of my own business. So, like, I rested a little bit and then embody came to mind, like the data collection. We always knew that data, data was a problem in robotics, and I'm sure I don't need to convince you on that, just like llms, you know, llms require a lot of data to be trained, same with robotic brains foundation models, but this data is not readily available, like the internet corpus, so companies resort to teleoperation, right? So, like, that's remote controlling the robot to capture data. But if you think of it, tele operation is not scalable, right? So like, if you look at figure, for instance, figure AI, they partner with BMW, what they've done is they've actually replicated BMW production cells in house to tell operate the robot there and collect data there. It's extremely expensive. Imagine trying to do that for every single task that you want to automate. And these robots are going to automate everything. So that's that's the whole point of humanoid robots, right? So that's not scalable. That's not something that you can do, especially on specialized tasks like manufacturing and all of that specialized tasks. It's not feasible. So the idea of embody is like, okay, instead of three entities being in the loop and three entities being the robot that the pilot who's piloting the robot and the worker who's been doing the work there. So you got the pilot needs to learn that task, right. Instead of three entities, we reduce it to one entity. So just just that one person who's doing the work has been doing that work for years. So we collect directly from them via some wearables. So like these are gloves and goggles. And there's a reason for this. A lot of companies you see out there are collecting data from video alone, right? So video alone, there's a lot of information lost in video, right? So how, hard are you pressing on the objects? If your hands are occluded, what's going to happen? Anyways? So some some hardware. We're going to process that data and convert that's human space. We're going to process that data, convert that to a robot space, as if the robot is doing that task, and basically give it to customers. So the market potential for this is massive, right? So like all these robotics companies who are data hungry, thirsty for data, they would need something like this to scale like without something like this, it's going to be really difficult to scale. I mean, there are other ways as well, but this is one of the ways that we're kind of like tackling the problem. Obviously, you mentioned simulation. Yes, simulation is viable, but there's always that centriole gap. So you see a lot of, you know, good work coming out of simulation for bulk movements of the robot, right? So the bulk movements are, you know, you see unitary robots dance, you know, Tesla robots walk like, just like humans. So that, that's our lha, that's basically reinforcement learning with human feedback,
And, yeah, I was leading that team for for for a few years. Last year, April, I decided to part ways from sanctuary to basically think of my own business. So, like, I rested a little bit and then embody came to mind, like the data collection. We always knew that data, data was a problem in robotics, and I'm sure I don't need to convince you on that, just like llms, you know, llms require a lot of data to be trained, same with robotic brains foundation models, but this data is not readily available, like the internet corpus, so companies resort to teleoperation, right? So, like, that's remote controlling the robot to capture data. But if you think of it, tele operation is not scalable, right? So like, if you look at figure, for instance, figure AI, they partner with BMW, what they've done is they've actually replicated BMW production cells in house to tell operate the robot there and collect data there. It's extremely expensive. Imagine trying to do that for every single task that you want to automate. And these robots are going to automate everything. So that's that's the whole point of humanoid robots, right? So that's not scalable. That's not something that you can do, especially on specialized tasks like manufacturing and all of that specialized tasks. It's not feasible. So the idea of embody is like, okay, instead of three entities being in the loop and three entities being the robot that the pilot who's piloting the robot and the worker who's been doing the work there. So you got the pilot needs to learn that task, right. Instead of three entities, we reduce it to one entity. So just just that one person who's doing the work has been doing that work for years. So we collect directly from them via some wearables. So like these are gloves and goggles. And there's a reason for this. A lot of companies you see out there are collecting data from video alone, right? So video alone, there's a lot of information lost in video, right? So how, hard are you pressing on the objects? If your hands are occluded, what's going to happen? Anyways? So some some hardware. We're going to process that data and convert that's human space. We're going to process that data, convert that to a robot space, as if the robot is doing that task, and basically give it to customers. So the market potential for this is massive, right? So like all these robotics companies who are data hungry, thirsty for data, they would need something like this to scale like without something like this, it's going to be really difficult to scale. I mean, there are other ways as well, but this is one of the ways that we're kind of like tackling the problem. Obviously, you mentioned simulation. Yes, simulation is viable, but there's always that centriole gap. So you see a lot of, you know, good work coming out of simulation for bulk movements of the robot, right? So the bulk movements are, you know, you see unitary robots dance, you know, Tesla robots walk like, just like humans. So that, that's our lha, that's basically reinforcement learning with human feedback,
And, yeah, I was leading that team for for for a few years. Last year, April, I decided to part ways from sanctuary to basically think of my own business. So, like, I rested a little bit and then embody came to mind, like the data collection. We always knew that data, data was a problem in robotics, and I'm sure I don't need to convince you on that, just like llms, you know, llms require a lot of data to be trained, same with robotic brains foundation models, but this data is not readily available, like the internet corpus, so companies resort to teleoperation, right? So, like, that's remote controlling the robot to capture data. But if you think of it, tele operation is not scalable, right? So like, if you look at figure, for instance, figure AI, they partner with BMW, what they've done is they've actually replicated BMW production cells in house to tell operate the robot there and collect data there. It's extremely expensive. Imagine trying to do that for every single task that you want to automate. And these robots are going to automate everything. So that's that's the whole point of humanoid robots, right? So that's not scalable. That's not something that you can do, especially on specialized tasks like manufacturing and all of that specialized tasks. It's not feasible. So the idea of embody is like, okay, instead of three entities being in the loop and three entities being the robot that the pilot who's piloting the robot and the worker who's been doing the work there. So you got the pilot needs to learn that task, right. Instead of three entities, we reduce it to one entity. So just just that one person who's doing the work has been doing that work for years. So we collect directly from them via some wearables. So like these are gloves and goggles. And there's a reason for this. A lot of companies you see out there are collecting data from video alone, right? So video alone, there's a lot of information lost in video, right? So how, hard are you pressing on the objects? If your hands are occluded, what's going to happen? Anyways? So some some hardware. We're going to process that data and convert that's human space. We're going to process that data, convert that to a robot space, as if the robot is doing that task, and basically give it to customers. So the market potential for this is massive, right? So like all these robotics companies who are data hungry, thirsty for data, they would need something like this to scale like without something like this, it's going to be really difficult to scale. I mean, there are other ways as well, but this is one of the ways that we're kind of like tackling the problem. Obviously, you mentioned simulation. Yes, simulation is viable, but there's always that centriole gap. So you see a lot of, you know, good work coming out of simulation for bulk movements of the robot, right? So the bulk movements are, you know, you see unitary robots dance, you know, Tesla robots walk like, just like humans. So that, that's our lha, that's basically reinforcement learning with human feedback,
S Speaker 212:23doesn't really translate when the environment is contact rich. So manipulation problem is a very contact rich problem. So like you, you're basically touching a lot of things and so on and so forth. And that becomes really difficult to do it in simulation and directly zero shot, transferring it to to robots. So people would still resort to kind of imitation learning, kind of techniques, which would require a lot of data. Our electif requires data as well, but not, not as data hungry, as as imitation models. So, long story short, data is a massive need. You know, humanoids are, I think arc investment had a good article on it's going to be, according to them, it's going to be a $24 trillion market. So that's insane. That's basically the labor market. And in order to get there, we need massive amounts of data. We need, like internet scale data. So and embody is positioned to be exactly that. And that's, you know, we're basically aiming to become the scale of physical uh, robot data. So, yeah, very,
doesn't really translate when the environment is contact rich. So manipulation problem is a very contact rich problem. So like you, you're basically touching a lot of things and so on and so forth. And that becomes really difficult to do it in simulation and directly zero shot, transferring it to to robots. So people would still resort to kind of imitation learning, kind of techniques, which would require a lot of data. Our electif requires data as well, but not, not as data hungry, as as imitation models. So, long story short, data is a massive need. You know, humanoids are, I think arc investment had a good article on it's going to be, according to them, it's going to be a $24 trillion market. So that's insane. That's basically the labor market. And in order to get there, we need massive amounts of data. We need, like internet scale data. So and embody is positioned to be exactly that. And that's, you know, we're basically aiming to become the scale of physical uh, robot data. So, yeah, very,
doesn't really translate when the environment is contact rich. So manipulation problem is a very contact rich problem. So like you, you're basically touching a lot of things and so on and so forth. And that becomes really difficult to do it in simulation and directly zero shot, transferring it to to robots. So people would still resort to kind of imitation learning, kind of techniques, which would require a lot of data. Our electif requires data as well, but not, not as data hungry, as as imitation models. So, long story short, data is a massive need. You know, humanoids are, I think arc investment had a good article on it's going to be, according to them, it's going to be a $24 trillion market. So that's insane. That's basically the labor market. And in order to get there, we need massive amounts of data. We need, like internet scale data. So and embody is positioned to be exactly that. And that's, you know, we're basically aiming to become the scale of physical uh, robot data. So, yeah, very,
doesn't really translate when the environment is contact rich. So manipulation problem is a very contact rich problem. So like you, you're basically touching a lot of things and so on and so forth. And that becomes really difficult to do it in simulation and directly zero shot, transferring it to to robots. So people would still resort to kind of imitation learning, kind of techniques, which would require a lot of data. Our electif requires data as well, but not, not as data hungry, as as imitation models. So, long story short, data is a massive need. You know, humanoids are, I think arc investment had a good article on it's going to be, according to them, it's going to be a $24 trillion market. So that's insane. That's basically the labor market. And in order to get there, we need massive amounts of data. We need, like internet scale data. So and embody is positioned to be exactly that. And that's, you know, we're basically aiming to become the scale of physical uh, robot data. So, yeah, very,
S Speaker 113:44very interesting overview and and a lot to unpack in the scale is one of our investor, one of our investments also understand the broader thesis here. Let's unpack a little bit on how the pipeline would look like. I went through the website so I can see that just not just collecting data, but you've also created a data pipeline which could feed into robotics training right now. Does it feed into foundation model training, or does it feed into simulations which can then be transferred into foundation model, would be my first question, and then. So
very interesting overview and and a lot to unpack in the scale is one of our investor, one of our investments also understand the broader thesis here. Let's unpack a little bit on how the pipeline would look like. I went through the website so I can see that just not just collecting data, but you've also created a data pipeline which could feed into robotics training right now. Does it feed into foundation model training, or does it feed into simulations which can then be transferred into foundation model, would be my first question, and then. So
very interesting overview and and a lot to unpack in the scale is one of our investor, one of our investments also understand the broader thesis here. Let's unpack a little bit on how the pipeline would look like. I went through the website so I can see that just not just collecting data, but you've also created a data pipeline which could feed into robotics training right now. Does it feed into foundation model training, or does it feed into simulations which can then be transferred into foundation model, would be my first question, and then. So
very interesting overview and and a lot to unpack in the scale is one of our investor, one of our investments also understand the broader thesis here. Let's unpack a little bit on how the pipeline would look like. I went through the website so I can see that just not just collecting data, but you've also created a data pipeline which could feed into robotics training right now. Does it feed into foundation model training, or does it feed into simulations which can then be transferred into foundation model, would be my first question, and then. So
S Speaker 214:18the idea is, like, especially in this initial stage, we don't want to, you know, create a massive platform, obviously, we can, but we want to be focused. Because, you know, funds are limited, you know, we want to make product as fast as possible. So we want to be hyper focused. So the initial, initial focus is to create a pipeline that we can capture data through, through this hardware. You know that data gets uploaded, so I'll give you a day in a life of exercise. So like a robotics company, for instance, figure comes to us say, like, okay, like what we can't scale tele operation and your product looks good, we want to use your product. So figure AI, you know, buys or leases or SaaS is our product, essentially, and this is a hardware software product. So, like, we'll give them the gloves and the goggles, and then we'll charge them monthly on the on the platform, right? So figure is incentivized to go and find customers, obviously, to automate their tasks. So they go find BMW, for instance, to to automate their tasks. They ask BMW workers to wear these wearables and go about their lives. So these are minimally invasive variables. They basically looks like PPE, essentially, they go about their lives. They do their daily, daily tasks, and at the end of the day, the data that they've collected gets uploaded to to our cloud system. So that data is
the idea is, like, especially in this initial stage, we don't want to, you know, create a massive platform, obviously, we can, but we want to be focused. Because, you know, funds are limited, you know, we want to make product as fast as possible. So we want to be hyper focused. So the initial, initial focus is to create a pipeline that we can capture data through, through this hardware. You know that data gets uploaded, so I'll give you a day in a life of exercise. So like a robotics company, for instance, figure comes to us say, like, okay, like what we can't scale tele operation and your product looks good, we want to use your product. So figure AI, you know, buys or leases or SaaS is our product, essentially, and this is a hardware software product. So, like, we'll give them the gloves and the goggles, and then we'll charge them monthly on the on the platform, right? So figure is incentivized to go and find customers, obviously, to automate their tasks. So they go find BMW, for instance, to to automate their tasks. They ask BMW workers to wear these wearables and go about their lives. So these are minimally invasive variables. They basically looks like PPE, essentially, they go about their lives. They do their daily, daily tasks, and at the end of the day, the data that they've collected gets uploaded to to our cloud system. So that data is
the idea is, like, especially in this initial stage, we don't want to, you know, create a massive platform, obviously, we can, but we want to be focused. Because, you know, funds are limited, you know, we want to make product as fast as possible. So we want to be hyper focused. So the initial, initial focus is to create a pipeline that we can capture data through, through this hardware. You know that data gets uploaded, so I'll give you a day in a life of exercise. So like a robotics company, for instance, figure comes to us say, like, okay, like what we can't scale tele operation and your product looks good, we want to use your product. So figure AI, you know, buys or leases or SaaS is our product, essentially, and this is a hardware software product. So, like, we'll give them the gloves and the goggles, and then we'll charge them monthly on the on the platform, right? So figure is incentivized to go and find customers, obviously, to automate their tasks. So they go find BMW, for instance, to to automate their tasks. They ask BMW workers to wear these wearables and go about their lives. So these are minimally invasive variables. They basically looks like PPE, essentially, they go about their lives. They do their daily, daily tasks, and at the end of the day, the data that they've collected gets uploaded to to our cloud system. So that data is
the idea is, like, especially in this initial stage, we don't want to, you know, create a massive platform, obviously, we can, but we want to be focused. Because, you know, funds are limited, you know, we want to make product as fast as possible. So we want to be hyper focused. So the initial, initial focus is to create a pipeline that we can capture data through, through this hardware. You know that data gets uploaded, so I'll give you a day in a life of exercise. So like a robotics company, for instance, figure comes to us say, like, okay, like what we can't scale tele operation and your product looks good, we want to use your product. So figure AI, you know, buys or leases or SaaS is our product, essentially, and this is a hardware software product. So, like, we'll give them the gloves and the goggles, and then we'll charge them monthly on the on the platform, right? So figure is incentivized to go and find customers, obviously, to automate their tasks. So they go find BMW, for instance, to to automate their tasks. They ask BMW workers to wear these wearables and go about their lives. So these are minimally invasive variables. They basically looks like PPE, essentially, they go about their lives. They do their daily, daily tasks, and at the end of the day, the data that they've collected gets uploaded to to our cloud system. So that data is
S Speaker 216:07human space data. So like the finger length and all of that that is captured as human space, that POV is human space. So the first thing that we need to do, we need to convert that, really, to annotate that, so like, what tasks are done, and then we have to convert that into robot space. So that's, that's a retargeting problem. Essentially, after that step, it is as if the robot has has done that task. It's tele operation, but it's, it's offline, tele operation. It's not human in the loop tell operation. So that's one aspect of it. Then it we have a two other pipelines, and in our platform, one is synthetic data generation. So this is very close to, if you're familiar with Dex, mimic Gen from Nvidia. So that's that's essentially takes in one teleoperation instance and replicates it in simulation. So we have, will have and domain randomizes. And what that means is, you know, if the cup, if you're picking up a cup, for instance, if the cup is green in simulation, you change the color of that cup to red, or change the position of that cup, or change other parameters slightly. This way you multiply the data by a lot. So that's that's our second pipeline that goes through that. The third pipeline is direct human video data. So with direct human video data, you you post process it. There's a model called latent action model, law model, and that basically estimates, imagines the human as a robot, and estimates the joint angles of the human robot. Essentially, this is proven to be extremely effective in training as well. So we have like three classes of data. One is tele operation analogous. Second is synthetic data, and third is direct human data. And we amalgamate all of this and pass this to figure. So figure can do whatever they want with this data. They can download it, they can train models with it, and so on and so forth. Now, in the future, we're planning on creating a platform where the training also can be done on embodies platform, but that's for the time being. That's that's not a priority. Kind of makes sense. Makes sense.
human space data. So like the finger length and all of that that is captured as human space, that POV is human space. So the first thing that we need to do, we need to convert that, really, to annotate that, so like, what tasks are done, and then we have to convert that into robot space. So that's, that's a retargeting problem. Essentially, after that step, it is as if the robot has has done that task. It's tele operation, but it's, it's offline, tele operation. It's not human in the loop tell operation. So that's one aspect of it. Then it we have a two other pipelines, and in our platform, one is synthetic data generation. So this is very close to, if you're familiar with Dex, mimic Gen from Nvidia. So that's that's essentially takes in one teleoperation instance and replicates it in simulation. So we have, will have and domain randomizes. And what that means is, you know, if the cup, if you're picking up a cup, for instance, if the cup is green in simulation, you change the color of that cup to red, or change the position of that cup, or change other parameters slightly. This way you multiply the data by a lot. So that's that's our second pipeline that goes through that. The third pipeline is direct human video data. So with direct human video data, you you post process it. There's a model called latent action model, law model, and that basically estimates, imagines the human as a robot, and estimates the joint angles of the human robot. Essentially, this is proven to be extremely effective in training as well. So we have like three classes of data. One is tele operation analogous. Second is synthetic data, and third is direct human data. And we amalgamate all of this and pass this to figure. So figure can do whatever they want with this data. They can download it, they can train models with it, and so on and so forth. Now, in the future, we're planning on creating a platform where the training also can be done on embodies platform, but that's for the time being. That's that's not a priority. Kind of makes sense. Makes sense.
human space data. So like the finger length and all of that that is captured as human space, that POV is human space. So the first thing that we need to do, we need to convert that, really, to annotate that, so like, what tasks are done, and then we have to convert that into robot space. So that's, that's a retargeting problem. Essentially, after that step, it is as if the robot has has done that task. It's tele operation, but it's, it's offline, tele operation. It's not human in the loop tell operation. So that's one aspect of it. Then it we have a two other pipelines, and in our platform, one is synthetic data generation. So this is very close to, if you're familiar with Dex, mimic Gen from Nvidia. So that's that's essentially takes in one teleoperation instance and replicates it in simulation. So we have, will have and domain randomizes. And what that means is, you know, if the cup, if you're picking up a cup, for instance, if the cup is green in simulation, you change the color of that cup to red, or change the position of that cup, or change other parameters slightly. This way you multiply the data by a lot. So that's that's our second pipeline that goes through that. The third pipeline is direct human video data. So with direct human video data, you you post process it. There's a model called latent action model, law model, and that basically estimates, imagines the human as a robot, and estimates the joint angles of the human robot. Essentially, this is proven to be extremely effective in training as well. So we have like three classes of data. One is tele operation analogous. Second is synthetic data, and third is direct human data. And we amalgamate all of this and pass this to figure. So figure can do whatever they want with this data. They can download it, they can train models with it, and so on and so forth. Now, in the future, we're planning on creating a platform where the training also can be done on embodies platform, but that's for the time being. That's that's not a priority. Kind of makes sense. Makes sense.
human space data. So like the finger length and all of that that is captured as human space, that POV is human space. So the first thing that we need to do, we need to convert that, really, to annotate that, so like, what tasks are done, and then we have to convert that into robot space. So that's, that's a retargeting problem. Essentially, after that step, it is as if the robot has has done that task. It's tele operation, but it's, it's offline, tele operation. It's not human in the loop tell operation. So that's one aspect of it. Then it we have a two other pipelines, and in our platform, one is synthetic data generation. So this is very close to, if you're familiar with Dex, mimic Gen from Nvidia. So that's that's essentially takes in one teleoperation instance and replicates it in simulation. So we have, will have and domain randomizes. And what that means is, you know, if the cup, if you're picking up a cup, for instance, if the cup is green in simulation, you change the color of that cup to red, or change the position of that cup, or change other parameters slightly. This way you multiply the data by a lot. So that's that's our second pipeline that goes through that. The third pipeline is direct human video data. So with direct human video data, you you post process it. There's a model called latent action model, law model, and that basically estimates, imagines the human as a robot, and estimates the joint angles of the human robot. Essentially, this is proven to be extremely effective in training as well. So we have like three classes of data. One is tele operation analogous. Second is synthetic data, and third is direct human data. And we amalgamate all of this and pass this to figure. So figure can do whatever they want with this data. They can download it, they can train models with it, and so on and so forth. Now, in the future, we're planning on creating a platform where the training also can be done on embodies platform, but that's for the time being. That's that's not a priority. Kind of makes sense. Makes sense.
S Speaker 118:41So one follow up question on what you mentioned, and like piggybacking on the figure example itself, you mentioned that you're sort of trying to make it as a hardware as a subscription model, where you supply the hardware to figure, Figure then gives it to their end customers. BMW, for this example, BMW workers use the hardware collect data. It goes to your cloud, and then figure can sort of take the data from your cloud and use it to train their own models. Who sort of owns this data in that in that fraction, do you have any kind of ownership or all over the data. Can you use that to sort of generate synthetic data for different customers? How is this going to work?
So one follow up question on what you mentioned, and like piggybacking on the figure example itself, you mentioned that you're sort of trying to make it as a hardware as a subscription model, where you supply the hardware to figure, Figure then gives it to their end customers. BMW, for this example, BMW workers use the hardware collect data. It goes to your cloud, and then figure can sort of take the data from your cloud and use it to train their own models. Who sort of owns this data in that in that fraction, do you have any kind of ownership or all over the data. Can you use that to sort of generate synthetic data for different customers? How is this going to work?
So one follow up question on what you mentioned, and like piggybacking on the figure example itself, you mentioned that you're sort of trying to make it as a hardware as a subscription model, where you supply the hardware to figure, Figure then gives it to their end customers. BMW, for this example, BMW workers use the hardware collect data. It goes to your cloud, and then figure can sort of take the data from your cloud and use it to train their own models. Who sort of owns this data in that in that fraction, do you have any kind of ownership or all over the data. Can you use that to sort of generate synthetic data for different customers? How is this going to work?
So one follow up question on what you mentioned, and like piggybacking on the figure example itself, you mentioned that you're sort of trying to make it as a hardware as a subscription model, where you supply the hardware to figure, Figure then gives it to their end customers. BMW, for this example, BMW workers use the hardware collect data. It goes to your cloud, and then figure can sort of take the data from your cloud and use it to train their own models. Who sort of owns this data in that in that fraction, do you have any kind of ownership or all over the data. Can you use that to sort of generate synthetic data for different customers? How is this going to work?
S Speaker 219:27Yeah. So our from our customer discovery data ownership is extremely tricky, yeah. So there's two types of data, right? So, like, especially if we're talking about like foundation models, there are two types of data. One type of data is, I call the pre training data. So this is the data. This is like menial tasks. So this could be constant data set of variety of tasks out there, and you use this to pre train your model. And what this does, it basically shows the brain, tells the brain about physics of the world, so how things move, and what is up, what is down. It's all sort of like abstract things, right? And this, this is usually a constant data set. You don't need to. I to Yeah. You don't need to expand on it. You just have a constant data set. You can, you know, data as a service to
Yeah. So our from our customer discovery data ownership is extremely tricky, yeah. So there's two types of data, right? So, like, especially if we're talking about like foundation models, there are two types of data. One type of data is, I call the pre training data. So this is the data. This is like menial tasks. So this could be constant data set of variety of tasks out there, and you use this to pre train your model. And what this does, it basically shows the brain, tells the brain about physics of the world, so how things move, and what is up, what is down. It's all sort of like abstract things, right? And this, this is usually a constant data set. You don't need to. I to Yeah. You don't need to expand on it. You just have a constant data set. You can, you know, data as a service to
Yeah. So our from our customer discovery data ownership is extremely tricky, yeah. So there's two types of data, right? So, like, especially if we're talking about like foundation models, there are two types of data. One type of data is, I call the pre training data. So this is the data. This is like menial tasks. So this could be constant data set of variety of tasks out there, and you use this to pre train your model. And what this does, it basically shows the brain, tells the brain about physics of the world, so how things move, and what is up, what is down. It's all sort of like abstract things, right? And this, this is usually a constant data set. You don't need to. I to Yeah. You don't need to expand on it. You just have a constant data set. You can, you know, data as a service to
Yeah. So our from our customer discovery data ownership is extremely tricky, yeah. So there's two types of data, right? So, like, especially if we're talking about like foundation models, there are two types of data. One type of data is, I call the pre training data. So this is the data. This is like menial tasks. So this could be constant data set of variety of tasks out there, and you use this to pre train your model. And what this does, it basically shows the brain, tells the brain about physics of the world, so how things move, and what is up, what is down. It's all sort of like abstract things, right? And this, this is usually a constant data set. You don't need to. I to Yeah. You don't need to expand on it. You just have a constant data set. You can, you know, data as a service to
S Speaker 120:30people, yeah, comparing to the LLM words this, this could be something like scraping the entire internet and just
people, yeah, comparing to the LLM words this, this could be something like scraping the entire internet and just
people, yeah, comparing to the LLM words this, this could be something like scraping the entire internet and just
people, yeah, comparing to the LLM words this, this could be something like scraping the entire internet and just
22:12something tricky, right? I
something tricky, right? I
something tricky, right? I
something tricky, right? I
S Speaker 122:13understand, and that's very fair, data is the entire core of this problem. So for sure, and the BMW handle makes sense as well. What I'm trying to understand how this here isn't something that I personally cannot sort of get a grasp because I haven't been in that industry. But taking a parallel from the LLM world data in general, or let's say, let's say, let's just take the example of figure. Again, what I know is figure is trying to create an entire RL engine which would let them, and it's not RL, HF, it's a pure reinforcement learning engine which would let them sort of specialize on one task and then generalize across different tasks, which could be similar in nature, and also sort of have one robot. Do it transfer the entire knowledge to the entire fleet? How do you see one, I would say is, how do you see data labeling? Or how exactly is data labeling in this world? I can sort of get a natural understanding of that in the LLM world, but for physical AI, there is a lot of different parameters to be logged. You mentioned initially, force, dexterity of the material, torque, angles, joint angle. So how, how is data labeling in this space, generally is? And do you see this as a problem which is continual in nature, like do you see at one point figure sort of generalizes from training across a few 1000 tasks to sort of expanding into a completely general humanoid robot, and they do not need embody, after that, something like that.
understand, and that's very fair, data is the entire core of this problem. So for sure, and the BMW handle makes sense as well. What I'm trying to understand how this here isn't something that I personally cannot sort of get a grasp because I haven't been in that industry. But taking a parallel from the LLM world data in general, or let's say, let's say, let's just take the example of figure. Again, what I know is figure is trying to create an entire RL engine which would let them, and it's not RL, HF, it's a pure reinforcement learning engine which would let them sort of specialize on one task and then generalize across different tasks, which could be similar in nature, and also sort of have one robot. Do it transfer the entire knowledge to the entire fleet? How do you see one, I would say is, how do you see data labeling? Or how exactly is data labeling in this world? I can sort of get a natural understanding of that in the LLM world, but for physical AI, there is a lot of different parameters to be logged. You mentioned initially, force, dexterity of the material, torque, angles, joint angle. So how, how is data labeling in this space, generally is? And do you see this as a problem which is continual in nature, like do you see at one point figure sort of generalizes from training across a few 1000 tasks to sort of expanding into a completely general humanoid robot, and they do not need embody, after that, something like that.
understand, and that's very fair, data is the entire core of this problem. So for sure, and the BMW handle makes sense as well. What I'm trying to understand how this here isn't something that I personally cannot sort of get a grasp because I haven't been in that industry. But taking a parallel from the LLM world data in general, or let's say, let's say, let's just take the example of figure. Again, what I know is figure is trying to create an entire RL engine which would let them, and it's not RL, HF, it's a pure reinforcement learning engine which would let them sort of specialize on one task and then generalize across different tasks, which could be similar in nature, and also sort of have one robot. Do it transfer the entire knowledge to the entire fleet? How do you see one, I would say is, how do you see data labeling? Or how exactly is data labeling in this world? I can sort of get a natural understanding of that in the LLM world, but for physical AI, there is a lot of different parameters to be logged. You mentioned initially, force, dexterity of the material, torque, angles, joint angle. So how, how is data labeling in this space, generally is? And do you see this as a problem which is continual in nature, like do you see at one point figure sort of generalizes from training across a few 1000 tasks to sort of expanding into a completely general humanoid robot, and they do not need embody, after that, something like that.
understand, and that's very fair, data is the entire core of this problem. So for sure, and the BMW handle makes sense as well. What I'm trying to understand how this here isn't something that I personally cannot sort of get a grasp because I haven't been in that industry. But taking a parallel from the LLM world data in general, or let's say, let's say, let's just take the example of figure. Again, what I know is figure is trying to create an entire RL engine which would let them, and it's not RL, HF, it's a pure reinforcement learning engine which would let them sort of specialize on one task and then generalize across different tasks, which could be similar in nature, and also sort of have one robot. Do it transfer the entire knowledge to the entire fleet? How do you see one, I would say is, how do you see data labeling? Or how exactly is data labeling in this world? I can sort of get a natural understanding of that in the LLM world, but for physical AI, there is a lot of different parameters to be logged. You mentioned initially, force, dexterity of the material, torque, angles, joint angle. So how, how is data labeling in this space, generally is? And do you see this as a problem which is continual in nature, like do you see at one point figure sort of generalizes from training across a few 1000 tasks to sort of expanding into a completely general humanoid robot, and they do not need embody, after that, something like that.
S Speaker 223:51Yeah, yeah. That's, that's a fair question. So there's a few kind of, like, hidden questions there, so I'm gonna try and, like, respond to them one by one. So first, with regards to, you know, the amount of data needed, you know, like, and the variety of tasks. So there, there's a nice kind of, like, data set of all the jobs out there. It's called whole net data sets. I don't know if you're familiar with that, but in that O net data set, it basically says what kind of tasks are done in a job. So imagine this is like, as comprehensive as it gets to a labor directory of like, all the jobs out there with all the tasks within those jobs. So if you analyze that data, they're around, I want to say more than 5 million tasks, sub tasks, unique sub tasks out there, right? So these vary in and by that, I mean, for instance, if you're kind of like operating a forklift, right? So what are the sub tasks? There you you get on it, get on the seat, you take the key, put it in the hole, turn it, use pedal. So those are all sub tasks, right? And, you know, they're around 5 million unique sub tasks out there with varying complexities. So like, some of them are easy, like picking up a cup, some of them are difficult, very difficult, like open heart surgery, like shootering, like an artery, yeah, and these, these vary, right? So, like, but they're unique, so that they're not necessarily transferable. They're unique. So like, sure, like putting in a key in a, I don't know, BMW
Yeah, yeah. That's, that's a fair question. So there's a few kind of, like, hidden questions there, so I'm gonna try and, like, respond to them one by one. So first, with regards to, you know, the amount of data needed, you know, like, and the variety of tasks. So there, there's a nice kind of, like, data set of all the jobs out there. It's called whole net data sets. I don't know if you're familiar with that, but in that O net data set, it basically says what kind of tasks are done in a job. So imagine this is like, as comprehensive as it gets to a labor directory of like, all the jobs out there with all the tasks within those jobs. So if you analyze that data, they're around, I want to say more than 5 million tasks, sub tasks, unique sub tasks out there, right? So these vary in and by that, I mean, for instance, if you're kind of like operating a forklift, right? So what are the sub tasks? There you you get on it, get on the seat, you take the key, put it in the hole, turn it, use pedal. So those are all sub tasks, right? And, you know, they're around 5 million unique sub tasks out there with varying complexities. So like, some of them are easy, like picking up a cup, some of them are difficult, very difficult, like open heart surgery, like shootering, like an artery, yeah, and these, these vary, right? So, like, but they're unique, so that they're not necessarily transferable. They're unique. So like, sure, like putting in a key in a, I don't know, BMW
Yeah, yeah. That's, that's a fair question. So there's a few kind of, like, hidden questions there, so I'm gonna try and, like, respond to them one by one. So first, with regards to, you know, the amount of data needed, you know, like, and the variety of tasks. So there, there's a nice kind of, like, data set of all the jobs out there. It's called whole net data sets. I don't know if you're familiar with that, but in that O net data set, it basically says what kind of tasks are done in a job. So imagine this is like, as comprehensive as it gets to a labor directory of like, all the jobs out there with all the tasks within those jobs. So if you analyze that data, they're around, I want to say more than 5 million tasks, sub tasks, unique sub tasks out there, right? So these vary in and by that, I mean, for instance, if you're kind of like operating a forklift, right? So what are the sub tasks? There you you get on it, get on the seat, you take the key, put it in the hole, turn it, use pedal. So those are all sub tasks, right? And, you know, they're around 5 million unique sub tasks out there with varying complexities. So like, some of them are easy, like picking up a cup, some of them are difficult, very difficult, like open heart surgery, like shootering, like an artery, yeah, and these, these vary, right? So, like, but they're unique, so that they're not necessarily transferable. They're unique. So like, sure, like putting in a key in a, I don't know, BMW
Yeah, yeah. That's, that's a fair question. So there's a few kind of, like, hidden questions there, so I'm gonna try and, like, respond to them one by one. So first, with regards to, you know, the amount of data needed, you know, like, and the variety of tasks. So there, there's a nice kind of, like, data set of all the jobs out there. It's called whole net data sets. I don't know if you're familiar with that, but in that O net data set, it basically says what kind of tasks are done in a job. So imagine this is like, as comprehensive as it gets to a labor directory of like, all the jobs out there with all the tasks within those jobs. So if you analyze that data, they're around, I want to say more than 5 million tasks, sub tasks, unique sub tasks out there, right? So these vary in and by that, I mean, for instance, if you're kind of like operating a forklift, right? So what are the sub tasks? There you you get on it, get on the seat, you take the key, put it in the hole, turn it, use pedal. So those are all sub tasks, right? And, you know, they're around 5 million unique sub tasks out there with varying complexities. So like, some of them are easy, like picking up a cup, some of them are difficult, very difficult, like open heart surgery, like shootering, like an artery, yeah, and these, these vary, right? So, like, but they're unique, so that they're not necessarily transferable. They're unique. So like, sure, like putting in a key in a, I don't know, BMW
25:53ignition versus putting a key in
ignition versus putting a key in
ignition versus putting a key in
ignition versus putting a key in
S Speaker 225:57at your home is very similar, so that that can translate, but putting a key doesn't translate to picking up a cup, right? So those are the unique tasks out there. So we calculated this, and we calculated how much effort does it take to, you know, tele operate this, to capture this data, and obviously for for more complex tasks, you need to collect more data. For less complex tasks, you you don't need to collect as much data.
at your home is very similar, so that that can translate, but putting a key doesn't translate to picking up a cup, right? So those are the unique tasks out there. So we calculated this, and we calculated how much effort does it take to, you know, tele operate this, to capture this data, and obviously for for more complex tasks, you need to collect more data. For less complex tasks, you you don't need to collect as much data.
at your home is very similar, so that that can translate, but putting a key doesn't translate to picking up a cup, right? So those are the unique tasks out there. So we calculated this, and we calculated how much effort does it take to, you know, tele operate this, to capture this data, and obviously for for more complex tasks, you need to collect more data. For less complex tasks, you you don't need to collect as much data.
at your home is very similar, so that that can translate, but putting a key doesn't translate to picking up a cup, right? So those are the unique tasks out there. So we calculated this, and we calculated how much effort does it take to, you know, tele operate this, to capture this data, and obviously for for more complex tasks, you need to collect more data. For less complex tasks, you you don't need to collect as much data.
26:33So we calculated this, and
So we calculated this, and
So we calculated this, and
So we calculated this, and
S Speaker 226:38we believe that, based on the kind of like parallels between the automotive industry. Actually, this kind of like scaling humanoids will take around 15 to 17 years, right? So, like, that's how long it's going to take just parallels to automotive industry to get to, you know, mass scale automation, once the first task is automated fully, so that there's, there's a quite a bit of runway there to kind of collect data there, and there's a lot of data that will be collected through tele operation. But we are offering a better way to to collect that. We have all the calculations there. I can direct you towards that as well, if you're curious, in more detailed calculation, but that's kind of like the gist of it. There's a long runway on how long this is gonna take, around like 1570, 20 years, and also the number of tasks out there are immense,
we believe that, based on the kind of like parallels between the automotive industry. Actually, this kind of like scaling humanoids will take around 15 to 17 years, right? So, like, that's how long it's going to take just parallels to automotive industry to get to, you know, mass scale automation, once the first task is automated fully, so that there's, there's a quite a bit of runway there to kind of collect data there, and there's a lot of data that will be collected through tele operation. But we are offering a better way to to collect that. We have all the calculations there. I can direct you towards that as well, if you're curious, in more detailed calculation, but that's kind of like the gist of it. There's a long runway on how long this is gonna take, around like 1570, 20 years, and also the number of tasks out there are immense,
we believe that, based on the kind of like parallels between the automotive industry. Actually, this kind of like scaling humanoids will take around 15 to 17 years, right? So, like, that's how long it's going to take just parallels to automotive industry to get to, you know, mass scale automation, once the first task is automated fully, so that there's, there's a quite a bit of runway there to kind of collect data there, and there's a lot of data that will be collected through tele operation. But we are offering a better way to to collect that. We have all the calculations there. I can direct you towards that as well, if you're curious, in more detailed calculation, but that's kind of like the gist of it. There's a long runway on how long this is gonna take, around like 1570, 20 years, and also the number of tasks out there are immense,
we believe that, based on the kind of like parallels between the automotive industry. Actually, this kind of like scaling humanoids will take around 15 to 17 years, right? So, like, that's how long it's going to take just parallels to automotive industry to get to, you know, mass scale automation, once the first task is automated fully, so that there's, there's a quite a bit of runway there to kind of collect data there, and there's a lot of data that will be collected through tele operation. But we are offering a better way to to collect that. We have all the calculations there. I can direct you towards that as well, if you're curious, in more detailed calculation, but that's kind of like the gist of it. There's a long runway on how long this is gonna take, around like 1570, 20 years, and also the number of tasks out there are immense,
S Speaker 127:46absolutely, I've got that, get that so make sense. Where are you in terms of, and I would love to read through some of the materials you mentioned. Would love to educate myself better on this for sure. But where are you on the entire process of product development and business traction today? And then we can, we can discuss fundraising as well. Yeah,
absolutely, I've got that, get that so make sense. Where are you in terms of, and I would love to read through some of the materials you mentioned. Would love to educate myself better on this for sure. But where are you on the entire process of product development and business traction today? And then we can, we can discuss fundraising as well. Yeah,
absolutely, I've got that, get that so make sense. Where are you in terms of, and I would love to read through some of the materials you mentioned. Would love to educate myself better on this for sure. But where are you on the entire process of product development and business traction today? And then we can, we can discuss fundraising as well. Yeah,
absolutely, I've got that, get that so make sense. Where are you in terms of, and I would love to read through some of the materials you mentioned. Would love to educate myself better on this for sure. But where are you on the entire process of product development and business traction today? And then we can, we can discuss fundraising as well. Yeah,
S Speaker 228:08so with regards to the product, so our, one of our first goals was to basically de risk that retargeting aspect. So I talked about the retargeting aspect. That's, I think that's the riskiest aspect that this product has. And in fact, we were able to do that in the first couple of months. So, you know, between January and March, we were working on a prototype showing that we can, in fact, translate direct capture of humans to robots in an offline fashion. So that's a possibility we have. We've demonstrated this again. We're working on a on a nice, polished video showing how to go from, you know, human demonstrations to robot. So that's that's coming up soon regarding customer traction. So we've talked to a lot of the robotics companies, up and coming robotics companies in North America. We've talked to kind of like a company called Foundation, Persona Jane Pratt, new company. I've talked to 1x few other companies out there as well. There's a lot of interests, but they want to see product, right? So, like they want to see a data set, they want to see a product. We're not there yet, obviously. So we're getting there. We need funds to get to that place. So we are working on Lois from these companies to show that they have interest in us in parallel to that, or we're working on partnerships. So obviously it'll be easier for us to partner with with few other companies to take on some of the technical challenges, including that kind of like synthetic data generation, right? And we are working with a company called glacial. Glacial is a new one of the new simulation companies out there. They're doing great work on basically converting real images and videos into simulated scenes, right automatically. And that's, that's a that's a part of our pipeline, that we we have a kind of partnership loi signed with them. Currently we're working on that. We're also working with a consortium, leading a consortium, essentially of open source physical data, robotic data initiatives. So our part of that as well. We really believe in kind of open sourcing as much as possible, so that people can, you know, we can build this community together, right? So build this automation together. So we're kind of like leading on that, leading that as well. And that's what we got with regard subtraction so far, and product
so with regards to the product, so our, one of our first goals was to basically de risk that retargeting aspect. So I talked about the retargeting aspect. That's, I think that's the riskiest aspect that this product has. And in fact, we were able to do that in the first couple of months. So, you know, between January and March, we were working on a prototype showing that we can, in fact, translate direct capture of humans to robots in an offline fashion. So that's a possibility we have. We've demonstrated this again. We're working on a on a nice, polished video showing how to go from, you know, human demonstrations to robot. So that's that's coming up soon regarding customer traction. So we've talked to a lot of the robotics companies, up and coming robotics companies in North America. We've talked to kind of like a company called Foundation, Persona Jane Pratt, new company. I've talked to 1x few other companies out there as well. There's a lot of interests, but they want to see product, right? So, like they want to see a data set, they want to see a product. We're not there yet, obviously. So we're getting there. We need funds to get to that place. So we are working on Lois from these companies to show that they have interest in us in parallel to that, or we're working on partnerships. So obviously it'll be easier for us to partner with with few other companies to take on some of the technical challenges, including that kind of like synthetic data generation, right? And we are working with a company called glacial. Glacial is a new one of the new simulation companies out there. They're doing great work on basically converting real images and videos into simulated scenes, right automatically. And that's, that's a that's a part of our pipeline, that we we have a kind of partnership loi signed with them. Currently we're working on that. We're also working with a consortium, leading a consortium, essentially of open source physical data, robotic data initiatives. So our part of that as well. We really believe in kind of open sourcing as much as possible, so that people can, you know, we can build this community together, right? So build this automation together. So we're kind of like leading on that, leading that as well. And that's what we got with regard subtraction so far, and product
so with regards to the product, so our, one of our first goals was to basically de risk that retargeting aspect. So I talked about the retargeting aspect. That's, I think that's the riskiest aspect that this product has. And in fact, we were able to do that in the first couple of months. So, you know, between January and March, we were working on a prototype showing that we can, in fact, translate direct capture of humans to robots in an offline fashion. So that's a possibility we have. We've demonstrated this again. We're working on a on a nice, polished video showing how to go from, you know, human demonstrations to robot. So that's that's coming up soon regarding customer traction. So we've talked to a lot of the robotics companies, up and coming robotics companies in North America. We've talked to kind of like a company called Foundation, Persona Jane Pratt, new company. I've talked to 1x few other companies out there as well. There's a lot of interests, but they want to see product, right? So, like they want to see a data set, they want to see a product. We're not there yet, obviously. So we're getting there. We need funds to get to that place. So we are working on Lois from these companies to show that they have interest in us in parallel to that, or we're working on partnerships. So obviously it'll be easier for us to partner with with few other companies to take on some of the technical challenges, including that kind of like synthetic data generation, right? And we are working with a company called glacial. Glacial is a new one of the new simulation companies out there. They're doing great work on basically converting real images and videos into simulated scenes, right automatically. And that's, that's a that's a part of our pipeline, that we we have a kind of partnership loi signed with them. Currently we're working on that. We're also working with a consortium, leading a consortium, essentially of open source physical data, robotic data initiatives. So our part of that as well. We really believe in kind of open sourcing as much as possible, so that people can, you know, we can build this community together, right? So build this automation together. So we're kind of like leading on that, leading that as well. And that's what we got with regard subtraction so far, and product
so with regards to the product, so our, one of our first goals was to basically de risk that retargeting aspect. So I talked about the retargeting aspect. That's, I think that's the riskiest aspect that this product has. And in fact, we were able to do that in the first couple of months. So, you know, between January and March, we were working on a prototype showing that we can, in fact, translate direct capture of humans to robots in an offline fashion. So that's a possibility we have. We've demonstrated this again. We're working on a on a nice, polished video showing how to go from, you know, human demonstrations to robot. So that's that's coming up soon regarding customer traction. So we've talked to a lot of the robotics companies, up and coming robotics companies in North America. We've talked to kind of like a company called Foundation, Persona Jane Pratt, new company. I've talked to 1x few other companies out there as well. There's a lot of interests, but they want to see product, right? So, like they want to see a data set, they want to see a product. We're not there yet, obviously. So we're getting there. We need funds to get to that place. So we are working on Lois from these companies to show that they have interest in us in parallel to that, or we're working on partnerships. So obviously it'll be easier for us to partner with with few other companies to take on some of the technical challenges, including that kind of like synthetic data generation, right? And we are working with a company called glacial. Glacial is a new one of the new simulation companies out there. They're doing great work on basically converting real images and videos into simulated scenes, right automatically. And that's, that's a that's a part of our pipeline, that we we have a kind of partnership loi signed with them. Currently we're working on that. We're also working with a consortium, leading a consortium, essentially of open source physical data, robotic data initiatives. So our part of that as well. We really believe in kind of open sourcing as much as possible, so that people can, you know, we can build this community together, right? So build this automation together. So we're kind of like leading on that, leading that as well. And that's what we got with regard subtraction so far, and product
S Speaker 131:18interesting Hatha, so makes sense and and you, I know you were raising a 10 million seed round. What's the progress on that? And what's the timeline you're looking at? Right?
interesting Hatha, so makes sense and and you, I know you were raising a 10 million seed round. What's the progress on that? And what's the timeline you're looking at? Right?
interesting Hatha, so makes sense and and you, I know you were raising a 10 million seed round. What's the progress on that? And what's the timeline you're looking at? Right?
interesting Hatha, so makes sense and and you, I know you were raising a 10 million seed round. What's the progress on that? And what's the timeline you're looking at? Right?
S Speaker 233:21sounds good. So what kind of, what kind of data, what kind of information do you need? Do
sounds good. So what kind of, what kind of data, what kind of information do you need? Do
sounds good. So what kind of, what kind of data, what kind of information do you need? Do
sounds good. So what kind of, what kind of data, what kind of information do you need? Do
S Speaker 133:25you have a general deck that you have today? Like, from our conversation, I have a few follow up questions that I can send your way, but would like to sort of do a little bit more digging before I come back and, like, take request more of your time. Absolutely,
you have a general deck that you have today? Like, from our conversation, I have a few follow up questions that I can send your way, but would like to sort of do a little bit more digging before I come back and, like, take request more of your time. Absolutely,
you have a general deck that you have today? Like, from our conversation, I have a few follow up questions that I can send your way, but would like to sort of do a little bit more digging before I come back and, like, take request more of your time. Absolutely,
you have a general deck that you have today? Like, from our conversation, I have a few follow up questions that I can send your way, but would like to sort of do a little bit more digging before I come back and, like, take request more of your time. Absolutely,
S Speaker 134:48Absolutely. Thanks, Adam, enjoyed the conversation today for sure, and I appreciate your time. Yeah, thank you, thank you. I'll see you again. Bye. See you, bye.
Absolutely. Thanks, Adam, enjoyed the conversation today for sure, and I appreciate your time. Yeah, thank you, thank you. I'll see you again. Bye. See you, bye.
Absolutely. Thanks, Adam, enjoyed the conversation today for sure, and I appreciate your time. Yeah, thank you, thank you. I'll see you again. Bye. See you, bye.
Absolutely. Thanks, Adam, enjoyed the conversation today for sure, and I appreciate your time. Yeah, thank you, thank you. I'll see you again. Bye. See you, bye.