Meeting: Luma AI
Wed, Mar 12
2:39 PM
31 min
Priyesh P
Luma AI's Models and Use Cases
0:00
Team Background and
URL: https://otter.ai/u/TlyscNAvsHsvCa4BlhEAN9Wtve4
Downloaded: 2025-12-22T12:54:52.444534
Method: text_extraction
============================================================

Speaker 1
General models that are able to produce a modality are also very good at understanding that value. So the another word here would be, if you're familiar with natural language processing, before trying to understand what a user is saying, was a big problem in argument that the world had to actually solve. Once we got a labs, that problem went away because, you know, lens are good at generating elements, also good and very good at understanding. But actually so our models show similar performance, attractive understanding. So immediate use case that we are going after is very large scale video cameras that are 1000s and hundreds of 1000s of cameras deployed for the same use case. How do you understand what's happening in that cell does actually really well. And finally, real time video, AI chat. So what we mean by that is being able to freestyle you type in and you get millions of words back and so forth. The future already is moving towards voice. The are the three areas,
General models that are able to produce a modality are also very good at understanding that value. So the another word here would be, if you're familiar with natural language processing, before trying to understand what a user is saying, was a big problem in argument that the world had to actually solve. Once we got a labs, that problem went away because, you know, lens are good at generating elements, also good and very good at understanding. But actually so our models show similar performance, attractive understanding. So immediate use case that we are going after is very large scale video cameras that are 1000s and hundreds of 1000s of cameras deployed for the same use case. How do you understand what's happening in that cell does actually really well. And finally, real time video, AI chat. So what we mean by that is being able to freestyle you type in and you get millions of words back and so forth. The future already is moving towards voice. The are the three areas,
General models that are able to produce a modality are also very good at understanding that value. So the another word here would be, if you're familiar with natural language processing, before trying to understand what a user is saying, was a big problem in argument that the world had to actually solve. Once we got a labs, that problem went away because, you know, lens are good at generating elements, also good and very good at understanding. But actually so our models show similar performance, attractive understanding. So immediate use case that we are going after is very large scale video cameras that are 1000s and hundreds of 1000s of cameras deployed for the same use case. How do you understand what's happening in that cell does actually really well. And finally, real time video, AI chat. So what we mean by that is being able to freestyle you type in and you get millions of words back and so forth. The future already is moving towards voice. The are the three areas,
General models that are able to produce a modality are also very good at understanding that value. So the another word here would be, if you're familiar with natural language processing, before trying to understand what a user is saying, was a big problem in argument that the world had to actually solve. Once we got a labs, that problem went away because, you know, lens are good at generating elements, also good and very good at understanding. But actually so our models show similar performance, attractive understanding. So immediate use case that we are going after is very large scale video cameras that are 1000s and hundreds of 1000s of cameras deployed for the same use case. How do you understand what's happening in that cell does actually really well. And finally, real time video, AI chat. So what we mean by that is being able to freestyle you type in and you get millions of words back and so forth. The future already is moving towards voice. The are the three areas,
well understanding, so I'll show you
well understanding, so I'll show you
well understanding, so I'll show you
well understanding, so I'll show you
Speaker 1
sort of how we see that. I think method of research and how understanding production actually
sort of how we see that. I think method of research and how understanding production actually
sort of how we see that. I think method of research and how understanding production actually
sort of how we see that. I think method of research and how understanding production actually
is very talented.
Speaker 1
So basically, the company is currently looking at the barrier, and a lot of us are our researchers, and most of our experience has been in building pieces of business. So a little bit of the work that our teams have done before the three pieces that takes the level of information and produces understandable instruments, the general model and then the COVID testing. These two data box, people on our team have built both of those parts, especially for video direction. So we had the bottleneck that actually made generate models possible. That made generate models possible. This is the work from our Josh Dylan, on our team, wasn't in line for 15 years. All the images you see today are possible because of this paper DM because that's work from RC 90 song he was before he joined us in 2023 is actually in the middle of 23 and this was his work from on this video, audio plus language, joint generation problem, then Google and Google's video.
So basically, the company is currently looking at the barrier, and a lot of us are our researchers, and most of our experience has been in building pieces of business. So a little bit of the work that our teams have done before the three pieces that takes the level of information and produces understandable instruments, the general model and then the COVID testing. These two data box, people on our team have built both of those parts, especially for video direction. So we had the bottleneck that actually made generate models possible. That made generate models possible. This is the work from our Josh Dylan, on our team, wasn't in line for 15 years. All the images you see today are possible because of this paper DM because that's work from RC 90 song he was before he joined us in 2023 is actually in the middle of 23 and this was his work from on this video, audio plus language, joint generation problem, then Google and Google's video.
So basically, the company is currently looking at the barrier, and a lot of us are our researchers, and most of our experience has been in building pieces of business. So a little bit of the work that our teams have done before the three pieces that takes the level of information and produces understandable instruments, the general model and then the COVID testing. These two data box, people on our team have built both of those parts, especially for video direction. So we had the bottleneck that actually made generate models possible. That made generate models possible. This is the work from our Josh Dylan, on our team, wasn't in line for 15 years. All the images you see today are possible because of this paper DM because that's work from RC 90 song he was before he joined us in 2023 is actually in the middle of 23 and this was his work from on this video, audio plus language, joint generation problem, then Google and Google's video.
So basically, the company is currently looking at the barrier, and a lot of us are our researchers, and most of our experience has been in building pieces of business. So a little bit of the work that our teams have done before the three pieces that takes the level of information and produces understandable instruments, the general model and then the COVID testing. These two data box, people on our team have built both of those parts, especially for video direction. So we had the bottleneck that actually made generate models possible. That made generate models possible. This is the work from our Josh Dylan, on our team, wasn't in line for 15 years. All the images you see today are possible because of this paper DM because that's work from RC 90 song he was before he joined us in 2023 is actually in the middle of 23 and this was his work from on this video, audio plus language, joint generation problem, then Google and Google's video.
So we have four people
So we have four people
So we have four people
So we have four people
Speaker 1
especially very science research. So that's sort of the position of the team research. We believe that to unlock the market, we need to build the use case you can optimize both at the same time. And that's actually a big part of it here is the same time, and that's actually a big part of, right, of course,
especially very science research. So that's sort of the position of the team research. We believe that to unlock the market, we need to build the use case you can optimize both at the same time. And that's actually a big part of it here is the same time, and that's actually a big part of, right, of course,
especially very science research. So that's sort of the position of the team research. We believe that to unlock the market, we need to build the use case you can optimize both at the same time. And that's actually a big part of it here is the same time, and that's actually a big part of, right, of course,
especially very science research. So that's sort of the position of the team research. We believe that to unlock the market, we need to build the use case you can optimize both at the same time. And that's actually a big part of it here is the same time, and that's actually a big part of, right, of course,
the research. So
Speaker 1
this discussion my background. I work on Apple vision Pro for topics, specifically on the internet of humans already. So vision Pro has the most expressive and most natural work on the Quantum
this discussion my background. I work on Apple vision Pro for topics, specifically on the internet of humans already. So vision Pro has the most expressive and most natural work on the Quantum
this discussion my background. I work on Apple vision Pro for topics, specifically on the internet of humans already. So vision Pro has the most expressive and most natural work on the Quantum
this discussion my background. I work on Apple vision Pro for topics, specifically on the internet of humans already. So vision Pro has the most expressive and most natural work on the Quantum
Speaker 1
Mechanics before that. Josh,
Mechanics before that. Josh,
Mechanics before that. Josh,
Mechanics before that. Josh,
train, if you did
Speaker 1
Speaker 1
our last round was led by recent, last price round, at least, was led by in recent Horowitz and video. Then we raised another round after that that was led by AWS. So series A was
our last round was led by recent, last price round, at least, was led by in recent Horowitz and video. Then we raised another round after that that was led by AWS. So series A was
our last round was led by recent, last price round, at least, was led by in recent Horowitz and video. Then we raised another round after that that was led by AWS. So series A was
our last round was led by recent, last price round, at least, was led by in recent Horowitz and video. Then we raised another round after that that was led by AWS. So series A was
led by amplify and matrix and
led by amplify and matrix and
led by amplify and matrix and
led by amplify and matrix and
Speaker 1
other angels. I Some people, but I didn't write.
other angels. I Some people, but I didn't write.
other angels. I Some people, but I didn't write.
other angels. I Some people, but I didn't write.
And recently, from their side, it's more in recent
And recently, from their side, it's more in recent
And recently, from their side, it's more in recent
And recently, from their side, it's more in recent
Speaker 1
Anjali manhau, if you're familiar with him, they are partners. Interviews. Is very interesting. I'll tell you in a second about that. And I think for this audience, we can't write that down. But I was mentioning to Albert, if you're familiar with inside the AI, if effort, the ten billion enterprise that they're creating in the next couple of months, they are leading on it.
Anjali manhau, if you're familiar with him, they are partners. Interviews. Is very interesting. I'll tell you in a second about that. And I think for this audience, we can't write that down. But I was mentioning to Albert, if you're familiar with inside the AI, if effort, the ten billion enterprise that they're creating in the next couple of months, they are leading on it.
Anjali manhau, if you're familiar with him, they are partners. Interviews. Is very interesting. I'll tell you in a second about that. And I think for this audience, we can't write that down. But I was mentioning to Albert, if you're familiar with inside the AI, if effort, the ten billion enterprise that they're creating in the next couple of months, they are leading on it.
Anjali manhau, if you're familiar with him, they are partners. Interviews. Is very interesting. I'll tell you in a second about that. And I think for this audience, we can't write that down. But I was mentioning to Albert, if you're familiar with inside the AI, if effort, the ten billion enterprise that they're creating in the next couple of months, they are leading on it.
Speaker 1
know, it's a distribution as well as infrastructure partnership to do a lot of things, sorry, discussion on valuation so
know, it's a distribution as well as infrastructure partnership to do a lot of things, sorry, discussion on valuation so
know, it's a distribution as well as infrastructure partnership to do a lot of things, sorry, discussion on valuation so
know, it's a distribution as well as infrastructure partnership to do a lot of things, sorry, discussion on valuation so
Speaker 1
but this is a bit interesting. In this space, every company you can imagine that you can imagine, they have a need for a loan. Pfizer. Pfizer spends 120 million to 400 million, depending on the year, on video advertising, right? Some of these companies either find out, especially so much so what matters is distribution of the productivity partnership is the only other equity investment that Amazon has done other than electronic So Amazon is not usually too relevant. And as you see, the fruits of that Amazon was looking around for same thing for same thing for video, because they have several customers who are either work the next thing that drives a lot of infrastructure
but this is a bit interesting. In this space, every company you can imagine that you can imagine, they have a need for a loan. Pfizer. Pfizer spends 120 million to 400 million, depending on the year, on video advertising, right? Some of these companies either find out, especially so much so what matters is distribution of the productivity partnership is the only other equity investment that Amazon has done other than electronic So Amazon is not usually too relevant. And as you see, the fruits of that Amazon was looking around for same thing for same thing for video, because they have several customers who are either work the next thing that drives a lot of infrastructure
but this is a bit interesting. In this space, every company you can imagine that you can imagine, they have a need for a loan. Pfizer. Pfizer spends 120 million to 400 million, depending on the year, on video advertising, right? Some of these companies either find out, especially so much so what matters is distribution of the productivity partnership is the only other equity investment that Amazon has done other than electronic So Amazon is not usually too relevant. And as you see, the fruits of that Amazon was looking around for same thing for same thing for video, because they have several customers who are either work the next thing that drives a lot of infrastructure
but this is a bit interesting. In this space, every company you can imagine that you can imagine, they have a need for a loan. Pfizer. Pfizer spends 120 million to 400 million, depending on the year, on video advertising, right? Some of these companies either find out, especially so much so what matters is distribution of the productivity partnership is the only other equity investment that Amazon has done other than electronic So Amazon is not usually too relevant. And as you see, the fruits of that Amazon was looking around for same thing for same thing for video, because they have several customers who are either work the next thing that drives a lot of infrastructure
space and maybe show them way too,
space and maybe show them way too,
space and maybe show them way too,
space and maybe show them way too,
Speaker 1
are modeling one that's available for Video Duration, basically today and rate two is going to be the first and probably the only, for real time approved model in Amazon Prime studios as the approval model that can be used At least
are modeling one that's available for Video Duration, basically today and rate two is going to be the first and probably the only, for real time approved model in Amazon Prime studios as the approval model that can be used At least
are modeling one that's available for Video Duration, basically today and rate two is going to be the first and probably the only, for real time approved model in Amazon Prime studios as the approval model that can be used At least
are modeling one that's available for Video Duration, basically today and rate two is going to be the first and probably the only, for real time approved model in Amazon Prime studios as the approval model that can be used At least
but it is because train with
but it is because train with
but it is because train with
but it is because train with
Speaker 1
licenses. So it was the largest stage that we had. So that's a little bit of fundamental company. So far, we have raised business coming from the our billion out of which two actually
licenses. So it was the largest stage that we had. So that's a little bit of fundamental company. So far, we have raised business coming from the our billion out of which two actually
licenses. So it was the largest stage that we had. So that's a little bit of fundamental company. So far, we have raised business coming from the our billion out of which two actually
licenses. So it was the largest stage that we had. So that's a little bit of fundamental company. So far, we have raised business coming from the our billion out of which two actually
Speaker 1
It understands audio, video, language, and it is that is able to produce whatever range audio video element.
It understands audio, video, language, and it is that is able to produce whatever range audio video element.
It understands audio, video, language, and it is that is able to produce whatever range audio video element.
It understands audio, video, language, and it is that is able to produce whatever range audio video element.
Speaker 1
So these are results generated by our users. That's the most important thing you should know about. These are not sharing the results that are called in our field in AI, right? If you generate 50 and then you pick the one best one, right? These are pretty much compiled from our users who generated it after the launch rate. That's generally how we do pretty much all marketing and benchmarking, not on what we generate, but our onboard. These are great, but engineers Great. Two is the highest quality model that's available on market today, and it's the only model that crosses the threshold of being useful for production became real. But pretty much all the models,
So these are results generated by our users. That's the most important thing you should know about. These are not sharing the results that are called in our field in AI, right? If you generate 50 and then you pick the one best one, right? These are pretty much compiled from our users who generated it after the launch rate. That's generally how we do pretty much all marketing and benchmarking, not on what we generate, but our onboard. These are great, but engineers Great. Two is the highest quality model that's available on market today, and it's the only model that crosses the threshold of being useful for production became real. But pretty much all the models,
So these are results generated by our users. That's the most important thing you should know about. These are not sharing the results that are called in our field in AI, right? If you generate 50 and then you pick the one best one, right? These are pretty much compiled from our users who generated it after the launch rate. That's generally how we do pretty much all marketing and benchmarking, not on what we generate, but our onboard. These are great, but engineers Great. Two is the highest quality model that's available on market today, and it's the only model that crosses the threshold of being useful for production became real. But pretty much all the models,
So these are results generated by our users. That's the most important thing you should know about. These are not sharing the results that are called in our field in AI, right? If you generate 50 and then you pick the one best one, right? These are pretty much compiled from our users who generated it after the launch rate. That's generally how we do pretty much all marketing and benchmarking, not on what we generate, but our onboard. These are great, but engineers Great. Two is the highest quality model that's available on market today, and it's the only model that crosses the threshold of being useful for production became real. But pretty much all the models,
including our first
model, but including our first
model, but including our first
model, but including our first
model, but including our first
Speaker 1
one, was just a demo that you can generate, but it Could mean some text, right? We can generate text here.
one, was just a demo that you can generate, but it Could mean some text, right? We can generate text here.
one, was just a demo that you can generate, but it Could mean some text, right? We can generate text here.
one, was just a demo that you can generate, but it Could mean some text, right? We can generate text here.
Speaker 1
because of what this giant region is making. So we compare ourselves against the rate to surpasses Sora. Basically,
because of what this giant region is making. So we compare ourselves against the rate to surpasses Sora. Basically,
because of what this giant region is making. So we compare ourselves against the rate to surpasses Sora. Basically,
because of what this giant region is making. So we compare ourselves against the rate to surpasses Sora. Basically,
Speaker 1
rate, just for elevation. So this is one
rate, just for elevation. So this is one
rate, just for elevation. So this is one
rate, just for elevation. So this is one
Speaker 1
that actually creates can use for now.
that actually creates can use for now.
that actually creates can use for now.
that actually creates can use for now.
Questions, how does it like?
Questions, how does it like?
Questions, how does it like?
Questions, how does it like?
Speaker 1
Every month we have a new generation model that comes out and then, and then we've got a benchmark competition similar to llms over there as well. So is there fundamentally, a significant leap in terms of so the models that you are generating now creating with Ray two or Ray three compared to others, because also now you have Google also, which is VO two also supposedly has good benchmark results. Gen mom supposedly has good modules as well. Clean, of course, you're showing them here, but we're seeing them like every month somebody releases a new reiteration model, and they're beating other benchmarks, right? So actually, there are no benchmarks in our field. There's only hype. That's the actual thing you have, actually benchmarks that you can look at, right? Like, you know, LGBTQA, or human success, all these other things. But basically it's a very much human eval driven field, and really interesting. Okay, how does it do across the various things that people are talking about? Right? So let's talk about all the ones people are raising and how different companies stack. So you can think about the current reiteration space in three states. You have on the bottom companies that are targeting like those are Totally very small in terms of quality, very small startups and
Every month we have a new generation model that comes out and then, and then we've got a benchmark competition similar to llms over there as well. So is there fundamentally, a significant leap in terms of so the models that you are generating now creating with Ray two or Ray three compared to others, because also now you have Google also, which is VO two also supposedly has good benchmark results. Gen mom supposedly has good modules as well. Clean, of course, you're showing them here, but we're seeing them like every month somebody releases a new reiteration model, and they're beating other benchmarks, right? So actually, there are no benchmarks in our field. There's only hype. That's the actual thing you have, actually benchmarks that you can look at, right? Like, you know, LGBTQA, or human success, all these other things. But basically it's a very much human eval driven field, and really interesting. Okay, how does it do across the various things that people are talking about? Right? So let's talk about all the ones people are raising and how different companies stack. So you can think about the current reiteration space in three states. You have on the bottom companies that are targeting like those are Totally very small in terms of quality, very small startups and
Every month we have a new generation model that comes out and then, and then we've got a benchmark competition similar to llms over there as well. So is there fundamentally, a significant leap in terms of so the models that you are generating now creating with Ray two or Ray three compared to others, because also now you have Google also, which is VO two also supposedly has good benchmark results. Gen mom supposedly has good modules as well. Clean, of course, you're showing them here, but we're seeing them like every month somebody releases a new reiteration model, and they're beating other benchmarks, right? So actually, there are no benchmarks in our field. There's only hype. That's the actual thing you have, actually benchmarks that you can look at, right? Like, you know, LGBTQA, or human success, all these other things. But basically it's a very much human eval driven field, and really interesting. Okay, how does it do across the various things that people are talking about? Right? So let's talk about all the ones people are raising and how different companies stack. So you can think about the current reiteration space in three states. You have on the bottom companies that are targeting like those are Totally very small in terms of quality, very small startups and
Every month we have a new generation model that comes out and then, and then we've got a benchmark competition similar to llms over there as well. So is there fundamentally, a significant leap in terms of so the models that you are generating now creating with Ray two or Ray three compared to others, because also now you have Google also, which is VO two also supposedly has good benchmark results. Gen mom supposedly has good modules as well. Clean, of course, you're showing them here, but we're seeing them like every month somebody releases a new reiteration model, and they're beating other benchmarks, right? So actually, there are no benchmarks in our field. There's only hype. That's the actual thing you have, actually benchmarks that you can look at, right? Like, you know, LGBTQA, or human success, all these other things. But basically it's a very much human eval driven field, and really interesting. Okay, how does it do across the various things that people are talking about? Right? So let's talk about all the ones people are raising and how different companies stack. So you can think about the current reiteration space in three states. You have on the bottom companies that are targeting like those are Totally very small in terms of quality, very small startups and
Speaker 1
the goal of those models is to a goal of that area. They operate video platforms. These are two places, and they want more of their users to post. The base model right now is the ratio it will post is one is to 1000 to 10,000 so these models are mostly geared towards the to make something right, and that's if You're a startup in that space, you're a startup in that space, you're not competing with these models that will be available, not only for free, but integrated into all these distributions. So we don't target that quality affects audience, because then you compete against free, and then competing against our goal is, how can we actually, then come to the middle companies that are targeting premium production? So here companies, all the ones you mentioned, any of our customers, they will never touch the whole enterprise. They need. These companies, what happened in language models, just how many companies like Dropbox and Databricks and every other random data company, because methods are known and you can actually produce a new empathy. Would anyone make? Well, what is the best so complete, basically entering that phase the alpha is mostly in. How can you make the highest quality months? And how do you do the research to solve the actual problems people have, right? So now, when you think about this, like runway and others, they are making video calls. That's a problem, because if you want to solve the problem of actual creative audience, the $1.1 trillion creation, they don't just want video module. What they actually want is the simulation of the world understanding. Let me go talk to directors at Netflix or Amazon Studios or whatever have you. They want to model the stats that it's an accurate tracking. And you're gonna have a five say, Okay, I want to replace this with this, right? Or I'm gonna take a video that is unique models that are actually very intelligent. So video model completes. Actually, I don't have any other example. This is a great mistake, because regular task,
the goal of those models is to a goal of that area. They operate video platforms. These are two places, and they want more of their users to post. The base model right now is the ratio it will post is one is to 1000 to 10,000 so these models are mostly geared towards the to make something right, and that's if You're a startup in that space, you're a startup in that space, you're not competing with these models that will be available, not only for free, but integrated into all these distributions. So we don't target that quality affects audience, because then you compete against free, and then competing against our goal is, how can we actually, then come to the middle companies that are targeting premium production? So here companies, all the ones you mentioned, any of our customers, they will never touch the whole enterprise. They need. These companies, what happened in language models, just how many companies like Dropbox and Databricks and every other random data company, because methods are known and you can actually produce a new empathy. Would anyone make? Well, what is the best so complete, basically entering that phase the alpha is mostly in. How can you make the highest quality months? And how do you do the research to solve the actual problems people have, right? So now, when you think about this, like runway and others, they are making video calls. That's a problem, because if you want to solve the problem of actual creative audience, the $1.1 trillion creation, they don't just want video module. What they actually want is the simulation of the world understanding. Let me go talk to directors at Netflix or Amazon Studios or whatever have you. They want to model the stats that it's an accurate tracking. And you're gonna have a five say, Okay, I want to replace this with this, right? Or I'm gonna take a video that is unique models that are actually very intelligent. So video model completes. Actually, I don't have any other example. This is a great mistake, because regular task,
the goal of those models is to a goal of that area. They operate video platforms. These are two places, and they want more of their users to post. The base model right now is the ratio it will post is one is to 1000 to 10,000 so these models are mostly geared towards the to make something right, and that's if You're a startup in that space, you're a startup in that space, you're not competing with these models that will be available, not only for free, but integrated into all these distributions. So we don't target that quality affects audience, because then you compete against free, and then competing against our goal is, how can we actually, then come to the middle companies that are targeting premium production? So here companies, all the ones you mentioned, any of our customers, they will never touch the whole enterprise. They need. These companies, what happened in language models, just how many companies like Dropbox and Databricks and every other random data company, because methods are known and you can actually produce a new empathy. Would anyone make? Well, what is the best so complete, basically entering that phase the alpha is mostly in. How can you make the highest quality months? And how do you do the research to solve the actual problems people have, right? So now, when you think about this, like runway and others, they are making video calls. That's a problem, because if you want to solve the problem of actual creative audience, the $1.1 trillion creation, they don't just want video module. What they actually want is the simulation of the world understanding. Let me go talk to directors at Netflix or Amazon Studios or whatever have you. They want to model the stats that it's an accurate tracking. And you're gonna have a five say, Okay, I want to replace this with this, right? Or I'm gonna take a video that is unique models that are actually very intelligent. So video model completes. Actually, I don't have any other example. This is a great mistake, because regular task,
the goal of those models is to a goal of that area. They operate video platforms. These are two places, and they want more of their users to post. The base model right now is the ratio it will post is one is to 1000 to 10,000 so these models are mostly geared towards the to make something right, and that's if You're a startup in that space, you're a startup in that space, you're not competing with these models that will be available, not only for free, but integrated into all these distributions. So we don't target that quality affects audience, because then you compete against free, and then competing against our goal is, how can we actually, then come to the middle companies that are targeting premium production? So here companies, all the ones you mentioned, any of our customers, they will never touch the whole enterprise. They need. These companies, what happened in language models, just how many companies like Dropbox and Databricks and every other random data company, because methods are known and you can actually produce a new empathy. Would anyone make? Well, what is the best so complete, basically entering that phase the alpha is mostly in. How can you make the highest quality months? And how do you do the research to solve the actual problems people have, right? So now, when you think about this, like runway and others, they are making video calls. That's a problem, because if you want to solve the problem of actual creative audience, the $1.1 trillion creation, they don't just want video module. What they actually want is the simulation of the world understanding. Let me go talk to directors at Netflix or Amazon Studios or whatever have you. They want to model the stats that it's an accurate tracking. And you're gonna have a five say, Okay, I want to replace this with this, right? Or I'm gonna take a video that is unique models that are actually very intelligent. So video model completes. Actually, I don't have any other example. This is a great mistake, because regular task,
Speaker 1
that's why you must focus on multi model research is the actual solution, right? So that's how we establish ourselves from these companies,
that's why you must focus on multi model research is the actual solution, right? So that's how we establish ourselves from these companies,
that's why you must focus on multi model research is the actual solution, right? So that's how we establish ourselves from these companies,
that's why you must focus on multi model research is the actual solution, right? So that's how we establish ourselves from these companies,
Speaker 1
you go in our space, at the moment, you're not going to find a company that can span across 3d across Language, across audio, but that's practically everybody's producing. This is this is by our research team, and our research is the big assets that you need to have. Who has similar research in terms of the part that I understood is essentially, you're focusing on world, better word, understanding better physics, understanding of the world, to be able to train these models, which are very different from some of the others. Video generation only model. Others, video generation only models. Then, how does like the research from these company or labs and your physical intelligence and skill? So is that similar research? Are they doing similar research in European or is that different categories, your biggest company and physical intelligence and others? Right? So bond Labs has taken a path which we consider to be technically so in the first year of our life, we actually did right? Because in 2022 it was not possible to train physically, the cards weren't there. The research was using the came from our team. So it should be. The issue with their approach of training with 3d is that data is not Where are you going to get 4d data that if you want to actually be able to train like if you've seen the demos, the demos are going from an image to the 3d right? Nothing in the world groups. It can't, because four dimensional data doesn't exist. There's no sensors. There's nothing like that. So if you're familiar with the singular truth of AI, that is the bitter lesson right again, from Rick Sutton, who just won, actually.
you go in our space, at the moment, you're not going to find a company that can span across 3d across Language, across audio, but that's practically everybody's producing. This is this is by our research team, and our research is the big assets that you need to have. Who has similar research in terms of the part that I understood is essentially, you're focusing on world, better word, understanding better physics, understanding of the world, to be able to train these models, which are very different from some of the others. Video generation only model. Others, video generation only models. Then, how does like the research from these company or labs and your physical intelligence and skill? So is that similar research? Are they doing similar research in European or is that different categories, your biggest company and physical intelligence and others? Right? So bond Labs has taken a path which we consider to be technically so in the first year of our life, we actually did right? Because in 2022 it was not possible to train physically, the cards weren't there. The research was using the came from our team. So it should be. The issue with their approach of training with 3d is that data is not Where are you going to get 4d data that if you want to actually be able to train like if you've seen the demos, the demos are going from an image to the 3d right? Nothing in the world groups. It can't, because four dimensional data doesn't exist. There's no sensors. There's nothing like that. So if you're familiar with the singular truth of AI, that is the bitter lesson right again, from Rick Sutton, who just won, actually.
you go in our space, at the moment, you're not going to find a company that can span across 3d across Language, across audio, but that's practically everybody's producing. This is this is by our research team, and our research is the big assets that you need to have. Who has similar research in terms of the part that I understood is essentially, you're focusing on world, better word, understanding better physics, understanding of the world, to be able to train these models, which are very different from some of the others. Video generation only model. Others, video generation only models. Then, how does like the research from these company or labs and your physical intelligence and skill? So is that similar research? Are they doing similar research in European or is that different categories, your biggest company and physical intelligence and others? Right? So bond Labs has taken a path which we consider to be technically so in the first year of our life, we actually did right? Because in 2022 it was not possible to train physically, the cards weren't there. The research was using the came from our team. So it should be. The issue with their approach of training with 3d is that data is not Where are you going to get 4d data that if you want to actually be able to train like if you've seen the demos, the demos are going from an image to the 3d right? Nothing in the world groups. It can't, because four dimensional data doesn't exist. There's no sensors. There's nothing like that. So if you're familiar with the singular truth of AI, that is the bitter lesson right again, from Rick Sutton, who just won, actually.
you go in our space, at the moment, you're not going to find a company that can span across 3d across Language, across audio, but that's practically everybody's producing. This is this is by our research team, and our research is the big assets that you need to have. Who has similar research in terms of the part that I understood is essentially, you're focusing on world, better word, understanding better physics, understanding of the world, to be able to train these models, which are very different from some of the others. Video generation only model. Others, video generation only models. Then, how does like the research from these company or labs and your physical intelligence and skill? So is that similar research? Are they doing similar research in European or is that different categories, your biggest company and physical intelligence and others? Right? So bond Labs has taken a path which we consider to be technically so in the first year of our life, we actually did right? Because in 2022 it was not possible to train physically, the cards weren't there. The research was using the came from our team. So it should be. The issue with their approach of training with 3d is that data is not Where are you going to get 4d data that if you want to actually be able to train like if you've seen the demos, the demos are going from an image to the 3d right? Nothing in the world groups. It can't, because four dimensional data doesn't exist. There's no sensors. There's nothing like that. So if you're familiar with the singular truth of AI, that is the bitter lesson right again, from Rick Sutton, who just won, actually.
Speaker 1
So the only truth in machine learning is that the only thing that works are general methods, transformers that can consume all of the data. If you don't have data, what are you going to do? Right? So modern labs,
So the only truth in machine learning is that the only thing that works are general methods, transformers that can consume all of the data. If you don't have data, what are you going to do? Right? So modern labs,
So the only truth in machine learning is that the only thing that works are general methods, transformers that can consume all of the data. If you don't have data, what are you going to do? Right? So modern labs,
So the only truth in machine learning is that the only thing that works are general methods, transformers that can consume all of the data. If you don't have data, what are you going to do? Right? So modern labs,
we know the team very well.
we know the team very well.
we know the team very well.
we know the team very well.
Speaker 1
But the final solution is our models,
But the final solution is our models,
But the final solution is our models,
But the final solution is our models,
Speaker 1
so they would rely but then, okay, I think we can pick that up Len as well later. But let's continue. Have you talked about that? Yeah, so maybe we can switch gear a little bit enterprise use case, but absolutely how you build up the revenue. So let's talk about the
so they would rely but then, okay, I think we can pick that up Len as well later. But let's continue. Have you talked about that? Yeah, so maybe we can switch gear a little bit enterprise use case, but absolutely how you build up the revenue. So let's talk about the
so they would rely but then, okay, I think we can pick that up Len as well later. But let's continue. Have you talked about that? Yeah, so maybe we can switch gear a little bit enterprise use case, but absolutely how you build up the revenue. So let's talk about the
so they would rely but then, okay, I think we can pick that up Len as well later. But let's continue. Have you talked about that? Yeah, so maybe we can switch gear a little bit enterprise use case, but absolutely how you build up the revenue. So let's talk about the
Speaker 1
The markets we are focused on is selling in real time in real time,
The markets we are focused on is selling in real time in real time,
The markets we are focused on is selling in real time in real time,
The markets we are focused on is selling in real time in real time,
Speaker 1
so in the premium production space, why we are working in this space, and like you know,
so in the premium production space, why we are working in this space, and like you know,
so in the premium production space, why we are working in this space, and like you know,
so in the premium production space, why we are working in this space, and like you know,
how to actually address so
how to actually address so
how to actually address so
how to actually address so
Speaker 1
video content, the is ridiculous. The easier you make it, the more spend actually goes into it, because the consumption side is so crazy. But if you give people a choice between consumer video versus, you know, anything else, they will go out today. So this is actually a time calculation that's done by Morgan Stanley on our behalf. So this is not like, you know, I pulled it out of thin air. Morgan Stanley wanted to lead our Series C, not Lee hasn't invest, but they wanted to, like, you know, as backers, but we didn't need them maybe next time when we raised the next so, but the problem with these targets is that, one, it's produced by professionals, and two unique models to be really, really good, right? Like, you know, how do you actually solve that? So we are targeting this, this particular market, and the way we think about how we go after it is as the world simulation accuracy increases. So your question on sequencing right, rate process, the boundary of producing media that can be used in production, and with rate three, that's when the world understanding and like in real time chat and these little things come up. So we actually track this, this metric on research side, well, simulation accuracy and the better it gets, the more you can do, the less you can do. Humanity, right? But today, grade two is being adopted very heavily by professionals who produce a so our approach actually, I'll skip that one, and then we'll come back to the next one. So I was also production is about $100,000 if you just look at our cost per minute of video. So we are targeting this market in two ways. One is through enterprise partnerships with large studios. So you should think about us as not a tool builder for large enterprises, but rather like Palantir. And then there's a specific reason why what we are changing is not like, Hey, this is how you do VFX. This is how you're
video content, the is ridiculous. The easier you make it, the more spend actually goes into it, because the consumption side is so crazy. But if you give people a choice between consumer video versus, you know, anything else, they will go out today. So this is actually a time calculation that's done by Morgan Stanley on our behalf. So this is not like, you know, I pulled it out of thin air. Morgan Stanley wanted to lead our Series C, not Lee hasn't invest, but they wanted to, like, you know, as backers, but we didn't need them maybe next time when we raised the next so, but the problem with these targets is that, one, it's produced by professionals, and two unique models to be really, really good, right? Like, you know, how do you actually solve that? So we are targeting this, this particular market, and the way we think about how we go after it is as the world simulation accuracy increases. So your question on sequencing right, rate process, the boundary of producing media that can be used in production, and with rate three, that's when the world understanding and like in real time chat and these little things come up. So we actually track this, this metric on research side, well, simulation accuracy and the better it gets, the more you can do, the less you can do. Humanity, right? But today, grade two is being adopted very heavily by professionals who produce a so our approach actually, I'll skip that one, and then we'll come back to the next one. So I was also production is about $100,000 if you just look at our cost per minute of video. So we are targeting this market in two ways. One is through enterprise partnerships with large studios. So you should think about us as not a tool builder for large enterprises, but rather like Palantir. And then there's a specific reason why what we are changing is not like, Hey, this is how you do VFX. This is how you're
video content, the is ridiculous. The easier you make it, the more spend actually goes into it, because the consumption side is so crazy. But if you give people a choice between consumer video versus, you know, anything else, they will go out today. So this is actually a time calculation that's done by Morgan Stanley on our behalf. So this is not like, you know, I pulled it out of thin air. Morgan Stanley wanted to lead our Series C, not Lee hasn't invest, but they wanted to, like, you know, as backers, but we didn't need them maybe next time when we raised the next so, but the problem with these targets is that, one, it's produced by professionals, and two unique models to be really, really good, right? Like, you know, how do you actually solve that? So we are targeting this, this particular market, and the way we think about how we go after it is as the world simulation accuracy increases. So your question on sequencing right, rate process, the boundary of producing media that can be used in production, and with rate three, that's when the world understanding and like in real time chat and these little things come up. So we actually track this, this metric on research side, well, simulation accuracy and the better it gets, the more you can do, the less you can do. Humanity, right? But today, grade two is being adopted very heavily by professionals who produce a so our approach actually, I'll skip that one, and then we'll come back to the next one. So I was also production is about $100,000 if you just look at our cost per minute of video. So we are targeting this market in two ways. One is through enterprise partnerships with large studios. So you should think about us as not a tool builder for large enterprises, but rather like Palantir. And then there's a specific reason why what we are changing is not like, Hey, this is how you do VFX. This is how you're
video content, the is ridiculous. The easier you make it, the more spend actually goes into it, because the consumption side is so crazy. But if you give people a choice between consumer video versus, you know, anything else, they will go out today. So this is actually a time calculation that's done by Morgan Stanley on our behalf. So this is not like, you know, I pulled it out of thin air. Morgan Stanley wanted to lead our Series C, not Lee hasn't invest, but they wanted to, like, you know, as backers, but we didn't need them maybe next time when we raised the next so, but the problem with these targets is that, one, it's produced by professionals, and two unique models to be really, really good, right? Like, you know, how do you actually solve that? So we are targeting this, this particular market, and the way we think about how we go after it is as the world simulation accuracy increases. So your question on sequencing right, rate process, the boundary of producing media that can be used in production, and with rate three, that's when the world understanding and like in real time chat and these little things come up. So we actually track this, this metric on research side, well, simulation accuracy and the better it gets, the more you can do, the less you can do. Humanity, right? But today, grade two is being adopted very heavily by professionals who produce a so our approach actually, I'll skip that one, and then we'll come back to the next one. So I was also production is about $100,000 if you just look at our cost per minute of video. So we are targeting this market in two ways. One is through enterprise partnerships with large studios. So you should think about us as not a tool builder for large enterprises, but rather like Palantir. And then there's a specific reason why what we are changing is not like, Hey, this is how you do VFX. This is how you're
changing production. These companies,
changing production. These companies,
changing production. These companies,
changing production. These companies,
Speaker 1
production is what they do every day. Is the 90% of the spend at the moment. So we have no discussion but live studios, those are the ones that have the most setting up, absolutely
production is what they do every day. Is the 90% of the spend at the moment. So we have no discussion but live studios, those are the ones that have the most setting up, absolutely
production is what they do every day. Is the 90% of the spend at the moment. So we have no discussion but live studios, those are the ones that have the most setting up, absolutely
production is what they do every day. Is the 90% of the spend at the moment. So we have no discussion but live studios, those are the ones that have the most setting up, absolutely
Speaker 1
doing 15 to 32nd heads kind of squarely fits by then, that's right. So that's why the second part comes into play, right? So we're not skipping one or the other. When you have the foundation models, you're able to build these products like the right thing, actually. But once you have really strong models, but you have my studio, so last year, in January, when we would approach these two news, they're like, oh, you know, we're taking our fans, we're trying it out, these kind of things. But today, when you push them, it's top down first of all, because they see that it can be reinvented the entire studio. So just within this group, I'll tell you a little bit about one of the publishers. It's extremely confidential, but it's Netflix. So Netflix spent last year 18.7 5 billion on production. 11 point I think three or 11.4 came outside of United States where there is no sign, right? They're also the most tech forward Studio you can ever imagine, and everybody copies what they do. So Luma and Netflix partnership is not again, a tool partnership. It's a joint research and commercial partnership, and Luma is the only company in the world with access to all of Netflix's data. This is every pixel that has been received and every pixel that has been captured by the camera and through edit processes and things like that, that allows us to invent the workflows that are necessary. Because, you know, you don't, if you're just scraping the internet, oh, you only have the final pixels. You never know how you got there, the process data, right? This is what everyone wants in languages. So that's a very unique partnership. And the way it will work is we are working with them jointly to build the workflows in those cases, as well as the foundation models. This allows us to not only understand premium production, but also target their support. So initially they'll start out at the 10 to 50 million range of others being deployed. They also identified the first two productions in which have been called regiment. They project in years 26 next year and the year after, they spent on alternative AI video was brought up to 500
doing 15 to 32nd heads kind of squarely fits by then, that's right. So that's why the second part comes into play, right? So we're not skipping one or the other. When you have the foundation models, you're able to build these products like the right thing, actually. But once you have really strong models, but you have my studio, so last year, in January, when we would approach these two news, they're like, oh, you know, we're taking our fans, we're trying it out, these kind of things. But today, when you push them, it's top down first of all, because they see that it can be reinvented the entire studio. So just within this group, I'll tell you a little bit about one of the publishers. It's extremely confidential, but it's Netflix. So Netflix spent last year 18.7 5 billion on production. 11 point I think three or 11.4 came outside of United States where there is no sign, right? They're also the most tech forward Studio you can ever imagine, and everybody copies what they do. So Luma and Netflix partnership is not again, a tool partnership. It's a joint research and commercial partnership, and Luma is the only company in the world with access to all of Netflix's data. This is every pixel that has been received and every pixel that has been captured by the camera and through edit processes and things like that, that allows us to invent the workflows that are necessary. Because, you know, you don't, if you're just scraping the internet, oh, you only have the final pixels. You never know how you got there, the process data, right? This is what everyone wants in languages. So that's a very unique partnership. And the way it will work is we are working with them jointly to build the workflows in those cases, as well as the foundation models. This allows us to not only understand premium production, but also target their support. So initially they'll start out at the 10 to 50 million range of others being deployed. They also identified the first two productions in which have been called regiment. They project in years 26 next year and the year after, they spent on alternative AI video was brought up to 500
doing 15 to 32nd heads kind of squarely fits by then, that's right. So that's why the second part comes into play, right? So we're not skipping one or the other. When you have the foundation models, you're able to build these products like the right thing, actually. But once you have really strong models, but you have my studio, so last year, in January, when we would approach these two news, they're like, oh, you know, we're taking our fans, we're trying it out, these kind of things. But today, when you push them, it's top down first of all, because they see that it can be reinvented the entire studio. So just within this group, I'll tell you a little bit about one of the publishers. It's extremely confidential, but it's Netflix. So Netflix spent last year 18.7 5 billion on production. 11 point I think three or 11.4 came outside of United States where there is no sign, right? They're also the most tech forward Studio you can ever imagine, and everybody copies what they do. So Luma and Netflix partnership is not again, a tool partnership. It's a joint research and commercial partnership, and Luma is the only company in the world with access to all of Netflix's data. This is every pixel that has been received and every pixel that has been captured by the camera and through edit processes and things like that, that allows us to invent the workflows that are necessary. Because, you know, you don't, if you're just scraping the internet, oh, you only have the final pixels. You never know how you got there, the process data, right? This is what everyone wants in languages. So that's a very unique partnership. And the way it will work is we are working with them jointly to build the workflows in those cases, as well as the foundation models. This allows us to not only understand premium production, but also target their support. So initially they'll start out at the 10 to 50 million range of others being deployed. They also identified the first two productions in which have been called regiment. They project in years 26 next year and the year after, they spent on alternative AI video was brought up to 500
doing 15 to 32nd heads kind of squarely fits by then, that's right. So that's why the second part comes into play, right? So we're not skipping one or the other. When you have the foundation models, you're able to build these products like the right thing, actually. But once you have really strong models, but you have my studio, so last year, in January, when we would approach these two news, they're like, oh, you know, we're taking our fans, we're trying it out, these kind of things. But today, when you push them, it's top down first of all, because they see that it can be reinvented the entire studio. So just within this group, I'll tell you a little bit about one of the publishers. It's extremely confidential, but it's Netflix. So Netflix spent last year 18.7 5 billion on production. 11 point I think three or 11.4 came outside of United States where there is no sign, right? They're also the most tech forward Studio you can ever imagine, and everybody copies what they do. So Luma and Netflix partnership is not again, a tool partnership. It's a joint research and commercial partnership, and Luma is the only company in the world with access to all of Netflix's data. This is every pixel that has been received and every pixel that has been captured by the camera and through edit processes and things like that, that allows us to invent the workflows that are necessary. Because, you know, you don't, if you're just scraping the internet, oh, you only have the final pixels. You never know how you got there, the process data, right? This is what everyone wants in languages. So that's a very unique partnership. And the way it will work is we are working with them jointly to build the workflows in those cases, as well as the foundation models. This allows us to not only understand premium production, but also target their support. So initially they'll start out at the 10 to 50 million range of others being deployed. They also identified the first two productions in which have been called regiment. They project in years 26 next year and the year after, they spent on alternative AI video was brought up to 500
million in the next year. So
million in the next year. So
million in the next year. So
million in the next year. So
Speaker 1
what we are doing, basically is with them, is not again as a tool provider for better. Okay?
what we are doing, basically is with them, is not again as a tool provider for better. Okay?
what we are doing, basically is with them, is not again as a tool provider for better. Okay?
what we are doing, basically is with them, is not again as a tool provider for better. Okay?
Speaker 1
So this is so why do we care about students, despite them having psych after and these infections? Because they will, actually, they spend immense amount of money. Hollywood is not actually dying, but they control IP. They control, like, you know, the distribution channels and things of that nature. They are actually investing impendence amount of money in general. They just don't know, right? So, for instance, like, you know John Chu if you know about him, the director of picket and crazy invitations, things like that. He is an investor in demand. Bono is investor in Dubai, the CEO of Warner Music and born animations and professor, but it just, you're never going to hear about it. It's going to happen. And the director of Black Mirror, David sled, is one of the most prolific users of the month. But of course, he will never tell you that it is happening. And it's happening at a very fast rate that you'll never find out. Secondly, Luma is the only approved model in Amazon Studios. Amazon, Amazon owns MGM. MGM is James Bond. If you want to jump one branch, I think of that Luma is the only approved model in Amazon Studios, and we are doing basically a similar kind of research and commercial partnership with them. Of course, it is a function of our relationship. It's another very tech forward study. So think about it this way. You learn premium production from these people, right? And then you're able to deploy it to every advertising studio. You're able to deploy them actually, which is produces Coca Cola ads, for instance, submitting a $15 billion budget. Is our self serve product. So it's the large studios that produce, like, you know, a lot of things we watch. And then there are small studios, small groups, small teams, that make majority of stuff that, like, you know, goes on YouTube, that goes on on smaller budget movies. And these people, you're you have to have a self serve. But their needs and their needs are actually not very different, because the way it is produced is not terribly different between these two items, but they're more sophisticated. So what we learned from here and at the workflows we built here, you're able to deploy self serve, which grows rapidly. So currently, rate two, we launched in January, 13, January 14, 2025, so about six, six weeks ago. Now, our self production machine is now at $17.8 million or run rate. This doesn't include any of our enterprise deals or APS.
So this is so why do we care about students, despite them having psych after and these infections? Because they will, actually, they spend immense amount of money. Hollywood is not actually dying, but they control IP. They control, like, you know, the distribution channels and things of that nature. They are actually investing impendence amount of money in general. They just don't know, right? So, for instance, like, you know John Chu if you know about him, the director of picket and crazy invitations, things like that. He is an investor in demand. Bono is investor in Dubai, the CEO of Warner Music and born animations and professor, but it just, you're never going to hear about it. It's going to happen. And the director of Black Mirror, David sled, is one of the most prolific users of the month. But of course, he will never tell you that it is happening. And it's happening at a very fast rate that you'll never find out. Secondly, Luma is the only approved model in Amazon Studios. Amazon, Amazon owns MGM. MGM is James Bond. If you want to jump one branch, I think of that Luma is the only approved model in Amazon Studios, and we are doing basically a similar kind of research and commercial partnership with them. Of course, it is a function of our relationship. It's another very tech forward study. So think about it this way. You learn premium production from these people, right? And then you're able to deploy it to every advertising studio. You're able to deploy them actually, which is produces Coca Cola ads, for instance, submitting a $15 billion budget. Is our self serve product. So it's the large studios that produce, like, you know, a lot of things we watch. And then there are small studios, small groups, small teams, that make majority of stuff that, like, you know, goes on YouTube, that goes on on smaller budget movies. And these people, you're you have to have a self serve. But their needs and their needs are actually not very different, because the way it is produced is not terribly different between these two items, but they're more sophisticated. So what we learned from here and at the workflows we built here, you're able to deploy self serve, which grows rapidly. So currently, rate two, we launched in January, 13, January 14, 2025, so about six, six weeks ago. Now, our self production machine is now at $17.8 million or run rate. This doesn't include any of our enterprise deals or APS.
So this is so why do we care about students, despite them having psych after and these infections? Because they will, actually, they spend immense amount of money. Hollywood is not actually dying, but they control IP. They control, like, you know, the distribution channels and things of that nature. They are actually investing impendence amount of money in general. They just don't know, right? So, for instance, like, you know John Chu if you know about him, the director of picket and crazy invitations, things like that. He is an investor in demand. Bono is investor in Dubai, the CEO of Warner Music and born animations and professor, but it just, you're never going to hear about it. It's going to happen. And the director of Black Mirror, David sled, is one of the most prolific users of the month. But of course, he will never tell you that it is happening. And it's happening at a very fast rate that you'll never find out. Secondly, Luma is the only approved model in Amazon Studios. Amazon, Amazon owns MGM. MGM is James Bond. If you want to jump one branch, I think of that Luma is the only approved model in Amazon Studios, and we are doing basically a similar kind of research and commercial partnership with them. Of course, it is a function of our relationship. It's another very tech forward study. So think about it this way. You learn premium production from these people, right? And then you're able to deploy it to every advertising studio. You're able to deploy them actually, which is produces Coca Cola ads, for instance, submitting a $15 billion budget. Is our self serve product. So it's the large studios that produce, like, you know, a lot of things we watch. And then there are small studios, small groups, small teams, that make majority of stuff that, like, you know, goes on YouTube, that goes on on smaller budget movies. And these people, you're you have to have a self serve. But their needs and their needs are actually not very different, because the way it is produced is not terribly different between these two items, but they're more sophisticated. So what we learned from here and at the workflows we built here, you're able to deploy self serve, which grows rapidly. So currently, rate two, we launched in January, 13, January 14, 2025, so about six, six weeks ago. Now, our self production machine is now at $17.8 million or run rate. This doesn't include any of our enterprise deals or APS.
So this is so why do we care about students, despite them having psych after and these infections? Because they will, actually, they spend immense amount of money. Hollywood is not actually dying, but they control IP. They control, like, you know, the distribution channels and things of that nature. They are actually investing impendence amount of money in general. They just don't know, right? So, for instance, like, you know John Chu if you know about him, the director of picket and crazy invitations, things like that. He is an investor in demand. Bono is investor in Dubai, the CEO of Warner Music and born animations and professor, but it just, you're never going to hear about it. It's going to happen. And the director of Black Mirror, David sled, is one of the most prolific users of the month. But of course, he will never tell you that it is happening. And it's happening at a very fast rate that you'll never find out. Secondly, Luma is the only approved model in Amazon Studios. Amazon, Amazon owns MGM. MGM is James Bond. If you want to jump one branch, I think of that Luma is the only approved model in Amazon Studios, and we are doing basically a similar kind of research and commercial partnership with them. Of course, it is a function of our relationship. It's another very tech forward study. So think about it this way. You learn premium production from these people, right? And then you're able to deploy it to every advertising studio. You're able to deploy them actually, which is produces Coca Cola ads, for instance, submitting a $15 billion budget. Is our self serve product. So it's the large studios that produce, like, you know, a lot of things we watch. And then there are small studios, small groups, small teams, that make majority of stuff that, like, you know, goes on YouTube, that goes on on smaller budget movies. And these people, you're you have to have a self serve. But their needs and their needs are actually not very different, because the way it is produced is not terribly different between these two items, but they're more sophisticated. So what we learned from here and at the workflows we built here, you're able to deploy self serve, which grows rapidly. So currently, rate two, we launched in January, 13, January 14, 2025, so about six, six weeks ago. Now, our self production machine is now at $17.8 million or run rate. This doesn't include any of our enterprise deals or APS.
So that is why we are,
So that is why we are,
So that is why we are,
So that is why we are,
Speaker 1
like, you know, you can, of course, be scared of the studios and go off make like, you know, these kind of things, but this is where the spend is. It's a concentrated industry, right? You can be the right approaches. You can actually, you can get entrenched in them. So that's why we're taking this research and commercial partnership approach together.
like, you know, you can, of course, be scared of the studios and go off make like, you know, these kind of things, but this is where the spend is. It's a concentrated industry, right? You can be the right approaches. You can actually, you can get entrenched in them. So that's why we're taking this research and commercial partnership approach together.
like, you know, you can, of course, be scared of the studios and go off make like, you know, these kind of things, but this is where the spend is. It's a concentrated industry, right? You can be the right approaches. You can actually, you can get entrenched in them. So that's why we're taking this research and commercial partnership approach together.
like, you know, you can, of course, be scared of the studios and go off make like, you know, these kind of things, but this is where the spend is. It's a concentrated industry, right? You can be the right approaches. You can actually, you can get entrenched in them. So that's why we're taking this research and commercial partnership approach together.
So this was the fixed thing. So
So this was the fixed thing. So
So this was the fixed thing. So
So this was the fixed thing. So
Speaker 1
yeah, that's, I think, probably where I should stop.
yeah, that's, I think, probably where I should stop.
yeah, that's, I think, probably where I should stop.
yeah, that's, I think, probably where I should stop.
So I guess I think there's a lot more to dig into, but second session,
So I guess I think there's a lot more to dig into, but second session,
So I guess I think there's a lot more to dig into, but second session,
So I guess I think there's a lot more to dig into, but second session,
Speaker 1
yeah, so we have next we have EU meetings, awesome. We're gonna switch over to that. What everybody online. Thank you very much. Thanks. Thank you. Amit. Yeah,
yeah, so we have next we have EU meetings, awesome. We're gonna switch over to that. What everybody online. Thank you very much. Thanks. Thank you. Amit. Yeah,
yeah, so we have next we have EU meetings, awesome. We're gonna switch over to that. What everybody online. Thank you very much. Thanks. Thank you. Amit. Yeah,
yeah, so we have next we have EU meetings, awesome. We're gonna switch over to that. What everybody online. Thank you very much. Thanks. Thank you. Amit. Yeah,
Speaker 1
General models that are able to produce a modality are also very good at understanding that value. So the another word here would be, if you're familiar with natural language processing, before trying to understand what a user is saying, was a big problem in argument that the world had to actually solve. Once we got a labs, that problem went away because, you know, lens are good at generating elements, also good and very good at understanding. But actually so our models show similar performance, attractive understanding. So immediate use case that we are going after is very large scale video cameras that are 1000s and hundreds of 1000s of cameras deployed for the same use case. How do you understand what's happening in that cell does actually really well. And finally, real time video, AI chat. So what we mean by that is being able to freestyle you type in and you get millions of words back and so forth. The future already is moving towards voice. The are the three areas,
well understanding, so I'll show you
Speaker 1
sort of how we see that. I think method of research and how understanding production actually
is very talented.
Speaker 1
So basically, the company is currently looking at the barrier, and a lot of us are our researchers, and most of our experience has been in building pieces of business. So a little bit of the work that our teams have done before the three pieces that takes the level of information and produces understandable instruments, the general model and then the COVID testing. These two data box, people on our team have built both of those parts, especially for video direction. So we had the bottleneck that actually made generate models possible. That made generate models possible. This is the work from our Josh Dylan, on our team, wasn't in line for 15 years. All the images you see today are possible because of this paper DM because that's work from RC 90 song he was before he joined us in 2023 is actually in the middle of 23 and this was his work from on this video, audio plus language, joint generation problem, then Google and Google's video.
So we have four people
responsible,
Speaker 1
especially very science research. So that's sort of the position of the team research. We believe that to unlock the market, we need to build the use case you can optimize both at the same time. And that's actually a big part of it here is the same time, and that's actually a big part of, right, of course,
the research. So
Speaker 1
this discussion my background. I work on Apple vision Pro for topics, specifically on the internet of humans already. So vision Pro has the most expressive and most natural work on the Quantum
Speaker 1
Mechanics before that. Josh,
if you
Speaker 1
train, want to
train, if you did
Speaker 1
Speaker 1
our last round was led by recent, last price round, at least, was led by in recent Horowitz and video. Then we raised another round after that that was led by AWS. So series A was
led by amplify and matrix and
Speaker 1
other angels. I Some people, but I didn't write.
And recently, from their side, it's more in recent
Speaker 1
Anjali manhau, if you're familiar with him, they are partners. Interviews. Is very interesting. I'll tell you in a second about that. And I think for this audience, we can't write that down. But I was mentioning to Albert, if you're familiar with inside the AI, if effort, the ten billion enterprise that they're creating in the next couple of months, they are leading on it.
And, you
Speaker 1
know, it's a distribution as well as infrastructure partnership to do a lot of things, sorry, discussion on valuation so
twice,
Speaker 1
but this is a bit interesting. In this space, every company you can imagine that you can imagine, they have a need for a loan. Pfizer. Pfizer spends 120 million to 400 million, depending on the year, on video advertising, right? Some of these companies either find out, especially so much so what matters is distribution of the productivity partnership is the only other equity investment that Amazon has done other than electronic So Amazon is not usually too relevant. And as you see, the fruits of that Amazon was looking around for same thing for same thing for video, because they have several customers who are either work the next thing that drives a lot of infrastructure
space and maybe show them way too,
Speaker 1
are modeling one that's available for Video Duration, basically today and rate two is going to be the first and probably the only, for real time approved model in Amazon Prime studios as the approval model that can be used At least
but it is because train with
Speaker 1
licenses. So it was the largest stage that we had. So that's a little bit of fundamental company. So far, we have raised business coming from the our billion out of which two actually
not only
understands
Speaker 1
It understands audio, video, language, and it is that is able to produce whatever range audio video element.
every night.
Speaker 1
So these are results generated by our users. That's the most important thing you should know about. These are not sharing the results that are called in our field in AI, right? If you generate 50 and then you pick the one best one, right? These are pretty much compiled from our users who generated it after the launch rate. That's generally how we do pretty much all marketing and benchmarking, not on what we generate, but our onboard. These are great, but engineers Great. Two is the highest quality model that's available on market today, and it's the only model that crosses the threshold of being useful for production became real. But pretty much all the models,
including our first
model, but including our first
Speaker 1
one, was just a demo that you can generate, but it Could mean some text, right? We can generate text here.
Speaker 1
because of what this giant region is making. So we compare ourselves against the rate to surpasses Sora. Basically,
Speaker 1
the bigger
one for
rate, just for elevation. So this is one
Speaker 1
that actually creates can use for now.
Questions, how does it like?
Speaker 1
Every month we have a new generation model that comes out and then, and then we've got a benchmark competition similar to llms over there as well. So is there fundamentally, a significant leap in terms of so the models that you are generating now creating with Ray two or Ray three compared to others, because also now you have Google also, which is VO two also supposedly has good benchmark results. Gen mom supposedly has good modules as well. Clean, of course, you're showing them here, but we're seeing them like every month somebody releases a new reiteration model, and they're beating other benchmarks, right? So actually, there are no benchmarks in our field. There's only hype. That's the actual thing you have, actually benchmarks that you can look at, right? Like, you know, LGBTQA, or human success, all these other things. But basically it's a very much human eval driven field, and really interesting. Okay, how does it do across the various things that people are talking about? Right? So let's talk about all the ones people are raising and how different companies stack. So you can think about the current reiteration space in three states. You have on the bottom companies that are targeting like those are Totally very small in terms of quality, very small startups and
investing
Speaker 1
the goal of those models is to a goal of that area. They operate video platforms. These are two places, and they want more of their users to post. The base model right now is the ratio it will post is one is to 1000 to 10,000 so these models are mostly geared towards the to make something right, and that's if You're a startup in that space, you're a startup in that space, you're not competing with these models that will be available, not only for free, but integrated into all these distributions. So we don't target that quality affects audience, because then you compete against free, and then competing against our goal is, how can we actually, then come to the middle companies that are targeting premium production? So here companies, all the ones you mentioned, any of our customers, they will never touch the whole enterprise. They need. These companies, what happened in language models, just how many companies like Dropbox and Databricks and every other random data company, because methods are known and you can actually produce a new empathy. Would anyone make? Well, what is the best so complete, basically entering that phase the alpha is mostly in. How can you make the highest quality months? And how do you do the research to solve the actual problems people have, right? So now, when you think about this, like runway and others, they are making video calls. That's a problem, because if you want to solve the problem of actual creative audience, the $1.1 trillion creation, they don't just want video module. What they actually want is the simulation of the world understanding. Let me go talk to directors at Netflix or Amazon Studios or whatever have you. They want to model the stats that it's an accurate tracking. And you're gonna have a five say, Okay, I want to replace this with this, right? Or I'm gonna take a video that is unique models that are actually very intelligent. So video model completes. Actually, I don't have any other example. This is a great mistake, because regular task,
Speaker 1
that's why you must focus on multi model research is the actual solution, right? So that's how we establish ourselves from these companies,
and if
Speaker 1
you go in our space, at the moment, you're not going to find a company that can span across 3d across Language, across audio, but that's practically everybody's producing. This is this is by our research team, and our research is the big assets that you need to have. Who has similar research in terms of the part that I understood is essentially, you're focusing on world, better word, understanding better physics, understanding of the world, to be able to train these models, which are very different from some of the others. Video generation only model. Others, video generation only models. Then, how does like the research from these company or labs and your physical intelligence and skill? So is that similar research? Are they doing similar research in European or is that different categories, your biggest company and physical intelligence and others? Right? So bond Labs has taken a path which we consider to be technically so in the first year of our life, we actually did right? Because in 2022 it was not possible to train physically, the cards weren't there. The research was using the came from our team. So it should be. The issue with their approach of training with 3d is that data is not Where are you going to get 4d data that if you want to actually be able to train like if you've seen the demos, the demos are going from an image to the 3d right? Nothing in the world groups. It can't, because four dimensional data doesn't exist. There's no sensors. There's nothing like that. So if you're familiar with the singular truth of AI, that is the bitter lesson right again, from Rick Sutton, who just won, actually.
Speaker 1
So the only truth in machine learning is that the only thing that works are general methods, transformers that can consume all of the data. If you don't have data, what are you going to do? Right? So modern labs,
we know the team very well.
Speaker 1
But the final solution is our models,
Speaker 1
so they would rely but then, okay, I think we can pick that up Len as well later. But let's continue. Have you talked about that? Yeah, so maybe we can switch gear a little bit enterprise use case, but absolutely how you build up the revenue. So let's talk about the
GTM view.
Speaker 1
The markets we are focused on is selling in real time in real time,
video chat,
Speaker 1
so in the premium production space, why we are working in this space, and like you know,
how to actually address so
Speaker 1
video content, the is ridiculous. The easier you make it, the more spend actually goes into it, because the consumption side is so crazy. But if you give people a choice between consumer video versus, you know, anything else, they will go out today. So this is actually a time calculation that's done by Morgan Stanley on our behalf. So this is not like, you know, I pulled it out of thin air. Morgan Stanley wanted to lead our Series C, not Lee hasn't invest, but they wanted to, like, you know, as backers, but we didn't need them maybe next time when we raised the next so, but the problem with these targets is that, one, it's produced by professionals, and two unique models to be really, really good, right? Like, you know, how do you actually solve that? So we are targeting this, this particular market, and the way we think about how we go after it is as the world simulation accuracy increases. So your question on sequencing right, rate process, the boundary of producing media that can be used in production, and with rate three, that's when the world understanding and like in real time chat and these little things come up. So we actually track this, this metric on research side, well, simulation accuracy and the better it gets, the more you can do, the less you can do. Humanity, right? But today, grade two is being adopted very heavily by professionals who produce a so our approach actually, I'll skip that one, and then we'll come back to the next one. So I was also production is about $100,000 if you just look at our cost per minute of video. So we are targeting this market in two ways. One is through enterprise partnerships with large studios. So you should think about us as not a tool builder for large enterprises, but rather like Palantir. And then there's a specific reason why what we are changing is not like, Hey, this is how you do VFX. This is how you're
changing production. These companies,
Speaker 1
production is what they do every day. Is the 90% of the spend at the moment. So we have no discussion but live studios, those are the ones that have the most setting up, absolutely
genetic models
Speaker 1
doing 15 to 32nd heads kind of squarely fits by then, that's right. So that's why the second part comes into play, right? So we're not skipping one or the other. When you have the foundation models, you're able to build these products like the right thing, actually. But once you have really strong models, but you have my studio, so last year, in January, when we would approach these two news, they're like, oh, you know, we're taking our fans, we're trying it out, these kind of things. But today, when you push them, it's top down first of all, because they see that it can be reinvented the entire studio. So just within this group, I'll tell you a little bit about one of the publishers. It's extremely confidential, but it's Netflix. So Netflix spent last year 18.7 5 billion on production. 11 point I think three or 11.4 came outside of United States where there is no sign, right? They're also the most tech forward Studio you can ever imagine, and everybody copies what they do. So Luma and Netflix partnership is not again, a tool partnership. It's a joint research and commercial partnership, and Luma is the only company in the world with access to all of Netflix's data. This is every pixel that has been received and every pixel that has been captured by the camera and through edit processes and things like that, that allows us to invent the workflows that are necessary. Because, you know, you don't, if you're just scraping the internet, oh, you only have the final pixels. You never know how you got there, the process data, right? This is what everyone wants in languages. So that's a very unique partnership. And the way it will work is we are working with them jointly to build the workflows in those cases, as well as the foundation models. This allows us to not only understand premium production, but also target their support. So initially they'll start out at the 10 to 50 million range of others being deployed. They also identified the first two productions in which have been called regiment. They project in years 26 next year and the year after, they spent on alternative AI video was brought up to 500
million in the next year. So
Speaker 1
what we are doing, basically is with them, is not again as a tool provider for better. Okay?
Speaker 1
So this is so why do we care about students, despite them having psych after and these infections? Because they will, actually, they spend immense amount of money. Hollywood is not actually dying, but they control IP. They control, like, you know, the distribution channels and things of that nature. They are actually investing impendence amount of money in general. They just don't know, right? So, for instance, like, you know John Chu if you know about him, the director of picket and crazy invitations, things like that. He is an investor in demand. Bono is investor in Dubai, the CEO of Warner Music and born animations and professor, but it just, you're never going to hear about it. It's going to happen. And the director of Black Mirror, David sled, is one of the most prolific users of the month. But of course, he will never tell you that it is happening. And it's happening at a very fast rate that you'll never find out. Secondly, Luma is the only approved model in Amazon Studios. Amazon, Amazon owns MGM. MGM is James Bond. If you want to jump one branch, I think of that Luma is the only approved model in Amazon Studios, and we are doing basically a similar kind of research and commercial partnership with them. Of course, it is a function of our relationship. It's another very tech forward study. So think about it this way. You learn premium production from these people, right? And then you're able to deploy it to every advertising studio. You're able to deploy them actually, which is produces Coca Cola ads, for instance, submitting a $15 billion budget. Is our self serve product. So it's the large studios that produce, like, you know, a lot of things we watch. And then there are small studios, small groups, small teams, that make majority of stuff that, like, you know, goes on YouTube, that goes on on smaller budget movies. And these people, you're you have to have a self serve. But their needs and their needs are actually not very different, because the way it is produced is not terribly different between these two items, but they're more sophisticated. So what we learned from here and at the workflows we built here, you're able to deploy self serve, which grows rapidly. So currently, rate two, we launched in January, 13, January 14, 2025, so about six, six weeks ago. Now, our self production machine is now at $17.8 million or run rate. This doesn't include any of our enterprise deals or APS.
So that is why we are,
Speaker 1
like, you know, you can, of course, be scared of the studios and go off make like, you know, these kind of things, but this is where the spend is. It's a concentrated industry, right? You can be the right approaches. You can actually, you can get entrenched in them. So that's why we're taking this research and commercial partnership approach together.
So this was the fixed thing. So
Speaker 1
yeah, that's, I think, probably where I should stop.
So I guess I think there's a lot more to dig into, but second session,
Speaker 1
yeah, so we have next we have EU meetings, awesome. We're gonna switch over to that. What everybody online. Thank you very much. Thanks. Thank you. Amit. Yeah,
How accurate was this transcription?