Meeting: Sapient + Tushar
Fri, Nov 14
9:00 AM
59 min
Priyesh P
Travel Plans and Company Updates
0:00
Company 
URL: https://otter.ai/u/ND_lQZ5zZVH4V3612ynpFi6Fev8
Downloaded: 2025-12-21T19:37:22.734153
Method: text_extraction
============================================================

S Speaker 10:00How's it going? Going good, going good. When did you reach back?
How's it going? Going good, going good. When did you reach back?
How's it going? Going good, going good. When did you reach back?
How's it going? Going good, going good. When did you reach back?
S Speaker 20:05I actually just came back to Beijing office yesterday. Got it. I'm going over to San Francisco, probably by the beginning of December, with one our founder actually
I actually just came back to Beijing office yesterday. Got it. I'm going over to San Francisco, probably by the beginning of December, with one our founder actually
I actually just came back to Beijing office yesterday. Got it. I'm going over to San Francisco, probably by the beginning of December, with one our founder actually
I actually just came back to Beijing office yesterday. Got it. I'm going over to San Francisco, probably by the beginning of December, with one our founder actually
S Speaker 10:19willing to be there? Yeah, and stay there for a while. That'd be amazing. Willem let us know on those travel plans and when those materialize, we would love to meet in person.
willing to be there? Yeah, and stay there for a while. That'd be amazing. Willem let us know on those travel plans and when those materialize, we would love to meet in person.
willing to be there? Yeah, and stay there for a while. That'd be amazing. Willem let us know on those travel plans and when those materialize, we would love to meet in person.
willing to be there? Yeah, and stay there for a while. That'd be amazing. Willem let us know on those travel plans and when those materialize, we would love to meet in person.
S Speaker 20:29Of course, of course, we do have a few big plans for December and upcoming January. Yeah, I mentioned about the generative model that we're working on that's about the time we're about to actually do some releases in the, probably in the San Francisco area.
Of course, of course, we do have a few big plans for December and upcoming January. Yeah, I mentioned about the generative model that we're working on that's about the time we're about to actually do some releases in the, probably in the San Francisco area.
Of course, of course, we do have a few big plans for December and upcoming January. Yeah, I mentioned about the generative model that we're working on that's about the time we're about to actually do some releases in the, probably in the San Francisco area.
Of course, of course, we do have a few big plans for December and upcoming January. Yeah, I mentioned about the generative model that we're working on that's about the time we're about to actually do some releases in the, probably in the San Francisco area.
S Speaker 10:48That'd be amazing. So you plan to launch both the image more the image model at that time, right?
That'd be amazing. So you plan to launch both the image more the image model at that time, right?
That'd be amazing. So you plan to launch both the image more the image model at that time, right?
That'd be amazing. So you plan to launch both the image more the image model at that time, right?
S Speaker 10:58a shot. Hey, the shark. You're on mute.
a shot. Hey, the shark. You're on mute.
a shot. Hey, the shark. You're on mute.
a shot. Hey, the shark. You're on mute.
1:04Hey, Bill, how are you doing?
Hey, Bill, how are you doing?
Hey, Bill, how are you doing?
Hey, Bill, how are you doing?
S Speaker 21:05Good, good. Nice meeting you. Nice to meet you too. Thank you. Thank you for connecting. Priyesh, absolutely sorry to I'm actually in the Beijing office right now. The lighting might be a little weird. I don't
Good, good. Nice meeting you. Nice to meet you too. Thank you. Thank you for connecting. Priyesh, absolutely sorry to I'm actually in the Beijing office right now. The lighting might be a little weird. I don't
Good, good. Nice meeting you. Nice to meet you too. Thank you. Thank you for connecting. Priyesh, absolutely sorry to I'm actually in the Beijing office right now. The lighting might be a little weird. I don't
Good, good. Nice meeting you. Nice to meet you too. Thank you. Thank you for connecting. Priyesh, absolutely sorry to I'm actually in the Beijing office right now. The lighting might be a little weird. I don't
S Speaker 31:21know if you guys late night for you, yeah,
know if you guys late night for you, yeah,
know if you guys late night for you, yeah,
know if you guys late night for you, yeah,
S Speaker 21:24but no worries. I just got back from a conference in San Diego yesterday, and we will be actually in the Palo Alto area by December. I was just talking to Priyesh With with our founder, Juan. You'll be there, and we're gonna stay there for a while. Got some big plans for the upcoming December and January.
but no worries. I just got back from a conference in San Diego yesterday, and we will be actually in the Palo Alto area by December. I was just talking to Priyesh With with our founder, Juan. You'll be there, and we're gonna stay there for a while. Got some big plans for the upcoming December and January.
but no worries. I just got back from a conference in San Diego yesterday, and we will be actually in the Palo Alto area by December. I was just talking to Priyesh With with our founder, Juan. You'll be there, and we're gonna stay there for a while. Got some big plans for the upcoming December and January.
but no worries. I just got back from a conference in San Diego yesterday, and we will be actually in the Palo Alto area by December. I was just talking to Priyesh With with our founder, Juan. You'll be there, and we're gonna stay there for a while. Got some big plans for the upcoming December and January.
1:46The company is based here, right?
The company is based here, right?
The company is based here, right?
The company is based here, right?
S Speaker 21:50We're based in Singapore, actually, but we do have a lot of folks there in the in the US. Some of them are this headquarter in Singapore, right? Headquarters in Singapore. We got a some folks in the US. Got some folks in China, because both one and I were graduated from university. It's like a second MIT
We're based in Singapore, actually, but we do have a lot of folks there in the in the US. Some of them are this headquarter in Singapore, right? Headquarters in Singapore. We got a some folks in the US. Got some folks in China, because both one and I were graduated from university. It's like a second MIT
We're based in Singapore, actually, but we do have a lot of folks there in the in the US. Some of them are this headquarter in Singapore, right? Headquarters in Singapore. We got a some folks in the US. Got some folks in China, because both one and I were graduated from university. It's like a second MIT
We're based in Singapore, actually, but we do have a lot of folks there in the in the US. Some of them are this headquarter in Singapore, right? Headquarters in Singapore. We got a some folks in the US. Got some folks in China, because both one and I were graduated from university. It's like a second MIT
2:12equipment, yeah, yeah.
equipment, yeah, yeah.
equipment, yeah, yeah.
equipment, yeah, yeah.
S Speaker 22:16And we got it, got some folks joining us from DeepMind open i as well. So that's why we do. We don't have a official office yet in Palo Alto, but we are going to build one. That's why Brian and I were going over in December.
And we got it, got some folks joining us from DeepMind open i as well. So that's why we do. We don't have a official office yet in Palo Alto, but we are going to build one. That's why Brian and I were going over in December.
And we got it, got some folks joining us from DeepMind open i as well. So that's why we do. We don't have a official office yet in Palo Alto, but we are going to build one. That's why Brian and I were going over in December.
And we got it, got some folks joining us from DeepMind open i as well. So that's why we do. We don't have a official office yet in Palo Alto, but we are going to build one. That's why Brian and I were going over in December.
S Speaker 12:35William, is there a plan to move the headquarters to us as well or not?
William, is there a plan to move the headquarters to us as well or not?
William, is there a plan to move the headquarters to us as well or not?
William, is there a plan to move the headquarters to us as well or not?
S Speaker 22:39Yeah, we are gonna slowly move our the most important, like research course, to to the US.
Yeah, we are gonna slowly move our the most important, like research course, to to the US.
Yeah, we are gonna slowly move our the most important, like research course, to to the US.
Yeah, we are gonna slowly move our the most important, like research course, to to the US.
S Speaker 32:50But what about the headquarter? I'm asking bill, because for us to engage.
But what about the headquarter? I'm asking bill, because for us to engage.
But what about the headquarter? I'm asking bill, because for us to engage.
But what about the headquarter? I'm asking bill, because for us to engage.
2:57Bill, I lost you.
S Speaker 23:12Cool, you're back. Yeah, sorry, I think the internet just screwed up.
Cool, you're back. Yeah, sorry, I think the internet just screwed up.
Cool, you're back. Yeah, sorry, I think the internet just screwed up.
Cool, you're back. Yeah, sorry, I think the internet just screwed up.
3:19Yeah, no worries. Okay, perfect. Everything,
Yeah, no worries. Okay, perfect. Everything,
Yeah, no worries. Okay, perfect. Everything,
Yeah, no worries. Okay, perfect. Everything,
S Speaker 23:22okay, perfect. Sorry, I don't know where I where I was, so we're going to slowly move everything to the US. And then Guan and some of the research core team, they're going to be based in hopefully, Palo Alto, starting from December, one will be staying there for, you know, as long as in future, at least, when
okay, perfect. Sorry, I don't know where I where I was, so we're going to slowly move everything to the US. And then Guan and some of the research core team, they're going to be based in hopefully, Palo Alto, starting from December, one will be staying there for, you know, as long as in future, at least, when
okay, perfect. Sorry, I don't know where I where I was, so we're going to slowly move everything to the US. And then Guan and some of the research core team, they're going to be based in hopefully, Palo Alto, starting from December, one will be staying there for, you know, as long as in future, at least, when
okay, perfect. Sorry, I don't know where I where I was, so we're going to slowly move everything to the US. And then Guan and some of the research core team, they're going to be based in hopefully, Palo Alto, starting from December, one will be staying there for, you know, as long as in future, at least, when
S Speaker 33:43are you thinking of moving the headquarter to you for us to engage. So the reason why is for us to engage we, we can only engage if you're a US company.
are you thinking of moving the headquarter to you for us to engage. So the reason why is for us to engage we, we can only engage if you're a US company.
are you thinking of moving the headquarter to you for us to engage. So the reason why is for us to engage we, we can only engage if you're a US company.
are you thinking of moving the headquarter to you for us to engage. So the reason why is for us to engage we, we can only engage if you're a US company.
S Speaker 23:53Oh, I see, I see, okay, yeah, so I think we can. We can probably go a little bit deeper on that. We don't have a set date yet, but what's going to happen is we're going to start a US entity, probably starting from December ish, and then we're going to slowly move everything from Singapore and Beijing to to the US entity. That's That's the plan for now. But we do have a we do want to leave a small office that's still in Beijing and Singapore, because just plethora of folks over there. That's, that's still part of the part of the research team.
Oh, I see, I see, okay, yeah, so I think we can. We can probably go a little bit deeper on that. We don't have a set date yet, but what's going to happen is we're going to start a US entity, probably starting from December ish, and then we're going to slowly move everything from Singapore and Beijing to to the US entity. That's That's the plan for now. But we do have a we do want to leave a small office that's still in Beijing and Singapore, because just plethora of folks over there. That's, that's still part of the part of the research team.
Oh, I see, I see, okay, yeah, so I think we can. We can probably go a little bit deeper on that. We don't have a set date yet, but what's going to happen is we're going to start a US entity, probably starting from December ish, and then we're going to slowly move everything from Singapore and Beijing to to the US entity. That's That's the plan for now. But we do have a we do want to leave a small office that's still in Beijing and Singapore, because just plethora of folks over there. That's, that's still part of the part of the research team.
Oh, I see, I see, okay, yeah, so I think we can. We can probably go a little bit deeper on that. We don't have a set date yet, but what's going to happen is we're going to start a US entity, probably starting from December ish, and then we're going to slowly move everything from Singapore and Beijing to to the US entity. That's That's the plan for now. But we do have a we do want to leave a small office that's still in Beijing and Singapore, because just plethora of folks over there. That's, that's still part of the part of the research team.
S Speaker 34:36Got it? Okay, yeah, why don't we spend the next few minutes, learning about what you're doing, and then we can decide, because for us to make an investment bill, we we would want you to be like a US headquartered company, like us incorporated and a US headquartered company.
Got it? Okay, yeah, why don't we spend the next few minutes, learning about what you're doing, and then we can decide, because for us to make an investment bill, we we would want you to be like a US headquartered company, like us incorporated and a US headquartered company.
Got it? Okay, yeah, why don't we spend the next few minutes, learning about what you're doing, and then we can decide, because for us to make an investment bill, we we would want you to be like a US headquartered company, like us incorporated and a US headquartered company.
Got it? Okay, yeah, why don't we spend the next few minutes, learning about what you're doing, and then we can decide, because for us to make an investment bill, we we would want you to be like a US headquartered company, like us incorporated and a US headquartered company.
S Speaker 24:57I think that happened, but I do need to talk to one. Just things make it more clear we're very excited for for potential collaboration.
I think that happened, but I do need to talk to one. Just things make it more clear we're very excited for for potential collaboration.
I think that happened, but I do need to talk to one. Just things make it more clear we're very excited for for potential collaboration.
I think that happened, but I do need to talk to one. Just things make it more clear we're very excited for for potential collaboration.
5:09Yeah, that'd be great, yeah. So
Yeah, that'd be great, yeah. So
Yeah, that'd be great, yeah. So
Yeah, that'd be great, yeah. So
S Speaker 35:13would be great to actually learn more about what you guys are doing.
would be great to actually learn more about what you guys are doing.
would be great to actually learn more about what you guys are doing.
would be great to actually learn more about what you guys are doing.
S Speaker 25:17Yeah, let me. Let me walk you through real quick. I have an intro deck. I went over this with Priyesh on a actual business deck that's gonna come out actually, actually in the record too. Okay, give me one second. Let me present this. Are you guys seeing this the site? It's loading, yes, okay, perfect. This is the size that we created back in September. Basically briefly introduced the sapient HR architecture and the reasoning models. So let me just one little bit about us. We do call ourselves a global AGI research company. We have people joining us from DeepMind, OpenAI, deep seek, qn, Xai, etc.
Yeah, let me. Let me walk you through real quick. I have an intro deck. I went over this with Priyesh on a actual business deck that's gonna come out actually, actually in the record too. Okay, give me one second. Let me present this. Are you guys seeing this the site? It's loading, yes, okay, perfect. This is the size that we created back in September. Basically briefly introduced the sapient HR architecture and the reasoning models. So let me just one little bit about us. We do call ourselves a global AGI research company. We have people joining us from DeepMind, OpenAI, deep seek, qn, Xai, etc.
Yeah, let me. Let me walk you through real quick. I have an intro deck. I went over this with Priyesh on a actual business deck that's gonna come out actually, actually in the record too. Okay, give me one second. Let me present this. Are you guys seeing this the site? It's loading, yes, okay, perfect. This is the size that we created back in September. Basically briefly introduced the sapient HR architecture and the reasoning models. So let me just one little bit about us. We do call ourselves a global AGI research company. We have people joining us from DeepMind, OpenAI, deep seek, qn, Xai, etc.
Yeah, let me. Let me walk you through real quick. I have an intro deck. I went over this with Priyesh on a actual business deck that's gonna come out actually, actually in the record too. Okay, give me one second. Let me present this. Are you guys seeing this the site? It's loading, yes, okay, perfect. This is the size that we created back in September. Basically briefly introduced the sapient HR architecture and the reasoning models. So let me just one little bit about us. We do call ourselves a global AGI research company. We have people joining us from DeepMind, OpenAI, deep seek, qn, Xai, etc.
S Speaker 26:23AGI is one of the, you know, Chloe grills, of all AI companies that they're targeting to. We do believe a a brain inspired approach will be a really interesting way to see it, because, you know, after billions of years of evolution, that's basically what's provided us, to us by mother nature. And then there's a lot of things that's going on with brain. With brains. It's very small and compact, but highly intellect. You only needs 20 watts of power to that's that's basically our vision and belief. And then if you look at some of the some of the benchmarks, some of the large language models that we know they are pretty good at, you know, science questions, because they are more like factually based. How many legs does an insect have? Everything's embedded inside a text. But for math, you actually require some sort of like, reasoning capabilities, and that's why it's very hard to do. It's not right now large that models are all probability models like, the more data you're provided with, the more accurate guess, right guess. So what we are doing is we are actually taking a thinking mechanism inside of human brains and put it inside AI architectures and then given them, giving them the capability to think like humans, or think like a human brain, at least, and to give them a much better reasoning capability. And on top of that, much better performance in general, you guys probably already heard of the skinning law as well. Elia is actually a big supporter of the new architecture, actually the popularity of the companies they started to switch from pre training, which is just cramming all the data is inside, into post training, starting from 2024 that's because we all know 2028 will grow some of the existing public data for AI model training will be depleted, and then The new data are not being generated fast enough. Then the speed we're actually training the models. So we are going to face a bottleneck if we still just keep on doing pre training, and that's why everyone's shifting to post training. And in fact, Gwen and I, we released a model back in 2023 called the open chat. It was we open sourced it, and it was pretty famous, both Berkeley and Stanford. Did you know? A lot of research on top of that, and they released a lot of articles as well, a lot of companies, including Nexus flow, they also use that model. We did. It was a 7b model, but it was better than some of the 14 V models released by some of the major companies, including nose research, mixtro llama that at the time. So it was pretty, pretty big. And we were not not bragging, but we were one of the first people to attempt using reinforcement learning as a post training method on a modeling that was in 2023 early 2023 action. So that's why it played such a significance in that. And Elon actually sent us message, sent us an email offering a few million dollars to ask her to join the x ai team. But we eventually, you know, chose to do a startup instead, because we Oh, really, yeah, it was. It was very interesting. He sent an email with Tony Wu, who was
AGI is one of the, you know, Chloe grills, of all AI companies that they're targeting to. We do believe a a brain inspired approach will be a really interesting way to see it, because, you know, after billions of years of evolution, that's basically what's provided us, to us by mother nature. And then there's a lot of things that's going on with brain. With brains. It's very small and compact, but highly intellect. You only needs 20 watts of power to that's that's basically our vision and belief. And then if you look at some of the some of the benchmarks, some of the large language models that we know they are pretty good at, you know, science questions, because they are more like factually based. How many legs does an insect have? Everything's embedded inside a text. But for math, you actually require some sort of like, reasoning capabilities, and that's why it's very hard to do. It's not right now large that models are all probability models like, the more data you're provided with, the more accurate guess, right guess. So what we are doing is we are actually taking a thinking mechanism inside of human brains and put it inside AI architectures and then given them, giving them the capability to think like humans, or think like a human brain, at least, and to give them a much better reasoning capability. And on top of that, much better performance in general, you guys probably already heard of the skinning law as well. Elia is actually a big supporter of the new architecture, actually the popularity of the companies they started to switch from pre training, which is just cramming all the data is inside, into post training, starting from 2024 that's because we all know 2028 will grow some of the existing public data for AI model training will be depleted, and then The new data are not being generated fast enough. Then the speed we're actually training the models. So we are going to face a bottleneck if we still just keep on doing pre training, and that's why everyone's shifting to post training. And in fact, Gwen and I, we released a model back in 2023 called the open chat. It was we open sourced it, and it was pretty famous, both Berkeley and Stanford. Did you know? A lot of research on top of that, and they released a lot of articles as well, a lot of companies, including Nexus flow, they also use that model. We did. It was a 7b model, but it was better than some of the 14 V models released by some of the major companies, including nose research, mixtro llama that at the time. So it was pretty, pretty big. And we were not not bragging, but we were one of the first people to attempt using reinforcement learning as a post training method on a modeling that was in 2023 early 2023 action. So that's why it played such a significance in that. And Elon actually sent us message, sent us an email offering a few million dollars to ask her to join the x ai team. But we eventually, you know, chose to do a startup instead, because we Oh, really, yeah, it was. It was very interesting. He sent an email with Tony Wu, who was
AGI is one of the, you know, Chloe grills, of all AI companies that they're targeting to. We do believe a a brain inspired approach will be a really interesting way to see it, because, you know, after billions of years of evolution, that's basically what's provided us, to us by mother nature. And then there's a lot of things that's going on with brain. With brains. It's very small and compact, but highly intellect. You only needs 20 watts of power to that's that's basically our vision and belief. And then if you look at some of the some of the benchmarks, some of the large language models that we know they are pretty good at, you know, science questions, because they are more like factually based. How many legs does an insect have? Everything's embedded inside a text. But for math, you actually require some sort of like, reasoning capabilities, and that's why it's very hard to do. It's not right now large that models are all probability models like, the more data you're provided with, the more accurate guess, right guess. So what we are doing is we are actually taking a thinking mechanism inside of human brains and put it inside AI architectures and then given them, giving them the capability to think like humans, or think like a human brain, at least, and to give them a much better reasoning capability. And on top of that, much better performance in general, you guys probably already heard of the skinning law as well. Elia is actually a big supporter of the new architecture, actually the popularity of the companies they started to switch from pre training, which is just cramming all the data is inside, into post training, starting from 2024 that's because we all know 2028 will grow some of the existing public data for AI model training will be depleted, and then The new data are not being generated fast enough. Then the speed we're actually training the models. So we are going to face a bottleneck if we still just keep on doing pre training, and that's why everyone's shifting to post training. And in fact, Gwen and I, we released a model back in 2023 called the open chat. It was we open sourced it, and it was pretty famous, both Berkeley and Stanford. Did you know? A lot of research on top of that, and they released a lot of articles as well, a lot of companies, including Nexus flow, they also use that model. We did. It was a 7b model, but it was better than some of the 14 V models released by some of the major companies, including nose research, mixtro llama that at the time. So it was pretty, pretty big. And we were not not bragging, but we were one of the first people to attempt using reinforcement learning as a post training method on a modeling that was in 2023 early 2023 action. So that's why it played such a significance in that. And Elon actually sent us message, sent us an email offering a few million dollars to ask her to join the x ai team. But we eventually, you know, chose to do a startup instead, because we Oh, really, yeah, it was. It was very interesting. He sent an email with Tony Wu, who was
AGI is one of the, you know, Chloe grills, of all AI companies that they're targeting to. We do believe a a brain inspired approach will be a really interesting way to see it, because, you know, after billions of years of evolution, that's basically what's provided us, to us by mother nature. And then there's a lot of things that's going on with brain. With brains. It's very small and compact, but highly intellect. You only needs 20 watts of power to that's that's basically our vision and belief. And then if you look at some of the some of the benchmarks, some of the large language models that we know they are pretty good at, you know, science questions, because they are more like factually based. How many legs does an insect have? Everything's embedded inside a text. But for math, you actually require some sort of like, reasoning capabilities, and that's why it's very hard to do. It's not right now large that models are all probability models like, the more data you're provided with, the more accurate guess, right guess. So what we are doing is we are actually taking a thinking mechanism inside of human brains and put it inside AI architectures and then given them, giving them the capability to think like humans, or think like a human brain, at least, and to give them a much better reasoning capability. And on top of that, much better performance in general, you guys probably already heard of the skinning law as well. Elia is actually a big supporter of the new architecture, actually the popularity of the companies they started to switch from pre training, which is just cramming all the data is inside, into post training, starting from 2024 that's because we all know 2028 will grow some of the existing public data for AI model training will be depleted, and then The new data are not being generated fast enough. Then the speed we're actually training the models. So we are going to face a bottleneck if we still just keep on doing pre training, and that's why everyone's shifting to post training. And in fact, Gwen and I, we released a model back in 2023 called the open chat. It was we open sourced it, and it was pretty famous, both Berkeley and Stanford. Did you know? A lot of research on top of that, and they released a lot of articles as well, a lot of companies, including Nexus flow, they also use that model. We did. It was a 7b model, but it was better than some of the 14 V models released by some of the major companies, including nose research, mixtro llama that at the time. So it was pretty, pretty big. And we were not not bragging, but we were one of the first people to attempt using reinforcement learning as a post training method on a modeling that was in 2023 early 2023 action. So that's why it played such a significance in that. And Elon actually sent us message, sent us an email offering a few million dollars to ask her to join the x ai team. But we eventually, you know, chose to do a startup instead, because we Oh, really, yeah, it was. It was very interesting. He sent an email with Tony Wu, who was
9:50the chief scientist of XA at the title member
the chief scientist of XA at the title member
the chief scientist of XA at the title member
the chief scientist of XA at the title member
S Speaker 29:53to our mailbox. It was very interesting. We were still, you know, in a very friendly relationship with Elon. He's still checking out on the X pages. It's very interesting. So we released that model open chat back in 2023
to our mailbox. It was very interesting. We were still, you know, in a very friendly relationship with Elon. He's still checking out on the X pages. It's very interesting. So we released that model open chat back in 2023
to our mailbox. It was very interesting. We were still, you know, in a very friendly relationship with Elon. He's still checking out on the X pages. It's very interesting. So we released that model open chat back in 2023
to our mailbox. It was very interesting. We were still, you know, in a very friendly relationship with Elon. He's still checking out on the X pages. It's very interesting. So we released that model open chat back in 2023
10:08it was, it got pretty big, actually. And then
it was, it got pretty big, actually. And then
it was, it got pretty big, actually. And then
it was, it got pretty big, actually. And then
S Speaker 210:12we sort of realized that even with post training, there's still a, you know, there's still a limit to how far you can take it with just transformer based models the old architecture. So that's why we're we're shifting towards a new architecture for this, and we call this the hierarchical reasoning model, or the architecture, is also named after the HRM. You know, the brain actually utilize a hierarchical and the current system to do the thinking process, like, right now I'm talking to the brain and actually break down into different modules, right like, I have the logic model that's handling all the reasoning, logic thinking that's behind inside my brain, and I have the bronze model that's responsible for the language part and their their data flowing in a recurrent fashion between these different
we sort of realized that even with post training, there's still a, you know, there's still a limit to how far you can take it with just transformer based models the old architecture. So that's why we're we're shifting towards a new architecture for this, and we call this the hierarchical reasoning model, or the architecture, is also named after the HRM. You know, the brain actually utilize a hierarchical and the current system to do the thinking process, like, right now I'm talking to the brain and actually break down into different modules, right like, I have the logic model that's handling all the reasoning, logic thinking that's behind inside my brain, and I have the bronze model that's responsible for the language part and their their data flowing in a recurrent fashion between these different
we sort of realized that even with post training, there's still a, you know, there's still a limit to how far you can take it with just transformer based models the old architecture. So that's why we're we're shifting towards a new architecture for this, and we call this the hierarchical reasoning model, or the architecture, is also named after the HRM. You know, the brain actually utilize a hierarchical and the current system to do the thinking process, like, right now I'm talking to the brain and actually break down into different modules, right like, I have the logic model that's handling all the reasoning, logic thinking that's behind inside my brain, and I have the bronze model that's responsible for the language part and their their data flowing in a recurrent fashion between these different
we sort of realized that even with post training, there's still a, you know, there's still a limit to how far you can take it with just transformer based models the old architecture. So that's why we're we're shifting towards a new architecture for this, and we call this the hierarchical reasoning model, or the architecture, is also named after the HRM. You know, the brain actually utilize a hierarchical and the current system to do the thinking process, like, right now I'm talking to the brain and actually break down into different modules, right like, I have the logic model that's handling all the reasoning, logic thinking that's behind inside my brain, and I have the bronze model that's responsible for the language part and their their data flowing in a recurrent fashion between these different
S Speaker 311:05Mark. Mark, I mean, essentially over simple system one and system two. Yes,
Mark. Mark, I mean, essentially over simple system one and system two. Yes,
Mark. Mark, I mean, essentially over simple system one and system two. Yes,
Mark. Mark, I mean, essentially over simple system one and system two. Yes,
S Speaker 211:11exactly. So we have a low level and a high, high level. It's basically, you know, mimicking that a low level is a faster working module that's, you know, handling detailed computation in that high level is actually monitoring and controlling the direction of convergence the model. So it's like a self correction algorithm. And that gives us a few advantages with that architecture. We are we are for inference. We are putting out a tokens inside encoder simultaneously or in parallel, as opposed to sequentially. So give us a few other advantages as well. First of all, we can dramatically increase the inference death. That's actually a very key factor in large animals. For transformer only models, they only have about 64 layers, for reason of reasoning that and for us, but we can stretch it to a few hundreds, even 1000s of layers. Right now. We keep it around 500 layers. It's just enough to most of the task. So if we were to ask a 200 layer question, a complex question to a transformer only model like GBT sequence stuff. It's the only way for it to properly do it. It's through a chain of thought, right? Yeah, it's gonna break down into smaller pieces and process. It's way of prominent. But for us, we can do it in one run. And also, on top of that, what
exactly. So we have a low level and a high, high level. It's basically, you know, mimicking that a low level is a faster working module that's, you know, handling detailed computation in that high level is actually monitoring and controlling the direction of convergence the model. So it's like a self correction algorithm. And that gives us a few advantages with that architecture. We are we are for inference. We are putting out a tokens inside encoder simultaneously or in parallel, as opposed to sequentially. So give us a few other advantages as well. First of all, we can dramatically increase the inference death. That's actually a very key factor in large animals. For transformer only models, they only have about 64 layers, for reason of reasoning that and for us, but we can stretch it to a few hundreds, even 1000s of layers. Right now. We keep it around 500 layers. It's just enough to most of the task. So if we were to ask a 200 layer question, a complex question to a transformer only model like GBT sequence stuff. It's the only way for it to properly do it. It's through a chain of thought, right? Yeah, it's gonna break down into smaller pieces and process. It's way of prominent. But for us, we can do it in one run. And also, on top of that, what
exactly. So we have a low level and a high, high level. It's basically, you know, mimicking that a low level is a faster working module that's, you know, handling detailed computation in that high level is actually monitoring and controlling the direction of convergence the model. So it's like a self correction algorithm. And that gives us a few advantages with that architecture. We are we are for inference. We are putting out a tokens inside encoder simultaneously or in parallel, as opposed to sequentially. So give us a few other advantages as well. First of all, we can dramatically increase the inference death. That's actually a very key factor in large animals. For transformer only models, they only have about 64 layers, for reason of reasoning that and for us, but we can stretch it to a few hundreds, even 1000s of layers. Right now. We keep it around 500 layers. It's just enough to most of the task. So if we were to ask a 200 layer question, a complex question to a transformer only model like GBT sequence stuff. It's the only way for it to properly do it. It's through a chain of thought, right? Yeah, it's gonna break down into smaller pieces and process. It's way of prominent. But for us, we can do it in one run. And also, on top of that, what
exactly. So we have a low level and a high, high level. It's basically, you know, mimicking that a low level is a faster working module that's, you know, handling detailed computation in that high level is actually monitoring and controlling the direction of convergence the model. So it's like a self correction algorithm. And that gives us a few advantages with that architecture. We are we are for inference. We are putting out a tokens inside encoder simultaneously or in parallel, as opposed to sequentially. So give us a few other advantages as well. First of all, we can dramatically increase the inference death. That's actually a very key factor in large animals. For transformer only models, they only have about 64 layers, for reason of reasoning that and for us, but we can stretch it to a few hundreds, even 1000s of layers. Right now. We keep it around 500 layers. It's just enough to most of the task. So if we were to ask a 200 layer question, a complex question to a transformer only model like GBT sequence stuff. It's the only way for it to properly do it. It's through a chain of thought, right? Yeah, it's gonna break down into smaller pieces and process. It's way of prominent. But for us, we can do it in one run. And also, on top of that, what
S Speaker 312:40does that mean? Which is, which is, in case of regular transformer, you know, I ask something, and then it's going to go through, you know, a complex chain of thought, depending on how much compute it has. Sometimes it'll go through a lot more depth using chain of thought or just a few steps, if it doesn't have enough computer, depending on the cost that I have to spend on compute for that particular task in HRM, what happens like, like, specifically, like, does it still, it's still going through a chain of thought or no chain of thought at all.
does that mean? Which is, which is, in case of regular transformer, you know, I ask something, and then it's going to go through, you know, a complex chain of thought, depending on how much compute it has. Sometimes it'll go through a lot more depth using chain of thought or just a few steps, if it doesn't have enough computer, depending on the cost that I have to spend on compute for that particular task in HRM, what happens like, like, specifically, like, does it still, it's still going through a chain of thought or no chain of thought at all.
does that mean? Which is, which is, in case of regular transformer, you know, I ask something, and then it's going to go through, you know, a complex chain of thought, depending on how much compute it has. Sometimes it'll go through a lot more depth using chain of thought or just a few steps, if it doesn't have enough computer, depending on the cost that I have to spend on compute for that particular task in HRM, what happens like, like, specifically, like, does it still, it's still going through a chain of thought or no chain of thought at all.
does that mean? Which is, which is, in case of regular transformer, you know, I ask something, and then it's going to go through, you know, a complex chain of thought, depending on how much compute it has. Sometimes it'll go through a lot more depth using chain of thought or just a few steps, if it doesn't have enough computer, depending on the cost that I have to spend on compute for that particular task in HRM, what happens like, like, specifically, like, does it still, it's still going through a chain of thought or no chain of thought at all.
S Speaker 213:23Yes, our model right now doesn't based on any pre training or chain of thought at all. We do not use pre training or nor chain of thought on our model, simply because we do have just sufficient inference then, and that's one of the key factors that we found out through extensive experiments that that's the one of the key factor why transformer models can't really do some of the complex reasonings and stuff.
Yes, our model right now doesn't based on any pre training or chain of thought at all. We do not use pre training or nor chain of thought on our model, simply because we do have just sufficient inference then, and that's one of the key factors that we found out through extensive experiments that that's the one of the key factor why transformer models can't really do some of the complex reasonings and stuff.
Yes, our model right now doesn't based on any pre training or chain of thought at all. We do not use pre training or nor chain of thought on our model, simply because we do have just sufficient inference then, and that's one of the key factors that we found out through extensive experiments that that's the one of the key factor why transformer models can't really do some of the complex reasonings and stuff.
Yes, our model right now doesn't based on any pre training or chain of thought at all. We do not use pre training or nor chain of thought on our model, simply because we do have just sufficient inference then, and that's one of the key factors that we found out through extensive experiments that that's the one of the key factor why transformer models can't really do some of the complex reasonings and stuff.
13:53Can you explain, for my simple brain?
Can you explain, for my simple brain?
Can you explain, for my simple brain?
Can you explain, for my simple brain?
S Speaker 313:59Can you explain, for example. Let's take an example of a task. Then, like, for example, like, pick any task that somebody wants to use the HRM for, and then what does it mean like to the end user? What would it seem feel like?
Can you explain, for example. Let's take an example of a task. Then, like, for example, like, pick any task that somebody wants to use the HRM for, and then what does it mean like to the end user? What would it seem feel like?
Can you explain, for example. Let's take an example of a task. Then, like, for example, like, pick any task that somebody wants to use the HRM for, and then what does it mean like to the end user? What would it seem feel like?
Can you explain, for example. Let's take an example of a task. Then, like, for example, like, pick any task that somebody wants to use the HRM for, and then what does it mean like to the end user? What would it seem feel like?
14:47When did you start the company?
When did you start the company?
When did you start the company?
When did you start the company?
S Speaker 214:48We started in August 2024 Okay, about about a year? Yeah, yeah, three years ago. Okay, in 20.3 were released open chat and you know, that's, that's the thing that caught Elon's attention. And then we kind of just, and then after that, for about a few months, we realized, even with preach post training and reinforcement learning, there's, there's just still a limit, I know, deep seat in January this year, right? They got really famous because the paper on grpo, which is also a reinforcement learning on post training stuff, but still, it's more like solving engineering problems as opposed to actually tracking the scientific problems. That's actually blocking us from achieving AGI. We could have done post training from 2023 and, you know, keep on doing that. But we just, we just think we do need a new architecture. That's, that's when we're building, we're building this new team and new startup from 2024 Okay, yeah, I'm gonna go back to the champ dot problem we've been talking about so one of the practical work around is to, you know explicitly, to break down a complex text into smaller steps, right? That's so, for example, if I want to get from a letter A to letter D as a thought process, right? For chain of thought, or for large language models, in general, they all utilize this technique. Is they're going to break down from, take it from A to B and explicitly tell you the answer B in their response. So that's, that's how, that's how, that's the only how far I can take it, 64 layers. For example, if we're, if we're to ask a 240 layer task or question that requires 240 layer and 40 layers to solve, it's going to run for 64 layers. It's going to explicitly give you the output, and you're, you're going to look at the output, and then you'll be like, I'm not sure that's it, try again. And then it's going to take that output, put it inside the input again, and it's going to run it for maybe 64 years, maybe give you something that's letter C. And then you're going to take that, take a look at that output, and be like, still not right there, and try again. It's going to take that again and then put in inside there. Eventually it might get the letter D, or eventually you will never get that letter, because it just might just diverge in the middle and just, you know, never come back. That's hooked this nation so but ideally for humans, right? We our brain, don't process it as we don't simply put out, give you speak out the middle thoughts. I'm only going to tell you my prepared speech, my generative speech, not not the thoughts that's in between. So ideally, the models the human brain will process everything internally or implicitly, inside. And then we'll output D directly, or that's what we call a Latin chain of time. And that's one of the techniques that's used by HRM. Basically, that's what and the reason behind is we are stretching the model into different hierarchies to provide it with, you know, sufficient reason in depth, and that can actually achieve that effect for most
We started in August 2024 Okay, about about a year? Yeah, yeah, three years ago. Okay, in 20.3 were released open chat and you know, that's, that's the thing that caught Elon's attention. And then we kind of just, and then after that, for about a few months, we realized, even with preach post training and reinforcement learning, there's, there's just still a limit, I know, deep seat in January this year, right? They got really famous because the paper on grpo, which is also a reinforcement learning on post training stuff, but still, it's more like solving engineering problems as opposed to actually tracking the scientific problems. That's actually blocking us from achieving AGI. We could have done post training from 2023 and, you know, keep on doing that. But we just, we just think we do need a new architecture. That's, that's when we're building, we're building this new team and new startup from 2024 Okay, yeah, I'm gonna go back to the champ dot problem we've been talking about so one of the practical work around is to, you know explicitly, to break down a complex text into smaller steps, right? That's so, for example, if I want to get from a letter A to letter D as a thought process, right? For chain of thought, or for large language models, in general, they all utilize this technique. Is they're going to break down from, take it from A to B and explicitly tell you the answer B in their response. So that's, that's how, that's how, that's the only how far I can take it, 64 layers. For example, if we're, if we're to ask a 240 layer task or question that requires 240 layer and 40 layers to solve, it's going to run for 64 layers. It's going to explicitly give you the output, and you're, you're going to look at the output, and then you'll be like, I'm not sure that's it, try again. And then it's going to take that output, put it inside the input again, and it's going to run it for maybe 64 years, maybe give you something that's letter C. And then you're going to take that, take a look at that output, and be like, still not right there, and try again. It's going to take that again and then put in inside there. Eventually it might get the letter D, or eventually you will never get that letter, because it just might just diverge in the middle and just, you know, never come back. That's hooked this nation so but ideally for humans, right? We our brain, don't process it as we don't simply put out, give you speak out the middle thoughts. I'm only going to tell you my prepared speech, my generative speech, not not the thoughts that's in between. So ideally, the models the human brain will process everything internally or implicitly, inside. And then we'll output D directly, or that's what we call a Latin chain of time. And that's one of the techniques that's used by HRM. Basically, that's what and the reason behind is we are stretching the model into different hierarchies to provide it with, you know, sufficient reason in depth, and that can actually achieve that effect for most
We started in August 2024 Okay, about about a year? Yeah, yeah, three years ago. Okay, in 20.3 were released open chat and you know, that's, that's the thing that caught Elon's attention. And then we kind of just, and then after that, for about a few months, we realized, even with preach post training and reinforcement learning, there's, there's just still a limit, I know, deep seat in January this year, right? They got really famous because the paper on grpo, which is also a reinforcement learning on post training stuff, but still, it's more like solving engineering problems as opposed to actually tracking the scientific problems. That's actually blocking us from achieving AGI. We could have done post training from 2023 and, you know, keep on doing that. But we just, we just think we do need a new architecture. That's, that's when we're building, we're building this new team and new startup from 2024 Okay, yeah, I'm gonna go back to the champ dot problem we've been talking about so one of the practical work around is to, you know explicitly, to break down a complex text into smaller steps, right? That's so, for example, if I want to get from a letter A to letter D as a thought process, right? For chain of thought, or for large language models, in general, they all utilize this technique. Is they're going to break down from, take it from A to B and explicitly tell you the answer B in their response. So that's, that's how, that's how, that's the only how far I can take it, 64 layers. For example, if we're, if we're to ask a 240 layer task or question that requires 240 layer and 40 layers to solve, it's going to run for 64 layers. It's going to explicitly give you the output, and you're, you're going to look at the output, and then you'll be like, I'm not sure that's it, try again. And then it's going to take that output, put it inside the input again, and it's going to run it for maybe 64 years, maybe give you something that's letter C. And then you're going to take that, take a look at that output, and be like, still not right there, and try again. It's going to take that again and then put in inside there. Eventually it might get the letter D, or eventually you will never get that letter, because it just might just diverge in the middle and just, you know, never come back. That's hooked this nation so but ideally for humans, right? We our brain, don't process it as we don't simply put out, give you speak out the middle thoughts. I'm only going to tell you my prepared speech, my generative speech, not not the thoughts that's in between. So ideally, the models the human brain will process everything internally or implicitly, inside. And then we'll output D directly, or that's what we call a Latin chain of time. And that's one of the techniques that's used by HRM. Basically, that's what and the reason behind is we are stretching the model into different hierarchies to provide it with, you know, sufficient reason in depth, and that can actually achieve that effect for most
We started in August 2024 Okay, about about a year? Yeah, yeah, three years ago. Okay, in 20.3 were released open chat and you know, that's, that's the thing that caught Elon's attention. And then we kind of just, and then after that, for about a few months, we realized, even with preach post training and reinforcement learning, there's, there's just still a limit, I know, deep seat in January this year, right? They got really famous because the paper on grpo, which is also a reinforcement learning on post training stuff, but still, it's more like solving engineering problems as opposed to actually tracking the scientific problems. That's actually blocking us from achieving AGI. We could have done post training from 2023 and, you know, keep on doing that. But we just, we just think we do need a new architecture. That's, that's when we're building, we're building this new team and new startup from 2024 Okay, yeah, I'm gonna go back to the champ dot problem we've been talking about so one of the practical work around is to, you know explicitly, to break down a complex text into smaller steps, right? That's so, for example, if I want to get from a letter A to letter D as a thought process, right? For chain of thought, or for large language models, in general, they all utilize this technique. Is they're going to break down from, take it from A to B and explicitly tell you the answer B in their response. So that's, that's how, that's how, that's the only how far I can take it, 64 layers. For example, if we're, if we're to ask a 240 layer task or question that requires 240 layer and 40 layers to solve, it's going to run for 64 layers. It's going to explicitly give you the output, and you're, you're going to look at the output, and then you'll be like, I'm not sure that's it, try again. And then it's going to take that output, put it inside the input again, and it's going to run it for maybe 64 years, maybe give you something that's letter C. And then you're going to take that, take a look at that output, and be like, still not right there, and try again. It's going to take that again and then put in inside there. Eventually it might get the letter D, or eventually you will never get that letter, because it just might just diverge in the middle and just, you know, never come back. That's hooked this nation so but ideally for humans, right? We our brain, don't process it as we don't simply put out, give you speak out the middle thoughts. I'm only going to tell you my prepared speech, my generative speech, not not the thoughts that's in between. So ideally, the models the human brain will process everything internally or implicitly, inside. And then we'll output D directly, or that's what we call a Latin chain of time. And that's one of the techniques that's used by HRM. Basically, that's what and the reason behind is we are stretching the model into different hierarchies to provide it with, you know, sufficient reason in depth, and that can actually achieve that effect for most
S Speaker 318:27of the task in one run in and this just happens. What and how does tool calling work? In this case, I ah, for example, in order to accomplish a certain task, you have tap into some other tools, right, right, letting kids of chain of thought, one of the steps is, I need to call a tool.
of the task in one run in and this just happens. What and how does tool calling work? In this case, I ah, for example, in order to accomplish a certain task, you have tap into some other tools, right, right, letting kids of chain of thought, one of the steps is, I need to call a tool.
of the task in one run in and this just happens. What and how does tool calling work? In this case, I ah, for example, in order to accomplish a certain task, you have tap into some other tools, right, right, letting kids of chain of thought, one of the steps is, I need to call a tool.
of the task in one run in and this just happens. What and how does tool calling work? In this case, I ah, for example, in order to accomplish a certain task, you have tap into some other tools, right, right, letting kids of chain of thought, one of the steps is, I need to call a tool.
S Speaker 218:53Yes, sorry, I'm gonna jump around different slides. Just yeah, good to explain stuff. Give me one second
Yes, sorry, I'm gonna jump around different slides. Just yeah, good to explain stuff. Give me one second
Yes, sorry, I'm gonna jump around different slides. Just yeah, good to explain stuff. Give me one second
Yes, sorry, I'm gonna jump around different slides. Just yeah, good to explain stuff. Give me one second
S Speaker 319:13these days, to accomplish a task, you have to call like, three or four different tools, right?
these days, to accomplish a task, you have to call like, three or four different tools, right?
these days, to accomplish a task, you have to call like, three or four different tools, right?
these days, to accomplish a task, you have to call like, three or four different tools, right?
S Speaker 219:31Okay, I'll use this as an example. So this is the, this is the framework that we're using for for a application, in this case, climate prediction. But it will be the it will be similar for other applications as well. So on the left, this is, this is basically a HR and model right here. I don't know if you can see the mouse. Yeah, yes. Okay, so we have, will we usually break down a task into different scales or different, as we say, hierarchies or different levels. It will really actually simplify the task. And then to make everything work better, for example, for climate prediction, right? You have the synaptic skill that's things changing a few days, a planter skill that's for things changing a few weeks, and intra seasonal skills for things that's changing a few months. And together with that, you can more more precisely predict the climate.
Okay, I'll use this as an example. So this is the, this is the framework that we're using for for a application, in this case, climate prediction. But it will be the it will be similar for other applications as well. So on the left, this is, this is basically a HR and model right here. I don't know if you can see the mouse. Yeah, yes. Okay, so we have, will we usually break down a task into different scales or different, as we say, hierarchies or different levels. It will really actually simplify the task. And then to make everything work better, for example, for climate prediction, right? You have the synaptic skill that's things changing a few days, a planter skill that's for things changing a few weeks, and intra seasonal skills for things that's changing a few months. And together with that, you can more more precisely predict the climate.
Okay, I'll use this as an example. So this is the, this is the framework that we're using for for a application, in this case, climate prediction. But it will be the it will be similar for other applications as well. So on the left, this is, this is basically a HR and model right here. I don't know if you can see the mouse. Yeah, yes. Okay, so we have, will we usually break down a task into different scales or different, as we say, hierarchies or different levels. It will really actually simplify the task. And then to make everything work better, for example, for climate prediction, right? You have the synaptic skill that's things changing a few days, a planter skill that's for things changing a few weeks, and intra seasonal skills for things that's changing a few months. And together with that, you can more more precisely predict the climate.
Okay, I'll use this as an example. So this is the, this is the framework that we're using for for a application, in this case, climate prediction. But it will be the it will be similar for other applications as well. So on the left, this is, this is basically a HR and model right here. I don't know if you can see the mouse. Yeah, yes. Okay, so we have, will we usually break down a task into different scales or different, as we say, hierarchies or different levels. It will really actually simplify the task. And then to make everything work better, for example, for climate prediction, right? You have the synaptic skill that's things changing a few days, a planter skill that's for things changing a few weeks, and intra seasonal skills for things that's changing a few months. And together with that, you can more more precisely predict the climate.
S Speaker 320:37Where do you decide which hierarchies you need to extract for any particular task,
Where do you decide which hierarchies you need to extract for any particular task,
Where do you decide which hierarchies you need to extract for any particular task,
Where do you decide which hierarchies you need to extract for any particular task,
S Speaker 220:47it usually depends on the nature of the task themselves. In this case, for climate prediction, these are actually the same scales are used in traditional climate prediction as models. And I'll give another
it usually depends on the nature of the task themselves. In this case, for climate prediction, these are actually the same scales are used in traditional climate prediction as models. And I'll give another
it usually depends on the nature of the task themselves. In this case, for climate prediction, these are actually the same scales are used in traditional climate prediction as models. And I'll give another
it usually depends on the nature of the task themselves. In this case, for climate prediction, these are actually the same scales are used in traditional climate prediction as models. And I'll give another
21:04example, where have you learned that in the model,
example, where have you learned that in the model,
example, where have you learned that in the model,
example, where have you learned that in the model,
21:10you can train the model to at least know that right,
you can train the model to at least know that right,
you can train the model to at least know that right,
you can train the model to at least know that right,
S Speaker 221:13right? So we are providing model with different timescales and different data from different timescales, then it actually you will learn the pattern over, over. The training
right? So we are providing model with different timescales and different data from different timescales, then it actually you will learn the pattern over, over. The training
right? So we are providing model with different timescales and different data from different timescales, then it actually you will learn the pattern over, over. The training
right? So we are providing model with different timescales and different data from different timescales, then it actually you will learn the pattern over, over. The training
S Speaker 321:23you train the model with, and then, like you train the model to actually learn hierarchies.
you train the model with, and then, like you train the model to actually learn hierarchies.
you train the model with, and then, like you train the model to actually learn hierarchies.
you train the model with, and then, like you train the model to actually learn hierarchies.
21:33Well, yes, yes, or
21:35any even particular
S Speaker 221:36task for for this, I'll use this as an example for climate prediction. Well we usually do is we put an encoder and in the input side and the output side of the model. Well, simply just put in a labeled image data. The image data will be the image of, for example, for climate prediction, it will be the image for the cloud graph for a particular region and the heat graph, or for temperature for a particular region. Well, those are labeled with, you know, the actual data points with a for example, the cloud distribution. Just put it in there, and then it will actually organize it with different scales. For example, things that's changing the few days will be under these different skills. There are different variables for the synaptic skill. There you get the air temperature, you get the air speed and stuff. You're going to put those in there. And then you have the planter skill. It will be something like the ocean waves and stuff changing relatively slower inside there. And then interesting room skills, more like hurricanes and and big ocean movements, bigger ocean movements to that. And we're going to categorize different variables into different skills, so we can do a multi skill and a multi variable network prediction, and that will actually make this model super, and I wouldn't say super, more accurate, much more accurate than some of the other models that we've been currently. The ECM, WF is the European equivalent of NASA. Yeah, that's a very famous model. Fusi is also a very famous Chinese model on the camera prediction. We're about 620, 5% more accurate than theirs and and also on top of that, we are a general reasoning model, as opposed to a specialized kind of prediction model. So that's pretty, actually pretty good. And some of the things that I just described, like, for example, the multi scale and the multi variable forecast capability, it's very it's basically impossible to actually accurately achieve using just normal, large language models to do so. So that's something that's really interesting. And this model is super small. It's like about tenths of the size of a normal, specialized model. So it's pretty pretty bill.
task for for this, I'll use this as an example for climate prediction. Well we usually do is we put an encoder and in the input side and the output side of the model. Well, simply just put in a labeled image data. The image data will be the image of, for example, for climate prediction, it will be the image for the cloud graph for a particular region and the heat graph, or for temperature for a particular region. Well, those are labeled with, you know, the actual data points with a for example, the cloud distribution. Just put it in there, and then it will actually organize it with different scales. For example, things that's changing the few days will be under these different skills. There are different variables for the synaptic skill. There you get the air temperature, you get the air speed and stuff. You're going to put those in there. And then you have the planter skill. It will be something like the ocean waves and stuff changing relatively slower inside there. And then interesting room skills, more like hurricanes and and big ocean movements, bigger ocean movements to that. And we're going to categorize different variables into different skills, so we can do a multi skill and a multi variable network prediction, and that will actually make this model super, and I wouldn't say super, more accurate, much more accurate than some of the other models that we've been currently. The ECM, WF is the European equivalent of NASA. Yeah, that's a very famous model. Fusi is also a very famous Chinese model on the camera prediction. We're about 620, 5% more accurate than theirs and and also on top of that, we are a general reasoning model, as opposed to a specialized kind of prediction model. So that's pretty, actually pretty good. And some of the things that I just described, like, for example, the multi scale and the multi variable forecast capability, it's very it's basically impossible to actually accurately achieve using just normal, large language models to do so. So that's something that's really interesting. And this model is super small. It's like about tenths of the size of a normal, specialized model. So it's pretty pretty bill.
task for for this, I'll use this as an example for climate prediction. Well we usually do is we put an encoder and in the input side and the output side of the model. Well, simply just put in a labeled image data. The image data will be the image of, for example, for climate prediction, it will be the image for the cloud graph for a particular region and the heat graph, or for temperature for a particular region. Well, those are labeled with, you know, the actual data points with a for example, the cloud distribution. Just put it in there, and then it will actually organize it with different scales. For example, things that's changing the few days will be under these different skills. There are different variables for the synaptic skill. There you get the air temperature, you get the air speed and stuff. You're going to put those in there. And then you have the planter skill. It will be something like the ocean waves and stuff changing relatively slower inside there. And then interesting room skills, more like hurricanes and and big ocean movements, bigger ocean movements to that. And we're going to categorize different variables into different skills, so we can do a multi skill and a multi variable network prediction, and that will actually make this model super, and I wouldn't say super, more accurate, much more accurate than some of the other models that we've been currently. The ECM, WF is the European equivalent of NASA. Yeah, that's a very famous model. Fusi is also a very famous Chinese model on the camera prediction. We're about 620, 5% more accurate than theirs and and also on top of that, we are a general reasoning model, as opposed to a specialized kind of prediction model. So that's pretty, actually pretty good. And some of the things that I just described, like, for example, the multi scale and the multi variable forecast capability, it's very it's basically impossible to actually accurately achieve using just normal, large language models to do so. So that's something that's really interesting. And this model is super small. It's like about tenths of the size of a normal, specialized model. So it's pretty pretty bill.
task for for this, I'll use this as an example for climate prediction. Well we usually do is we put an encoder and in the input side and the output side of the model. Well, simply just put in a labeled image data. The image data will be the image of, for example, for climate prediction, it will be the image for the cloud graph for a particular region and the heat graph, or for temperature for a particular region. Well, those are labeled with, you know, the actual data points with a for example, the cloud distribution. Just put it in there, and then it will actually organize it with different scales. For example, things that's changing the few days will be under these different skills. There are different variables for the synaptic skill. There you get the air temperature, you get the air speed and stuff. You're going to put those in there. And then you have the planter skill. It will be something like the ocean waves and stuff changing relatively slower inside there. And then interesting room skills, more like hurricanes and and big ocean movements, bigger ocean movements to that. And we're going to categorize different variables into different skills, so we can do a multi skill and a multi variable network prediction, and that will actually make this model super, and I wouldn't say super, more accurate, much more accurate than some of the other models that we've been currently. The ECM, WF is the European equivalent of NASA. Yeah, that's a very famous model. Fusi is also a very famous Chinese model on the camera prediction. We're about 620, 5% more accurate than theirs and and also on top of that, we are a general reasoning model, as opposed to a specialized kind of prediction model. So that's pretty, actually pretty good. And some of the things that I just described, like, for example, the multi scale and the multi variable forecast capability, it's very it's basically impossible to actually accurately achieve using just normal, large language models to do so. So that's something that's really interesting. And this model is super small. It's like about tenths of the size of a normal, specialized model. So it's pretty pretty bill.
S Speaker 324:21Let me take you like, how like, how about like, knowledge work, which is for tasks inside an enterprise. Let's say, you know, I'll give you an example of a task that we have used bunch of AI tools for internally, which is, you know, we have we come like our customers. Imagine our customers like Samsung, or all those customers, they send us logs. Okay, like logs from cell phones, right? Error Logs. We have a task that goes you need to go fetch these logs from the tickets, then you need to fetch information from Salesforce, and then you need to process these logs for errors, and you need to draw diagrams and also suggest where the where the problems are, and at the same time, suggest where you can make changes in the code, let's say, for example, to actually fix that and then resolve that ticket and then send it, attach the, you know, log or attach the message back to the customer, and close The ticket, right? So that's like a complex workflow, and if you were to do that so today, the way this thing happens is you have a regular I mean, the tool that we use is uses a regular LLM comes up with the chain of thought. In some cases, these tools also change every now and then chain of thought and goes and processes each one of these. Right? How does HRM change that?
Let me take you like, how like, how about like, knowledge work, which is for tasks inside an enterprise. Let's say, you know, I'll give you an example of a task that we have used bunch of AI tools for internally, which is, you know, we have we come like our customers. Imagine our customers like Samsung, or all those customers, they send us logs. Okay, like logs from cell phones, right? Error Logs. We have a task that goes you need to go fetch these logs from the tickets, then you need to fetch information from Salesforce, and then you need to process these logs for errors, and you need to draw diagrams and also suggest where the where the problems are, and at the same time, suggest where you can make changes in the code, let's say, for example, to actually fix that and then resolve that ticket and then send it, attach the, you know, log or attach the message back to the customer, and close The ticket, right? So that's like a complex workflow, and if you were to do that so today, the way this thing happens is you have a regular I mean, the tool that we use is uses a regular LLM comes up with the chain of thought. In some cases, these tools also change every now and then chain of thought and goes and processes each one of these. Right? How does HRM change that?
Let me take you like, how like, how about like, knowledge work, which is for tasks inside an enterprise. Let's say, you know, I'll give you an example of a task that we have used bunch of AI tools for internally, which is, you know, we have we come like our customers. Imagine our customers like Samsung, or all those customers, they send us logs. Okay, like logs from cell phones, right? Error Logs. We have a task that goes you need to go fetch these logs from the tickets, then you need to fetch information from Salesforce, and then you need to process these logs for errors, and you need to draw diagrams and also suggest where the where the problems are, and at the same time, suggest where you can make changes in the code, let's say, for example, to actually fix that and then resolve that ticket and then send it, attach the, you know, log or attach the message back to the customer, and close The ticket, right? So that's like a complex workflow, and if you were to do that so today, the way this thing happens is you have a regular I mean, the tool that we use is uses a regular LLM comes up with the chain of thought. In some cases, these tools also change every now and then chain of thought and goes and processes each one of these. Right? How does HRM change that?
Let me take you like, how like, how about like, knowledge work, which is for tasks inside an enterprise. Let's say, you know, I'll give you an example of a task that we have used bunch of AI tools for internally, which is, you know, we have we come like our customers. Imagine our customers like Samsung, or all those customers, they send us logs. Okay, like logs from cell phones, right? Error Logs. We have a task that goes you need to go fetch these logs from the tickets, then you need to fetch information from Salesforce, and then you need to process these logs for errors, and you need to draw diagrams and also suggest where the where the problems are, and at the same time, suggest where you can make changes in the code, let's say, for example, to actually fix that and then resolve that ticket and then send it, attach the, you know, log or attach the message back to the customer, and close The ticket, right? So that's like a complex workflow, and if you were to do that so today, the way this thing happens is you have a regular I mean, the tool that we use is uses a regular LLM comes up with the chain of thought. In some cases, these tools also change every now and then chain of thought and goes and processes each one of these. Right? How does HRM change that?
S Speaker 226:08I got it. Okay. Sorry to divert just just now for tool calling as you, as you were asking, we can embed it in any part of the stream that I was, I was showing on there, so that wouldn't be actually a problem, but I have to stress a little bit about the nature of this architecture, as Maryam, so what I've been showing on this deck, particular deck, is actually a very, Very straightforward reasoning model. So it doesn't, doesn't really do generative work as of right now, as for this model particular architecture, so it's forte is strictly on
I got it. Okay. Sorry to divert just just now for tool calling as you, as you were asking, we can embed it in any part of the stream that I was, I was showing on there, so that wouldn't be actually a problem, but I have to stress a little bit about the nature of this architecture, as Maryam, so what I've been showing on this deck, particular deck, is actually a very, Very straightforward reasoning model. So it doesn't, doesn't really do generative work as of right now, as for this model particular architecture, so it's forte is strictly on
I got it. Okay. Sorry to divert just just now for tool calling as you, as you were asking, we can embed it in any part of the stream that I was, I was showing on there, so that wouldn't be actually a problem, but I have to stress a little bit about the nature of this architecture, as Maryam, so what I've been showing on this deck, particular deck, is actually a very, Very straightforward reasoning model. So it doesn't, doesn't really do generative work as of right now, as for this model particular architecture, so it's forte is strictly on
I got it. Okay. Sorry to divert just just now for tool calling as you, as you were asking, we can embed it in any part of the stream that I was, I was showing on there, so that wouldn't be actually a problem, but I have to stress a little bit about the nature of this architecture, as Maryam, so what I've been showing on this deck, particular deck, is actually a very, Very straightforward reasoning model. So it doesn't, doesn't really do generative work as of right now, as for this model particular architecture, so it's forte is strictly on
S Speaker 226:55and, you know, reasoning and stuff. We are, as I was in talking to Priyesh. We are releasing a generative model that will be that will be very capable of what you just just mentioned, and it will be, right now we're looking at a very interesting way to do I'll make it sure. I know we're running out of time.
and, you know, reasoning and stuff. We are, as I was in talking to Priyesh. We are releasing a generative model that will be that will be very capable of what you just just mentioned, and it will be, right now we're looking at a very interesting way to do I'll make it sure. I know we're running out of time.
and, you know, reasoning and stuff. We are, as I was in talking to Priyesh. We are releasing a generative model that will be that will be very capable of what you just just mentioned, and it will be, right now we're looking at a very interesting way to do I'll make it sure. I know we're running out of time.
and, you know, reasoning and stuff. We are, as I was in talking to Priyesh. We are releasing a generative model that will be that will be very capable of what you just just mentioned, and it will be, right now we're looking at a very interesting way to do I'll make it sure. I know we're running out of time.
S Speaker 327:17You know what you are. Let me see. Let me see.
You know what you are. Let me see. Let me see.
You know what you are. Let me see. Let me see.
You know what you are. Let me see. Let me see.
27:23For me, this is very interesting. So
For me, this is very interesting. So
For me, this is very interesting. So
For me, this is very interesting. So
27:29I can stay for longer.
I can stay for longer.
I can stay for longer.
I can stay for longer.
S Speaker 327:30I can stay. I can stay for longer as well. I have like another 30 minutes, depending on how long you have, definitely interesting for me. So I can, okay,
I can stay. I can stay for longer as well. I have like another 30 minutes, depending on how long you have, definitely interesting for me. So I can, okay,
I can stay. I can stay for longer as well. I have like another 30 minutes, depending on how long you have, definitely interesting for me. So I can, okay,
I can stay. I can stay for longer as well. I have like another 30 minutes, depending on how long you have, definitely interesting for me. So I can, okay,
S Speaker 327:46Take your time to explain, because I want to understand. So here's what I want to do. I want to have some basic understanding such that I can translate. I can understand it myself. I can translate it to our guys on the research side and on the product side to get them excited enough to say, hey, let's this is interesting enough. Let's learn more. And then the goal would be, if this is interesting enough, because the idea is that this is significantly smaller models, then does this change the paradigm of being able to run models on device. So that's, that's, that's what I want to get to, but, but you but, but I need to have that much understanding that I can translate it. I'm not as technical as you are.
Take your time to explain, because I want to understand. So here's what I want to do. I want to have some basic understanding such that I can translate. I can understand it myself. I can translate it to our guys on the research side and on the product side to get them excited enough to say, hey, let's this is interesting enough. Let's learn more. And then the goal would be, if this is interesting enough, because the idea is that this is significantly smaller models, then does this change the paradigm of being able to run models on device. So that's, that's, that's what I want to get to, but, but you but, but I need to have that much understanding that I can translate it. I'm not as technical as you are.
Take your time to explain, because I want to understand. So here's what I want to do. I want to have some basic understanding such that I can translate. I can understand it myself. I can translate it to our guys on the research side and on the product side to get them excited enough to say, hey, let's this is interesting enough. Let's learn more. And then the goal would be, if this is interesting enough, because the idea is that this is significantly smaller models, then does this change the paradigm of being able to run models on device. So that's, that's, that's what I want to get to, but, but you but, but I need to have that much understanding that I can translate it. I'm not as technical as you are.
Take your time to explain, because I want to understand. So here's what I want to do. I want to have some basic understanding such that I can translate. I can understand it myself. I can translate it to our guys on the research side and on the product side to get them excited enough to say, hey, let's this is interesting enough. Let's learn more. And then the goal would be, if this is interesting enough, because the idea is that this is significantly smaller models, then does this change the paradigm of being able to run models on device. So that's, that's, that's what I want to get to, but, but you but, but I need to have that much understanding that I can translate it. I'm not as technical as you are.
S Speaker 228:32So yes, of course. I gotcha. I gotcha. I'll, I'll do my best. I'm sure you be very interested in this. So I'll go from from from the back all the way to back to the front. So I'll first introduce some of the what the four very important advantages that we can provide as as model right now. For first of all, it's a very, very small model. We're talking about 10s of 1000s times, or 30,000 30,000 times. That's smaller than age. You're
So yes, of course. I gotcha. I gotcha. I'll, I'll do my best. I'm sure you be very interested in this. So I'll go from from from the back all the way to back to the front. So I'll first introduce some of the what the four very important advantages that we can provide as as model right now. For first of all, it's a very, very small model. We're talking about 10s of 1000s times, or 30,000 30,000 times. That's smaller than age. You're
So yes, of course. I gotcha. I gotcha. I'll, I'll do my best. I'm sure you be very interested in this. So I'll go from from from the back all the way to back to the front. So I'll first introduce some of the what the four very important advantages that we can provide as as model right now. For first of all, it's a very, very small model. We're talking about 10s of 1000s times, or 30,000 30,000 times. That's smaller than age. You're
So yes, of course. I gotcha. I gotcha. I'll, I'll do my best. I'm sure you be very interested in this. So I'll go from from from the back all the way to back to the front. So I'll first introduce some of the what the four very important advantages that we can provide as as model right now. For first of all, it's a very, very small model. We're talking about 10s of 1000s times, or 30,000 30,000 times. That's smaller than age. You're
S Speaker 329:05saying only this is what? 200 or 2 million parameters?
saying only this is what? 200 or 2 million parameters?
saying only this is what? 200 or 2 million parameters?
saying only this is what? 200 or 2 million parameters?
S Speaker 229:11Is that 27 million? Yes, 27,000,020 7 million, okay, yeah, yes. It's a crazy small model with no pre training, no chain of thought required, and we can outperform some of the, you know, super huge, 200 plus b models in reasoning, for reasoning tasks, yes, for reasoning tasks, but very soon, with generative tasks, too. We are waiting for series eight to land and we're going to release that generative model. We're actually pretty much done with that, to be, to be very honest, but we're still keeping it just to wait for the next round, so it will be optimal for edge or lightweight employment. Right now, we're already collaborate, collaborating with some of the clinical research facilities and hospitals. They have a very strict policy on, you know, on their data not going to cloud. They have to run it on their local machines, like local servers and stuff. And to be honest, they have really shitty servers, and that's something we were really good at. Currently, our model can be deployed on any, basically, any Snapdragon chips, any phones, any laptops, you name it, with the same capability to do reasoning task. So that's, that's very, that's, that's the thing that's very excited about. You know, for our collaboration, my I myself, and I'm a Qualcomm fan, to be honest. Devices, good. Yeah, we actually already ran this model on, I think it's a agent three, just a few months back. It run pretty, very smoothly, and they can run on any CPU as well, because it's just so lightweight. And in fact, you can train this model on a laptop for for a laptop with Nvidia, 39 GPU, you only need to train it for 30 minutes, and it will be it will be done. You will be able to achieve a specialized it will be a specialized model for it, if you like, train it on medical data, for example, or if you train it on any other there. So that's how small this model is for it to train on a business laptop that doesn't have a discrete GPU. That will take the just a few more hours, but it will still do the job. So it's super small. And secondly, it's very sensitive numbers and patterns, like we already mentioned, some of the applications. It's optimal for time series, data training, like forecast prediction, numeric data processing right now, the reasoning model, as we're advertising right now, it's a symbolic model, so unfortunately, it doesn't talk to you like a chatbot, but it will take all different types of data, including image, by the way, inside I will do crazy pattern recognition, and you know, prediction and forecast we're doing, we're doing HR based quant trading, or doing HR based Climate Prediction, or doing HR based AI medical models. So that's something we're already working on. Thirdly, it has very good, complex, what we call multi skill problem reasoning capability. We already actually covered that I can do. It will be great for scenarios that require multi step or multi scale reasoning. Scenarios that requires, you know, calculations for 40 different variables in the 100 different variables over different timescale. That's something we're really, really good at. And for lastly, we're we have extraordinary small sample learning capability. So usually, if you train a model, any type of model you need, you might need, like, at least 100,000 samples or even millions of samples, to actually achieve a optimal result. But for us, it will reach, actually the about the same level of performance with just 1000s, even hundreds, of samples that I work and I'll go back to some of the examples.
Is that 27 million? Yes, 27,000,020 7 million, okay, yeah, yes. It's a crazy small model with no pre training, no chain of thought required, and we can outperform some of the, you know, super huge, 200 plus b models in reasoning, for reasoning tasks, yes, for reasoning tasks, but very soon, with generative tasks, too. We are waiting for series eight to land and we're going to release that generative model. We're actually pretty much done with that, to be, to be very honest, but we're still keeping it just to wait for the next round, so it will be optimal for edge or lightweight employment. Right now, we're already collaborate, collaborating with some of the clinical research facilities and hospitals. They have a very strict policy on, you know, on their data not going to cloud. They have to run it on their local machines, like local servers and stuff. And to be honest, they have really shitty servers, and that's something we were really good at. Currently, our model can be deployed on any, basically, any Snapdragon chips, any phones, any laptops, you name it, with the same capability to do reasoning task. So that's, that's very, that's, that's the thing that's very excited about. You know, for our collaboration, my I myself, and I'm a Qualcomm fan, to be honest. Devices, good. Yeah, we actually already ran this model on, I think it's a agent three, just a few months back. It run pretty, very smoothly, and they can run on any CPU as well, because it's just so lightweight. And in fact, you can train this model on a laptop for for a laptop with Nvidia, 39 GPU, you only need to train it for 30 minutes, and it will be it will be done. You will be able to achieve a specialized it will be a specialized model for it, if you like, train it on medical data, for example, or if you train it on any other there. So that's how small this model is for it to train on a business laptop that doesn't have a discrete GPU. That will take the just a few more hours, but it will still do the job. So it's super small. And secondly, it's very sensitive numbers and patterns, like we already mentioned, some of the applications. It's optimal for time series, data training, like forecast prediction, numeric data processing right now, the reasoning model, as we're advertising right now, it's a symbolic model, so unfortunately, it doesn't talk to you like a chatbot, but it will take all different types of data, including image, by the way, inside I will do crazy pattern recognition, and you know, prediction and forecast we're doing, we're doing HR based quant trading, or doing HR based Climate Prediction, or doing HR based AI medical models. So that's something we're already working on. Thirdly, it has very good, complex, what we call multi skill problem reasoning capability. We already actually covered that I can do. It will be great for scenarios that require multi step or multi scale reasoning. Scenarios that requires, you know, calculations for 40 different variables in the 100 different variables over different timescale. That's something we're really, really good at. And for lastly, we're we have extraordinary small sample learning capability. So usually, if you train a model, any type of model you need, you might need, like, at least 100,000 samples or even millions of samples, to actually achieve a optimal result. But for us, it will reach, actually the about the same level of performance with just 1000s, even hundreds, of samples that I work and I'll go back to some of the examples.
Is that 27 million? Yes, 27,000,020 7 million, okay, yeah, yes. It's a crazy small model with no pre training, no chain of thought required, and we can outperform some of the, you know, super huge, 200 plus b models in reasoning, for reasoning tasks, yes, for reasoning tasks, but very soon, with generative tasks, too. We are waiting for series eight to land and we're going to release that generative model. We're actually pretty much done with that, to be, to be very honest, but we're still keeping it just to wait for the next round, so it will be optimal for edge or lightweight employment. Right now, we're already collaborate, collaborating with some of the clinical research facilities and hospitals. They have a very strict policy on, you know, on their data not going to cloud. They have to run it on their local machines, like local servers and stuff. And to be honest, they have really shitty servers, and that's something we were really good at. Currently, our model can be deployed on any, basically, any Snapdragon chips, any phones, any laptops, you name it, with the same capability to do reasoning task. So that's, that's very, that's, that's the thing that's very excited about. You know, for our collaboration, my I myself, and I'm a Qualcomm fan, to be honest. Devices, good. Yeah, we actually already ran this model on, I think it's a agent three, just a few months back. It run pretty, very smoothly, and they can run on any CPU as well, because it's just so lightweight. And in fact, you can train this model on a laptop for for a laptop with Nvidia, 39 GPU, you only need to train it for 30 minutes, and it will be it will be done. You will be able to achieve a specialized it will be a specialized model for it, if you like, train it on medical data, for example, or if you train it on any other there. So that's how small this model is for it to train on a business laptop that doesn't have a discrete GPU. That will take the just a few more hours, but it will still do the job. So it's super small. And secondly, it's very sensitive numbers and patterns, like we already mentioned, some of the applications. It's optimal for time series, data training, like forecast prediction, numeric data processing right now, the reasoning model, as we're advertising right now, it's a symbolic model, so unfortunately, it doesn't talk to you like a chatbot, but it will take all different types of data, including image, by the way, inside I will do crazy pattern recognition, and you know, prediction and forecast we're doing, we're doing HR based quant trading, or doing HR based Climate Prediction, or doing HR based AI medical models. So that's something we're already working on. Thirdly, it has very good, complex, what we call multi skill problem reasoning capability. We already actually covered that I can do. It will be great for scenarios that require multi step or multi scale reasoning. Scenarios that requires, you know, calculations for 40 different variables in the 100 different variables over different timescale. That's something we're really, really good at. And for lastly, we're we have extraordinary small sample learning capability. So usually, if you train a model, any type of model you need, you might need, like, at least 100,000 samples or even millions of samples, to actually achieve a optimal result. But for us, it will reach, actually the about the same level of performance with just 1000s, even hundreds, of samples that I work and I'll go back to some of the examples.
Is that 27 million? Yes, 27,000,020 7 million, okay, yeah, yes. It's a crazy small model with no pre training, no chain of thought required, and we can outperform some of the, you know, super huge, 200 plus b models in reasoning, for reasoning tasks, yes, for reasoning tasks, but very soon, with generative tasks, too. We are waiting for series eight to land and we're going to release that generative model. We're actually pretty much done with that, to be, to be very honest, but we're still keeping it just to wait for the next round, so it will be optimal for edge or lightweight employment. Right now, we're already collaborate, collaborating with some of the clinical research facilities and hospitals. They have a very strict policy on, you know, on their data not going to cloud. They have to run it on their local machines, like local servers and stuff. And to be honest, they have really shitty servers, and that's something we were really good at. Currently, our model can be deployed on any, basically, any Snapdragon chips, any phones, any laptops, you name it, with the same capability to do reasoning task. So that's, that's very, that's, that's the thing that's very excited about. You know, for our collaboration, my I myself, and I'm a Qualcomm fan, to be honest. Devices, good. Yeah, we actually already ran this model on, I think it's a agent three, just a few months back. It run pretty, very smoothly, and they can run on any CPU as well, because it's just so lightweight. And in fact, you can train this model on a laptop for for a laptop with Nvidia, 39 GPU, you only need to train it for 30 minutes, and it will be it will be done. You will be able to achieve a specialized it will be a specialized model for it, if you like, train it on medical data, for example, or if you train it on any other there. So that's how small this model is for it to train on a business laptop that doesn't have a discrete GPU. That will take the just a few more hours, but it will still do the job. So it's super small. And secondly, it's very sensitive numbers and patterns, like we already mentioned, some of the applications. It's optimal for time series, data training, like forecast prediction, numeric data processing right now, the reasoning model, as we're advertising right now, it's a symbolic model, so unfortunately, it doesn't talk to you like a chatbot, but it will take all different types of data, including image, by the way, inside I will do crazy pattern recognition, and you know, prediction and forecast we're doing, we're doing HR based quant trading, or doing HR based Climate Prediction, or doing HR based AI medical models. So that's something we're already working on. Thirdly, it has very good, complex, what we call multi skill problem reasoning capability. We already actually covered that I can do. It will be great for scenarios that require multi step or multi scale reasoning. Scenarios that requires, you know, calculations for 40 different variables in the 100 different variables over different timescale. That's something we're really, really good at. And for lastly, we're we have extraordinary small sample learning capability. So usually, if you train a model, any type of model you need, you might need, like, at least 100,000 samples or even millions of samples, to actually achieve a optimal result. But for us, it will reach, actually the about the same level of performance with just 1000s, even hundreds, of samples that I work and I'll go back to some of the examples.
S Speaker 333:18Yeah, on the when you have the generative model
Yeah, on the when you have the generative model
Yeah, on the when you have the generative model
Yeah, on the when you have the generative model
S Speaker 333:27you know, in terms of benchmarks, what are you targeting like for the first, what would be the size of the generative model, and then approximately, and then, what would you target in terms of bench like the same, maybe rkgi is more extreme benchmark, but even rkgi, or like the generative benchmarks,
you know, in terms of benchmarks, what are you targeting like for the first, what would be the size of the generative model, and then approximately, and then, what would you target in terms of bench like the same, maybe rkgi is more extreme benchmark, but even rkgi, or like the generative benchmarks,
you know, in terms of benchmarks, what are you targeting like for the first, what would be the size of the generative model, and then approximately, and then, what would you target in terms of bench like the same, maybe rkgi is more extreme benchmark, but even rkgi, or like the generative benchmarks,
you know, in terms of benchmarks, what are you targeting like for the first, what would be the size of the generative model, and then approximately, and then, what would you target in terms of bench like the same, maybe rkgi is more extreme benchmark, but even rkgi, or like the generative benchmarks,
S Speaker 233:52so for the generative model, it will actually be a general model, instead of the general reasoning model as we as we call it Right now it will actually be able to do language, image, even video and stuff.
so for the generative model, it will actually be a general model, instead of the general reasoning model as we as we call it Right now it will actually be able to do language, image, even video and stuff.
so for the generative model, it will actually be a general model, instead of the general reasoning model as we as we call it Right now it will actually be able to do language, image, even video and stuff.
so for the generative model, it will actually be a general model, instead of the general reasoning model as we as we call it Right now it will actually be able to do language, image, even video and stuff.
34:06Oh, really, yes,
S Speaker 334:08modality with that has no you have, you know, like how state space models today, they're good for speech, but they're not that good for text or images. Diffusion is good for images and video, and it's only okay for text, but in this case, are you think like, Are there any limitations, and then, fundamentally, is this like a different architecture from transformers altogether?
modality with that has no you have, you know, like how state space models today, they're good for speech, but they're not that good for text or images. Diffusion is good for images and video, and it's only okay for text, but in this case, are you think like, Are there any limitations, and then, fundamentally, is this like a different architecture from transformers altogether?
modality with that has no you have, you know, like how state space models today, they're good for speech, but they're not that good for text or images. Diffusion is good for images and video, and it's only okay for text, but in this case, are you think like, Are there any limitations, and then, fundamentally, is this like a different architecture from transformers altogether?
modality with that has no you have, you know, like how state space models today, they're good for speech, but they're not that good for text or images. Diffusion is good for images and video, and it's only okay for text, but in this case, are you think like, Are there any limitations, and then, fundamentally, is this like a different architecture from transformers altogether?
S Speaker 234:44Like, yes, this is a different architecture from transforming, totally different and all. And the generative model will be based on the very same architecture, but there'll be a few changes to it, but it will be still the same HRM architecture, the one we're talking right now.
Like, yes, this is a different architecture from transforming, totally different and all. And the generative model will be based on the very same architecture, but there'll be a few changes to it, but it will be still the same HRM architecture, the one we're talking right now.
Like, yes, this is a different architecture from transforming, totally different and all. And the generative model will be based on the very same architecture, but there'll be a few changes to it, but it will be still the same HRM architecture, the one we're talking right now.
Like, yes, this is a different architecture from transforming, totally different and all. And the generative model will be based on the very same architecture, but there'll be a few changes to it, but it will be still the same HRM architecture, the one we're talking right now.
35:03And for the generator model,
And for the generator model,
And for the generator model,
And for the generator model,
S Speaker 235:07let's see how passionate faces. There are things that I might not be able to disclose as of right now, but I'll see how if I can explain this. So for generator model, you know, we're actually working on what we call the architecture side, it's more basic than the model themselves. So for for when I say it's a general architecture, it can be, actually the very same architecture can be made into any sorts of models, like, for example, video model, a image model, and, you know, text model, depending on what type of data you provided to train it. That's so the model is itself. It's very general. You can put in literally any task, like, like, right now, we can put it in quant trading, put it in climate prediction, when put in medical. So that's, that's the that's how the mod the new generator model can do. It can do basically any sort of generation works. I hope that that explained that. And for model, I can't say for sure, but right now, we're looking at at the size, about 8080, M, 80 million. That's it. For end of generation, we haven't, we haven't get to, you know, video generation yet. But for inner generation, it will be about a size for
let's see how passionate faces. There are things that I might not be able to disclose as of right now, but I'll see how if I can explain this. So for generator model, you know, we're actually working on what we call the architecture side, it's more basic than the model themselves. So for for when I say it's a general architecture, it can be, actually the very same architecture can be made into any sorts of models, like, for example, video model, a image model, and, you know, text model, depending on what type of data you provided to train it. That's so the model is itself. It's very general. You can put in literally any task, like, like, right now, we can put it in quant trading, put it in climate prediction, when put in medical. So that's, that's the that's how the mod the new generator model can do. It can do basically any sort of generation works. I hope that that explained that. And for model, I can't say for sure, but right now, we're looking at at the size, about 8080, M, 80 million. That's it. For end of generation, we haven't, we haven't get to, you know, video generation yet. But for inner generation, it will be about a size for
let's see how passionate faces. There are things that I might not be able to disclose as of right now, but I'll see how if I can explain this. So for generator model, you know, we're actually working on what we call the architecture side, it's more basic than the model themselves. So for for when I say it's a general architecture, it can be, actually the very same architecture can be made into any sorts of models, like, for example, video model, a image model, and, you know, text model, depending on what type of data you provided to train it. That's so the model is itself. It's very general. You can put in literally any task, like, like, right now, we can put it in quant trading, put it in climate prediction, when put in medical. So that's, that's the that's how the mod the new generator model can do. It can do basically any sort of generation works. I hope that that explained that. And for model, I can't say for sure, but right now, we're looking at at the size, about 8080, M, 80 million. That's it. For end of generation, we haven't, we haven't get to, you know, video generation yet. But for inner generation, it will be about a size for
let's see how passionate faces. There are things that I might not be able to disclose as of right now, but I'll see how if I can explain this. So for generator model, you know, we're actually working on what we call the architecture side, it's more basic than the model themselves. So for for when I say it's a general architecture, it can be, actually the very same architecture can be made into any sorts of models, like, for example, video model, a image model, and, you know, text model, depending on what type of data you provided to train it. That's so the model is itself. It's very general. You can put in literally any task, like, like, right now, we can put it in quant trading, put it in climate prediction, when put in medical. So that's, that's the that's how the mod the new generator model can do. It can do basically any sort of generation works. I hope that that explained that. And for model, I can't say for sure, but right now, we're looking at at the size, about 8080, M, 80 million. That's it. For end of generation, we haven't, we haven't get to, you know, video generation yet. But for inner generation, it will be about a size for
S Speaker 336:29ADA, this includes the reasoning model, or this would include that will
ADA, this includes the reasoning model, or this would include that will
ADA, this includes the reasoning model, or this would include that will
ADA, this includes the reasoning model, or this would include that will
S Speaker 236:34that will retain the reasoning inside of the model. Yes, it will have the same level of reasoning as the model we're looking at right
that will retain the reasoning inside of the model. Yes, it will have the same level of reasoning as the model we're looking at right
that will retain the reasoning inside of the model. Yes, it will have the same level of reasoning as the model we're looking at right
that will retain the reasoning inside of the model. Yes, it will have the same level of reasoning as the model we're looking at right
S Speaker 336:43now, and that model, that generative model, plus, on this you said, is only image
now, and that model, that generative model, plus, on this you said, is only image
now, and that model, that generative model, plus, on this you said, is only image
now, and that model, that generative model, plus, on this you said, is only image
S Speaker 236:50right for the 80 million sizes. And
right for the 80 million sizes. And
right for the 80 million sizes. And
right for the 80 million sizes. And
36:53what about, like, text plus?
what about, like, text plus?
what about, like, text plus?
what about, like, text plus?
S Speaker 236:57I can't say for sure, because we do approximately. Do you know? So for tests, it actually depends on how much knowledge you will actually put into the model. But by
I can't say for sure, because we do approximately. Do you know? So for tests, it actually depends on how much knowledge you will actually put into the model. But by
I can't say for sure, because we do approximately. Do you know? So for tests, it actually depends on how much knowledge you will actually put into the model. But by
I can't say for sure, because we do approximately. Do you know? So for tests, it actually depends on how much knowledge you will actually put into the model. But by
S Speaker 337:12you have to be on par with the large language models of capabilities, then how big
you have to be on par with the large language models of capabilities, then how big
you have to be on par with the large language models of capabilities, then how big
you have to be on par with the large language models of capabilities, then how big
S Speaker 237:24it gets? So we're looking at a scale about a few hundreds. So for for based to be model, based on the HR and architecture, it will be on par with about a, let's say a few one, about a 1t model for, for, for the old transformer architectures. That's something we can we are predicting for, for the performance at least a few hundreds scale. Well, just just to clarify, it doesn't mean the amount of knowledge that's inside the model, but it will be amount of the amount of performance the model can provide. If you you know, if you provide it with an internet connection, right? It will, you'll be able to fetch all the other knowledge from so it would be what we are, not seeing that as a bigger problem, but for the generation, we haven't get that far with text yet, because it's something that we're not really putting our efforts in because, you know, for chat, GPT and stuff, they're already pretty good at text generation. But for image, we do think there's still a a lot of improvement that can be still, can be made. So we're starting with image first, and then we're going to do text and then video. Well, we are also collaborating with some of the companies. We will be on on the data providing side, because we do, we will, we will need a lot of data for text. That's something you just can't get around with to be honest. So but for image generation, we are predicting the model to be about 80 million in size, but it can also be about 500 times faster than a VIP model, visual transformer model, and that means you will be able to reach about a real time performance for image generation, it will be really big for, you know, embody AI stuff. They really want something that's running on real time for the control theory to work. That's actually the VLA are based on the generation for the models. So that's something we might be able to really track down. That's something we're looking forward to. And also on top of that, just imagine a nano banana, but running locally on it, though, there will be, that'll be also very, very interesting.
it gets? So we're looking at a scale about a few hundreds. So for for based to be model, based on the HR and architecture, it will be on par with about a, let's say a few one, about a 1t model for, for, for the old transformer architectures. That's something we can we are predicting for, for the performance at least a few hundreds scale. Well, just just to clarify, it doesn't mean the amount of knowledge that's inside the model, but it will be amount of the amount of performance the model can provide. If you you know, if you provide it with an internet connection, right? It will, you'll be able to fetch all the other knowledge from so it would be what we are, not seeing that as a bigger problem, but for the generation, we haven't get that far with text yet, because it's something that we're not really putting our efforts in because, you know, for chat, GPT and stuff, they're already pretty good at text generation. But for image, we do think there's still a a lot of improvement that can be still, can be made. So we're starting with image first, and then we're going to do text and then video. Well, we are also collaborating with some of the companies. We will be on on the data providing side, because we do, we will, we will need a lot of data for text. That's something you just can't get around with to be honest. So but for image generation, we are predicting the model to be about 80 million in size, but it can also be about 500 times faster than a VIP model, visual transformer model, and that means you will be able to reach about a real time performance for image generation, it will be really big for, you know, embody AI stuff. They really want something that's running on real time for the control theory to work. That's actually the VLA are based on the generation for the models. So that's something we might be able to really track down. That's something we're looking forward to. And also on top of that, just imagine a nano banana, but running locally on it, though, there will be, that'll be also very, very interesting.
it gets? So we're looking at a scale about a few hundreds. So for for based to be model, based on the HR and architecture, it will be on par with about a, let's say a few one, about a 1t model for, for, for the old transformer architectures. That's something we can we are predicting for, for the performance at least a few hundreds scale. Well, just just to clarify, it doesn't mean the amount of knowledge that's inside the model, but it will be amount of the amount of performance the model can provide. If you you know, if you provide it with an internet connection, right? It will, you'll be able to fetch all the other knowledge from so it would be what we are, not seeing that as a bigger problem, but for the generation, we haven't get that far with text yet, because it's something that we're not really putting our efforts in because, you know, for chat, GPT and stuff, they're already pretty good at text generation. But for image, we do think there's still a a lot of improvement that can be still, can be made. So we're starting with image first, and then we're going to do text and then video. Well, we are also collaborating with some of the companies. We will be on on the data providing side, because we do, we will, we will need a lot of data for text. That's something you just can't get around with to be honest. So but for image generation, we are predicting the model to be about 80 million in size, but it can also be about 500 times faster than a VIP model, visual transformer model, and that means you will be able to reach about a real time performance for image generation, it will be really big for, you know, embody AI stuff. They really want something that's running on real time for the control theory to work. That's actually the VLA are based on the generation for the models. So that's something we might be able to really track down. That's something we're looking forward to. And also on top of that, just imagine a nano banana, but running locally on it, though, there will be, that'll be also very, very interesting.
it gets? So we're looking at a scale about a few hundreds. So for for based to be model, based on the HR and architecture, it will be on par with about a, let's say a few one, about a 1t model for, for, for the old transformer architectures. That's something we can we are predicting for, for the performance at least a few hundreds scale. Well, just just to clarify, it doesn't mean the amount of knowledge that's inside the model, but it will be amount of the amount of performance the model can provide. If you you know, if you provide it with an internet connection, right? It will, you'll be able to fetch all the other knowledge from so it would be what we are, not seeing that as a bigger problem, but for the generation, we haven't get that far with text yet, because it's something that we're not really putting our efforts in because, you know, for chat, GPT and stuff, they're already pretty good at text generation. But for image, we do think there's still a a lot of improvement that can be still, can be made. So we're starting with image first, and then we're going to do text and then video. Well, we are also collaborating with some of the companies. We will be on on the data providing side, because we do, we will, we will need a lot of data for text. That's something you just can't get around with to be honest. So but for image generation, we are predicting the model to be about 80 million in size, but it can also be about 500 times faster than a VIP model, visual transformer model, and that means you will be able to reach about a real time performance for image generation, it will be really big for, you know, embody AI stuff. They really want something that's running on real time for the control theory to work. That's actually the VLA are based on the generation for the models. So that's something we might be able to really track down. That's something we're looking forward to. And also on top of that, just imagine a nano banana, but running locally on it, though, there will be, that'll be also very, very interesting.
S Speaker 339:56I mean that, see, the key is bill to be able to do it at the same quality. Because, see, fundamentally, what happens is the users are happy taking, paying the cost, the marginal cost, and the network round trip, as long as the quality is significantly superior. Users don't care about, you know, running it on the edge as much, right? But if you can provide almost the same quality as the one running in the cloud, then I think like edge has so many other benefits of cost, privacy, latency, that there's no doubt that people can use edge then at that point, and we I mean what you're describing. Me think about it right that next our current gen Snapdragon processors, you should be able to run two to 3 billion parameter models easily on it, and then the next gen Snapdragon processors will be like significantly more so. And then the arpcs, you can already run like 20 billion parameters. So I think like, then you can but, but the granite, the quality has to be similar,
I mean that, see, the key is bill to be able to do it at the same quality. Because, see, fundamentally, what happens is the users are happy taking, paying the cost, the marginal cost, and the network round trip, as long as the quality is significantly superior. Users don't care about, you know, running it on the edge as much, right? But if you can provide almost the same quality as the one running in the cloud, then I think like edge has so many other benefits of cost, privacy, latency, that there's no doubt that people can use edge then at that point, and we I mean what you're describing. Me think about it right that next our current gen Snapdragon processors, you should be able to run two to 3 billion parameter models easily on it, and then the next gen Snapdragon processors will be like significantly more so. And then the arpcs, you can already run like 20 billion parameters. So I think like, then you can but, but the granite, the quality has to be similar,
I mean that, see, the key is bill to be able to do it at the same quality. Because, see, fundamentally, what happens is the users are happy taking, paying the cost, the marginal cost, and the network round trip, as long as the quality is significantly superior. Users don't care about, you know, running it on the edge as much, right? But if you can provide almost the same quality as the one running in the cloud, then I think like edge has so many other benefits of cost, privacy, latency, that there's no doubt that people can use edge then at that point, and we I mean what you're describing. Me think about it right that next our current gen Snapdragon processors, you should be able to run two to 3 billion parameter models easily on it, and then the next gen Snapdragon processors will be like significantly more so. And then the arpcs, you can already run like 20 billion parameters. So I think like, then you can but, but the granite, the quality has to be similar,
I mean that, see, the key is bill to be able to do it at the same quality. Because, see, fundamentally, what happens is the users are happy taking, paying the cost, the marginal cost, and the network round trip, as long as the quality is significantly superior. Users don't care about, you know, running it on the edge as much, right? But if you can provide almost the same quality as the one running in the cloud, then I think like edge has so many other benefits of cost, privacy, latency, that there's no doubt that people can use edge then at that point, and we I mean what you're describing. Me think about it right that next our current gen Snapdragon processors, you should be able to run two to 3 billion parameter models easily on it, and then the next gen Snapdragon processors will be like significantly more so. And then the arpcs, you can already run like 20 billion parameters. So I think like, then you can but, but the granite, the quality has to be similar,
S Speaker 241:27Let me tell you this. I can't guarantee anything as of right now, because, you know, it's still in the developing process. But there's something called the FID for video effort for image generations. It's basically a it's like basically a benchmark. It's like the Weissman score for a compression fid benchmark. It's not really a benchmark, but it's more like a measurement for how, how accurate a image generation model is. Okay, let me, let me look it up.
Let me tell you this. I can't guarantee anything as of right now, because, you know, it's still in the developing process. But there's something called the FID for video effort for image generations. It's basically a it's like basically a benchmark. It's like the Weissman score for a compression fid benchmark. It's not really a benchmark, but it's more like a measurement for how, how accurate a image generation model is. Okay, let me, let me look it up.
Let me tell you this. I can't guarantee anything as of right now, because, you know, it's still in the developing process. But there's something called the FID for video effort for image generations. It's basically a it's like basically a benchmark. It's like the Weissman score for a compression fid benchmark. It's not really a benchmark, but it's more like a measurement for how, how accurate a image generation model is. Okay, let me, let me look it up.
Let me tell you this. I can't guarantee anything as of right now, because, you know, it's still in the developing process. But there's something called the FID for video effort for image generations. It's basically a it's like basically a benchmark. It's like the Weissman score for a compression fid benchmark. It's not really a benchmark, but it's more like a measurement for how, how accurate a image generation model is. Okay, let me, let me look it up.
42:01Fresh inception, distance.
Fresh inception, distance.
Fresh inception, distance.
Fresh inception, distance.
42:04Yes, I think that's the one.
Yes, I think that's the one.
Yes, I think that's the one.
Yes, I think that's the one.
42:07Oh, that's an interesting I know about that. Okay, so,
Oh, that's an interesting I know about that. Okay, so,
Oh, that's an interesting I know about that. Okay, so,
Oh, that's an interesting I know about that. Okay, so,
42:12so we have reached about a
so we have reached about a
so we have reached about a
so we have reached about a
S Speaker 242:16right now I can say I can very safely conclude that we are on a score under five. So that's a very, very good score for for us, model that size, and we are still working on to shrink that even more
right now I can say I can very safely conclude that we are on a score under five. So that's a very, very good score for for us, model that size, and we are still working on to shrink that even more
right now I can say I can very safely conclude that we are on a score under five. So that's a very, very good score for for us, model that size, and we are still working on to shrink that even more
right now I can say I can very safely conclude that we are on a score under five. So that's a very, very good score for for us, model that size, and we are still working on to shrink that even more
S Speaker 242:34comparison for that's about like two or three, I think. So it's pretty close. It's pretty close.
comparison for that's about like two or three, I think. So it's pretty close. It's pretty close.
comparison for that's about like two or three, I think. So it's pretty close. It's pretty close.
comparison for that's about like two or three, I think. So it's pretty close. It's pretty close.
S Speaker 342:42Okay, okay, good, that's good. Yeah, this is exciting. So, in terms of so you guys started last year, you raised seed funding.
Okay, okay, good, that's good. Yeah, this is exciting. So, in terms of so you guys started last year, you raised seed funding.
Okay, okay, good, that's good. Yeah, this is exciting. So, in terms of so you guys started last year, you raised seed funding.
Okay, okay, good, that's good. Yeah, this is exciting. So, in terms of so you guys started last year, you raised seed funding.
S Speaker 242:54Yes, we raised, we raised a 22 million seed round. Yeah, and we're about series eight for generative model. It's vertex. They're, they're from Singapore. That's why we based in Singapore. But I'm sure they're pretty, pretty open for, you know, the further First up, and we're looking to move in anyways. But you know, I still have to to actually talk to them and talk to our own investors and
Yes, we raised, we raised a 22 million seed round. Yeah, and we're about series eight for generative model. It's vertex. They're, they're from Singapore. That's why we based in Singapore. But I'm sure they're pretty, pretty open for, you know, the further First up, and we're looking to move in anyways. But you know, I still have to to actually talk to them and talk to our own investors and
Yes, we raised, we raised a 22 million seed round. Yeah, and we're about series eight for generative model. It's vertex. They're, they're from Singapore. That's why we based in Singapore. But I'm sure they're pretty, pretty open for, you know, the further First up, and we're looking to move in anyways. But you know, I still have to to actually talk to them and talk to our own investors and
Yes, we raised, we raised a 22 million seed round. Yeah, and we're about series eight for generative model. It's vertex. They're, they're from Singapore. That's why we based in Singapore. But I'm sure they're pretty, pretty open for, you know, the further First up, and we're looking to move in anyways. But you know, I still have to to actually talk to them and talk to our own investors and
43:27founders as well.
S Speaker 243:29So we were vertex, was the lead other, including the Somi tomo Sumitomo group in from Japan. Yeah, those are some of the major, major quarters from the next round,
So we were vertex, was the lead other, including the Somi tomo Sumitomo group in from Japan. Yeah, those are some of the major, major quarters from the next round,
So we were vertex, was the lead other, including the Somi tomo Sumitomo group in from Japan. Yeah, those are some of the major, major quarters from the next round,
So we were vertex, was the lead other, including the Somi tomo Sumitomo group in from Japan. Yeah, those are some of the major, major quarters from the next round,
S Speaker 343:49from the last round. And then right now you're raising about 5200 is that
from the last round. And then right now you're raising about 5200 is that
from the last round. And then right now you're raising about 5200 is that
from the last round. And then right now you're raising about 5200 is that
43:53yes, or looking for 5200
yes, or looking for 5200
yes, or looking for 5200
yes, or looking for 5200
43:56that's more for the next way be able to
that's more for the next way be able to
that's more for the next way be able to
that's more for the next way be able to
43:58move to the US before the round.
move to the US before the round.
move to the US before the round.
move to the US before the round.
S Speaker 244:03I can't say for sure, because that's happening. The round is happening pretty fast. It might just happen somewhere, and we're looking into somewhere in December or January.
I can't say for sure, because that's happening. The round is happening pretty fast. It might just happen somewhere, and we're looking into somewhere in December or January.
I can't say for sure, because that's happening. The round is happening pretty fast. It might just happen somewhere, and we're looking into somewhere in December or January.
I can't say for sure, because that's happening. The round is happening pretty fast. It might just happen somewhere, and we're looking into somewhere in December or January.
44:12We will, I have to, I have to talk. You already have a lead.
We will, I have to, I have to talk. You already have a lead.
We will, I have to, I have to talk. You already have a lead.
We will, I have to, I have to talk. You already have a lead.
S Speaker 244:17No, not yet. We haven't, to be honest, we haven't officially kick start this round yet, because
No, not yet. We haven't, to be honest, we haven't officially kick start this round yet, because
No, not yet. We haven't, to be honest, we haven't officially kick start this round yet, because
No, not yet. We haven't, to be honest, we haven't officially kick start this round yet, because
S Speaker 344:22yeah, it'll happen. It can happen fast. The only thing is for us
yeah, it'll happen. It can happen fast. The only thing is for us
yeah, it'll happen. It can happen fast. The only thing is for us
yeah, it'll happen. It can happen fast. The only thing is for us
44:28to be loved is because, for us to come
to be loved is because, for us to come
to be loved is because, for us to come
to be loved is because, for us to come
44:33we need to be like a US
we need to be like a US
we need to be like a US
we need to be like a US
S Speaker 244:35company. So just a just little clarification on that, can we be a US based company, but with, you know, so with offices overseas, is that?
company. So just a just little clarification on that, can we be a US based company, but with, you know, so with offices overseas, is that?
company. So just a just little clarification on that, can we be a US based company, but with, you know, so with offices overseas, is that?
company. So just a just little clarification on that, can we be a US based company, but with, you know, so with offices overseas, is that?
S Speaker 344:48Or you can have offices overseas, okay, but eventually you'll have to move HQ here. Okay, got it? Okay,
Or you can have offices overseas, okay, but eventually you'll have to move HQ here. Okay, got it? Okay,
Or you can have offices overseas, okay, but eventually you'll have to move HQ here. Okay, got it? Okay,
Or you can have offices overseas, okay, but eventually you'll have to move HQ here. Okay, got it? Okay,
S Speaker 244:57I'll talk to one about this. We're very excited for this, but maybe our
I'll talk to one about this. We're very excited for this, but maybe our
I'll talk to one about this. We're very excited for this, but maybe our
I'll talk to one about this. We're very excited for this, but maybe our