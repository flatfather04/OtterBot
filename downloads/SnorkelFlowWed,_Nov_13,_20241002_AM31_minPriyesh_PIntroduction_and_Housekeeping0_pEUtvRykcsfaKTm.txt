Meeting: SnorkelFlow
Wed, Nov 13, 2024
10:02 AM
31 min
Priyesh P
Introduction and Housekeeping
0:00
Overview 
URL: https://otter.ai/u/pEUtvRykcsfaKTmsl056B4HMNoc
Downloaded: 2025-12-22T13:45:29.073974
Method: text_extraction
============================================================

S Speaker 10:00Questions, whether they are regarding the demo or start with flow or AIML in general, we're happy to answer your questions and discuss these topics. Couple little things. First, you will see the Q and A button at the bottom. Please feel free to ask questions throughout the demo. Michael and I will be sure to get as many of those as we can when we wrap up. And there is a short survey when it's finished. And we always appreciate if we can take just a minute or two to fill out that survey. It lets us know if we're doing well, and certainly areas where we can do better in the future. But before we have Chris jump into the demo first. Michael, do you want to introduce yourself to
Questions, whether they are regarding the demo or start with flow or AIML in general, we're happy to answer your questions and discuss these topics. Couple little things. First, you will see the Q and A button at the bottom. Please feel free to ask questions throughout the demo. Michael and I will be sure to get as many of those as we can when we wrap up. And there is a short survey when it's finished. And we always appreciate if we can take just a minute or two to fill out that survey. It lets us know if we're doing well, and certainly areas where we can do better in the future. But before we have Chris jump into the demo first. Michael, do you want to introduce yourself to
Questions, whether they are regarding the demo or start with flow or AIML in general, we're happy to answer your questions and discuss these topics. Couple little things. First, you will see the Q and A button at the bottom. Please feel free to ask questions throughout the demo. Michael and I will be sure to get as many of those as we can when we wrap up. And there is a short survey when it's finished. And we always appreciate if we can take just a minute or two to fill out that survey. It lets us know if we're doing well, and certainly areas where we can do better in the future. But before we have Chris jump into the demo first. Michael, do you want to introduce yourself to
Questions, whether they are regarding the demo or start with flow or AIML in general, we're happy to answer your questions and discuss these topics. Couple little things. First, you will see the Q and A button at the bottom. Please feel free to ask questions throughout the demo. Michael and I will be sure to get as many of those as we can when we wrap up. And there is a short survey when it's finished. And we always appreciate if we can take just a minute or two to fill out that survey. It lets us know if we're doing well, and certainly areas where we can do better in the future. But before we have Chris jump into the demo first. Michael, do you want to introduce yourself to
S Speaker 11:01great. Well, with that said, let's go ahead and take a look at Chris's demo class by chance. Today I'm going
great. Well, with that said, let's go ahead and take a look at Chris's demo class by chance. Today I'm going
great. Well, with that said, let's go ahead and take a look at Chris's demo class by chance. Today I'm going
great. Well, with that said, let's go ahead and take a look at Chris's demo class by chance. Today I'm going
S Speaker 31:19to give a demonstration on the snorkel flow platform around chat bot intent classification. And before I get started with that, I just want to introduce a few slides to give you some background on snorkel, and then set up the context for the challenge that we're going to solve today at snorkel. What we're seeing across many enterprises is that for organizations attempting attempting to deploy large language models, production quality results typically require quite a bit of customization and data development. For many generic prototyping tasks or product demos, there are some AI systems that tend to work out of the box. But for enterprise, production, quality models that require some domain expertise. There's quite a bit of data development that we see needs to happen during that process. And in that process, we see quite a few pain points, a few of which I've highlighted here. While many organizations know this, there are some common pain points they encounter during this process. One of them is that much of the data is unstructured and not ready for AI applications. This could be PDFs or unstructured texts. And the second is that they have too much data to label manually, and so they are reliant on small ground truth data sets, or require subject matter experts to spend many, many hours on simple labeling tasks. Snorkel provides a unique approach to solving these pain points via programmatic data development. The snorkel flow platform combines four parts of the data development cycle in one place, combining data development, model development and error analysis in a single streamlined workflow. The process starts here with data development, but instead of manually labeling data, snorkel flow enables your team to encode your internal expertise and annotate data faster using programmatic labeling and weak supervision techniques. The unique value prop here is that instead of manually labeling all data necessary to fine tune your models, your subject matter experts and data scientists work together to encode heuristics, which we call labeling functions to label your data automatically. Once your team writes some labeling functions, we will develop a small model in the loop here to check the quality of that labeling when combined with the error analysis tools on the platform your team can quickly identify common error modes and iterate towards production accuracy. The task today will be to develop a data set that has labels for queries coming into a chat bot based on the intent of that message or question. I've pulled together some examples from the data set here to give you an idea of the kinds of questions being asked. So we can see a few of the questions here, and they all have some intent classifier associated with that. So in this case, some of these might be the refund isn't showing up. One of these might be activate my card, and the other two are age limit. In total, we have 77 different classes that we want to differentiate between. And so we're going to go through that same workflow that I just mentioned on the previous slide, where we're starting with this historical conversational data, and then we're going to build a set of labeling functions that will programmatically label this data set and train an ML model to essentially guide the error analysis and test how well are these labeling functions labeling this data set, and once we hit production accuracy, we're going to deploy that downstream model. And in order for this machine learning model here in the loop to know if the labeling functions are doing well or not, we are using 300 hand labeled queries, so it's a much smaller data set than you would typically need to use for manual labeling. And to put some numbers to this process, we can quickly review a case study that we did on this problem with Google and their palm two model, starting off with just simple prompting and advanced prompting techniques, we were able to iterate on their model, but not to a point where we got to production quality, and using programmatic labeling, we're able to get to a model that was much more performant. So what I want to do today is to walk through an example of this data development workflow on the platform, to show how we start with some simple prompting and eventually get to a high quality model via programmatic labeling. Here I am in the snorkel flow platform, and I'm in an area of the platform called the studio. This is where I'm going to spend the majority of my time iterating on our data development workflow. I've brought in a few records here so we can see a bit of the content around each query coming into our chat bot. And if I click through them, we can just get an idea of the kinds of questions that are typically being asked. And we can imagine that for all these records, if I was going to label them manually, it would take quite a bit of time. This is a good time to note. We do have a full annotation suite here, and that's where I've done the small amount of hand labeling that 300 that we're going to use as our test set. So there are three distinct ways in which we can create labeling functions. In the snorkel flow platform, there's pattern based methods, prompt based methods and embeddings based methods. One of the things that our customers like to do is to start with a prompt based method, because it's a really easy way to capture a lot of the obvious signal in the data set. So I'm going to click on prompts, and what we see is a template for prompting a suite of large language models. What I can see here is that I'm giving it a set of all the possible labels, and then I'm asking it to essentially do that labeling for me, and then return to me what it thinks the labels are. And depending on which llms you have access to you can create connections to a variety of various models here. So I've connected that this LLM, and I've asked it essentially, what are all the possible labels? And as I come down here, I can preview this prompt on a sample size of just 100 records. Now is a good time to introduce two metrics that are important to us here, precision and coverage. And so precision is essentially how well did this LLM determine the labels between that ground truth and what it was predicted? And the coverage is how many documents is essentially voting on, or is it able to discern a actual label for if it doesn't have enough confidence, it won't actually discern a label on this subset. It voted on all of them, and we can see which ones are correct and we can see which ones are incorrect, but this precision isn't very high. However, this is a good place to start, because it captures some amount of signal, and so we don't want to throw all this work away. What I'll do is I'll code this into one of our labeling functions here on the left, and so I'll click down, and you'll see I have this here as a labeling function, but the precision and coverage are a bit different, and that's because I've applied it to the entire data set, so the precision goes down and there's coverage that also goes down now that I have a labeling function, the Next step that we take is to train a model in the loop here to test the quality of this labeling function. So I'm going to click down here, and when I click train a model, I get options for how to select what kind of architecture I want here. And snorkel is agnostic to what kind of architecture you might want to use. So you could use a basic logistic regression or xgboost model here, or if you have your own architecture, you can bring that. And then there are quite a few different training options, and I'll just continue with all the default but you can bring a lot of those tuning to the table as well. Once you train this model, it takes a few minutes, so I've pre trained it here. We get a sense of the accuracy of this labeling function and how many points it labeled. So for this single labeling function, it's able to label 11,682 points in our data set, and it enables them with an accuracy of 58.4% obviously, this isn't production quality, and we'd like to improve upon it, and the way that we do that is to come and use our error analysis tools. So I'm going to go down to our error analysis tools here. On the bottom left, you'll see in this clarity matrix, we get suggestions for how to refine or create new labeling functions. And so there are 89 that are suggested to refine. 43 that are suggested to refine the model, and 28 to write new ones. If I click here, I get options for different visualizations. I'm going to click into the confusion matrix, and I'm going to take a look at the ones where there are errors, and then I'm going to say, okay, which ones are maybe most important or have the most errors? Which one would be interesting to take a look at. Why don't we take a look at this balance not updated after bank transfer, where it was predicted to be pending transfer. So I could see why panel might be confused about that. And so I can view the records here, what I would like to do is actually view all of them at the same time. So I'm going to go to the snippet view, and I can see four different records here that you know had a ground truth, where it was balanced, not updated, but the model was predicting pending transfer. And so what I want to do now is try to encode a heuristic that sort of captures the signal and relabels them as the balance not updated. What I can see is there's some commonalities here. You know, they all contain the word transfer, but obviously a lot of the queries may contain the word transfer, but I also see that, you know, there's things like, Oh, my transfer is missing, or it hasn't arrived, or it's not showing up. And so what I'll do is I'll write a pattern based query that matches those commonalities between them. So I'll click up here in the pattern based and I'll click Full Text regex, and the first thing I'll do is look for the word transfer,
to give a demonstration on the snorkel flow platform around chat bot intent classification. And before I get started with that, I just want to introduce a few slides to give you some background on snorkel, and then set up the context for the challenge that we're going to solve today at snorkel. What we're seeing across many enterprises is that for organizations attempting attempting to deploy large language models, production quality results typically require quite a bit of customization and data development. For many generic prototyping tasks or product demos, there are some AI systems that tend to work out of the box. But for enterprise, production, quality models that require some domain expertise. There's quite a bit of data development that we see needs to happen during that process. And in that process, we see quite a few pain points, a few of which I've highlighted here. While many organizations know this, there are some common pain points they encounter during this process. One of them is that much of the data is unstructured and not ready for AI applications. This could be PDFs or unstructured texts. And the second is that they have too much data to label manually, and so they are reliant on small ground truth data sets, or require subject matter experts to spend many, many hours on simple labeling tasks. Snorkel provides a unique approach to solving these pain points via programmatic data development. The snorkel flow platform combines four parts of the data development cycle in one place, combining data development, model development and error analysis in a single streamlined workflow. The process starts here with data development, but instead of manually labeling data, snorkel flow enables your team to encode your internal expertise and annotate data faster using programmatic labeling and weak supervision techniques. The unique value prop here is that instead of manually labeling all data necessary to fine tune your models, your subject matter experts and data scientists work together to encode heuristics, which we call labeling functions to label your data automatically. Once your team writes some labeling functions, we will develop a small model in the loop here to check the quality of that labeling when combined with the error analysis tools on the platform your team can quickly identify common error modes and iterate towards production accuracy. The task today will be to develop a data set that has labels for queries coming into a chat bot based on the intent of that message or question. I've pulled together some examples from the data set here to give you an idea of the kinds of questions being asked. So we can see a few of the questions here, and they all have some intent classifier associated with that. So in this case, some of these might be the refund isn't showing up. One of these might be activate my card, and the other two are age limit. In total, we have 77 different classes that we want to differentiate between. And so we're going to go through that same workflow that I just mentioned on the previous slide, where we're starting with this historical conversational data, and then we're going to build a set of labeling functions that will programmatically label this data set and train an ML model to essentially guide the error analysis and test how well are these labeling functions labeling this data set, and once we hit production accuracy, we're going to deploy that downstream model. And in order for this machine learning model here in the loop to know if the labeling functions are doing well or not, we are using 300 hand labeled queries, so it's a much smaller data set than you would typically need to use for manual labeling. And to put some numbers to this process, we can quickly review a case study that we did on this problem with Google and their palm two model, starting off with just simple prompting and advanced prompting techniques, we were able to iterate on their model, but not to a point where we got to production quality, and using programmatic labeling, we're able to get to a model that was much more performant. So what I want to do today is to walk through an example of this data development workflow on the platform, to show how we start with some simple prompting and eventually get to a high quality model via programmatic labeling. Here I am in the snorkel flow platform, and I'm in an area of the platform called the studio. This is where I'm going to spend the majority of my time iterating on our data development workflow. I've brought in a few records here so we can see a bit of the content around each query coming into our chat bot. And if I click through them, we can just get an idea of the kinds of questions that are typically being asked. And we can imagine that for all these records, if I was going to label them manually, it would take quite a bit of time. This is a good time to note. We do have a full annotation suite here, and that's where I've done the small amount of hand labeling that 300 that we're going to use as our test set. So there are three distinct ways in which we can create labeling functions. In the snorkel flow platform, there's pattern based methods, prompt based methods and embeddings based methods. One of the things that our customers like to do is to start with a prompt based method, because it's a really easy way to capture a lot of the obvious signal in the data set. So I'm going to click on prompts, and what we see is a template for prompting a suite of large language models. What I can see here is that I'm giving it a set of all the possible labels, and then I'm asking it to essentially do that labeling for me, and then return to me what it thinks the labels are. And depending on which llms you have access to you can create connections to a variety of various models here. So I've connected that this LLM, and I've asked it essentially, what are all the possible labels? And as I come down here, I can preview this prompt on a sample size of just 100 records. Now is a good time to introduce two metrics that are important to us here, precision and coverage. And so precision is essentially how well did this LLM determine the labels between that ground truth and what it was predicted? And the coverage is how many documents is essentially voting on, or is it able to discern a actual label for if it doesn't have enough confidence, it won't actually discern a label on this subset. It voted on all of them, and we can see which ones are correct and we can see which ones are incorrect, but this precision isn't very high. However, this is a good place to start, because it captures some amount of signal, and so we don't want to throw all this work away. What I'll do is I'll code this into one of our labeling functions here on the left, and so I'll click down, and you'll see I have this here as a labeling function, but the precision and coverage are a bit different, and that's because I've applied it to the entire data set, so the precision goes down and there's coverage that also goes down now that I have a labeling function, the Next step that we take is to train a model in the loop here to test the quality of this labeling function. So I'm going to click down here, and when I click train a model, I get options for how to select what kind of architecture I want here. And snorkel is agnostic to what kind of architecture you might want to use. So you could use a basic logistic regression or xgboost model here, or if you have your own architecture, you can bring that. And then there are quite a few different training options, and I'll just continue with all the default but you can bring a lot of those tuning to the table as well. Once you train this model, it takes a few minutes, so I've pre trained it here. We get a sense of the accuracy of this labeling function and how many points it labeled. So for this single labeling function, it's able to label 11,682 points in our data set, and it enables them with an accuracy of 58.4% obviously, this isn't production quality, and we'd like to improve upon it, and the way that we do that is to come and use our error analysis tools. So I'm going to go down to our error analysis tools here. On the bottom left, you'll see in this clarity matrix, we get suggestions for how to refine or create new labeling functions. And so there are 89 that are suggested to refine. 43 that are suggested to refine the model, and 28 to write new ones. If I click here, I get options for different visualizations. I'm going to click into the confusion matrix, and I'm going to take a look at the ones where there are errors, and then I'm going to say, okay, which ones are maybe most important or have the most errors? Which one would be interesting to take a look at. Why don't we take a look at this balance not updated after bank transfer, where it was predicted to be pending transfer. So I could see why panel might be confused about that. And so I can view the records here, what I would like to do is actually view all of them at the same time. So I'm going to go to the snippet view, and I can see four different records here that you know had a ground truth, where it was balanced, not updated, but the model was predicting pending transfer. And so what I want to do now is try to encode a heuristic that sort of captures the signal and relabels them as the balance not updated. What I can see is there's some commonalities here. You know, they all contain the word transfer, but obviously a lot of the queries may contain the word transfer, but I also see that, you know, there's things like, Oh, my transfer is missing, or it hasn't arrived, or it's not showing up. And so what I'll do is I'll write a pattern based query that matches those commonalities between them. So I'll click up here in the pattern based and I'll click Full Text regex, and the first thing I'll do is look for the word transfer,
to give a demonstration on the snorkel flow platform around chat bot intent classification. And before I get started with that, I just want to introduce a few slides to give you some background on snorkel, and then set up the context for the challenge that we're going to solve today at snorkel. What we're seeing across many enterprises is that for organizations attempting attempting to deploy large language models, production quality results typically require quite a bit of customization and data development. For many generic prototyping tasks or product demos, there are some AI systems that tend to work out of the box. But for enterprise, production, quality models that require some domain expertise. There's quite a bit of data development that we see needs to happen during that process. And in that process, we see quite a few pain points, a few of which I've highlighted here. While many organizations know this, there are some common pain points they encounter during this process. One of them is that much of the data is unstructured and not ready for AI applications. This could be PDFs or unstructured texts. And the second is that they have too much data to label manually, and so they are reliant on small ground truth data sets, or require subject matter experts to spend many, many hours on simple labeling tasks. Snorkel provides a unique approach to solving these pain points via programmatic data development. The snorkel flow platform combines four parts of the data development cycle in one place, combining data development, model development and error analysis in a single streamlined workflow. The process starts here with data development, but instead of manually labeling data, snorkel flow enables your team to encode your internal expertise and annotate data faster using programmatic labeling and weak supervision techniques. The unique value prop here is that instead of manually labeling all data necessary to fine tune your models, your subject matter experts and data scientists work together to encode heuristics, which we call labeling functions to label your data automatically. Once your team writes some labeling functions, we will develop a small model in the loop here to check the quality of that labeling when combined with the error analysis tools on the platform your team can quickly identify common error modes and iterate towards production accuracy. The task today will be to develop a data set that has labels for queries coming into a chat bot based on the intent of that message or question. I've pulled together some examples from the data set here to give you an idea of the kinds of questions being asked. So we can see a few of the questions here, and they all have some intent classifier associated with that. So in this case, some of these might be the refund isn't showing up. One of these might be activate my card, and the other two are age limit. In total, we have 77 different classes that we want to differentiate between. And so we're going to go through that same workflow that I just mentioned on the previous slide, where we're starting with this historical conversational data, and then we're going to build a set of labeling functions that will programmatically label this data set and train an ML model to essentially guide the error analysis and test how well are these labeling functions labeling this data set, and once we hit production accuracy, we're going to deploy that downstream model. And in order for this machine learning model here in the loop to know if the labeling functions are doing well or not, we are using 300 hand labeled queries, so it's a much smaller data set than you would typically need to use for manual labeling. And to put some numbers to this process, we can quickly review a case study that we did on this problem with Google and their palm two model, starting off with just simple prompting and advanced prompting techniques, we were able to iterate on their model, but not to a point where we got to production quality, and using programmatic labeling, we're able to get to a model that was much more performant. So what I want to do today is to walk through an example of this data development workflow on the platform, to show how we start with some simple prompting and eventually get to a high quality model via programmatic labeling. Here I am in the snorkel flow platform, and I'm in an area of the platform called the studio. This is where I'm going to spend the majority of my time iterating on our data development workflow. I've brought in a few records here so we can see a bit of the content around each query coming into our chat bot. And if I click through them, we can just get an idea of the kinds of questions that are typically being asked. And we can imagine that for all these records, if I was going to label them manually, it would take quite a bit of time. This is a good time to note. We do have a full annotation suite here, and that's where I've done the small amount of hand labeling that 300 that we're going to use as our test set. So there are three distinct ways in which we can create labeling functions. In the snorkel flow platform, there's pattern based methods, prompt based methods and embeddings based methods. One of the things that our customers like to do is to start with a prompt based method, because it's a really easy way to capture a lot of the obvious signal in the data set. So I'm going to click on prompts, and what we see is a template for prompting a suite of large language models. What I can see here is that I'm giving it a set of all the possible labels, and then I'm asking it to essentially do that labeling for me, and then return to me what it thinks the labels are. And depending on which llms you have access to you can create connections to a variety of various models here. So I've connected that this LLM, and I've asked it essentially, what are all the possible labels? And as I come down here, I can preview this prompt on a sample size of just 100 records. Now is a good time to introduce two metrics that are important to us here, precision and coverage. And so precision is essentially how well did this LLM determine the labels between that ground truth and what it was predicted? And the coverage is how many documents is essentially voting on, or is it able to discern a actual label for if it doesn't have enough confidence, it won't actually discern a label on this subset. It voted on all of them, and we can see which ones are correct and we can see which ones are incorrect, but this precision isn't very high. However, this is a good place to start, because it captures some amount of signal, and so we don't want to throw all this work away. What I'll do is I'll code this into one of our labeling functions here on the left, and so I'll click down, and you'll see I have this here as a labeling function, but the precision and coverage are a bit different, and that's because I've applied it to the entire data set, so the precision goes down and there's coverage that also goes down now that I have a labeling function, the Next step that we take is to train a model in the loop here to test the quality of this labeling function. So I'm going to click down here, and when I click train a model, I get options for how to select what kind of architecture I want here. And snorkel is agnostic to what kind of architecture you might want to use. So you could use a basic logistic regression or xgboost model here, or if you have your own architecture, you can bring that. And then there are quite a few different training options, and I'll just continue with all the default but you can bring a lot of those tuning to the table as well. Once you train this model, it takes a few minutes, so I've pre trained it here. We get a sense of the accuracy of this labeling function and how many points it labeled. So for this single labeling function, it's able to label 11,682 points in our data set, and it enables them with an accuracy of 58.4% obviously, this isn't production quality, and we'd like to improve upon it, and the way that we do that is to come and use our error analysis tools. So I'm going to go down to our error analysis tools here. On the bottom left, you'll see in this clarity matrix, we get suggestions for how to refine or create new labeling functions. And so there are 89 that are suggested to refine. 43 that are suggested to refine the model, and 28 to write new ones. If I click here, I get options for different visualizations. I'm going to click into the confusion matrix, and I'm going to take a look at the ones where there are errors, and then I'm going to say, okay, which ones are maybe most important or have the most errors? Which one would be interesting to take a look at. Why don't we take a look at this balance not updated after bank transfer, where it was predicted to be pending transfer. So I could see why panel might be confused about that. And so I can view the records here, what I would like to do is actually view all of them at the same time. So I'm going to go to the snippet view, and I can see four different records here that you know had a ground truth, where it was balanced, not updated, but the model was predicting pending transfer. And so what I want to do now is try to encode a heuristic that sort of captures the signal and relabels them as the balance not updated. What I can see is there's some commonalities here. You know, they all contain the word transfer, but obviously a lot of the queries may contain the word transfer, but I also see that, you know, there's things like, Oh, my transfer is missing, or it hasn't arrived, or it's not showing up. And so what I'll do is I'll write a pattern based query that matches those commonalities between them. So I'll click up here in the pattern based and I'll click Full Text regex, and the first thing I'll do is look for the word transfer,
to give a demonstration on the snorkel flow platform around chat bot intent classification. And before I get started with that, I just want to introduce a few slides to give you some background on snorkel, and then set up the context for the challenge that we're going to solve today at snorkel. What we're seeing across many enterprises is that for organizations attempting attempting to deploy large language models, production quality results typically require quite a bit of customization and data development. For many generic prototyping tasks or product demos, there are some AI systems that tend to work out of the box. But for enterprise, production, quality models that require some domain expertise. There's quite a bit of data development that we see needs to happen during that process. And in that process, we see quite a few pain points, a few of which I've highlighted here. While many organizations know this, there are some common pain points they encounter during this process. One of them is that much of the data is unstructured and not ready for AI applications. This could be PDFs or unstructured texts. And the second is that they have too much data to label manually, and so they are reliant on small ground truth data sets, or require subject matter experts to spend many, many hours on simple labeling tasks. Snorkel provides a unique approach to solving these pain points via programmatic data development. The snorkel flow platform combines four parts of the data development cycle in one place, combining data development, model development and error analysis in a single streamlined workflow. The process starts here with data development, but instead of manually labeling data, snorkel flow enables your team to encode your internal expertise and annotate data faster using programmatic labeling and weak supervision techniques. The unique value prop here is that instead of manually labeling all data necessary to fine tune your models, your subject matter experts and data scientists work together to encode heuristics, which we call labeling functions to label your data automatically. Once your team writes some labeling functions, we will develop a small model in the loop here to check the quality of that labeling when combined with the error analysis tools on the platform your team can quickly identify common error modes and iterate towards production accuracy. The task today will be to develop a data set that has labels for queries coming into a chat bot based on the intent of that message or question. I've pulled together some examples from the data set here to give you an idea of the kinds of questions being asked. So we can see a few of the questions here, and they all have some intent classifier associated with that. So in this case, some of these might be the refund isn't showing up. One of these might be activate my card, and the other two are age limit. In total, we have 77 different classes that we want to differentiate between. And so we're going to go through that same workflow that I just mentioned on the previous slide, where we're starting with this historical conversational data, and then we're going to build a set of labeling functions that will programmatically label this data set and train an ML model to essentially guide the error analysis and test how well are these labeling functions labeling this data set, and once we hit production accuracy, we're going to deploy that downstream model. And in order for this machine learning model here in the loop to know if the labeling functions are doing well or not, we are using 300 hand labeled queries, so it's a much smaller data set than you would typically need to use for manual labeling. And to put some numbers to this process, we can quickly review a case study that we did on this problem with Google and their palm two model, starting off with just simple prompting and advanced prompting techniques, we were able to iterate on their model, but not to a point where we got to production quality, and using programmatic labeling, we're able to get to a model that was much more performant. So what I want to do today is to walk through an example of this data development workflow on the platform, to show how we start with some simple prompting and eventually get to a high quality model via programmatic labeling. Here I am in the snorkel flow platform, and I'm in an area of the platform called the studio. This is where I'm going to spend the majority of my time iterating on our data development workflow. I've brought in a few records here so we can see a bit of the content around each query coming into our chat bot. And if I click through them, we can just get an idea of the kinds of questions that are typically being asked. And we can imagine that for all these records, if I was going to label them manually, it would take quite a bit of time. This is a good time to note. We do have a full annotation suite here, and that's where I've done the small amount of hand labeling that 300 that we're going to use as our test set. So there are three distinct ways in which we can create labeling functions. In the snorkel flow platform, there's pattern based methods, prompt based methods and embeddings based methods. One of the things that our customers like to do is to start with a prompt based method, because it's a really easy way to capture a lot of the obvious signal in the data set. So I'm going to click on prompts, and what we see is a template for prompting a suite of large language models. What I can see here is that I'm giving it a set of all the possible labels, and then I'm asking it to essentially do that labeling for me, and then return to me what it thinks the labels are. And depending on which llms you have access to you can create connections to a variety of various models here. So I've connected that this LLM, and I've asked it essentially, what are all the possible labels? And as I come down here, I can preview this prompt on a sample size of just 100 records. Now is a good time to introduce two metrics that are important to us here, precision and coverage. And so precision is essentially how well did this LLM determine the labels between that ground truth and what it was predicted? And the coverage is how many documents is essentially voting on, or is it able to discern a actual label for if it doesn't have enough confidence, it won't actually discern a label on this subset. It voted on all of them, and we can see which ones are correct and we can see which ones are incorrect, but this precision isn't very high. However, this is a good place to start, because it captures some amount of signal, and so we don't want to throw all this work away. What I'll do is I'll code this into one of our labeling functions here on the left, and so I'll click down, and you'll see I have this here as a labeling function, but the precision and coverage are a bit different, and that's because I've applied it to the entire data set, so the precision goes down and there's coverage that also goes down now that I have a labeling function, the Next step that we take is to train a model in the loop here to test the quality of this labeling function. So I'm going to click down here, and when I click train a model, I get options for how to select what kind of architecture I want here. And snorkel is agnostic to what kind of architecture you might want to use. So you could use a basic logistic regression or xgboost model here, or if you have your own architecture, you can bring that. And then there are quite a few different training options, and I'll just continue with all the default but you can bring a lot of those tuning to the table as well. Once you train this model, it takes a few minutes, so I've pre trained it here. We get a sense of the accuracy of this labeling function and how many points it labeled. So for this single labeling function, it's able to label 11,682 points in our data set, and it enables them with an accuracy of 58.4% obviously, this isn't production quality, and we'd like to improve upon it, and the way that we do that is to come and use our error analysis tools. So I'm going to go down to our error analysis tools here. On the bottom left, you'll see in this clarity matrix, we get suggestions for how to refine or create new labeling functions. And so there are 89 that are suggested to refine. 43 that are suggested to refine the model, and 28 to write new ones. If I click here, I get options for different visualizations. I'm going to click into the confusion matrix, and I'm going to take a look at the ones where there are errors, and then I'm going to say, okay, which ones are maybe most important or have the most errors? Which one would be interesting to take a look at. Why don't we take a look at this balance not updated after bank transfer, where it was predicted to be pending transfer. So I could see why panel might be confused about that. And so I can view the records here, what I would like to do is actually view all of them at the same time. So I'm going to go to the snippet view, and I can see four different records here that you know had a ground truth, where it was balanced, not updated, but the model was predicting pending transfer. And so what I want to do now is try to encode a heuristic that sort of captures the signal and relabels them as the balance not updated. What I can see is there's some commonalities here. You know, they all contain the word transfer, but obviously a lot of the queries may contain the word transfer, but I also see that, you know, there's things like, Oh, my transfer is missing, or it hasn't arrived, or it's not showing up. And so what I'll do is I'll write a pattern based query that matches those commonalities between them. So I'll click up here in the pattern based and I'll click Full Text regex, and the first thing I'll do is look for the word transfer,
S Speaker 311:13and then I'll add another builder and another regex, and I'll either look for missing or arrived or showing
and then I'll add another builder and another regex, and I'll either look for missing or arrived or showing
and then I'll add another builder and another regex, and I'll either look for missing or arrived or showing
and then I'll add another builder and another regex, and I'll either look for missing or arrived or showing
11:24and when I preview this labeling function,
and when I preview this labeling function,
and when I preview this labeling function,
and when I preview this labeling function,
S Speaker 311:28it's going to tell us now that, based on all of the queries that are in this data set, it's able to confidently vote on a very small amount of them, but it's able to do them perfectly Based on our ground truth data. So this is a really high precision labeling function. Even if it has low coverage, it's still going to improve the signal in our model. So what I'm going to do is click Create labeling function, and we'll see it get thrown over here on the left side. Okay, so that was pattern based labeling functions, and we already did a prompt based one. So the last one that I want to illustrate are embeddings based labeling functions. And so let's click on embeddings.
it's going to tell us now that, based on all of the queries that are in this data set, it's able to confidently vote on a very small amount of them, but it's able to do them perfectly Based on our ground truth data. So this is a really high precision labeling function. Even if it has low coverage, it's still going to improve the signal in our model. So what I'm going to do is click Create labeling function, and we'll see it get thrown over here on the left side. Okay, so that was pattern based labeling functions, and we already did a prompt based one. So the last one that I want to illustrate are embeddings based labeling functions. And so let's click on embeddings.
it's going to tell us now that, based on all of the queries that are in this data set, it's able to confidently vote on a very small amount of them, but it's able to do them perfectly Based on our ground truth data. So this is a really high precision labeling function. Even if it has low coverage, it's still going to improve the signal in our model. So what I'm going to do is click Create labeling function, and we'll see it get thrown over here on the left side. Okay, so that was pattern based labeling functions, and we already did a prompt based one. So the last one that I want to illustrate are embeddings based labeling functions. And so let's click on embeddings.
it's going to tell us now that, based on all of the queries that are in this data set, it's able to confidently vote on a very small amount of them, but it's able to do them perfectly Based on our ground truth data. So this is a really high precision labeling function. Even if it has low coverage, it's still going to improve the signal in our model. So what I'm going to do is click Create labeling function, and we'll see it get thrown over here on the left side. Okay, so that was pattern based labeling functions, and we already did a prompt based one. So the last one that I want to illustrate are embeddings based labeling functions. And so let's click on embeddings.
S Speaker 219:12Yeah, absolutely. It's a really good question, and the question that a lot of people have so I think you're probably not the only one asking that, the answer is pretty simply. It's very robust. You can import things from like an s3 bucket on Amazon, but also to very large data warehouses like Google. I think they have BigQuery, AWS is redshift, Microsoft, Azure, and then we also work very closely with smaller or other teams like snowflake and Databricks. It's very easy to integrate your data from any of those big data warehouses, data lakes, into snorkel, ingest it, do your data development, and then put it back into your data
Yeah, absolutely. It's a really good question, and the question that a lot of people have so I think you're probably not the only one asking that, the answer is pretty simply. It's very robust. You can import things from like an s3 bucket on Amazon, but also to very large data warehouses like Google. I think they have BigQuery, AWS is redshift, Microsoft, Azure, and then we also work very closely with smaller or other teams like snowflake and Databricks. It's very easy to integrate your data from any of those big data warehouses, data lakes, into snorkel, ingest it, do your data development, and then put it back into your data
Yeah, absolutely. It's a really good question, and the question that a lot of people have so I think you're probably not the only one asking that, the answer is pretty simply. It's very robust. You can import things from like an s3 bucket on Amazon, but also to very large data warehouses like Google. I think they have BigQuery, AWS is redshift, Microsoft, Azure, and then we also work very closely with smaller or other teams like snowflake and Databricks. It's very easy to integrate your data from any of those big data warehouses, data lakes, into snorkel, ingest it, do your data development, and then put it back into your data
Yeah, absolutely. It's a really good question, and the question that a lot of people have so I think you're probably not the only one asking that, the answer is pretty simply. It's very robust. You can import things from like an s3 bucket on Amazon, but also to very large data warehouses like Google. I think they have BigQuery, AWS is redshift, Microsoft, Azure, and then we also work very closely with smaller or other teams like snowflake and Databricks. It's very easy to integrate your data from any of those big data warehouses, data lakes, into snorkel, ingest it, do your data development, and then put it back into your data
S Speaker 119:54lake. Yeah, I would just add as a fallback option, you know, barring the specifics, I believe there's any, you know, any real SQL database with JDBC connector, right? So if you're building a data warehouse on Postgres or MySQL, that's an option too. But for some of the bigger players, we certainly have more integration.
lake. Yeah, I would just add as a fallback option, you know, barring the specifics, I believe there's any, you know, any real SQL database with JDBC connector, right? So if you're building a data warehouse on Postgres or MySQL, that's an option too. But for some of the bigger players, we certainly have more integration.
lake. Yeah, I would just add as a fallback option, you know, barring the specifics, I believe there's any, you know, any real SQL database with JDBC connector, right? So if you're building a data warehouse on Postgres or MySQL, that's an option too. But for some of the bigger players, we certainly have more integration.
lake. Yeah, I would just add as a fallback option, you know, barring the specifics, I believe there's any, you know, any real SQL database with JDBC connector, right? So if you're building a data warehouse on Postgres or MySQL, that's an option too. But for some of the bigger players, we certainly have more integration.
20:13That's a great point.
That's a great point.
That's a great point.
That's a great point.
S Speaker 120:16Yeah, you know, another one I thought was kind of interesting when you did touch on it before is, I think sometimes it's easy to want to turn to an LLM kind of as step one, even for something about classification. No surprise. I mean, his models are pretty fantastic and more powerful every day. So why can't I just ask chatgpt to tell me the classification of one of these utterances. And I imagine there's some of the pros and cons of that. And if everyone was looking closely, you probably saw that, you know, accuracy percentage in there, which is a little bit surprising to me, but I want to see if you wanted to rip on that for a few more minutes, give us a little more context. Yeah,
Yeah, you know, another one I thought was kind of interesting when you did touch on it before is, I think sometimes it's easy to want to turn to an LLM kind of as step one, even for something about classification. No surprise. I mean, his models are pretty fantastic and more powerful every day. So why can't I just ask chatgpt to tell me the classification of one of these utterances. And I imagine there's some of the pros and cons of that. And if everyone was looking closely, you probably saw that, you know, accuracy percentage in there, which is a little bit surprising to me, but I want to see if you wanted to rip on that for a few more minutes, give us a little more context. Yeah,
Yeah, you know, another one I thought was kind of interesting when you did touch on it before is, I think sometimes it's easy to want to turn to an LLM kind of as step one, even for something about classification. No surprise. I mean, his models are pretty fantastic and more powerful every day. So why can't I just ask chatgpt to tell me the classification of one of these utterances. And I imagine there's some of the pros and cons of that. And if everyone was looking closely, you probably saw that, you know, accuracy percentage in there, which is a little bit surprising to me, but I want to see if you wanted to rip on that for a few more minutes, give us a little more context. Yeah,
Yeah, you know, another one I thought was kind of interesting when you did touch on it before is, I think sometimes it's easy to want to turn to an LLM kind of as step one, even for something about classification. No surprise. I mean, his models are pretty fantastic and more powerful every day. So why can't I just ask chatgpt to tell me the classification of one of these utterances. And I imagine there's some of the pros and cons of that. And if everyone was looking closely, you probably saw that, you know, accuracy percentage in there, which is a little bit surprising to me, but I want to see if you wanted to rip on that for a few more minutes, give us a little more context. Yeah,
S Speaker 220:55absolutely there. There is that kind of shock, because we're also impressed with these, these foundation models, and they are very impressive, but they do give up some nuance for breath. So while it can answer what's the best peach cobbler recipe and how do I properly get a soup tailored, it also struggles to get a ton of nuance in your data. And so what you can do is you can train it to spread that all out, kind of lose some of that knowledge. You don't need to expand the knowledge within your specific domain to get much better information on the nuances of your domain. In addition to that, even if you can with really good prompting, you know, you're a prompting wizard, I'll believe you. You can get it to answer all the questions, deploying one of these big, chunky, 8 billion parameter models, whatever it may be, is expensive, it's slow and it's difficult, and so being able to go to more of a small language model, so people are calling them, having this much more nuanced model is cheaper, faster and more accurate. There's a ton of reasons why this little bit of process, through data centric iteration provides some really great results. There used to be this big barrier to getting from that large language to a smaller model, with keeping the accuracy or improving the accuracy, but processes like snorkel does really take down that barrier of entry.
absolutely there. There is that kind of shock, because we're also impressed with these, these foundation models, and they are very impressive, but they do give up some nuance for breath. So while it can answer what's the best peach cobbler recipe and how do I properly get a soup tailored, it also struggles to get a ton of nuance in your data. And so what you can do is you can train it to spread that all out, kind of lose some of that knowledge. You don't need to expand the knowledge within your specific domain to get much better information on the nuances of your domain. In addition to that, even if you can with really good prompting, you know, you're a prompting wizard, I'll believe you. You can get it to answer all the questions, deploying one of these big, chunky, 8 billion parameter models, whatever it may be, is expensive, it's slow and it's difficult, and so being able to go to more of a small language model, so people are calling them, having this much more nuanced model is cheaper, faster and more accurate. There's a ton of reasons why this little bit of process, through data centric iteration provides some really great results. There used to be this big barrier to getting from that large language to a smaller model, with keeping the accuracy or improving the accuracy, but processes like snorkel does really take down that barrier of entry.
absolutely there. There is that kind of shock, because we're also impressed with these, these foundation models, and they are very impressive, but they do give up some nuance for breath. So while it can answer what's the best peach cobbler recipe and how do I properly get a soup tailored, it also struggles to get a ton of nuance in your data. And so what you can do is you can train it to spread that all out, kind of lose some of that knowledge. You don't need to expand the knowledge within your specific domain to get much better information on the nuances of your domain. In addition to that, even if you can with really good prompting, you know, you're a prompting wizard, I'll believe you. You can get it to answer all the questions, deploying one of these big, chunky, 8 billion parameter models, whatever it may be, is expensive, it's slow and it's difficult, and so being able to go to more of a small language model, so people are calling them, having this much more nuanced model is cheaper, faster and more accurate. There's a ton of reasons why this little bit of process, through data centric iteration provides some really great results. There used to be this big barrier to getting from that large language to a smaller model, with keeping the accuracy or improving the accuracy, but processes like snorkel does really take down that barrier of entry.
absolutely there. There is that kind of shock, because we're also impressed with these, these foundation models, and they are very impressive, but they do give up some nuance for breath. So while it can answer what's the best peach cobbler recipe and how do I properly get a soup tailored, it also struggles to get a ton of nuance in your data. And so what you can do is you can train it to spread that all out, kind of lose some of that knowledge. You don't need to expand the knowledge within your specific domain to get much better information on the nuances of your domain. In addition to that, even if you can with really good prompting, you know, you're a prompting wizard, I'll believe you. You can get it to answer all the questions, deploying one of these big, chunky, 8 billion parameter models, whatever it may be, is expensive, it's slow and it's difficult, and so being able to go to more of a small language model, so people are calling them, having this much more nuanced model is cheaper, faster and more accurate. There's a ton of reasons why this little bit of process, through data centric iteration provides some really great results. There used to be this big barrier to getting from that large language to a smaller model, with keeping the accuracy or improving the accuracy, but processes like snorkel does really take down that barrier of entry.
23:46to the Q and A pain as well. Here, that's what I was thinking.
to the Q and A pain as well. Here, that's what I was thinking.
to the Q and A pain as well. Here, that's what I was thinking.
to the Q and A pain as well. Here, that's what I was thinking.
S Speaker 123:49They're pretty similar, so I might be able to bundle them together a little bit. But I think, you know, for everyone that's listening, what we're really trying to get after here is, how can we, how can we resolve the conflicts and ensure the accuracy if we have lots of labeling functions, right? Some might be super great. Some might not be so great. How do we think about it, and how do we get to that point where we have a bunch of accurate labels?
They're pretty similar, so I might be able to bundle them together a little bit. But I think, you know, for everyone that's listening, what we're really trying to get after here is, how can we, how can we resolve the conflicts and ensure the accuracy if we have lots of labeling functions, right? Some might be super great. Some might not be so great. How do we think about it, and how do we get to that point where we have a bunch of accurate labels?
They're pretty similar, so I might be able to bundle them together a little bit. But I think, you know, for everyone that's listening, what we're really trying to get after here is, how can we, how can we resolve the conflicts and ensure the accuracy if we have lots of labeling functions, right? Some might be super great. Some might not be so great. How do we think about it, and how do we get to that point where we have a bunch of accurate labels?
They're pretty similar, so I might be able to bundle them together a little bit. But I think, you know, for everyone that's listening, what we're really trying to get after here is, how can we, how can we resolve the conflicts and ensure the accuracy if we have lots of labeling functions, right? Some might be super great. Some might not be so great. How do we think about it, and how do we get to that point where we have a bunch of accurate labels?
S Speaker 224:17Yeah, the beauty is, you don't have to have perfect labeling functions. And so your labeling functions, your heuristics, really just have to have some general truth to them. They have to be pretty good, and they're better off when they're nuanced or when they're specific, instead of just in the example we saw, if it ever says the word transfer, well that's going to be a lot of your data. That's probably going to cause some bad signal other places. And so a good labeling function is more specific, like we saw Chris add it has to have transfer and one of these missing show or things like that. But then also, it doesn't have to be perfect. You know that that could be only 80% accurate across your data, and that's good enough. And that's the beauty of some of this. This learning here is that it's able, this week supervision, is able to account for a lot of that, and it's able to use the different, imperfect heuristics together to come up with a statistically, probabilistically correct label for all of your data. And yes has a question there as well. How do we know that a label function is 100% accurate if we don't have labels in the first place? And that really touches on a critical piece here, as I'm talking about, is it accurate across our data? Is it 80% accurate? Well, how can we get those numbers if we don't have labeled data? And you're absolutely right, you can't. And so that's why snorkel adheres to all the data science best practices. We really require a small nugget of hand, manual annotated golden data. That golden data is your compass to getting the rest of your data. So like Chris showed in the beginning, there, there was that bar chart, 300 data points manually labeled, and 1.3 1000 not labeled. That was his example. So we can use those 300 to write our labeling functions and compare the accuracy of our labeling functions against those known good labels. See if we're creating good labeling functions or not. We'll use that to guide Well, the rest of the data is probably similar to that, if we did a good job of sprinkling our our manually little data throughout it, then we can use that and apply to the rest of the data and see how it performs based on there. That's a really good question.
Yeah, the beauty is, you don't have to have perfect labeling functions. And so your labeling functions, your heuristics, really just have to have some general truth to them. They have to be pretty good, and they're better off when they're nuanced or when they're specific, instead of just in the example we saw, if it ever says the word transfer, well that's going to be a lot of your data. That's probably going to cause some bad signal other places. And so a good labeling function is more specific, like we saw Chris add it has to have transfer and one of these missing show or things like that. But then also, it doesn't have to be perfect. You know that that could be only 80% accurate across your data, and that's good enough. And that's the beauty of some of this. This learning here is that it's able, this week supervision, is able to account for a lot of that, and it's able to use the different, imperfect heuristics together to come up with a statistically, probabilistically correct label for all of your data. And yes has a question there as well. How do we know that a label function is 100% accurate if we don't have labels in the first place? And that really touches on a critical piece here, as I'm talking about, is it accurate across our data? Is it 80% accurate? Well, how can we get those numbers if we don't have labeled data? And you're absolutely right, you can't. And so that's why snorkel adheres to all the data science best practices. We really require a small nugget of hand, manual annotated golden data. That golden data is your compass to getting the rest of your data. So like Chris showed in the beginning, there, there was that bar chart, 300 data points manually labeled, and 1.3 1000 not labeled. That was his example. So we can use those 300 to write our labeling functions and compare the accuracy of our labeling functions against those known good labels. See if we're creating good labeling functions or not. We'll use that to guide Well, the rest of the data is probably similar to that, if we did a good job of sprinkling our our manually little data throughout it, then we can use that and apply to the rest of the data and see how it performs based on there. That's a really good question.
Yeah, the beauty is, you don't have to have perfect labeling functions. And so your labeling functions, your heuristics, really just have to have some general truth to them. They have to be pretty good, and they're better off when they're nuanced or when they're specific, instead of just in the example we saw, if it ever says the word transfer, well that's going to be a lot of your data. That's probably going to cause some bad signal other places. And so a good labeling function is more specific, like we saw Chris add it has to have transfer and one of these missing show or things like that. But then also, it doesn't have to be perfect. You know that that could be only 80% accurate across your data, and that's good enough. And that's the beauty of some of this. This learning here is that it's able, this week supervision, is able to account for a lot of that, and it's able to use the different, imperfect heuristics together to come up with a statistically, probabilistically correct label for all of your data. And yes has a question there as well. How do we know that a label function is 100% accurate if we don't have labels in the first place? And that really touches on a critical piece here, as I'm talking about, is it accurate across our data? Is it 80% accurate? Well, how can we get those numbers if we don't have labeled data? And you're absolutely right, you can't. And so that's why snorkel adheres to all the data science best practices. We really require a small nugget of hand, manual annotated golden data. That golden data is your compass to getting the rest of your data. So like Chris showed in the beginning, there, there was that bar chart, 300 data points manually labeled, and 1.3 1000 not labeled. That was his example. So we can use those 300 to write our labeling functions and compare the accuracy of our labeling functions against those known good labels. See if we're creating good labeling functions or not. We'll use that to guide Well, the rest of the data is probably similar to that, if we did a good job of sprinkling our our manually little data throughout it, then we can use that and apply to the rest of the data and see how it performs based on there. That's a really good question.
Yeah, the beauty is, you don't have to have perfect labeling functions. And so your labeling functions, your heuristics, really just have to have some general truth to them. They have to be pretty good, and they're better off when they're nuanced or when they're specific, instead of just in the example we saw, if it ever says the word transfer, well that's going to be a lot of your data. That's probably going to cause some bad signal other places. And so a good labeling function is more specific, like we saw Chris add it has to have transfer and one of these missing show or things like that. But then also, it doesn't have to be perfect. You know that that could be only 80% accurate across your data, and that's good enough. And that's the beauty of some of this. This learning here is that it's able, this week supervision, is able to account for a lot of that, and it's able to use the different, imperfect heuristics together to come up with a statistically, probabilistically correct label for all of your data. And yes has a question there as well. How do we know that a label function is 100% accurate if we don't have labels in the first place? And that really touches on a critical piece here, as I'm talking about, is it accurate across our data? Is it 80% accurate? Well, how can we get those numbers if we don't have labeled data? And you're absolutely right, you can't. And so that's why snorkel adheres to all the data science best practices. We really require a small nugget of hand, manual annotated golden data. That golden data is your compass to getting the rest of your data. So like Chris showed in the beginning, there, there was that bar chart, 300 data points manually labeled, and 1.3 1000 not labeled. That was his example. So we can use those 300 to write our labeling functions and compare the accuracy of our labeling functions against those known good labels. See if we're creating good labeling functions or not. We'll use that to guide Well, the rest of the data is probably similar to that, if we did a good job of sprinkling our our manually little data throughout it, then we can use that and apply to the rest of the data and see how it performs based on there. That's a really good question.
S Speaker 126:34Ben, what thing I'll add to that you know is that the denoising particular it's a little bit like the secret sauce on your favorite cheeseburger, so it's probably a little outside the scope of our 30 minutes here. But what I wanted to kind of reinforce is that there is a lot of intelligence in that denoising process. So it's looking at which of these functions are often more correct, right or accurate? Is there, you know, two or three agreeing on something, and one's really kind of the outlier. So I would say it's multifaceted, and it's looking at lots of different perspectives. And that's kind of, yeah, I'd say that the key to its success, as opposed to more traditional approaches of simple, simple voting, right? Isn't really very accurate, because you might have, you know, two poor functions that you know are agreeing on the wrong answer. So hey, let's, you know, let's move forward with that one. But yeah, hopefully, I hopefully I'm not not sounding yeah here, but there's a lot that goes into it. And so I would want to reassure our audience that it isn't just simple, you know, the majority wins here, right? We're looking a lot of the behavior and the details of these label functions to figure out the winner. That's right?
Ben, what thing I'll add to that you know is that the denoising particular it's a little bit like the secret sauce on your favorite cheeseburger, so it's probably a little outside the scope of our 30 minutes here. But what I wanted to kind of reinforce is that there is a lot of intelligence in that denoising process. So it's looking at which of these functions are often more correct, right or accurate? Is there, you know, two or three agreeing on something, and one's really kind of the outlier. So I would say it's multifaceted, and it's looking at lots of different perspectives. And that's kind of, yeah, I'd say that the key to its success, as opposed to more traditional approaches of simple, simple voting, right? Isn't really very accurate, because you might have, you know, two poor functions that you know are agreeing on the wrong answer. So hey, let's, you know, let's move forward with that one. But yeah, hopefully, I hopefully I'm not not sounding yeah here, but there's a lot that goes into it. And so I would want to reassure our audience that it isn't just simple, you know, the majority wins here, right? We're looking a lot of the behavior and the details of these label functions to figure out the winner. That's right?
Ben, what thing I'll add to that you know is that the denoising particular it's a little bit like the secret sauce on your favorite cheeseburger, so it's probably a little outside the scope of our 30 minutes here. But what I wanted to kind of reinforce is that there is a lot of intelligence in that denoising process. So it's looking at which of these functions are often more correct, right or accurate? Is there, you know, two or three agreeing on something, and one's really kind of the outlier. So I would say it's multifaceted, and it's looking at lots of different perspectives. And that's kind of, yeah, I'd say that the key to its success, as opposed to more traditional approaches of simple, simple voting, right? Isn't really very accurate, because you might have, you know, two poor functions that you know are agreeing on the wrong answer. So hey, let's, you know, let's move forward with that one. But yeah, hopefully, I hopefully I'm not not sounding yeah here, but there's a lot that goes into it. And so I would want to reassure our audience that it isn't just simple, you know, the majority wins here, right? We're looking a lot of the behavior and the details of these label functions to figure out the winner. That's right?
Ben, what thing I'll add to that you know is that the denoising particular it's a little bit like the secret sauce on your favorite cheeseburger, so it's probably a little outside the scope of our 30 minutes here. But what I wanted to kind of reinforce is that there is a lot of intelligence in that denoising process. So it's looking at which of these functions are often more correct, right or accurate? Is there, you know, two or three agreeing on something, and one's really kind of the outlier. So I would say it's multifaceted, and it's looking at lots of different perspectives. And that's kind of, yeah, I'd say that the key to its success, as opposed to more traditional approaches of simple, simple voting, right? Isn't really very accurate, because you might have, you know, two poor functions that you know are agreeing on the wrong answer. So hey, let's, you know, let's move forward with that one. But yeah, hopefully, I hopefully I'm not not sounding yeah here, but there's a lot that goes into it. And so I would want to reassure our audience that it isn't just simple, you know, the majority wins here, right? We're looking a lot of the behavior and the details of these label functions to figure out the winner. That's right?
S Speaker 227:53And there's a lot of difference in the different projects. And you will care about this is some like Shane said, this is under the hood. A lot of the benefit you get by using snorkels platform is under the hood. We are able to optimize for your specific project and be able to de noise them in a way that gets you to the best end result, not just majority rules or whichever you said, prioritize whichever label came first or whichever labeling function you wrote first. No, no, it's all done in the best way for your project. And that's a lot of the that's a lot of the secret sauce under
And there's a lot of difference in the different projects. And you will care about this is some like Shane said, this is under the hood. A lot of the benefit you get by using snorkels platform is under the hood. We are able to optimize for your specific project and be able to de noise them in a way that gets you to the best end result, not just majority rules or whichever you said, prioritize whichever label came first or whichever labeling function you wrote first. No, no, it's all done in the best way for your project. And that's a lot of the that's a lot of the secret sauce under
And there's a lot of difference in the different projects. And you will care about this is some like Shane said, this is under the hood. A lot of the benefit you get by using snorkels platform is under the hood. We are able to optimize for your specific project and be able to de noise them in a way that gets you to the best end result, not just majority rules or whichever you said, prioritize whichever label came first or whichever labeling function you wrote first. No, no, it's all done in the best way for your project. And that's a lot of the that's a lot of the secret sauce under
And there's a lot of difference in the different projects. And you will care about this is some like Shane said, this is under the hood. A lot of the benefit you get by using snorkels platform is under the hood. We are able to optimize for your specific project and be able to de noise them in a way that gets you to the best end result, not just majority rules or whichever you said, prioritize whichever label came first or whichever labeling function you wrote first. No, no, it's all done in the best way for your project. And that's a lot of the that's a lot of the secret sauce under
28:25the hood for snorkel. Josh,
the hood for snorkel. Josh,
the hood for snorkel. Josh,
the hood for snorkel. Josh,
S Speaker 128:28so we are just about at the moment here, but I think maybe I'll squeeze one more in, because I think this is the one. Is an interesting, interesting one to me. And this is, you know, can we combine multiple label functions into single function. So just wondering, Michael, how, how do we think about that? If we have lots of different dimensions, do we try to group those, or we just keep them separate?
so we are just about at the moment here, but I think maybe I'll squeeze one more in, because I think this is the one. Is an interesting, interesting one to me. And this is, you know, can we combine multiple label functions into single function. So just wondering, Michael, how, how do we think about that? If we have lots of different dimensions, do we try to group those, or we just keep them separate?
so we are just about at the moment here, but I think maybe I'll squeeze one more in, because I think this is the one. Is an interesting, interesting one to me. And this is, you know, can we combine multiple label functions into single function. So just wondering, Michael, how, how do we think about that? If we have lots of different dimensions, do we try to group those, or we just keep them separate?
so we are just about at the moment here, but I think maybe I'll squeeze one more in, because I think this is the one. Is an interesting, interesting one to me. And this is, you know, can we combine multiple label functions into single function. So just wondering, Michael, how, how do we think about that? If we have lots of different dimensions, do we try to group those, or we just keep them separate?
S Speaker 228:49Yeah, now you're kind of getting into the art, as well as the science here, of data, data, iteration here and in general, if they both go together logically. There's, there's actually a really great human analog to a lot of this. If you were training a new hire to go through your data and label it yourself, how you would teach them is often how you want to create these labeling functions. And so if to you logically, there is that connection between, you know, requirement a and requirement B, that really is one data slice together. Then, yeah, create a single labeling function so that they don't disagree with each other. It has to have this or that or or these two have to be simultaneously or within a few lines of each other. You have to have both of these. Those should be combined into a single labeling function. However, you don't have to overdo that. There's this, there's this idea of, I don't want these to conflict. Let me just bundle everything into a single labeling function. Not required is, if they are not intrinsically tied in their heuristic, in their idea, leave it as separate labeling functions. And snorkel is really good at figuring out which one should apply stronger in which area. So if you have specific questions, as you're doing that, that's a great question to reach out and get some circle support on saying, Hey, I'm seeing this. What should I do? Or even try both? And that's the beauty of quick iteration. Is run it with all of them combined, three combined into one labeling function. Run it three separate, and you'll see the information yourself. Yeah,
Yeah, now you're kind of getting into the art, as well as the science here, of data, data, iteration here and in general, if they both go together logically. There's, there's actually a really great human analog to a lot of this. If you were training a new hire to go through your data and label it yourself, how you would teach them is often how you want to create these labeling functions. And so if to you logically, there is that connection between, you know, requirement a and requirement B, that really is one data slice together. Then, yeah, create a single labeling function so that they don't disagree with each other. It has to have this or that or or these two have to be simultaneously or within a few lines of each other. You have to have both of these. Those should be combined into a single labeling function. However, you don't have to overdo that. There's this, there's this idea of, I don't want these to conflict. Let me just bundle everything into a single labeling function. Not required is, if they are not intrinsically tied in their heuristic, in their idea, leave it as separate labeling functions. And snorkel is really good at figuring out which one should apply stronger in which area. So if you have specific questions, as you're doing that, that's a great question to reach out and get some circle support on saying, Hey, I'm seeing this. What should I do? Or even try both? And that's the beauty of quick iteration. Is run it with all of them combined, three combined into one labeling function. Run it three separate, and you'll see the information yourself. Yeah,
Yeah, now you're kind of getting into the art, as well as the science here, of data, data, iteration here and in general, if they both go together logically. There's, there's actually a really great human analog to a lot of this. If you were training a new hire to go through your data and label it yourself, how you would teach them is often how you want to create these labeling functions. And so if to you logically, there is that connection between, you know, requirement a and requirement B, that really is one data slice together. Then, yeah, create a single labeling function so that they don't disagree with each other. It has to have this or that or or these two have to be simultaneously or within a few lines of each other. You have to have both of these. Those should be combined into a single labeling function. However, you don't have to overdo that. There's this, there's this idea of, I don't want these to conflict. Let me just bundle everything into a single labeling function. Not required is, if they are not intrinsically tied in their heuristic, in their idea, leave it as separate labeling functions. And snorkel is really good at figuring out which one should apply stronger in which area. So if you have specific questions, as you're doing that, that's a great question to reach out and get some circle support on saying, Hey, I'm seeing this. What should I do? Or even try both? And that's the beauty of quick iteration. Is run it with all of them combined, three combined into one labeling function. Run it three separate, and you'll see the information yourself. Yeah,
Yeah, now you're kind of getting into the art, as well as the science here, of data, data, iteration here and in general, if they both go together logically. There's, there's actually a really great human analog to a lot of this. If you were training a new hire to go through your data and label it yourself, how you would teach them is often how you want to create these labeling functions. And so if to you logically, there is that connection between, you know, requirement a and requirement B, that really is one data slice together. Then, yeah, create a single labeling function so that they don't disagree with each other. It has to have this or that or or these two have to be simultaneously or within a few lines of each other. You have to have both of these. Those should be combined into a single labeling function. However, you don't have to overdo that. There's this, there's this idea of, I don't want these to conflict. Let me just bundle everything into a single labeling function. Not required is, if they are not intrinsically tied in their heuristic, in their idea, leave it as separate labeling functions. And snorkel is really good at figuring out which one should apply stronger in which area. So if you have specific questions, as you're doing that, that's a great question to reach out and get some circle support on saying, Hey, I'm seeing this. What should I do? Or even try both? And that's the beauty of quick iteration. Is run it with all of them combined, three combined into one labeling function. Run it three separate, and you'll see the information yourself. Yeah,
S Speaker 130:18I think that's one more great point to emphasize, before we wrap up here, is that notion of rapid iteration and correct me if I stray too far from the path here, Michael, but to me, what's great about is you can add and modify these labeling functions as well, right? This could be a structured process that takes you, you know, weeks and requires a plan, right? I mean, just get in there, experiment, try new ones, discard the ones that you're not happy with, but really get in there and just start working with it on day one. In my mind, is the best way to go about it.
I think that's one more great point to emphasize, before we wrap up here, is that notion of rapid iteration and correct me if I stray too far from the path here, Michael, but to me, what's great about is you can add and modify these labeling functions as well, right? This could be a structured process that takes you, you know, weeks and requires a plan, right? I mean, just get in there, experiment, try new ones, discard the ones that you're not happy with, but really get in there and just start working with it on day one. In my mind, is the best way to go about it.
I think that's one more great point to emphasize, before we wrap up here, is that notion of rapid iteration and correct me if I stray too far from the path here, Michael, but to me, what's great about is you can add and modify these labeling functions as well, right? This could be a structured process that takes you, you know, weeks and requires a plan, right? I mean, just get in there, experiment, try new ones, discard the ones that you're not happy with, but really get in there and just start working with it on day one. In my mind, is the best way to go about it.
I think that's one more great point to emphasize, before we wrap up here, is that notion of rapid iteration and correct me if I stray too far from the path here, Michael, but to me, what's great about is you can add and modify these labeling functions as well, right? This could be a structured process that takes you, you know, weeks and requires a plan, right? I mean, just get in there, experiment, try new ones, discard the ones that you're not happy with, but really get in there and just start working with it on day one. In my mind, is the best way to go about it.
S Speaker 230:54That's exactly right. Yeah. Being able to iterate, build quickly is really fantastic for day iteration.
That's exactly right. Yeah. Being able to iterate, build quickly is really fantastic for day iteration.
That's exactly right. Yeah. Being able to iterate, build quickly is really fantastic for day iteration.
That's exactly right. Yeah. Being able to iterate, build quickly is really fantastic for day iteration.