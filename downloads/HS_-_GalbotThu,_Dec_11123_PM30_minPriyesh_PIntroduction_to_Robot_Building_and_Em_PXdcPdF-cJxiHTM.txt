Meeting: HS - Galbot
Thu, Dec 11
1:23 PM
30 min
Priyesh P
Introduction to Robot Building and Embodied AI
0:00
URL: https://otter.ai/u/PXdcPdF-cJxiHTMMeIgvG0_CCRM
Downloaded: 2025-12-21T19:13:07.328489
Method: text_extraction
============================================================

S Speaker 10:00Company. Our mission is to make robots for every industry and every home. I think you know, like how to build a generous robots. It's just very similar how to build a human. You need to have body, a physical body, along with a good brain that is mainly composed of cerebrum and serial regulum. And the cerebrum is in charge of high level policies, deciding what to do. Well, the ribulum is responsible for low level monitor policies, determine how to execute the actions in a faster, accurate and reliable manner. So when we have robots, then the body becomes the embodiment, and our robot brain, also at the same time, is composed of cerebrum and cerebellum. So I will start with introducing our like strategy for building survival, basically, you know, following the main trend a lot of people now believe in below a, it is a large, embodied AI model that takes language vision as inputs and outputs robot actions. The advantage of using VLA is that is end to end model, and it also benefit a lot from pre training large parts of vision, language data pairs, and that's why this model can understand all kinds of human instructions and also all kinds of environments, but right now, I think the major limitations of VL A is the limited zero shot performance. It is way lagging behind the zero shot performance of llms and vlms in most of cases, if you just deploy a VL a in a new environment, just ask it to do something, it just fail. The success rate is almost zero. I think the bottleneck truly lies in data right now. Most of the VLA relay on real world tether operation data to work. And you know, Tesla, a lot of companies, they employ a team of people ranging from, you know, filters a few hundreds of people to do daily teleoperation to collect data. And even, even if this is the common case, the largest public data set of, you know, embodied AI coming from kind of operation is just at the scale of 1 million trajectories. It will be a very good question, how many trajectory do we need to train a truly generalist model? I would say it may take trillion of trajectories, just comparable to trillions of token in LLM, and like 10 trillions of tokens in VLM. So we may need trillions of trajectories for building a general purpose VRA, and this is way going beyond the limit can be collected via teleoperation, and so that's why I don't think teleoperation wouldn't be a scalable way to build a VLA. So we need some alternative for data. So in Gambol, we believe, we believe in synthetic data. We believe synthetic data going to be the game changer, and so we build this pipeline. We start with, you know, building upon robot model. We start with building the environment. It is basically you need to gather all kinds of object assets to make sure they are physically interactive, and you need to compose the physical interactive environment that has all kind of realistic layout and the complex like objects in it. And after constructing this environment, the most important thing is how to get physically plausible and meaningful trajectories. We get all kinds of mechanisms to generate trajectories. For example, you can turn human videos into like animation like data. There are, you know, very human leg, and just like mimic what human doing in the real world. And also you can relate on physics, space and energy optimization to find you know the correct way to grasp interact with objects. And also, you can do large scale reinforcement learning just through trial and error, that your network finds this way to interact with objects. And when you have different mechanisms to generate trajectories, you will use simulators to examine their like, whether they are valid, and if they are valid, you can use render physical renders that use free tracing to give you photorealistic image. At the end, you get a lot of simulation data, synthetic data you can use sim to real make sure the model trigger of those synthetic data can be directly deployed in the real world. It sounds a very complex pipeline, but we have validated this pipeline in many applications and tasks. So given that most of you know male a right now, focus on pick and place. We start with key. So worldwide, we are the first to synthesize billion scale synthetic data for grasping so it covers diverse environment, illumination, object layouts, object names, etc. We try to, you know, exhaust exhaustively, search all the variables, make sure the civilian skill this cover everything that can be, you know, varied in the real world, and then we just use this data to train our grasp of VRA, which is an end to end VLA model that also leverage embodied chain of thought. And this like a grasp vra is CO trained first with our 1 million VLA tuples that purity from society world, no any real world actions show up in our action data and also web grounding data we need to understand, you know, if you come up with a name of an object, we need to Make sure it is covered by our data, not limited to our Cincinnati data. That's why we leverage more than 100 million, you know, bounding boxes, basically grounding data on the internet so that our model can even grasp things that not show it in our Cincinnati data. So again, we show very, very impressive zero shot performance, starting from simulator in this very famous liberal environment benchmark, we compare our model to Pi Zero and open mirror a in this case, our model has never seen any data In this liberal benchmark, where we allow Pi Zero and open mirror a to be fine tuned on the training squeeze of this liberal data set, the results show that even without any fine tuning, our Model Zero sharply outperform Pi Zero and open mirror a after data fine tuned So our success rate is about 82% and the other model only achieve about
Company. Our mission is to make robots for every industry and every home. I think you know, like how to build a generous robots. It's just very similar how to build a human. You need to have body, a physical body, along with a good brain that is mainly composed of cerebrum and serial regulum. And the cerebrum is in charge of high level policies, deciding what to do. Well, the ribulum is responsible for low level monitor policies, determine how to execute the actions in a faster, accurate and reliable manner. So when we have robots, then the body becomes the embodiment, and our robot brain, also at the same time, is composed of cerebrum and cerebellum. So I will start with introducing our like strategy for building survival, basically, you know, following the main trend a lot of people now believe in below a, it is a large, embodied AI model that takes language vision as inputs and outputs robot actions. The advantage of using VLA is that is end to end model, and it also benefit a lot from pre training large parts of vision, language data pairs, and that's why this model can understand all kinds of human instructions and also all kinds of environments, but right now, I think the major limitations of VL A is the limited zero shot performance. It is way lagging behind the zero shot performance of llms and vlms in most of cases, if you just deploy a VL a in a new environment, just ask it to do something, it just fail. The success rate is almost zero. I think the bottleneck truly lies in data right now. Most of the VLA relay on real world tether operation data to work. And you know, Tesla, a lot of companies, they employ a team of people ranging from, you know, filters a few hundreds of people to do daily teleoperation to collect data. And even, even if this is the common case, the largest public data set of, you know, embodied AI coming from kind of operation is just at the scale of 1 million trajectories. It will be a very good question, how many trajectory do we need to train a truly generalist model? I would say it may take trillion of trajectories, just comparable to trillions of token in LLM, and like 10 trillions of tokens in VLM. So we may need trillions of trajectories for building a general purpose VRA, and this is way going beyond the limit can be collected via teleoperation, and so that's why I don't think teleoperation wouldn't be a scalable way to build a VLA. So we need some alternative for data. So in Gambol, we believe, we believe in synthetic data. We believe synthetic data going to be the game changer, and so we build this pipeline. We start with, you know, building upon robot model. We start with building the environment. It is basically you need to gather all kinds of object assets to make sure they are physically interactive, and you need to compose the physical interactive environment that has all kind of realistic layout and the complex like objects in it. And after constructing this environment, the most important thing is how to get physically plausible and meaningful trajectories. We get all kinds of mechanisms to generate trajectories. For example, you can turn human videos into like animation like data. There are, you know, very human leg, and just like mimic what human doing in the real world. And also you can relate on physics, space and energy optimization to find you know the correct way to grasp interact with objects. And also, you can do large scale reinforcement learning just through trial and error, that your network finds this way to interact with objects. And when you have different mechanisms to generate trajectories, you will use simulators to examine their like, whether they are valid, and if they are valid, you can use render physical renders that use free tracing to give you photorealistic image. At the end, you get a lot of simulation data, synthetic data you can use sim to real make sure the model trigger of those synthetic data can be directly deployed in the real world. It sounds a very complex pipeline, but we have validated this pipeline in many applications and tasks. So given that most of you know male a right now, focus on pick and place. We start with key. So worldwide, we are the first to synthesize billion scale synthetic data for grasping so it covers diverse environment, illumination, object layouts, object names, etc. We try to, you know, exhaust exhaustively, search all the variables, make sure the civilian skill this cover everything that can be, you know, varied in the real world, and then we just use this data to train our grasp of VRA, which is an end to end VLA model that also leverage embodied chain of thought. And this like a grasp vra is CO trained first with our 1 million VLA tuples that purity from society world, no any real world actions show up in our action data and also web grounding data we need to understand, you know, if you come up with a name of an object, we need to Make sure it is covered by our data, not limited to our Cincinnati data. That's why we leverage more than 100 million, you know, bounding boxes, basically grounding data on the internet so that our model can even grasp things that not show it in our Cincinnati data. So again, we show very, very impressive zero shot performance, starting from simulator in this very famous liberal environment benchmark, we compare our model to Pi Zero and open mirror a in this case, our model has never seen any data In this liberal benchmark, where we allow Pi Zero and open mirror a to be fine tuned on the training squeeze of this liberal data set, the results show that even without any fine tuning, our Model Zero sharply outperform Pi Zero and open mirror a after data fine tuned So our success rate is about 82% and the other model only achieve about
Company. Our mission is to make robots for every industry and every home. I think you know, like how to build a generous robots. It's just very similar how to build a human. You need to have body, a physical body, along with a good brain that is mainly composed of cerebrum and serial regulum. And the cerebrum is in charge of high level policies, deciding what to do. Well, the ribulum is responsible for low level monitor policies, determine how to execute the actions in a faster, accurate and reliable manner. So when we have robots, then the body becomes the embodiment, and our robot brain, also at the same time, is composed of cerebrum and cerebellum. So I will start with introducing our like strategy for building survival, basically, you know, following the main trend a lot of people now believe in below a, it is a large, embodied AI model that takes language vision as inputs and outputs robot actions. The advantage of using VLA is that is end to end model, and it also benefit a lot from pre training large parts of vision, language data pairs, and that's why this model can understand all kinds of human instructions and also all kinds of environments, but right now, I think the major limitations of VL A is the limited zero shot performance. It is way lagging behind the zero shot performance of llms and vlms in most of cases, if you just deploy a VL a in a new environment, just ask it to do something, it just fail. The success rate is almost zero. I think the bottleneck truly lies in data right now. Most of the VLA relay on real world tether operation data to work. And you know, Tesla, a lot of companies, they employ a team of people ranging from, you know, filters a few hundreds of people to do daily teleoperation to collect data. And even, even if this is the common case, the largest public data set of, you know, embodied AI coming from kind of operation is just at the scale of 1 million trajectories. It will be a very good question, how many trajectory do we need to train a truly generalist model? I would say it may take trillion of trajectories, just comparable to trillions of token in LLM, and like 10 trillions of tokens in VLM. So we may need trillions of trajectories for building a general purpose VRA, and this is way going beyond the limit can be collected via teleoperation, and so that's why I don't think teleoperation wouldn't be a scalable way to build a VLA. So we need some alternative for data. So in Gambol, we believe, we believe in synthetic data. We believe synthetic data going to be the game changer, and so we build this pipeline. We start with, you know, building upon robot model. We start with building the environment. It is basically you need to gather all kinds of object assets to make sure they are physically interactive, and you need to compose the physical interactive environment that has all kind of realistic layout and the complex like objects in it. And after constructing this environment, the most important thing is how to get physically plausible and meaningful trajectories. We get all kinds of mechanisms to generate trajectories. For example, you can turn human videos into like animation like data. There are, you know, very human leg, and just like mimic what human doing in the real world. And also you can relate on physics, space and energy optimization to find you know the correct way to grasp interact with objects. And also, you can do large scale reinforcement learning just through trial and error, that your network finds this way to interact with objects. And when you have different mechanisms to generate trajectories, you will use simulators to examine their like, whether they are valid, and if they are valid, you can use render physical renders that use free tracing to give you photorealistic image. At the end, you get a lot of simulation data, synthetic data you can use sim to real make sure the model trigger of those synthetic data can be directly deployed in the real world. It sounds a very complex pipeline, but we have validated this pipeline in many applications and tasks. So given that most of you know male a right now, focus on pick and place. We start with key. So worldwide, we are the first to synthesize billion scale synthetic data for grasping so it covers diverse environment, illumination, object layouts, object names, etc. We try to, you know, exhaust exhaustively, search all the variables, make sure the civilian skill this cover everything that can be, you know, varied in the real world, and then we just use this data to train our grasp of VRA, which is an end to end VLA model that also leverage embodied chain of thought. And this like a grasp vra is CO trained first with our 1 million VLA tuples that purity from society world, no any real world actions show up in our action data and also web grounding data we need to understand, you know, if you come up with a name of an object, we need to Make sure it is covered by our data, not limited to our Cincinnati data. That's why we leverage more than 100 million, you know, bounding boxes, basically grounding data on the internet so that our model can even grasp things that not show it in our Cincinnati data. So again, we show very, very impressive zero shot performance, starting from simulator in this very famous liberal environment benchmark, we compare our model to Pi Zero and open mirror a in this case, our model has never seen any data In this liberal benchmark, where we allow Pi Zero and open mirror a to be fine tuned on the training squeeze of this liberal data set, the results show that even without any fine tuning, our Model Zero sharply outperform Pi Zero and open mirror a after data fine tuned So our success rate is about 82% and the other model only achieve about
Company. Our mission is to make robots for every industry and every home. I think you know, like how to build a generous robots. It's just very similar how to build a human. You need to have body, a physical body, along with a good brain that is mainly composed of cerebrum and serial regulum. And the cerebrum is in charge of high level policies, deciding what to do. Well, the ribulum is responsible for low level monitor policies, determine how to execute the actions in a faster, accurate and reliable manner. So when we have robots, then the body becomes the embodiment, and our robot brain, also at the same time, is composed of cerebrum and cerebellum. So I will start with introducing our like strategy for building survival, basically, you know, following the main trend a lot of people now believe in below a, it is a large, embodied AI model that takes language vision as inputs and outputs robot actions. The advantage of using VLA is that is end to end model, and it also benefit a lot from pre training large parts of vision, language data pairs, and that's why this model can understand all kinds of human instructions and also all kinds of environments, but right now, I think the major limitations of VL A is the limited zero shot performance. It is way lagging behind the zero shot performance of llms and vlms in most of cases, if you just deploy a VL a in a new environment, just ask it to do something, it just fail. The success rate is almost zero. I think the bottleneck truly lies in data right now. Most of the VLA relay on real world tether operation data to work. And you know, Tesla, a lot of companies, they employ a team of people ranging from, you know, filters a few hundreds of people to do daily teleoperation to collect data. And even, even if this is the common case, the largest public data set of, you know, embodied AI coming from kind of operation is just at the scale of 1 million trajectories. It will be a very good question, how many trajectory do we need to train a truly generalist model? I would say it may take trillion of trajectories, just comparable to trillions of token in LLM, and like 10 trillions of tokens in VLM. So we may need trillions of trajectories for building a general purpose VRA, and this is way going beyond the limit can be collected via teleoperation, and so that's why I don't think teleoperation wouldn't be a scalable way to build a VLA. So we need some alternative for data. So in Gambol, we believe, we believe in synthetic data. We believe synthetic data going to be the game changer, and so we build this pipeline. We start with, you know, building upon robot model. We start with building the environment. It is basically you need to gather all kinds of object assets to make sure they are physically interactive, and you need to compose the physical interactive environment that has all kind of realistic layout and the complex like objects in it. And after constructing this environment, the most important thing is how to get physically plausible and meaningful trajectories. We get all kinds of mechanisms to generate trajectories. For example, you can turn human videos into like animation like data. There are, you know, very human leg, and just like mimic what human doing in the real world. And also you can relate on physics, space and energy optimization to find you know the correct way to grasp interact with objects. And also, you can do large scale reinforcement learning just through trial and error, that your network finds this way to interact with objects. And when you have different mechanisms to generate trajectories, you will use simulators to examine their like, whether they are valid, and if they are valid, you can use render physical renders that use free tracing to give you photorealistic image. At the end, you get a lot of simulation data, synthetic data you can use sim to real make sure the model trigger of those synthetic data can be directly deployed in the real world. It sounds a very complex pipeline, but we have validated this pipeline in many applications and tasks. So given that most of you know male a right now, focus on pick and place. We start with key. So worldwide, we are the first to synthesize billion scale synthetic data for grasping so it covers diverse environment, illumination, object layouts, object names, etc. We try to, you know, exhaust exhaustively, search all the variables, make sure the civilian skill this cover everything that can be, you know, varied in the real world, and then we just use this data to train our grasp of VRA, which is an end to end VLA model that also leverage embodied chain of thought. And this like a grasp vra is CO trained first with our 1 million VLA tuples that purity from society world, no any real world actions show up in our action data and also web grounding data we need to understand, you know, if you come up with a name of an object, we need to Make sure it is covered by our data, not limited to our Cincinnati data. That's why we leverage more than 100 million, you know, bounding boxes, basically grounding data on the internet so that our model can even grasp things that not show it in our Cincinnati data. So again, we show very, very impressive zero shot performance, starting from simulator in this very famous liberal environment benchmark, we compare our model to Pi Zero and open mirror a in this case, our model has never seen any data In this liberal benchmark, where we allow Pi Zero and open mirror a to be fine tuned on the training squeeze of this liberal data set, the results show that even without any fine tuning, our Model Zero sharply outperform Pi Zero and open mirror a after data fine tuned So our success rate is about 82% and the other model only achieve about
S Speaker 18:0660 ish. And also we can directly test our model in real world. You can change the illumination as dramatically as you want, and we find you just name the object, and our model can reliably pick it up. And also you can try something real, really weird, not shown in our synthetic data. For example, shopping cart, excavator, swimming bubble, like electric tax pen, because the model is CO trained with internet grounding data, it actually know what you are meaning and can localize the object. And at the same time, it can transfer the grasping skew from the object category shown in the synthetic data to those real world object categories. And what is even amazing is that in a very challenging, real world scenario where you may have, you know, four by 520, bottles, densely packing in a box. You only need to collect, in total, 200 trajectories to find you our grasp of array, to let it understand your preference you want to pick the bottle from the like the nearest row, one by one from the left to the right, you only need to find your model using 200 trajectories for all the models. It just take a person for four hours after doing to collect this data, and we find the success, success rate of this pickup already like beyond the 90% and you can watch this video clip, it just go all success until the last bottle. And what is even more like amazing is that we observe strong emergent behavior on similar objects. You only need to tally operate on one bottle brand, and you can directly test on novel like drink brand, for example. Here this like Ryan bottle, we on purpose make every roll three bottles change from four bottle to three bottles. And also on the right case, we actually change the size of the cap to from small to large, and our model doesn't get bothered from those chimps. And it still works. I think this basically tell choose that you really need to scale up your data, and then the easiest way to scale up data is from synthetic world. So we observe actually very strong, you know, scaling law from using synthetic grasping data. The takeaway message is the performance scales sub linearly with data. If you want additional success rate, it may require exponentially more training frames. That's why, if you all really tally operation, you really need to go million trajectory or 10 million trajectory to make your demonstration real world deployable. And also, we observe that the same to real gap diminish dramatically when scaling up the training data. When we scale the training data to 1 billion frames, then we find the curve of real world and the same world. They actually converge together. So that's why we do billion scale data. This hasn't been collected in real world by any entities, and we want to show you that not just limited to Google Cloud for us right now, we have leveraged this pipeline to work on realistic grocery shops, and you can just place order on iPad. And we have our robot in front of this really realistic grocery shelf that has, you know, 40 different object categories, and they are densely packed with each other. And our robot can even use two hand, one hand to like, carefully remove the peanuts from the hook, and another hand to catch the bottle, densely packed with other bottles, and this is fully autonomous without any you know tele operation and the training data To conquer this realistic growth shock is composed of 99% Cincinnati data, plus less than 1% real world collaboration. By the way, we show this real world demo in a world robotic conference this August, 1000s of guests have placed order later for our demos, and right now we have even make it a commercial store. We are now running GABA store in 10 inch China city in China now the public completes order, and our robot gonna fetch all kinds of drinks, coffee, you know, souvenirs, and in our campus and this shelf, they weekly, they change the layout, have new product. It's okay. The model generalize across those categories. It doesn't care about if you have new atoms falling into non object categories. And also we, like now, are operating smart pharmacy warehouse using our gel but and the very first smart pharmacy warehouse was opened one year ago in December 2024 since since then, we now have operating more than 30 ish warehouse. You basically can place order online, and the delivery guy will come to the warehouse to pick up the items. And inside the warehouse there is only our humanoid robot, just a while, and it's completely online during the last one year, we like keep improving the model at the same time, just to improve our hardware, we get rid of a lot of problems. We significantly improve the reliability. And right now, the robot can work six hours without stopping per every charge, and also, like the mean mechanically function time has passed one month, so we don't even need to send any engineer to our warehouse every month. And so that's why we are quickly expanding this warehouse things in China across several like first tier cities, just now limited to pick and pick and place, we now can synthesize long horizon, deformable manipulation demo, and that's Why now, even for close holding, we relay synthetic data to achieve seem to real and not just limited to like a parallel breaker. Back to 2023, we come up with the worldwide first Amelia scale Dexter's grasp data set, that is Dex grasp net, which won't aircraft and three outstanding manipulation paper finalists, and in 24 we extend to
60 ish. And also we can directly test our model in real world. You can change the illumination as dramatically as you want, and we find you just name the object, and our model can reliably pick it up. And also you can try something real, really weird, not shown in our synthetic data. For example, shopping cart, excavator, swimming bubble, like electric tax pen, because the model is CO trained with internet grounding data, it actually know what you are meaning and can localize the object. And at the same time, it can transfer the grasping skew from the object category shown in the synthetic data to those real world object categories. And what is even amazing is that in a very challenging, real world scenario where you may have, you know, four by 520, bottles, densely packing in a box. You only need to collect, in total, 200 trajectories to find you our grasp of array, to let it understand your preference you want to pick the bottle from the like the nearest row, one by one from the left to the right, you only need to find your model using 200 trajectories for all the models. It just take a person for four hours after doing to collect this data, and we find the success, success rate of this pickup already like beyond the 90% and you can watch this video clip, it just go all success until the last bottle. And what is even more like amazing is that we observe strong emergent behavior on similar objects. You only need to tally operate on one bottle brand, and you can directly test on novel like drink brand, for example. Here this like Ryan bottle, we on purpose make every roll three bottles change from four bottle to three bottles. And also on the right case, we actually change the size of the cap to from small to large, and our model doesn't get bothered from those chimps. And it still works. I think this basically tell choose that you really need to scale up your data, and then the easiest way to scale up data is from synthetic world. So we observe actually very strong, you know, scaling law from using synthetic grasping data. The takeaway message is the performance scales sub linearly with data. If you want additional success rate, it may require exponentially more training frames. That's why, if you all really tally operation, you really need to go million trajectory or 10 million trajectory to make your demonstration real world deployable. And also, we observe that the same to real gap diminish dramatically when scaling up the training data. When we scale the training data to 1 billion frames, then we find the curve of real world and the same world. They actually converge together. So that's why we do billion scale data. This hasn't been collected in real world by any entities, and we want to show you that not just limited to Google Cloud for us right now, we have leveraged this pipeline to work on realistic grocery shops, and you can just place order on iPad. And we have our robot in front of this really realistic grocery shelf that has, you know, 40 different object categories, and they are densely packed with each other. And our robot can even use two hand, one hand to like, carefully remove the peanuts from the hook, and another hand to catch the bottle, densely packed with other bottles, and this is fully autonomous without any you know tele operation and the training data To conquer this realistic growth shock is composed of 99% Cincinnati data, plus less than 1% real world collaboration. By the way, we show this real world demo in a world robotic conference this August, 1000s of guests have placed order later for our demos, and right now we have even make it a commercial store. We are now running GABA store in 10 inch China city in China now the public completes order, and our robot gonna fetch all kinds of drinks, coffee, you know, souvenirs, and in our campus and this shelf, they weekly, they change the layout, have new product. It's okay. The model generalize across those categories. It doesn't care about if you have new atoms falling into non object categories. And also we, like now, are operating smart pharmacy warehouse using our gel but and the very first smart pharmacy warehouse was opened one year ago in December 2024 since since then, we now have operating more than 30 ish warehouse. You basically can place order online, and the delivery guy will come to the warehouse to pick up the items. And inside the warehouse there is only our humanoid robot, just a while, and it's completely online during the last one year, we like keep improving the model at the same time, just to improve our hardware, we get rid of a lot of problems. We significantly improve the reliability. And right now, the robot can work six hours without stopping per every charge, and also, like the mean mechanically function time has passed one month, so we don't even need to send any engineer to our warehouse every month. And so that's why we are quickly expanding this warehouse things in China across several like first tier cities, just now limited to pick and pick and place, we now can synthesize long horizon, deformable manipulation demo, and that's Why now, even for close holding, we relay synthetic data to achieve seem to real and not just limited to like a parallel breaker. Back to 2023, we come up with the worldwide first Amelia scale Dexter's grasp data set, that is Dex grasp net, which won't aircraft and three outstanding manipulation paper finalists, and in 24 we extend to
60 ish. And also we can directly test our model in real world. You can change the illumination as dramatically as you want, and we find you just name the object, and our model can reliably pick it up. And also you can try something real, really weird, not shown in our synthetic data. For example, shopping cart, excavator, swimming bubble, like electric tax pen, because the model is CO trained with internet grounding data, it actually know what you are meaning and can localize the object. And at the same time, it can transfer the grasping skew from the object category shown in the synthetic data to those real world object categories. And what is even amazing is that in a very challenging, real world scenario where you may have, you know, four by 520, bottles, densely packing in a box. You only need to collect, in total, 200 trajectories to find you our grasp of array, to let it understand your preference you want to pick the bottle from the like the nearest row, one by one from the left to the right, you only need to find your model using 200 trajectories for all the models. It just take a person for four hours after doing to collect this data, and we find the success, success rate of this pickup already like beyond the 90% and you can watch this video clip, it just go all success until the last bottle. And what is even more like amazing is that we observe strong emergent behavior on similar objects. You only need to tally operate on one bottle brand, and you can directly test on novel like drink brand, for example. Here this like Ryan bottle, we on purpose make every roll three bottles change from four bottle to three bottles. And also on the right case, we actually change the size of the cap to from small to large, and our model doesn't get bothered from those chimps. And it still works. I think this basically tell choose that you really need to scale up your data, and then the easiest way to scale up data is from synthetic world. So we observe actually very strong, you know, scaling law from using synthetic grasping data. The takeaway message is the performance scales sub linearly with data. If you want additional success rate, it may require exponentially more training frames. That's why, if you all really tally operation, you really need to go million trajectory or 10 million trajectory to make your demonstration real world deployable. And also, we observe that the same to real gap diminish dramatically when scaling up the training data. When we scale the training data to 1 billion frames, then we find the curve of real world and the same world. They actually converge together. So that's why we do billion scale data. This hasn't been collected in real world by any entities, and we want to show you that not just limited to Google Cloud for us right now, we have leveraged this pipeline to work on realistic grocery shops, and you can just place order on iPad. And we have our robot in front of this really realistic grocery shelf that has, you know, 40 different object categories, and they are densely packed with each other. And our robot can even use two hand, one hand to like, carefully remove the peanuts from the hook, and another hand to catch the bottle, densely packed with other bottles, and this is fully autonomous without any you know tele operation and the training data To conquer this realistic growth shock is composed of 99% Cincinnati data, plus less than 1% real world collaboration. By the way, we show this real world demo in a world robotic conference this August, 1000s of guests have placed order later for our demos, and right now we have even make it a commercial store. We are now running GABA store in 10 inch China city in China now the public completes order, and our robot gonna fetch all kinds of drinks, coffee, you know, souvenirs, and in our campus and this shelf, they weekly, they change the layout, have new product. It's okay. The model generalize across those categories. It doesn't care about if you have new atoms falling into non object categories. And also we, like now, are operating smart pharmacy warehouse using our gel but and the very first smart pharmacy warehouse was opened one year ago in December 2024 since since then, we now have operating more than 30 ish warehouse. You basically can place order online, and the delivery guy will come to the warehouse to pick up the items. And inside the warehouse there is only our humanoid robot, just a while, and it's completely online during the last one year, we like keep improving the model at the same time, just to improve our hardware, we get rid of a lot of problems. We significantly improve the reliability. And right now, the robot can work six hours without stopping per every charge, and also, like the mean mechanically function time has passed one month, so we don't even need to send any engineer to our warehouse every month. And so that's why we are quickly expanding this warehouse things in China across several like first tier cities, just now limited to pick and pick and place, we now can synthesize long horizon, deformable manipulation demo, and that's Why now, even for close holding, we relay synthetic data to achieve seem to real and not just limited to like a parallel breaker. Back to 2023, we come up with the worldwide first Amelia scale Dexter's grasp data set, that is Dex grasp net, which won't aircraft and three outstanding manipulation paper finalists, and in 24 we extend to
60 ish. And also we can directly test our model in real world. You can change the illumination as dramatically as you want, and we find you just name the object, and our model can reliably pick it up. And also you can try something real, really weird, not shown in our synthetic data. For example, shopping cart, excavator, swimming bubble, like electric tax pen, because the model is CO trained with internet grounding data, it actually know what you are meaning and can localize the object. And at the same time, it can transfer the grasping skew from the object category shown in the synthetic data to those real world object categories. And what is even amazing is that in a very challenging, real world scenario where you may have, you know, four by 520, bottles, densely packing in a box. You only need to collect, in total, 200 trajectories to find you our grasp of array, to let it understand your preference you want to pick the bottle from the like the nearest row, one by one from the left to the right, you only need to find your model using 200 trajectories for all the models. It just take a person for four hours after doing to collect this data, and we find the success, success rate of this pickup already like beyond the 90% and you can watch this video clip, it just go all success until the last bottle. And what is even more like amazing is that we observe strong emergent behavior on similar objects. You only need to tally operate on one bottle brand, and you can directly test on novel like drink brand, for example. Here this like Ryan bottle, we on purpose make every roll three bottles change from four bottle to three bottles. And also on the right case, we actually change the size of the cap to from small to large, and our model doesn't get bothered from those chimps. And it still works. I think this basically tell choose that you really need to scale up your data, and then the easiest way to scale up data is from synthetic world. So we observe actually very strong, you know, scaling law from using synthetic grasping data. The takeaway message is the performance scales sub linearly with data. If you want additional success rate, it may require exponentially more training frames. That's why, if you all really tally operation, you really need to go million trajectory or 10 million trajectory to make your demonstration real world deployable. And also, we observe that the same to real gap diminish dramatically when scaling up the training data. When we scale the training data to 1 billion frames, then we find the curve of real world and the same world. They actually converge together. So that's why we do billion scale data. This hasn't been collected in real world by any entities, and we want to show you that not just limited to Google Cloud for us right now, we have leveraged this pipeline to work on realistic grocery shops, and you can just place order on iPad. And we have our robot in front of this really realistic grocery shelf that has, you know, 40 different object categories, and they are densely packed with each other. And our robot can even use two hand, one hand to like, carefully remove the peanuts from the hook, and another hand to catch the bottle, densely packed with other bottles, and this is fully autonomous without any you know tele operation and the training data To conquer this realistic growth shock is composed of 99% Cincinnati data, plus less than 1% real world collaboration. By the way, we show this real world demo in a world robotic conference this August, 1000s of guests have placed order later for our demos, and right now we have even make it a commercial store. We are now running GABA store in 10 inch China city in China now the public completes order, and our robot gonna fetch all kinds of drinks, coffee, you know, souvenirs, and in our campus and this shelf, they weekly, they change the layout, have new product. It's okay. The model generalize across those categories. It doesn't care about if you have new atoms falling into non object categories. And also we, like now, are operating smart pharmacy warehouse using our gel but and the very first smart pharmacy warehouse was opened one year ago in December 2024 since since then, we now have operating more than 30 ish warehouse. You basically can place order online, and the delivery guy will come to the warehouse to pick up the items. And inside the warehouse there is only our humanoid robot, just a while, and it's completely online during the last one year, we like keep improving the model at the same time, just to improve our hardware, we get rid of a lot of problems. We significantly improve the reliability. And right now, the robot can work six hours without stopping per every charge, and also, like the mean mechanically function time has passed one month, so we don't even need to send any engineer to our warehouse every month. And so that's why we are quickly expanding this warehouse things in China across several like first tier cities, just now limited to pick and pick and place, we now can synthesize long horizon, deformable manipulation demo, and that's Why now, even for close holding, we relay synthetic data to achieve seem to real and not just limited to like a parallel breaker. Back to 2023, we come up with the worldwide first Amelia scale Dexter's grasp data set, that is Dex grasp net, which won't aircraft and three outstanding manipulation paper finalists, and in 24 we extend to
S Speaker 115:47we extend it to a billion scale Dexter scrub data set in clutter. And using this billion scale data set, we achieve generalizable broadcasting unseen objects you just like, for a pile of objects, no matter it is transparent or specular, our DEXA San can just do this object clearing with more than 93% of success rate. And all the data comes from that world. There is no real data. And also all the objects are unseen. Their categories can be novel. And this year, we further propose a new synthetic data pipeline called dexonomy. It tried to, you know, synthesize all kinds of grasp types, like dexterous grasp taxonomy, not just limited to power grass. And right now, we can also accommodate all kinds of different Dexter's hand. And also, you can just give me a mesh and I'm gonna synthesize the grass type, you name on it. We believe that, you know, the synthesized method can be extended to a lot of different things, and even to navigation. This is the large scale design data set for navigation that we since set for learning, human tracking and using those navigation data set now we come up with the world first cross embodiment, multi task map enabled navigation foundation model called navform. So navform leverage like a panoramic view, like cameras, and it can form 360 degree environmental perception, cross embodiment, cross type, generalization, generalized navigation. And you can see that, you know, we can deploy it on like a wheel based robot, leg based robots, and even drone. And it can achieve longer than 30 minutes, longer human track is completely vision based, and this is the input to our model, and the blue curve shows the predicted trajectories from our VLA model. And the dog can just attract the person in the wild, even with all kinds of traffic, and you just need to describe the closest that this person wear. The Robin can know which person the dog should follow. And also it's in we can integrate maps into navigation system so that right now, you can just say, use a Google Map, use title map. You can just say, set the destination and the dog can, you know, just to walk in, in the outdoor environment, cross street, across the bridge, autonomously. And all the data come from synthetic world, because we don't have human driver voluntarily to drive dogs, robotic dogs. So that's why, you know, synthetic data plays a big role, and we believe, you know, for to view the robot cerebrum, it's really important to leverage the scalable synthetic data. So far, most of the model are learned via imitation learning. I believe that reinforcement learning will also like take a bigger role for the next step. But speaking of cerebellum, which is in charge of motion control, trajectory tracking, stability, error feedback, and in terms of our human noise robots, it should do whole body and whole hand control. This should learn via reinforcement learning, through interactions, through trial and error, instead of imitation learning. So right now, in Gallup, we have built this pipeline, basically from human motion capture data, one day to reinforcement learning in the simulator and another day to deployment in the real world. And it has been a very mature pipeline to turn any human motion into rocket motion. And behind this pipeline is our ani to track technology. Is a specialist policy that can track any motion with any disturbance. Well, our when the robot is dancing a human, can, you know,
we extend it to a billion scale Dexter scrub data set in clutter. And using this billion scale data set, we achieve generalizable broadcasting unseen objects you just like, for a pile of objects, no matter it is transparent or specular, our DEXA San can just do this object clearing with more than 93% of success rate. And all the data comes from that world. There is no real data. And also all the objects are unseen. Their categories can be novel. And this year, we further propose a new synthetic data pipeline called dexonomy. It tried to, you know, synthesize all kinds of grasp types, like dexterous grasp taxonomy, not just limited to power grass. And right now, we can also accommodate all kinds of different Dexter's hand. And also, you can just give me a mesh and I'm gonna synthesize the grass type, you name on it. We believe that, you know, the synthesized method can be extended to a lot of different things, and even to navigation. This is the large scale design data set for navigation that we since set for learning, human tracking and using those navigation data set now we come up with the world first cross embodiment, multi task map enabled navigation foundation model called navform. So navform leverage like a panoramic view, like cameras, and it can form 360 degree environmental perception, cross embodiment, cross type, generalization, generalized navigation. And you can see that, you know, we can deploy it on like a wheel based robot, leg based robots, and even drone. And it can achieve longer than 30 minutes, longer human track is completely vision based, and this is the input to our model, and the blue curve shows the predicted trajectories from our VLA model. And the dog can just attract the person in the wild, even with all kinds of traffic, and you just need to describe the closest that this person wear. The Robin can know which person the dog should follow. And also it's in we can integrate maps into navigation system so that right now, you can just say, use a Google Map, use title map. You can just say, set the destination and the dog can, you know, just to walk in, in the outdoor environment, cross street, across the bridge, autonomously. And all the data come from synthetic world, because we don't have human driver voluntarily to drive dogs, robotic dogs. So that's why, you know, synthetic data plays a big role, and we believe, you know, for to view the robot cerebrum, it's really important to leverage the scalable synthetic data. So far, most of the model are learned via imitation learning. I believe that reinforcement learning will also like take a bigger role for the next step. But speaking of cerebellum, which is in charge of motion control, trajectory tracking, stability, error feedback, and in terms of our human noise robots, it should do whole body and whole hand control. This should learn via reinforcement learning, through interactions, through trial and error, instead of imitation learning. So right now, in Gallup, we have built this pipeline, basically from human motion capture data, one day to reinforcement learning in the simulator and another day to deployment in the real world. And it has been a very mature pipeline to turn any human motion into rocket motion. And behind this pipeline is our ani to track technology. Is a specialist policy that can track any motion with any disturbance. Well, our when the robot is dancing a human, can, you know,
we extend it to a billion scale Dexter scrub data set in clutter. And using this billion scale data set, we achieve generalizable broadcasting unseen objects you just like, for a pile of objects, no matter it is transparent or specular, our DEXA San can just do this object clearing with more than 93% of success rate. And all the data comes from that world. There is no real data. And also all the objects are unseen. Their categories can be novel. And this year, we further propose a new synthetic data pipeline called dexonomy. It tried to, you know, synthesize all kinds of grasp types, like dexterous grasp taxonomy, not just limited to power grass. And right now, we can also accommodate all kinds of different Dexter's hand. And also, you can just give me a mesh and I'm gonna synthesize the grass type, you name on it. We believe that, you know, the synthesized method can be extended to a lot of different things, and even to navigation. This is the large scale design data set for navigation that we since set for learning, human tracking and using those navigation data set now we come up with the world first cross embodiment, multi task map enabled navigation foundation model called navform. So navform leverage like a panoramic view, like cameras, and it can form 360 degree environmental perception, cross embodiment, cross type, generalization, generalized navigation. And you can see that, you know, we can deploy it on like a wheel based robot, leg based robots, and even drone. And it can achieve longer than 30 minutes, longer human track is completely vision based, and this is the input to our model, and the blue curve shows the predicted trajectories from our VLA model. And the dog can just attract the person in the wild, even with all kinds of traffic, and you just need to describe the closest that this person wear. The Robin can know which person the dog should follow. And also it's in we can integrate maps into navigation system so that right now, you can just say, use a Google Map, use title map. You can just say, set the destination and the dog can, you know, just to walk in, in the outdoor environment, cross street, across the bridge, autonomously. And all the data come from synthetic world, because we don't have human driver voluntarily to drive dogs, robotic dogs. So that's why, you know, synthetic data plays a big role, and we believe, you know, for to view the robot cerebrum, it's really important to leverage the scalable synthetic data. So far, most of the model are learned via imitation learning. I believe that reinforcement learning will also like take a bigger role for the next step. But speaking of cerebellum, which is in charge of motion control, trajectory tracking, stability, error feedback, and in terms of our human noise robots, it should do whole body and whole hand control. This should learn via reinforcement learning, through interactions, through trial and error, instead of imitation learning. So right now, in Gallup, we have built this pipeline, basically from human motion capture data, one day to reinforcement learning in the simulator and another day to deployment in the real world. And it has been a very mature pipeline to turn any human motion into rocket motion. And behind this pipeline is our ani to track technology. Is a specialist policy that can track any motion with any disturbance. Well, our when the robot is dancing a human, can, you know,
we extend it to a billion scale Dexter scrub data set in clutter. And using this billion scale data set, we achieve generalizable broadcasting unseen objects you just like, for a pile of objects, no matter it is transparent or specular, our DEXA San can just do this object clearing with more than 93% of success rate. And all the data comes from that world. There is no real data. And also all the objects are unseen. Their categories can be novel. And this year, we further propose a new synthetic data pipeline called dexonomy. It tried to, you know, synthesize all kinds of grasp types, like dexterous grasp taxonomy, not just limited to power grass. And right now, we can also accommodate all kinds of different Dexter's hand. And also, you can just give me a mesh and I'm gonna synthesize the grass type, you name on it. We believe that, you know, the synthesized method can be extended to a lot of different things, and even to navigation. This is the large scale design data set for navigation that we since set for learning, human tracking and using those navigation data set now we come up with the world first cross embodiment, multi task map enabled navigation foundation model called navform. So navform leverage like a panoramic view, like cameras, and it can form 360 degree environmental perception, cross embodiment, cross type, generalization, generalized navigation. And you can see that, you know, we can deploy it on like a wheel based robot, leg based robots, and even drone. And it can achieve longer than 30 minutes, longer human track is completely vision based, and this is the input to our model, and the blue curve shows the predicted trajectories from our VLA model. And the dog can just attract the person in the wild, even with all kinds of traffic, and you just need to describe the closest that this person wear. The Robin can know which person the dog should follow. And also it's in we can integrate maps into navigation system so that right now, you can just say, use a Google Map, use title map. You can just say, set the destination and the dog can, you know, just to walk in, in the outdoor environment, cross street, across the bridge, autonomously. And all the data come from synthetic world, because we don't have human driver voluntarily to drive dogs, robotic dogs. So that's why, you know, synthetic data plays a big role, and we believe, you know, for to view the robot cerebrum, it's really important to leverage the scalable synthetic data. So far, most of the model are learned via imitation learning. I believe that reinforcement learning will also like take a bigger role for the next step. But speaking of cerebellum, which is in charge of motion control, trajectory tracking, stability, error feedback, and in terms of our human noise robots, it should do whole body and whole hand control. This should learn via reinforcement learning, through interactions, through trial and error, instead of imitation learning. So right now, in Gallup, we have built this pipeline, basically from human motion capture data, one day to reinforcement learning in the simulator and another day to deployment in the real world. And it has been a very mature pipeline to turn any human motion into rocket motion. And behind this pipeline is our ani to track technology. Is a specialist policy that can track any motion with any disturbance. Well, our when the robot is dancing a human, can, you know,
20:32disturb the robot,
S Speaker 120:37whatever it was and the robot was still being hit stable. And you can see all the you know, things on the floor are very clever and very messy, and the learned motion still remain reliable. And I know this looks very fancy, but again, it's still a specialist policy, which means it specialized in one dance. What we really need is a generalist policy that can do general purpose body whole body control. This is our human domain, GPT, and you can give real time command to the robot, and it will follow your command to do any motion you want. And this is an example. I believe this motion tracking, whole body control things, is getting more and more matured, and the major power behind it engine, the major engine behind it is seem to real. Again, seem to real is not always successful, but for lack of robots right now, it is working. I think a more challenging cases would be the controller for Dexter scan back to 2023 we have this SCV 23 best paper award channel is Unix grasper plus plus, which is a large scale RL work for generalizable dexterous manipulation. We come up with the idea of hierarchical reinforcement learning to form a lot of specialists that can grasp a specific type of things with specific poses. And then we use, you know, hierarchical policy distillations to gradually form generalist policy at the end the policy can grasp unseen objects using end to end positive through RL in the simulator. But we found, oh, this is a big problem. We find it cannot seem too real in the real world when the finger touches the, you know, the tabletop and easily get broken. So only recently, we find the correct ingredient to fix this inter real problem. So just one month ago, we announced this DAX in the end, Dexter's neural turning the generalist policy in the simulator, we can collect a little bit real world data, not to learn a neurodynamics model, or, in other words, a world model. In the real world, this neurodynamics model is used to fix the central gap via back propagation, because it's completely differentiable after like using it to fix Synchro gap, we find now our Dexter's hand can use unseen novel objects, and you can just use command to let it rotate in any orientation, any directions, and it can be used to do assembly. So I think you know, in general, I want to give you guys our philosophy about embodied AI data. I believe at this time, we are facing a severe cold start, rather than the warm start cases in LLM, VLM and opponent scraping, and that's why synthetic data at this stage is so important. Is the key to make the breakthrough? Is the key to do to form the game changer, we have to make sure syntactic first and the real world data is just a compliment. I believe we are using this philosophy we can significantly boost the progress of generalist robots. Yeah, that's my talk. Thank you for your question.
whatever it was and the robot was still being hit stable. And you can see all the you know, things on the floor are very clever and very messy, and the learned motion still remain reliable. And I know this looks very fancy, but again, it's still a specialist policy, which means it specialized in one dance. What we really need is a generalist policy that can do general purpose body whole body control. This is our human domain, GPT, and you can give real time command to the robot, and it will follow your command to do any motion you want. And this is an example. I believe this motion tracking, whole body control things, is getting more and more matured, and the major power behind it engine, the major engine behind it is seem to real. Again, seem to real is not always successful, but for lack of robots right now, it is working. I think a more challenging cases would be the controller for Dexter scan back to 2023 we have this SCV 23 best paper award channel is Unix grasper plus plus, which is a large scale RL work for generalizable dexterous manipulation. We come up with the idea of hierarchical reinforcement learning to form a lot of specialists that can grasp a specific type of things with specific poses. And then we use, you know, hierarchical policy distillations to gradually form generalist policy at the end the policy can grasp unseen objects using end to end positive through RL in the simulator. But we found, oh, this is a big problem. We find it cannot seem too real in the real world when the finger touches the, you know, the tabletop and easily get broken. So only recently, we find the correct ingredient to fix this inter real problem. So just one month ago, we announced this DAX in the end, Dexter's neural turning the generalist policy in the simulator, we can collect a little bit real world data, not to learn a neurodynamics model, or, in other words, a world model. In the real world, this neurodynamics model is used to fix the central gap via back propagation, because it's completely differentiable after like using it to fix Synchro gap, we find now our Dexter's hand can use unseen novel objects, and you can just use command to let it rotate in any orientation, any directions, and it can be used to do assembly. So I think you know, in general, I want to give you guys our philosophy about embodied AI data. I believe at this time, we are facing a severe cold start, rather than the warm start cases in LLM, VLM and opponent scraping, and that's why synthetic data at this stage is so important. Is the key to make the breakthrough? Is the key to do to form the game changer, we have to make sure syntactic first and the real world data is just a compliment. I believe we are using this philosophy we can significantly boost the progress of generalist robots. Yeah, that's my talk. Thank you for your question.
whatever it was and the robot was still being hit stable. And you can see all the you know, things on the floor are very clever and very messy, and the learned motion still remain reliable. And I know this looks very fancy, but again, it's still a specialist policy, which means it specialized in one dance. What we really need is a generalist policy that can do general purpose body whole body control. This is our human domain, GPT, and you can give real time command to the robot, and it will follow your command to do any motion you want. And this is an example. I believe this motion tracking, whole body control things, is getting more and more matured, and the major power behind it engine, the major engine behind it is seem to real. Again, seem to real is not always successful, but for lack of robots right now, it is working. I think a more challenging cases would be the controller for Dexter scan back to 2023 we have this SCV 23 best paper award channel is Unix grasper plus plus, which is a large scale RL work for generalizable dexterous manipulation. We come up with the idea of hierarchical reinforcement learning to form a lot of specialists that can grasp a specific type of things with specific poses. And then we use, you know, hierarchical policy distillations to gradually form generalist policy at the end the policy can grasp unseen objects using end to end positive through RL in the simulator. But we found, oh, this is a big problem. We find it cannot seem too real in the real world when the finger touches the, you know, the tabletop and easily get broken. So only recently, we find the correct ingredient to fix this inter real problem. So just one month ago, we announced this DAX in the end, Dexter's neural turning the generalist policy in the simulator, we can collect a little bit real world data, not to learn a neurodynamics model, or, in other words, a world model. In the real world, this neurodynamics model is used to fix the central gap via back propagation, because it's completely differentiable after like using it to fix Synchro gap, we find now our Dexter's hand can use unseen novel objects, and you can just use command to let it rotate in any orientation, any directions, and it can be used to do assembly. So I think you know, in general, I want to give you guys our philosophy about embodied AI data. I believe at this time, we are facing a severe cold start, rather than the warm start cases in LLM, VLM and opponent scraping, and that's why synthetic data at this stage is so important. Is the key to make the breakthrough? Is the key to do to form the game changer, we have to make sure syntactic first and the real world data is just a compliment. I believe we are using this philosophy we can significantly boost the progress of generalist robots. Yeah, that's my talk. Thank you for your question.
whatever it was and the robot was still being hit stable. And you can see all the you know, things on the floor are very clever and very messy, and the learned motion still remain reliable. And I know this looks very fancy, but again, it's still a specialist policy, which means it specialized in one dance. What we really need is a generalist policy that can do general purpose body whole body control. This is our human domain, GPT, and you can give real time command to the robot, and it will follow your command to do any motion you want. And this is an example. I believe this motion tracking, whole body control things, is getting more and more matured, and the major power behind it engine, the major engine behind it is seem to real. Again, seem to real is not always successful, but for lack of robots right now, it is working. I think a more challenging cases would be the controller for Dexter scan back to 2023 we have this SCV 23 best paper award channel is Unix grasper plus plus, which is a large scale RL work for generalizable dexterous manipulation. We come up with the idea of hierarchical reinforcement learning to form a lot of specialists that can grasp a specific type of things with specific poses. And then we use, you know, hierarchical policy distillations to gradually form generalist policy at the end the policy can grasp unseen objects using end to end positive through RL in the simulator. But we found, oh, this is a big problem. We find it cannot seem too real in the real world when the finger touches the, you know, the tabletop and easily get broken. So only recently, we find the correct ingredient to fix this inter real problem. So just one month ago, we announced this DAX in the end, Dexter's neural turning the generalist policy in the simulator, we can collect a little bit real world data, not to learn a neurodynamics model, or, in other words, a world model. In the real world, this neurodynamics model is used to fix the central gap via back propagation, because it's completely differentiable after like using it to fix Synchro gap, we find now our Dexter's hand can use unseen novel objects, and you can just use command to let it rotate in any orientation, any directions, and it can be used to do assembly. So I think you know, in general, I want to give you guys our philosophy about embodied AI data. I believe at this time, we are facing a severe cold start, rather than the warm start cases in LLM, VLM and opponent scraping, and that's why synthetic data at this stage is so important. Is the key to make the breakthrough? Is the key to do to form the game changer, we have to make sure syntactic first and the real world data is just a compliment. I believe we are using this philosophy we can significantly boost the progress of generalist robots. Yeah, that's my talk. Thank you for your question.
S Speaker 224:46I have good and bad news. We have a little extra time for Q and A so I'm curious if there's any questions from the audience, anyone who has
I have good and bad news. We have a little extra time for Q and A so I'm curious if there's any questions from the audience, anyone who has
I have good and bad news. We have a little extra time for Q and A so I'm curious if there's any questions from the audience, anyone who has
I have good and bad news. We have a little extra time for Q and A so I'm curious if there's any questions from the audience, anyone who has
24:56something they'd love to get
something they'd love to get
something they'd love to get
something they'd love to get
25:01answered, really. Oh, here we go. All right,
answered, really. Oh, here we go. All right,
answered, really. Oh, here we go. All right,
answered, really. Oh, here we go. All right,
25:02can we grab the microphone here?
can we grab the microphone here?
can we grab the microphone here?
can we grab the microphone here?
S Speaker 325:16Hello. Nice to meet you, Professor Wang, I'm Brian from accents, so from your first page, our mocap suit zone is on the left corner with Tesla robots. So I have a question. So what do you think compared to the humanoid robots, to the autonomous driving cars? So a lot of people believe that autonomous driving cars dare need a lot of real data, real real world data, like they have to drive the cars go around to collect all kind of sensor data. So what do you think of the humanoid robots? So do you believe that by only doing the synthetic the humanoid will be as successful as the autonomous driving car?
Hello. Nice to meet you, Professor Wang, I'm Brian from accents, so from your first page, our mocap suit zone is on the left corner with Tesla robots. So I have a question. So what do you think compared to the humanoid robots, to the autonomous driving cars? So a lot of people believe that autonomous driving cars dare need a lot of real data, real real world data, like they have to drive the cars go around to collect all kind of sensor data. So what do you think of the humanoid robots? So do you believe that by only doing the synthetic the humanoid will be as successful as the autonomous driving car?
Hello. Nice to meet you, Professor Wang, I'm Brian from accents, so from your first page, our mocap suit zone is on the left corner with Tesla robots. So I have a question. So what do you think compared to the humanoid robots, to the autonomous driving cars? So a lot of people believe that autonomous driving cars dare need a lot of real data, real real world data, like they have to drive the cars go around to collect all kind of sensor data. So what do you think of the humanoid robots? So do you believe that by only doing the synthetic the humanoid will be as successful as the autonomous driving car?
Hello. Nice to meet you, Professor Wang, I'm Brian from accents, so from your first page, our mocap suit zone is on the left corner with Tesla robots. So I have a question. So what do you think compared to the humanoid robots, to the autonomous driving cars? So a lot of people believe that autonomous driving cars dare need a lot of real data, real real world data, like they have to drive the cars go around to collect all kind of sensor data. So what do you think of the humanoid robots? So do you believe that by only doing the synthetic the humanoid will be as successful as the autonomous driving car?
S Speaker 125:58So thank you. I think this is a great question. A lot of people ask me, you know why autos driving? They don't use simulator so much. They don't use since adding data so much. First of all, they do use the setting data. A lot of nerf work, three Gs work, are trying to provide a good simulation environment for auto driving. But biggest bottleneck for using completely using simulator synthetic data for a home driving that you really need smart agents. You need every pedestrian in your simulator have intelligence. You need to make sure every car in your simulator, they are also a clone grabbing right? So that's why it becomes a chicken and egg problem. How to construct such an environment to enable RL or other training in using synthetic data for autonomous driving, for human noise. The good things at this moment, we are mainly dealing with objects instead of human in the future, we do need to do the human microwaves. DO need to deal with human. And if we want to simulate human, then it become a problem, but at this stage, we only need to comply with the physics behind the physical interaction with objects. I think this is an easier problem compared to simulating those smart agents. And so far, I I cannot say you think the setting data will achieve the success, but it just, you know, the force that can get all get us out of this cold start. And I believe at this stage, we relay on 99% of synthetic data plus 1% real data. In the future, when we have our robots largely massively deployed in the real world, we get the data flight will run, and then we will get more real data at that moment, probably the person the percentage of synthetic data and real data can change at the end. I also like real world data, but I think this belongs to the future.
So thank you. I think this is a great question. A lot of people ask me, you know why autos driving? They don't use simulator so much. They don't use since adding data so much. First of all, they do use the setting data. A lot of nerf work, three Gs work, are trying to provide a good simulation environment for auto driving. But biggest bottleneck for using completely using simulator synthetic data for a home driving that you really need smart agents. You need every pedestrian in your simulator have intelligence. You need to make sure every car in your simulator, they are also a clone grabbing right? So that's why it becomes a chicken and egg problem. How to construct such an environment to enable RL or other training in using synthetic data for autonomous driving, for human noise. The good things at this moment, we are mainly dealing with objects instead of human in the future, we do need to do the human microwaves. DO need to deal with human. And if we want to simulate human, then it become a problem, but at this stage, we only need to comply with the physics behind the physical interaction with objects. I think this is an easier problem compared to simulating those smart agents. And so far, I I cannot say you think the setting data will achieve the success, but it just, you know, the force that can get all get us out of this cold start. And I believe at this stage, we relay on 99% of synthetic data plus 1% real data. In the future, when we have our robots largely massively deployed in the real world, we get the data flight will run, and then we will get more real data at that moment, probably the person the percentage of synthetic data and real data can change at the end. I also like real world data, but I think this belongs to the future.
So thank you. I think this is a great question. A lot of people ask me, you know why autos driving? They don't use simulator so much. They don't use since adding data so much. First of all, they do use the setting data. A lot of nerf work, three Gs work, are trying to provide a good simulation environment for auto driving. But biggest bottleneck for using completely using simulator synthetic data for a home driving that you really need smart agents. You need every pedestrian in your simulator have intelligence. You need to make sure every car in your simulator, they are also a clone grabbing right? So that's why it becomes a chicken and egg problem. How to construct such an environment to enable RL or other training in using synthetic data for autonomous driving, for human noise. The good things at this moment, we are mainly dealing with objects instead of human in the future, we do need to do the human microwaves. DO need to deal with human. And if we want to simulate human, then it become a problem, but at this stage, we only need to comply with the physics behind the physical interaction with objects. I think this is an easier problem compared to simulating those smart agents. And so far, I I cannot say you think the setting data will achieve the success, but it just, you know, the force that can get all get us out of this cold start. And I believe at this stage, we relay on 99% of synthetic data plus 1% real data. In the future, when we have our robots largely massively deployed in the real world, we get the data flight will run, and then we will get more real data at that moment, probably the person the percentage of synthetic data and real data can change at the end. I also like real world data, but I think this belongs to the future.
So thank you. I think this is a great question. A lot of people ask me, you know why autos driving? They don't use simulator so much. They don't use since adding data so much. First of all, they do use the setting data. A lot of nerf work, three Gs work, are trying to provide a good simulation environment for auto driving. But biggest bottleneck for using completely using simulator synthetic data for a home driving that you really need smart agents. You need every pedestrian in your simulator have intelligence. You need to make sure every car in your simulator, they are also a clone grabbing right? So that's why it becomes a chicken and egg problem. How to construct such an environment to enable RL or other training in using synthetic data for autonomous driving, for human noise. The good things at this moment, we are mainly dealing with objects instead of human in the future, we do need to do the human microwaves. DO need to deal with human. And if we want to simulate human, then it become a problem, but at this stage, we only need to comply with the physics behind the physical interaction with objects. I think this is an easier problem compared to simulating those smart agents. And so far, I I cannot say you think the setting data will achieve the success, but it just, you know, the force that can get all get us out of this cold start. And I believe at this stage, we relay on 99% of synthetic data plus 1% real data. In the future, when we have our robots largely massively deployed in the real world, we get the data flight will run, and then we will get more real data at that moment, probably the person the percentage of synthetic data and real data can change at the end. I also like real world data, but I think this belongs to the future.
S Speaker 228:09Perhaps one more question, if there's something on your mind, anyone,
Perhaps one more question, if there's something on your mind, anyone,
Perhaps one more question, if there's something on your mind, anyone,
Perhaps one more question, if there's something on your mind, anyone,
S Speaker 428:15here we go. One last question, do I
here we go. One last question, do I
here we go. One last question, do I
here we go. One last question, do I
S Speaker 528:23cannot understand the benefit of doing simulation for pick and place, because you can vary and then do a lot of permutation
cannot understand the benefit of doing simulation for pick and place, because you can vary and then do a lot of permutation
cannot understand the benefit of doing simulation for pick and place, because you can vary and then do a lot of permutation
cannot understand the benefit of doing simulation for pick and place, because you can vary and then do a lot of permutation
S Speaker 128:32for different texture and environment. But when you want to go from simple pick and place to long horizon test, how do you generate automatically in simulation for those long trajectory tests? It seems not obvious. Yeah, I think this is a very good question for you know, deformable objects, like garments. It takes us a while to figure out how to synthesize long horizon folding. And the thing is that every action is just a peak end place, right? So in this case, you only need to decide where to start and where to end right. Now, you have many tools to enable doing that, and you can write heuristic you can when you like, synthesize those garment assets you can like, automatically form key points on them. And those key key points can directly tell you where to better pick and where to better place for folding. And this is just for folding, I believe, for other more complex manipulations, beckerville app will give you more cue on where to start, where to end, and the good things. You can examine all those things in the simulator before you store the trajectory. So this form pattern again for another round of trial and error at the end, it depends on the data collection efficiency, whether you can efficiently gather many like physically correct and plausible trajectories. I don't have a final answer right now, but I think there are more ways that people can imagine. Got it. Thank you so much.
for different texture and environment. But when you want to go from simple pick and place to long horizon test, how do you generate automatically in simulation for those long trajectory tests? It seems not obvious. Yeah, I think this is a very good question for you know, deformable objects, like garments. It takes us a while to figure out how to synthesize long horizon folding. And the thing is that every action is just a peak end place, right? So in this case, you only need to decide where to start and where to end right. Now, you have many tools to enable doing that, and you can write heuristic you can when you like, synthesize those garment assets you can like, automatically form key points on them. And those key key points can directly tell you where to better pick and where to better place for folding. And this is just for folding, I believe, for other more complex manipulations, beckerville app will give you more cue on where to start, where to end, and the good things. You can examine all those things in the simulator before you store the trajectory. So this form pattern again for another round of trial and error at the end, it depends on the data collection efficiency, whether you can efficiently gather many like physically correct and plausible trajectories. I don't have a final answer right now, but I think there are more ways that people can imagine. Got it. Thank you so much.
for different texture and environment. But when you want to go from simple pick and place to long horizon test, how do you generate automatically in simulation for those long trajectory tests? It seems not obvious. Yeah, I think this is a very good question for you know, deformable objects, like garments. It takes us a while to figure out how to synthesize long horizon folding. And the thing is that every action is just a peak end place, right? So in this case, you only need to decide where to start and where to end right. Now, you have many tools to enable doing that, and you can write heuristic you can when you like, synthesize those garment assets you can like, automatically form key points on them. And those key key points can directly tell you where to better pick and where to better place for folding. And this is just for folding, I believe, for other more complex manipulations, beckerville app will give you more cue on where to start, where to end, and the good things. You can examine all those things in the simulator before you store the trajectory. So this form pattern again for another round of trial and error at the end, it depends on the data collection efficiency, whether you can efficiently gather many like physically correct and plausible trajectories. I don't have a final answer right now, but I think there are more ways that people can imagine. Got it. Thank you so much.
for different texture and environment. But when you want to go from simple pick and place to long horizon test, how do you generate automatically in simulation for those long trajectory tests? It seems not obvious. Yeah, I think this is a very good question for you know, deformable objects, like garments. It takes us a while to figure out how to synthesize long horizon folding. And the thing is that every action is just a peak end place, right? So in this case, you only need to decide where to start and where to end right. Now, you have many tools to enable doing that, and you can write heuristic you can when you like, synthesize those garment assets you can like, automatically form key points on them. And those key key points can directly tell you where to better pick and where to better place for folding. And this is just for folding, I believe, for other more complex manipulations, beckerville app will give you more cue on where to start, where to end, and the good things. You can examine all those things in the simulator before you store the trajectory. So this form pattern again for another round of trial and error at the end, it depends on the data collection efficiency, whether you can efficiently gather many like physically correct and plausible trajectories. I don't have a final answer right now, but I think there are more ways that people can imagine. Got it. Thank you so much.
30:22Thank you so much. Really appreciate it. Please give up.
Thank you so much. Really appreciate it. Please give up.
Thank you so much. Really appreciate it. Please give up.
Thank you so much. Really appreciate it. Please give up.
S Speaker 10:00Company. Our mission is to make robots for every industry and every home. I think you know, like how to build a generous robots. It's just very similar how to build a human. You need to have body, a physical body, along with a good brain that is mainly composed of cerebrum and serial regulum. And the cerebrum is in charge of high level policies, deciding what to do. Well, the ribulum is responsible for low level monitor policies, determine how to execute the actions in a faster, accurate and reliable manner. So when we have robots, then the body becomes the embodiment, and our robot brain, also at the same time, is composed of cerebrum and cerebellum. So I will start with introducing our like strategy for building survival, basically, you know, following the main trend a lot of people now believe in below a, it is a large, embodied AI model that takes language vision as inputs and outputs robot actions. The advantage of using VLA is that is end to end model, and it also benefit a lot from pre training large parts of vision, language data pairs, and that's why this model can understand all kinds of human instructions and also all kinds of environments, but right now, I think the major limitations of VL A is the limited zero shot performance. It is way lagging behind the zero shot performance of llms and vlms in most of cases, if you just deploy a VL a in a new environment, just ask it to do something, it just fail. The success rate is almost zero. I think the bottleneck truly lies in data right now. Most of the VLA relay on real world tether operation data to work. And you know, Tesla, a lot of companies, they employ a team of people ranging from, you know, filters a few hundreds of people to do daily teleoperation to collect data. And even, even if this is the common case, the largest public data set of, you know, embodied AI coming from kind of operation is just at the scale of 1 million trajectories. It will be a very good question, how many trajectory do we need to train a truly generalist model? I would say it may take trillion of trajectories, just comparable to trillions of token in LLM, and like 10 trillions of tokens in VLM. So we may need trillions of trajectories for building a general purpose VRA, and this is way going beyond the limit can be collected via teleoperation, and so that's why I don't think teleoperation wouldn't be a scalable way to build a VLA. So we need some alternative for data. So in Gambol, we believe, we believe in synthetic data. We believe synthetic data going to be the game changer, and so we build this pipeline. We start with, you know, building upon robot model. We start with building the environment. It is basically you need to gather all kinds of object assets to make sure they are physically interactive, and you need to compose the physical interactive environment that has all kind of realistic layout and the complex like objects in it. And after constructing this environment, the most important thing is how to get physically plausible and meaningful trajectories. We get all kinds of mechanisms to generate trajectories. For example, you can turn human videos into like animation like data. There are, you know, very human leg, and just like mimic what human doing in the real world. And also you can relate on physics, space and energy optimization to find you know the correct way to grasp interact with objects. And also, you can do large scale reinforcement learning just through trial and error, that your network finds this way to interact with objects. And when you have different mechanisms to generate trajectories, you will use simulators to examine their like, whether they are valid, and if they are valid, you can use render physical renders that use free tracing to give you photorealistic image. At the end, you get a lot of simulation data, synthetic data you can use sim to real make sure the model trigger of those synthetic data can be directly deployed in the real world. It sounds a very complex pipeline, but we have validated this pipeline in many applications and tasks. So given that most of you know male a right now, focus on pick and place. We start with key. So worldwide, we are the first to synthesize billion scale synthetic data for grasping so it covers diverse environment, illumination, object layouts, object names, etc. We try to, you know, exhaust exhaustively, search all the variables, make sure the civilian skill this cover everything that can be, you know, varied in the real world, and then we just use this data to train our grasp of VRA, which is an end to end VLA model that also leverage embodied chain of thought. And this like a grasp vra is CO trained first with our 1 million VLA tuples that purity from society world, no any real world actions show up in our action data and also web grounding data we need to understand, you know, if you come up with a name of an object, we need to Make sure it is covered by our data, not limited to our Cincinnati data. That's why we leverage more than 100 million, you know, bounding boxes, basically grounding data on the internet so that our model can even grasp things that not show it in our Cincinnati data. So again, we show very, very impressive zero shot performance, starting from simulator in this very famous liberal environment benchmark, we compare our model to Pi Zero and open mirror a in this case, our model has never seen any data In this liberal benchmark, where we allow Pi Zero and open mirror a to be fine tuned on the training squeeze of this liberal data set, the results show that even without any fine tuning, our Model Zero sharply outperform Pi Zero and open mirror a after data fine tuned So our success rate is about 82% and the other model only achieve aboutS Speaker 18:0660 ish. And also we can directly test our model in real world. You can change the illumination as dramatically as you want, and we find you just name the object, and our model can reliably pick it up. And also you can try something real, really weird, not shown in our synthetic data. For example, shopping cart, excavator, swimming bubble, like electric tax pen, because the model is CO trained with internet grounding data, it actually know what you are meaning and can localize the object. And at the same time, it can transfer the grasping skew from the object category shown in the synthetic data to those real world object categories. And what is even amazing is that in a very challenging, real world scenario where you may have, you know, four by 520, bottles, densely packing in a box. You only need to collect, in total, 200 trajectories to find you our grasp of array, to let it understand your preference you want to pick the bottle from the like the nearest row, one by one from the left to the right, you only need to find your model using 200 trajectories for all the models. It just take a person for four hours after doing to collect this data, and we find the success, success rate of this pickup already like beyond the 90% and you can watch this video clip, it just go all success until the last bottle. And what is even more like amazing is that we observe strong emergent behavior on similar objects. You only need to tally operate on one bottle brand, and you can directly test on novel like drink brand, for example. Here this like Ryan bottle, we on purpose make every roll three bottles change from four bottle to three bottles. And also on the right case, we actually change the size of the cap to from small to large, and our model doesn't get bothered from those chimps. And it still works. I think this basically tell choose that you really need to scale up your data, and then the easiest way to scale up data is from synthetic world. So we observe actually very strong, you know, scaling law from using synthetic grasping data. The takeaway message is the performance scales sub linearly with data. If you want additional success rate, it may require exponentially more training frames. That's why, if you all really tally operation, you really need to go million trajectory or 10 million trajectory to make your demonstration real world deployable. And also, we observe that the same to real gap diminish dramatically when scaling up the training data. When we scale the training data to 1 billion frames, then we find the curve of real world and the same world. They actually converge together. So that's why we do billion scale data. This hasn't been collected in real world by any entities, and we want to show you that not just limited to Google Cloud for us right now, we have leveraged this pipeline to work on realistic grocery shops, and you can just place order on iPad. And we have our robot in front of this really realistic grocery shelf that has, you know, 40 different object categories, and they are densely packed with each other. And our robot can even use two hand, one hand to like, carefully remove the peanuts from the hook, and another hand to catch the bottle, densely packed with other bottles, and this is fully autonomous without any you know tele operation and the training data To conquer this realistic growth shock is composed of 99% Cincinnati data, plus less than 1% real world collaboration. By the way, we show this real world demo in a world robotic conference this August, 1000s of guests have placed order later for our demos, and right now we have even make it a commercial store. We are now running GABA store in 10 inch China city in China now the public completes order, and our robot gonna fetch all kinds of drinks, coffee, you know, souvenirs, and in our campus and this shelf, they weekly, they change the layout, have new product. It's okay. The model generalize across those categories. It doesn't care about if you have new atoms falling into non object categories. And also we, like now, are operating smart pharmacy warehouse using our gel but and the very first smart pharmacy warehouse was opened one year ago in December 2024 since since then, we now have operating more than 30 ish warehouse. You basically can place order online, and the delivery guy will come to the warehouse to pick up the items. And inside the warehouse there is only our humanoid robot, just a while, and it's completely online during the last one year, we like keep improving the model at the same time, just to improve our hardware, we get rid of a lot of problems. We significantly improve the reliability. And right now, the robot can work six hours without stopping per every charge, and also, like the mean mechanically function time has passed one month, so we don't even need to send any engineer to our warehouse every month. And so that's why we are quickly expanding this warehouse things in China across several like first tier cities, just now limited to pick and pick and place, we now can synthesize long horizon, deformable manipulation demo, and that's Why now, even for close holding, we relay synthetic data to achieve seem to real and not just limited to like a parallel breaker. Back to 2023, we come up with the worldwide first Amelia scale Dexter's grasp data set, that is Dex grasp net, which won't aircraft and three outstanding manipulation paper finalists, and in 24 we extend toS Speaker 115:47we extend it to a billion scale Dexter scrub data set in clutter. And using this billion scale data set, we achieve generalizable broadcasting unseen objects you just like, for a pile of objects, no matter it is transparent or specular, our DEXA San can just do this object clearing with more than 93% of success rate. And all the data comes from that world. There is no real data. And also all the objects are unseen. Their categories can be novel. And this year, we further propose a new synthetic data pipeline called dexonomy. It tried to, you know, synthesize all kinds of grasp types, like dexterous grasp taxonomy, not just limited to power grass. And right now, we can also accommodate all kinds of different Dexter's hand. And also, you can just give me a mesh and I'm gonna synthesize the grass type, you name on it. We believe that, you know, the synthesized method can be extended to a lot of different things, and even to navigation. This is the large scale design data set for navigation that we since set for learning, human tracking and using those navigation data set now we come up with the world first cross embodiment, multi task map enabled navigation foundation model called navform. So navform leverage like a panoramic view, like cameras, and it can form 360 degree environmental perception, cross embodiment, cross type, generalization, generalized navigation. And you can see that, you know, we can deploy it on like a wheel based robot, leg based robots, and even drone. And it can achieve longer than 30 minutes, longer human track is completely vision based, and this is the input to our model, and the blue curve shows the predicted trajectories from our VLA model. And the dog can just attract the person in the wild, even with all kinds of traffic, and you just need to describe the closest that this person wear. The Robin can know which person the dog should follow. And also it's in we can integrate maps into navigation system so that right now, you can just say, use a Google Map, use title map. You can just say, set the destination and the dog can, you know, just to walk in, in the outdoor environment, cross street, across the bridge, autonomously. And all the data come from synthetic world, because we don't have human driver voluntarily to drive dogs, robotic dogs. So that's why, you know, synthetic data plays a big role, and we believe, you know, for to view the robot cerebrum, it's really important to leverage the scalable synthetic data. So far, most of the model are learned via imitation learning. I believe that reinforcement learning will also like take a bigger role for the next step. But speaking of cerebellum, which is in charge of motion control, trajectory tracking, stability, error feedback, and in terms of our human noise robots, it should do whole body and whole hand control. This should learn via reinforcement learning, through interactions, through trial and error, instead of imitation learning. So right now, in Gallup, we have built this pipeline, basically from human motion capture data, one day to reinforcement learning in the simulator and another day to deployment in the real world. And it has been a very mature pipeline to turn any human motion into rocket motion. And behind this pipeline is our ani to track technology. Is a specialist policy that can track any motion with any disturbance. Well, our when the robot is dancing a human, can, you know,20:32disturb the robot,S Speaker 120:37whatever it was and the robot was still being hit stable. And you can see all the you know, things on the floor are very clever and very messy, and the learned motion still remain reliable. And I know this looks very fancy, but again, it's still a specialist policy, which means it specialized in one dance. What we really need is a generalist policy that can do general purpose body whole body control. This is our human domain, GPT, and you can give real time command to the robot, and it will follow your command to do any motion you want. And this is an example. I believe this motion tracking, whole body control things, is getting more and more matured, and the major power behind it engine, the major engine behind it is seem to real. Again, seem to real is not always successful, but for lack of robots right now, it is working. I think a more challenging cases would be the controller for Dexter scan back to 2023 we have this SCV 23 best paper award channel is Unix grasper plus plus, which is a large scale RL work for generalizable dexterous manipulation. We come up with the idea of hierarchical reinforcement learning to form a lot of specialists that can grasp a specific type of things with specific poses. And then we use, you know, hierarchical policy distillations to gradually form generalist policy at the end the policy can grasp unseen objects using end to end positive through RL in the simulator. But we found, oh, this is a big problem. We find it cannot seem too real in the real world when the finger touches the, you know, the tabletop and easily get broken. So only recently, we find the correct ingredient to fix this inter real problem. So just one month ago, we announced this DAX in the end, Dexter's neural turning the generalist policy in the simulator, we can collect a little bit real world data, not to learn a neurodynamics model, or, in other words, a world model. In the real world, this neurodynamics model is used to fix the central gap via back propagation, because it's completely differentiable after like using it to fix Synchro gap, we find now our Dexter's hand can use unseen novel objects, and you can just use command to let it rotate in any orientation, any directions, and it can be used to do assembly. So I think you know, in general, I want to give you guys our philosophy about embodied AI data. I believe at this time, we are facing a severe cold start, rather than the warm start cases in LLM, VLM and opponent scraping, and that's why synthetic data at this stage is so important. Is the key to make the breakthrough? Is the key to do to form the game changer, we have to make sure syntactic first and the real world data is just a compliment. I believe we are using this philosophy we can significantly boost the progress of generalist robots. Yeah, that's my talk. Thank you for your question.S Speaker 224:46I have good and bad news. We have a little extra time for Q and A so I'm curious if there's any questions from the audience, anyone who has24:56something they'd love to get25:01answered, really. Oh, here we go. All right,25:02can we grab the microphone here?S Speaker 325:16Hello. Nice to meet you, Professor Wang, I'm Brian from accents, so from your first page, our mocap suit zone is on the left corner with Tesla robots. So I have a question. So what do you think compared to the humanoid robots, to the autonomous driving cars? So a lot of people believe that autonomous driving cars dare need a lot of real data, real real world data, like they have to drive the cars go around to collect all kind of sensor data. So what do you think of the humanoid robots? So do you believe that by only doing the synthetic the humanoid will be as successful as the autonomous driving car?S Speaker 125:58So thank you. I think this is a great question. A lot of people ask me, you know why autos driving? They don't use simulator so much. They don't use since adding data so much. First of all, they do use the setting data. A lot of nerf work, three Gs work, are trying to provide a good simulation environment for auto driving. But biggest bottleneck for using completely using simulator synthetic data for a home driving that you really need smart agents. You need every pedestrian in your simulator have intelligence. You need to make sure every car in your simulator, they are also a clone grabbing right? So that's why it becomes a chicken and egg problem. How to construct such an environment to enable RL or other training in using synthetic data for autonomous driving, for human noise. The good things at this moment, we are mainly dealing with objects instead of human in the future, we do need to do the human microwaves. DO need to deal with human. And if we want to simulate human, then it become a problem, but at this stage, we only need to comply with the physics behind the physical interaction with objects. I think this is an easier problem compared to simulating those smart agents. And so far, I I cannot say you think the setting data will achieve the success, but it just, you know, the force that can get all get us out of this cold start. And I believe at this stage, we relay on 99% of synthetic data plus 1% real data. In the future, when we have our robots largely massively deployed in the real world, we get the data flight will run, and then we will get more real data at that moment, probably the person the percentage of synthetic data and real data can change at the end. I also like real world data, but I think this belongs to the future.S Speaker 228:09Perhaps one more question, if there's something on your mind, anyone,S Speaker 428:15here we go. One last question, do IS Speaker 528:23cannot understand the benefit of doing simulation for pick and place, because you can vary and then do a lot of permutationS Speaker 128:32for different texture and environment. But when you want to go from simple pick and place to long horizon test, how do you generate automatically in simulation for those long trajectory tests? It seems not obvious. Yeah, I think this is a very good question for you know, deformable objects, like garments. It takes us a while to figure out how to synthesize long horizon folding. And the thing is that every action is just a peak end place, right? So in this case, you only need to decide where to start and where to end right. Now, you have many tools to enable doing that, and you can write heuristic you can when you like, synthesize those garment assets you can like, automatically form key points on them. And those key key points can directly tell you where to better pick and where to better place for folding. And this is just for folding, I believe, for other more complex manipulations, beckerville app will give you more cue on where to start, where to end, and the good things. You can examine all those things in the simulator before you store the trajectory. So this form pattern again for another round of trial and error at the end, it depends on the data collection efficiency, whether you can efficiently gather many like physically correct and plausible trajectories. I don't have a final answer right now, but I think there are more ways that people can imagine. Got it. Thank you so much.30:22Thank you so much. Really appreciate it. Please give up.How accurate was this transcription?