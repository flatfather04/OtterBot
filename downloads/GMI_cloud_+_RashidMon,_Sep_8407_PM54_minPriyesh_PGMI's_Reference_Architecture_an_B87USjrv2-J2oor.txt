Meeting: GMI cloud + Rashid
Mon, Sep 8
4:07 PM
54 min
Priyesh P
GMI's Reference Architecture and Global Deplo
URL: https://otter.ai/u/B87USjrv2-J2oorwvdF_70nkWM8
Downloaded: 2025-12-21T20:25:22.680015
Method: text_extraction
============================================================

S Speaker 10:00Depending on what type of model that customer is looking for. In the latest slide, I'm going to do some live demos so you guys can see what the capability on the inferencing is. So GMI is actually one of the one out of six of the reference architect honors from a media we are also one part of the media cloud honors, so we can very easy to get the GPU locations. And because of this reference architect honors, you can see here is that there's GMI, it's it's, we have two we have two parts, right? We taking care of the USA parts. And we also have the APAC. And there's only two parties. I mean, two companies in APAC actually qualified for this reference architecture honors. One is GMI, one SoftBank. So it's actually a very unique place. I
Depending on what type of model that customer is looking for. In the latest slide, I'm going to do some live demos so you guys can see what the capability on the inferencing is. So GMI is actually one of the one out of six of the reference architect honors from a media we are also one part of the media cloud honors, so we can very easy to get the GPU locations. And because of this reference architect honors, you can see here is that there's GMI, it's it's, we have two we have two parts, right? We taking care of the USA parts. And we also have the APAC. And there's only two parties. I mean, two companies in APAC actually qualified for this reference architecture honors. One is GMI, one SoftBank. So it's actually a very unique place. I
Depending on what type of model that customer is looking for. In the latest slide, I'm going to do some live demos so you guys can see what the capability on the inferencing is. So GMI is actually one of the one out of six of the reference architect honors from a media we are also one part of the media cloud honors, so we can very easy to get the GPU locations. And because of this reference architect honors, you can see here is that there's GMI, it's it's, we have two we have two parts, right? We taking care of the USA parts. And we also have the APAC. And there's only two parties. I mean, two companies in APAC actually qualified for this reference architecture honors. One is GMI, one SoftBank. So it's actually a very unique place. I
Depending on what type of model that customer is looking for. In the latest slide, I'm going to do some live demos so you guys can see what the capability on the inferencing is. So GMI is actually one of the one out of six of the reference architect honors from a media we are also one part of the media cloud honors, so we can very easy to get the GPU locations. And because of this reference architect honors, you can see here is that there's GMI, it's it's, we have two we have two parts, right? We taking care of the USA parts. And we also have the APAC. And there's only two parties. I mean, two companies in APAC actually qualified for this reference architecture honors. One is GMI, one SoftBank. So it's actually a very unique place. I
1:03so we the first I was mentioning about.
so we the first I was mentioning about.
so we the first I was mentioning about.
so we the first I was mentioning about.
S Speaker 11:07So what we do is we providing the solutions, providing the platform for customer, easy to use, right? We have HTTP, 100 280 200 GB, 200 and that's crossing the globally. So total, we have seven location right now. We have one. We have cluster in Singapore, Malaysia, Thailand, Taiwan, Japan, us. So those are the majority that we deploy our GPUs. And since we are Nvidia cloud partners, so all our designs based on the NVIDIA reference architectures, so we follow all the guidelines make sure the system is having the best performance. So really quick to go over to cluster engines. And I'm going to switch the screen after this. So the cluster engine you can think about is just Infrastructure as a Service. So we design everything's actually here is designed by GMI. So we designed this software to manage all the GPUs, and very easy for customer to do bare metal container. And later on, we're going to support HPC slur. So it's easy for those people in the GPU world, right? And this is just some sort of the pictures you can take a look. So we supporting. You can see here, you can do the orchestrations and also container management. We also can monitor all the GPU clusters in utilize this tool, and at the same time, because we have the IAM type to do the VPC for a customer. So there you can consider this is very secure environments for customers. Some of the enterprise has a very care about this.
So what we do is we providing the solutions, providing the platform for customer, easy to use, right? We have HTTP, 100 280 200 GB, 200 and that's crossing the globally. So total, we have seven location right now. We have one. We have cluster in Singapore, Malaysia, Thailand, Taiwan, Japan, us. So those are the majority that we deploy our GPUs. And since we are Nvidia cloud partners, so all our designs based on the NVIDIA reference architectures, so we follow all the guidelines make sure the system is having the best performance. So really quick to go over to cluster engines. And I'm going to switch the screen after this. So the cluster engine you can think about is just Infrastructure as a Service. So we design everything's actually here is designed by GMI. So we designed this software to manage all the GPUs, and very easy for customer to do bare metal container. And later on, we're going to support HPC slur. So it's easy for those people in the GPU world, right? And this is just some sort of the pictures you can take a look. So we supporting. You can see here, you can do the orchestrations and also container management. We also can monitor all the GPU clusters in utilize this tool, and at the same time, because we have the IAM type to do the VPC for a customer. So there you can consider this is very secure environments for customers. Some of the enterprise has a very care about this.
So what we do is we providing the solutions, providing the platform for customer, easy to use, right? We have HTTP, 100 280 200 GB, 200 and that's crossing the globally. So total, we have seven location right now. We have one. We have cluster in Singapore, Malaysia, Thailand, Taiwan, Japan, us. So those are the majority that we deploy our GPUs. And since we are Nvidia cloud partners, so all our designs based on the NVIDIA reference architectures, so we follow all the guidelines make sure the system is having the best performance. So really quick to go over to cluster engines. And I'm going to switch the screen after this. So the cluster engine you can think about is just Infrastructure as a Service. So we design everything's actually here is designed by GMI. So we designed this software to manage all the GPUs, and very easy for customer to do bare metal container. And later on, we're going to support HPC slur. So it's easy for those people in the GPU world, right? And this is just some sort of the pictures you can take a look. So we supporting. You can see here, you can do the orchestrations and also container management. We also can monitor all the GPU clusters in utilize this tool, and at the same time, because we have the IAM type to do the VPC for a customer. So there you can consider this is very secure environments for customers. Some of the enterprise has a very care about this.
So what we do is we providing the solutions, providing the platform for customer, easy to use, right? We have HTTP, 100 280 200 GB, 200 and that's crossing the globally. So total, we have seven location right now. We have one. We have cluster in Singapore, Malaysia, Thailand, Taiwan, Japan, us. So those are the majority that we deploy our GPUs. And since we are Nvidia cloud partners, so all our designs based on the NVIDIA reference architectures, so we follow all the guidelines make sure the system is having the best performance. So really quick to go over to cluster engines. And I'm going to switch the screen after this. So the cluster engine you can think about is just Infrastructure as a Service. So we design everything's actually here is designed by GMI. So we designed this software to manage all the GPUs, and very easy for customer to do bare metal container. And later on, we're going to support HPC slur. So it's easy for those people in the GPU world, right? And this is just some sort of the pictures you can take a look. So we supporting. You can see here, you can do the orchestrations and also container management. We also can monitor all the GPU clusters in utilize this tool, and at the same time, because we have the IAM type to do the VPC for a customer. So there you can consider this is very secure environments for customers. Some of the enterprise has a very care about this.
S Speaker 23:13and this cluster, and then Andy, I remember Alex had mentioned before that cluster engine you can also license to to customers that also want to deploy it on their own, correct?
and this cluster, and then Andy, I remember Alex had mentioned before that cluster engine you can also license to to customers that also want to deploy it on their own, correct?
and this cluster, and then Andy, I remember Alex had mentioned before that cluster engine you can also license to to customers that also want to deploy it on their own, correct?
and this cluster, and then Andy, I remember Alex had mentioned before that cluster engine you can also license to to customers that also want to deploy it on their own, correct?
S Speaker 13:29So we sell this as a software license and charge by subscription, right? So if the customer deploying their own private cloud, for example, right? And they require some sort of software. Normally, the media will tell you to buy their license. But at the same time, we also provide this type of service to customers if they don't necessarily have to go with a media license. So this is just I want to show you, what is the capability on our cluster engines. Once you get into the class engine websites, actually our platform website, you can see here, it indicated what exactly the location for the data centers, and indicate how many resources you have for that data centers. So for some customer want to do on the mail, it's very easy for them just get on the platform and lease the system right away. So I want to show you here is if I do a search, for example, the L 40s, I can see there's a resource available in Taiwan lab. And I can do bare metal service. I can do container service or cluster depends, right? So to give you ideas on the bare metals we doing the rent, you can easy to see because it's bare metal. So they come with a GPU cards, right? And in our platforms, you can, depending on how long the contract you want to do for your cluster. You can select from here, and once you select it, go next. And depending on customer want to do, the SSH key, you can type in here. If the customer want to add the storage to assistance, you can also add it to it. And then you can just, very easy to add it to the car.
So we sell this as a software license and charge by subscription, right? So if the customer deploying their own private cloud, for example, right? And they require some sort of software. Normally, the media will tell you to buy their license. But at the same time, we also provide this type of service to customers if they don't necessarily have to go with a media license. So this is just I want to show you, what is the capability on our cluster engines. Once you get into the class engine websites, actually our platform website, you can see here, it indicated what exactly the location for the data centers, and indicate how many resources you have for that data centers. So for some customer want to do on the mail, it's very easy for them just get on the platform and lease the system right away. So I want to show you here is if I do a search, for example, the L 40s, I can see there's a resource available in Taiwan lab. And I can do bare metal service. I can do container service or cluster depends, right? So to give you ideas on the bare metals we doing the rent, you can easy to see because it's bare metal. So they come with a GPU cards, right? And in our platforms, you can, depending on how long the contract you want to do for your cluster. You can select from here, and once you select it, go next. And depending on customer want to do, the SSH key, you can type in here. If the customer want to add the storage to assistance, you can also add it to it. And then you can just, very easy to add it to the car.
So we sell this as a software license and charge by subscription, right? So if the customer deploying their own private cloud, for example, right? And they require some sort of software. Normally, the media will tell you to buy their license. But at the same time, we also provide this type of service to customers if they don't necessarily have to go with a media license. So this is just I want to show you, what is the capability on our cluster engines. Once you get into the class engine websites, actually our platform website, you can see here, it indicated what exactly the location for the data centers, and indicate how many resources you have for that data centers. So for some customer want to do on the mail, it's very easy for them just get on the platform and lease the system right away. So I want to show you here is if I do a search, for example, the L 40s, I can see there's a resource available in Taiwan lab. And I can do bare metal service. I can do container service or cluster depends, right? So to give you ideas on the bare metals we doing the rent, you can easy to see because it's bare metal. So they come with a GPU cards, right? And in our platforms, you can, depending on how long the contract you want to do for your cluster. You can select from here, and once you select it, go next. And depending on customer want to do, the SSH key, you can type in here. If the customer want to add the storage to assistance, you can also add it to it. And then you can just, very easy to add it to the car.
So we sell this as a software license and charge by subscription, right? So if the customer deploying their own private cloud, for example, right? And they require some sort of software. Normally, the media will tell you to buy their license. But at the same time, we also provide this type of service to customers if they don't necessarily have to go with a media license. So this is just I want to show you, what is the capability on our cluster engines. Once you get into the class engine websites, actually our platform website, you can see here, it indicated what exactly the location for the data centers, and indicate how many resources you have for that data centers. So for some customer want to do on the mail, it's very easy for them just get on the platform and lease the system right away. So I want to show you here is if I do a search, for example, the L 40s, I can see there's a resource available in Taiwan lab. And I can do bare metal service. I can do container service or cluster depends, right? So to give you ideas on the bare metals we doing the rent, you can easy to see because it's bare metal. So they come with a GPU cards, right? And in our platforms, you can, depending on how long the contract you want to do for your cluster. You can select from here, and once you select it, go next. And depending on customer want to do, the SSH key, you can type in here. If the customer want to add the storage to assistance, you can also add it to it. And then you can just, very easy to add it to the car.
S Speaker 15:29If we do the container, say, for example, the PI torch, you can just spread it. And in a container service, you can do one GPU, two GPU, four GPU, or AGP depends, right? So depending on what really customer looking for, it's and then at the same time, you can also open all the ports. So customer can easy management on their firewall, right? You can open whatever the ports they require for their environments at the center, also storage. You can also add it to it.
If we do the container, say, for example, the PI torch, you can just spread it. And in a container service, you can do one GPU, two GPU, four GPU, or AGP depends, right? So depending on what really customer looking for, it's and then at the same time, you can also open all the ports. So customer can easy management on their firewall, right? You can open whatever the ports they require for their environments at the center, also storage. You can also add it to it.
If we do the container, say, for example, the PI torch, you can just spread it. And in a container service, you can do one GPU, two GPU, four GPU, or AGP depends, right? So depending on what really customer looking for, it's and then at the same time, you can also open all the ports. So customer can easy management on their firewall, right? You can open whatever the ports they require for their environments at the center, also storage. You can also add it to it.
If we do the container, say, for example, the PI torch, you can just spread it. And in a container service, you can do one GPU, two GPU, four GPU, or AGP depends, right? So depending on what really customer looking for, it's and then at the same time, you can also open all the ports. So customer can easy management on their firewall, right? You can open whatever the ports they require for their environments at the center, also storage. You can also add it to it.
S Speaker 36:06So can I ask a couple of quick questions? Yes, in terms of your primary sort of pitch to customers or value prop, can you speak to that? Yeah.
So can I ask a couple of quick questions? Yes, in terms of your primary sort of pitch to customers or value prop, can you speak to that? Yeah.
So can I ask a couple of quick questions? Yes, in terms of your primary sort of pitch to customers or value prop, can you speak to that? Yeah.
So can I ask a couple of quick questions? Yes, in terms of your primary sort of pitch to customers or value prop, can you speak to that? Yeah.
S Speaker 16:22So this type of software typically for other NCP, they just utilize by themselves, right? They don't necessarily resell those. But as a GMI, we start this software for one of our big customers, and then we find out that the software is actually very unique and very, actually, very easy for customer to use. So we, we decided to make as a product to resell to the customer, and at the same time we utilize it. So the customer very easy to do the either public cloud or they want to do the private cloud deployments, they can utilize this software, this software itself. It can very easy for customer if they try to management large clusters, for example, if they want to do 1000 2000 GPUs, and they can just utilize this
So this type of software typically for other NCP, they just utilize by themselves, right? They don't necessarily resell those. But as a GMI, we start this software for one of our big customers, and then we find out that the software is actually very unique and very, actually, very easy for customer to use. So we, we decided to make as a product to resell to the customer, and at the same time we utilize it. So the customer very easy to do the either public cloud or they want to do the private cloud deployments, they can utilize this software, this software itself. It can very easy for customer if they try to management large clusters, for example, if they want to do 1000 2000 GPUs, and they can just utilize this
So this type of software typically for other NCP, they just utilize by themselves, right? They don't necessarily resell those. But as a GMI, we start this software for one of our big customers, and then we find out that the software is actually very unique and very, actually, very easy for customer to use. So we, we decided to make as a product to resell to the customer, and at the same time we utilize it. So the customer very easy to do the either public cloud or they want to do the private cloud deployments, they can utilize this software, this software itself. It can very easy for customer if they try to management large clusters, for example, if they want to do 1000 2000 GPUs, and they can just utilize this
So this type of software typically for other NCP, they just utilize by themselves, right? They don't necessarily resell those. But as a GMI, we start this software for one of our big customers, and then we find out that the software is actually very unique and very, actually, very easy for customer to use. So we, we decided to make as a product to resell to the customer, and at the same time we utilize it. So the customer very easy to do the either public cloud or they want to do the private cloud deployments, they can utilize this software, this software itself. It can very easy for customer if they try to management large clusters, for example, if they want to do 1000 2000 GPUs, and they can just utilize this
S Speaker 37:18so it's largely sort of resource management and so on that, that layer of software,
so it's largely sort of resource management and so on that, that layer of software,
so it's largely sort of resource management and so on that, that layer of software,
so it's largely sort of resource management and so on that, that layer of software,
S Speaker 17:25correct, okay, so, like, if you see here, if we go back to the main, main page, right, you can very easy to to management, all the containers, bare metals, what's Whatever the service, the customer rent, and they can just monitor everything here. So you can see here, I do the one bare metals. I also do the one container. So you can on this platforms, customer can easily to do the removal or recycle the systems, right? They don't have to necessarily contact the IT guy or contact our take support. They can just do it by themselves.
correct, okay, so, like, if you see here, if we go back to the main, main page, right, you can very easy to to management, all the containers, bare metals, what's Whatever the service, the customer rent, and they can just monitor everything here. So you can see here, I do the one bare metals. I also do the one container. So you can on this platforms, customer can easily to do the removal or recycle the systems, right? They don't have to necessarily contact the IT guy or contact our take support. They can just do it by themselves.
correct, okay, so, like, if you see here, if we go back to the main, main page, right, you can very easy to to management, all the containers, bare metals, what's Whatever the service, the customer rent, and they can just monitor everything here. So you can see here, I do the one bare metals. I also do the one container. So you can on this platforms, customer can easily to do the removal or recycle the systems, right? They don't have to necessarily contact the IT guy or contact our take support. They can just do it by themselves.
correct, okay, so, like, if you see here, if we go back to the main, main page, right, you can very easy to to management, all the containers, bare metals, what's Whatever the service, the customer rent, and they can just monitor everything here. So you can see here, I do the one bare metals. I also do the one container. So you can on this platforms, customer can easily to do the removal or recycle the systems, right? They don't have to necessarily contact the IT guy or contact our take support. They can just do it by themselves.
S Speaker 28:09So Andy, if you were to compare this with something like a core beef, do they like? How like their cluster management software versus your cluster management software? What's the Delta?
So Andy, if you were to compare this with something like a core beef, do they like? How like their cluster management software versus your cluster management software? What's the Delta?
So Andy, if you were to compare this with something like a core beef, do they like? How like their cluster management software versus your cluster management software? What's the Delta?
So Andy, if you were to compare this with something like a core beef, do they like? How like their cluster management software versus your cluster management software? What's the Delta?
S Speaker 18:22My understanding? Cool. We've also doing, oh, only doing bare metal, right? And we're doing bare metal container, and we're also doing start. So this is something different than other providers. Same thing for containers. You can just do this. And I was mentioning about the security. So in organizations, you can do owner and members. So for example, if you rent the space here, for example, if there's a one company CEO, he has probably has a the ownership for everything, or the it, the IT administrator. They can, he can very easy to assign the resource to someone else in the company. So it's, it's very easy for them to management the entire cluster, whatever they rent.
My understanding? Cool. We've also doing, oh, only doing bare metal, right? And we're doing bare metal container, and we're also doing start. So this is something different than other providers. Same thing for containers. You can just do this. And I was mentioning about the security. So in organizations, you can do owner and members. So for example, if you rent the space here, for example, if there's a one company CEO, he has probably has a the ownership for everything, or the it, the IT administrator. They can, he can very easy to assign the resource to someone else in the company. So it's, it's very easy for them to management the entire cluster, whatever they rent.
My understanding? Cool. We've also doing, oh, only doing bare metal, right? And we're doing bare metal container, and we're also doing start. So this is something different than other providers. Same thing for containers. You can just do this. And I was mentioning about the security. So in organizations, you can do owner and members. So for example, if you rent the space here, for example, if there's a one company CEO, he has probably has a the ownership for everything, or the it, the IT administrator. They can, he can very easy to assign the resource to someone else in the company. So it's, it's very easy for them to management the entire cluster, whatever they rent.
My understanding? Cool. We've also doing, oh, only doing bare metal, right? And we're doing bare metal container, and we're also doing start. So this is something different than other providers. Same thing for containers. You can just do this. And I was mentioning about the security. So in organizations, you can do owner and members. So for example, if you rent the space here, for example, if there's a one company CEO, he has probably has a the ownership for everything, or the it, the IT administrator. They can, he can very easy to assign the resource to someone else in the company. So it's, it's very easy for them to management the entire cluster, whatever they rent.
S Speaker 39:21Okay, I guess the other question. So, how do you so I understand the value prop. How do you go about securing customers?
Okay, I guess the other question. So, how do you so I understand the value prop. How do you go about securing customers?
Okay, I guess the other question. So, how do you so I understand the value prop. How do you go about securing customers?
Okay, I guess the other question. So, how do you so I understand the value prop. How do you go about securing customers?
S Speaker 49:34Yeah, it was my question. So what's your customer acquisition strategy on this?
Yeah, it was my question. So what's your customer acquisition strategy on this?
Yeah, it was my question. So what's your customer acquisition strategy on this?
Yeah, it was my question. So what's your customer acquisition strategy on this?
S Speaker 19:38Right? So for resell the licenses is, we have a lot of customer in APAC. They don't necessarily have the software capability to design or design the software by themselves. So we actually resell this license. For example, in Japan, we have a lot of customer that ask you for this. So if you ask into the regarding to the value propositions, I think the very unique places, we have this software ready for customer to use. And like I mentioned to you before,
Right? So for resell the licenses is, we have a lot of customer in APAC. They don't necessarily have the software capability to design or design the software by themselves. So we actually resell this license. For example, in Japan, we have a lot of customer that ask you for this. So if you ask into the regarding to the value propositions, I think the very unique places, we have this software ready for customer to use. And like I mentioned to you before,
Right? So for resell the licenses is, we have a lot of customer in APAC. They don't necessarily have the software capability to design or design the software by themselves. So we actually resell this license. For example, in Japan, we have a lot of customer that ask you for this. So if you ask into the regarding to the value propositions, I think the very unique places, we have this software ready for customer to use. And like I mentioned to you before,
Right? So for resell the licenses is, we have a lot of customer in APAC. They don't necessarily have the software capability to design or design the software by themselves. So we actually resell this license. For example, in Japan, we have a lot of customer that ask you for this. So if you ask into the regarding to the value propositions, I think the very unique places, we have this software ready for customer to use. And like I mentioned to you before,
S Speaker 210:20a lot, I think, let me so which is a question, and Russia The question is, which is also right now, you know, GMA has three cards, just cluster engine, inference engine and and also they have their own GPUs. So is a question more around customer acquisition overall, or customer acquisition just for this cluster and product.
a lot, I think, let me so which is a question, and Russia The question is, which is also right now, you know, GMA has three cards, just cluster engine, inference engine and and also they have their own GPUs. So is a question more around customer acquisition overall, or customer acquisition just for this cluster and product.
a lot, I think, let me so which is a question, and Russia The question is, which is also right now, you know, GMA has three cards, just cluster engine, inference engine and and also they have their own GPUs. So is a question more around customer acquisition overall, or customer acquisition just for this cluster and product.
a lot, I think, let me so which is a question, and Russia The question is, which is also right now, you know, GMA has three cards, just cluster engine, inference engine and and also they have their own GPUs. So is a question more around customer acquisition overall, or customer acquisition just for this cluster and product.
S Speaker 410:45I'm trying to understand personally. I don't know what we're saying. I'm trying to understand why people go to GMI, and is my HMI doing an active customer reach out and customer acquisition campaign, versus not. So what's your good and what type of customer are you eventually, yeah.
I'm trying to understand personally. I don't know what we're saying. I'm trying to understand why people go to GMI, and is my HMI doing an active customer reach out and customer acquisition campaign, versus not. So what's your good and what type of customer are you eventually, yeah.
I'm trying to understand personally. I don't know what we're saying. I'm trying to understand why people go to GMI, and is my HMI doing an active customer reach out and customer acquisition campaign, versus not. So what's your good and what type of customer are you eventually, yeah.
I'm trying to understand personally. I don't know what we're saying. I'm trying to understand why people go to GMI, and is my HMI doing an active customer reach out and customer acquisition campaign, versus not. So what's your good and what type of customer are you eventually, yeah.
11:10yeah. So we have,
S Speaker 511:21yeah. So we have a couple customers. First of all, target customers are from CSA to see startups all the way to Fortune 500 to Fortune Philippines. It proves one is software services and one is GPUs. And in terms of GPUs, it's self explanatory, right? They either want API services or endpoints on demand via computers bare metal or credit based
yeah. So we have a couple customers. First of all, target customers are from CSA to see startups all the way to Fortune 500 to Fortune Philippines. It proves one is software services and one is GPUs. And in terms of GPUs, it's self explanatory, right? They either want API services or endpoints on demand via computers bare metal or credit based
yeah. So we have a couple customers. First of all, target customers are from CSA to see startups all the way to Fortune 500 to Fortune Philippines. It proves one is software services and one is GPUs. And in terms of GPUs, it's self explanatory, right? They either want API services or endpoints on demand via computers bare metal or credit based
yeah. So we have a couple customers. First of all, target customers are from CSA to see startups all the way to Fortune 500 to Fortune Philippines. It proves one is software services and one is GPUs. And in terms of GPUs, it's self explanatory, right? They either want API services or endpoints on demand via computers bare metal or credit based
S Speaker 511:53why people choose us one our locations, right? We're the only company that offer operates in both Asia and us multiple locations, coast to coast and all across East Asia and Southeast Asia. Second is our products, right? We offer one of the fastest tokens, as well as highest input compared to anyone else in the market. You can look at this in the artificial analysis in terms of any for example, and our speed, our latency, are just two degrees faster than any other competitor out there. And there is service right? Our industry is called CSP for reasons, cloud service provider, right? But most of the hyperscalers right now, it's just CP and it's cloud provider. They have no service. Unless you have a 20, $30 million per year of spend, they are not going to give you an AE, right? So we're a small customer, sorry, small, small cloud players. And if a customer come to us with 20, 30 million check per year, we're going to service them very, very well. In fact, this is part of our SLA to any customer. It's 10 one hour, two hours, it's 10 minutes response, one hour full system diagnosis and two hours full system recovery. That is our guarantee to our customer. So in summary, those three reasons allow us to win customers globally, not just from the US, but Asia and some in Europe
why people choose us one our locations, right? We're the only company that offer operates in both Asia and us multiple locations, coast to coast and all across East Asia and Southeast Asia. Second is our products, right? We offer one of the fastest tokens, as well as highest input compared to anyone else in the market. You can look at this in the artificial analysis in terms of any for example, and our speed, our latency, are just two degrees faster than any other competitor out there. And there is service right? Our industry is called CSP for reasons, cloud service provider, right? But most of the hyperscalers right now, it's just CP and it's cloud provider. They have no service. Unless you have a 20, $30 million per year of spend, they are not going to give you an AE, right? So we're a small customer, sorry, small, small cloud players. And if a customer come to us with 20, 30 million check per year, we're going to service them very, very well. In fact, this is part of our SLA to any customer. It's 10 one hour, two hours, it's 10 minutes response, one hour full system diagnosis and two hours full system recovery. That is our guarantee to our customer. So in summary, those three reasons allow us to win customers globally, not just from the US, but Asia and some in Europe
why people choose us one our locations, right? We're the only company that offer operates in both Asia and us multiple locations, coast to coast and all across East Asia and Southeast Asia. Second is our products, right? We offer one of the fastest tokens, as well as highest input compared to anyone else in the market. You can look at this in the artificial analysis in terms of any for example, and our speed, our latency, are just two degrees faster than any other competitor out there. And there is service right? Our industry is called CSP for reasons, cloud service provider, right? But most of the hyperscalers right now, it's just CP and it's cloud provider. They have no service. Unless you have a 20, $30 million per year of spend, they are not going to give you an AE, right? So we're a small customer, sorry, small, small cloud players. And if a customer come to us with 20, 30 million check per year, we're going to service them very, very well. In fact, this is part of our SLA to any customer. It's 10 one hour, two hours, it's 10 minutes response, one hour full system diagnosis and two hours full system recovery. That is our guarantee to our customer. So in summary, those three reasons allow us to win customers globally, not just from the US, but Asia and some in Europe
why people choose us one our locations, right? We're the only company that offer operates in both Asia and us multiple locations, coast to coast and all across East Asia and Southeast Asia. Second is our products, right? We offer one of the fastest tokens, as well as highest input compared to anyone else in the market. You can look at this in the artificial analysis in terms of any for example, and our speed, our latency, are just two degrees faster than any other competitor out there. And there is service right? Our industry is called CSP for reasons, cloud service provider, right? But most of the hyperscalers right now, it's just CP and it's cloud provider. They have no service. Unless you have a 20, $30 million per year of spend, they are not going to give you an AE, right? So we're a small customer, sorry, small, small cloud players. And if a customer come to us with 20, 30 million check per year, we're going to service them very, very well. In fact, this is part of our SLA to any customer. It's 10 one hour, two hours, it's 10 minutes response, one hour full system diagnosis and two hours full system recovery. That is our guarantee to our customer. So in summary, those three reasons allow us to win customers globally, not just from the US, but Asia and some in Europe
S Speaker 213:26as well. Why are these customers today?
as well. Why are these customers today?
as well. Why are these customers today?
as well. Why are these customers today?
S Speaker 513:31Yeah, so we have both organic and and and outreach, right? So we have cafes marketing, we have some ads. We also have a lot of organic traffic and well, what, what amount, where our satisfied customer introduce other other founders that likes our products. So yeah, on our IE, we have,
Yeah, so we have both organic and and and outreach, right? So we have cafes marketing, we have some ads. We also have a lot of organic traffic and well, what, what amount, where our satisfied customer introduce other other founders that likes our products. So yeah, on our IE, we have,
Yeah, so we have both organic and and and outreach, right? So we have cafes marketing, we have some ads. We also have a lot of organic traffic and well, what, what amount, where our satisfied customer introduce other other founders that likes our products. So yeah, on our IE, we have,
Yeah, so we have both organic and and and outreach, right? So we have cafes marketing, we have some ads. We also have a lot of organic traffic and well, what, what amount, where our satisfied customer introduce other other founders that likes our products. So yeah, on our IE, we have,
13:55right now, 25,000
S Speaker 513:57users on it. Regard, when I was visiting, it was 14,000 25,000
users on it. Regard, when I was visiting, it was 14,000 25,000
users on it. Regard, when I was visiting, it was 14,000 25,000
users on it. Regard, when I was visiting, it was 14,000 25,000
S Speaker 514:06productivity, they'll find out more. We have more users using that as well. So so it's a mixed bag of both organic and outreach, our sales team and our marketing and, of course, controls from different other teams.
productivity, they'll find out more. We have more users using that as well. So so it's a mixed bag of both organic and outreach, our sales team and our marketing and, of course, controls from different other teams.
productivity, they'll find out more. We have more users using that as well. So so it's a mixed bag of both organic and outreach, our sales team and our marketing and, of course, controls from different other teams.
productivity, they'll find out more. We have more users using that as well. So so it's a mixed bag of both organic and outreach, our sales team and our marketing and, of course, controls from different other teams.
S Speaker 314:24So, so, Alex, you also mentioned one of the best cost options. You're using similar Nvidia hardware as others where we deliver, yeah,
So, so, Alex, you also mentioned one of the best cost options. You're using similar Nvidia hardware as others where we deliver, yeah,
So, so, Alex, you also mentioned one of the best cost options. You're using similar Nvidia hardware as others where we deliver, yeah,
So, so, Alex, you also mentioned one of the best cost options. You're using similar Nvidia hardware as others where we deliver, yeah,
S Speaker 514:37so this, so this is a great thing, is we were able to break below CUDA, and we're able to squeeze 70% more tokens out of the same GPUs.
so this, so this is a great thing, is we were able to break below CUDA, and we're able to squeeze 70% more tokens out of the same GPUs.
so this, so this is a great thing, is we were able to break below CUDA, and we're able to squeeze 70% more tokens out of the same GPUs.
so this, so this is a great thing, is we were able to break below CUDA, and we're able to squeeze 70% more tokens out of the same GPUs.
14:48We do something called
We do something called
We do something called
We do something called
S Speaker 514:51Parallel inferencing, so we actually use a batch of GPUs through intrinsic this is across compute intensive and memory bound systems. We split the workloads into two. Another trick that we and our trick we do is basically break down all the experts into into each GPU so that we actually run inference much faster. And this shows on a lot of public benchmarks. So all in all, right, if the dollar, it's $1 particularly the same, but you get one token output, then customers are saving more money,
Parallel inferencing, so we actually use a batch of GPUs through intrinsic this is across compute intensive and memory bound systems. We split the workloads into two. Another trick that we and our trick we do is basically break down all the experts into into each GPU so that we actually run inference much faster. And this shows on a lot of public benchmarks. So all in all, right, if the dollar, it's $1 particularly the same, but you get one token output, then customers are saving more money,
Parallel inferencing, so we actually use a batch of GPUs through intrinsic this is across compute intensive and memory bound systems. We split the workloads into two. Another trick that we and our trick we do is basically break down all the experts into into each GPU so that we actually run inference much faster. And this shows on a lot of public benchmarks. So all in all, right, if the dollar, it's $1 particularly the same, but you get one token output, then customers are saving more money,
Parallel inferencing, so we actually use a batch of GPUs through intrinsic this is across compute intensive and memory bound systems. We split the workloads into two. Another trick that we and our trick we do is basically break down all the experts into into each GPU so that we actually run inference much faster. And this shows on a lot of public benchmarks. So all in all, right, if the dollar, it's $1 particularly the same, but you get one token output, then customers are saving more money,
S Speaker 315:27okay? And then you said, somewhere I was also looking at your website in parallel. So you said, somewhere you support like 30 something models,
okay? And then you said, somewhere I was also looking at your website in parallel. So you said, somewhere you support like 30 something models,
okay? And then you said, somewhere I was also looking at your website in parallel. So you said, somewhere you support like 30 something models,
okay? And then you said, somewhere I was also looking at your website in parallel. So you said, somewhere you support like 30 something models,
S Speaker 515:37yes. So then you want to sell the inference page? Yes, sure.
yes. So then you want to sell the inference page? Yes, sure.
yes. So then you want to sell the inference page? Yes, sure.
yes. So then you want to sell the inference page? Yes, sure.
S Speaker 115:49So this is our website. If you go into the products, inference engines, you're going to this page. And once you click the Get start, you automatically bring into our inference engine platforms. So you can see here, there's a large language model posting and the video hosting. So if I, if I click on the large language models, can see all the models posting on this platforms, so including the open AI Lama, four, number three, DC wing queueing and moonshots. For most popular right now, it's actually the video. So we host a lot of video type modeling on our websites, so you can do either text to videos or image to videos. And those are we also have to google vo three on this. Okay,
So this is our website. If you go into the products, inference engines, you're going to this page. And once you click the Get start, you automatically bring into our inference engine platforms. So you can see here, there's a large language model posting and the video hosting. So if I, if I click on the large language models, can see all the models posting on this platforms, so including the open AI Lama, four, number three, DC wing queueing and moonshots. For most popular right now, it's actually the video. So we host a lot of video type modeling on our websites, so you can do either text to videos or image to videos. And those are we also have to google vo three on this. Okay,
So this is our website. If you go into the products, inference engines, you're going to this page. And once you click the Get start, you automatically bring into our inference engine platforms. So you can see here, there's a large language model posting and the video hosting. So if I, if I click on the large language models, can see all the models posting on this platforms, so including the open AI Lama, four, number three, DC wing queueing and moonshots. For most popular right now, it's actually the video. So we host a lot of video type modeling on our websites, so you can do either text to videos or image to videos. And those are we also have to google vo three on this. Okay,
So this is our website. If you go into the products, inference engines, you're going to this page. And once you click the Get start, you automatically bring into our inference engine platforms. So you can see here, there's a large language model posting and the video hosting. So if I, if I click on the large language models, can see all the models posting on this platforms, so including the open AI Lama, four, number three, DC wing queueing and moonshots. For most popular right now, it's actually the video. So we host a lot of video type modeling on our websites, so you can do either text to videos or image to videos. And those are we also have to google vo three on this. Okay,
16:56so I was going, Yeah, go ahead.
so I was going, Yeah, go ahead.
so I was going, Yeah, go ahead.
so I was going, Yeah, go ahead.
S Speaker 417:01Go ahead. Russia, you I think it's probably a
Go ahead. Russia, you I think it's probably a
Go ahead. Russia, you I think it's probably a
Go ahead. Russia, you I think it's probably a
17:09obvious question, but all your arteries in video right now, right?
obvious question, but all your arteries in video right now, right?
obvious question, but all your arteries in video right now, right?
obvious question, but all your arteries in video right now, right?
17:14No, the video is very popular, and we also do like Nvidia
No, the video is very popular, and we also do like Nvidia
No, the video is very popular, and we also do like Nvidia
No, the video is very popular, and we also do like Nvidia
17:19hardware. All the hardware is Nvidia, right?
hardware. All the hardware is Nvidia, right?
hardware. All the hardware is Nvidia, right?
hardware. All the hardware is Nvidia, right?
S Speaker 117:21Yes, only Nvidia. We can only do Nvidia correct.
Yes, only Nvidia. We can only do Nvidia correct.
Yes, only Nvidia. We can only do Nvidia correct.
Yes, only Nvidia. We can only do Nvidia correct.
S Speaker 517:27I think the cool thing is we So, yes, we haven't explored this, but
I think the cool thing is we So, yes, we haven't explored this, but
I think the cool thing is we So, yes, we haven't explored this, but
I think the cool thing is we So, yes, we haven't explored this, but
17:34ours is supposed to identify
ours is supposed to identify
ours is supposed to identify
ours is supposed to identify
S Speaker 117:42so we do immediate. But remember, I was showing the cluster engines. The cluster engine still actually do AMD, Nvidia, everything, so it doesn't really matter. So we try on the AMD mi 350 you also can manage by that
so we do immediate. But remember, I was showing the cluster engines. The cluster engine still actually do AMD, Nvidia, everything, so it doesn't really matter. So we try on the AMD mi 350 you also can manage by that
so we do immediate. But remember, I was showing the cluster engines. The cluster engine still actually do AMD, Nvidia, everything, so it doesn't really matter. So we try on the AMD mi 350 you also can manage by that
so we do immediate. But remember, I was showing the cluster engines. The cluster engine still actually do AMD, Nvidia, everything, so it doesn't really matter. So we try on the AMD mi 350 you also can manage by that
S Speaker 518:00just FYI, yeah. So sorry. I would say so. I would say we can split so the systems or the techniques that we're using, like I was mentioning, we split the workloads, inflicting workloads into compute bound and memory bound problems. And so a smart person would think, Okay, well, I don't have to use it in across h1 100 and h2 100, but split into two different type of GPUs, which is doable, right? So memory valve problem, it's great, huge memory so that could be on the decoder side, where we can use P field for compute. So if quantum chips are I don't know if it's compute or memory, basically advantages, then we can consider using one one or basically the memory valve and stable compute constraint once to optimize for total lower cost for everything. But this is 100% doable, but we just haven't had time to research into heterogeneous cards, but the techniques that we use, typically is allowed for us to think through. So does that make sense?
just FYI, yeah. So sorry. I would say so. I would say we can split so the systems or the techniques that we're using, like I was mentioning, we split the workloads, inflicting workloads into compute bound and memory bound problems. And so a smart person would think, Okay, well, I don't have to use it in across h1 100 and h2 100, but split into two different type of GPUs, which is doable, right? So memory valve problem, it's great, huge memory so that could be on the decoder side, where we can use P field for compute. So if quantum chips are I don't know if it's compute or memory, basically advantages, then we can consider using one one or basically the memory valve and stable compute constraint once to optimize for total lower cost for everything. But this is 100% doable, but we just haven't had time to research into heterogeneous cards, but the techniques that we use, typically is allowed for us to think through. So does that make sense?
just FYI, yeah. So sorry. I would say so. I would say we can split so the systems or the techniques that we're using, like I was mentioning, we split the workloads, inflicting workloads into compute bound and memory bound problems. And so a smart person would think, Okay, well, I don't have to use it in across h1 100 and h2 100, but split into two different type of GPUs, which is doable, right? So memory valve problem, it's great, huge memory so that could be on the decoder side, where we can use P field for compute. So if quantum chips are I don't know if it's compute or memory, basically advantages, then we can consider using one one or basically the memory valve and stable compute constraint once to optimize for total lower cost for everything. But this is 100% doable, but we just haven't had time to research into heterogeneous cards, but the techniques that we use, typically is allowed for us to think through. So does that make sense?
just FYI, yeah. So sorry. I would say so. I would say we can split so the systems or the techniques that we're using, like I was mentioning, we split the workloads, inflicting workloads into compute bound and memory bound problems. And so a smart person would think, Okay, well, I don't have to use it in across h1 100 and h2 100, but split into two different type of GPUs, which is doable, right? So memory valve problem, it's great, huge memory so that could be on the decoder side, where we can use P field for compute. So if quantum chips are I don't know if it's compute or memory, basically advantages, then we can consider using one one or basically the memory valve and stable compute constraint once to optimize for total lower cost for everything. But this is 100% doable, but we just haven't had time to research into heterogeneous cards, but the techniques that we use, typically is allowed for us to think through. So does that make sense?
S Speaker 119:52Are you asking about the h1, 100, 200 pricing?
Are you asking about the h1, 100, 200 pricing?
Are you asking about the h1, 100, 200 pricing?
Are you asking about the h1, 100, 200 pricing?
S Speaker 219:55Let's say, for example, you were to evaluate bringing welcome, right? Okay, so you need to have a value prop to it, and then let's say the value prop is cost. Then what would the cost have to be for? Let's pick a few models, which is where should the cost be for you to actually bring in a new hardware, right? Like, because you have to beat the cost because, so it seems like the cost that you're showing here are they the final cost or, like, for example, for an enterprise user, you're going to discount them further.
Let's say, for example, you were to evaluate bringing welcome, right? Okay, so you need to have a value prop to it, and then let's say the value prop is cost. Then what would the cost have to be for? Let's pick a few models, which is where should the cost be for you to actually bring in a new hardware, right? Like, because you have to beat the cost because, so it seems like the cost that you're showing here are they the final cost or, like, for example, for an enterprise user, you're going to discount them further.
Let's say, for example, you were to evaluate bringing welcome, right? Okay, so you need to have a value prop to it, and then let's say the value prop is cost. Then what would the cost have to be for? Let's pick a few models, which is where should the cost be for you to actually bring in a new hardware, right? Like, because you have to beat the cost because, so it seems like the cost that you're showing here are they the final cost or, like, for example, for an enterprise user, you're going to discount them further.
Let's say, for example, you were to evaluate bringing welcome, right? Okay, so you need to have a value prop to it, and then let's say the value prop is cost. Then what would the cost have to be for? Let's pick a few models, which is where should the cost be for you to actually bring in a new hardware, right? Like, because you have to beat the cost because, so it seems like the cost that you're showing here are they the final cost or, like, for example, for an enterprise user, you're going to discount them further.
S Speaker 120:38I think that's the price showing. Here is the token price, right? So it depending on the models. My understanding is Qualcomm chip is doing very good on the inference. And my understanding also is because of the total I mean, doing the inference is really ending on the token performance per second, like how much token you can generate in seconds, and that's really depend we never had a chance to try on the Qualcomm chip. So I can't really tell what the difference between the I and the Qualcomm in terms of the inference, right? So my I can just tell you that if, if there's a chance to try it, and if there's a chance to see what exactly the output for the token performance, then I can tell if that's suitable for doing all the interest. But my to tell you this, like deep sick, you don't necessarily have to go very high performance GPU, like in terms of each 100 you can run deep sea without any problems, right? And some of the video model might require a little bit higher performance on the GPU, but text to text, typically you just utilize single card or two cards should be fine, yeah, so it's very hard for me. So yeah, okay,
I think that's the price showing. Here is the token price, right? So it depending on the models. My understanding is Qualcomm chip is doing very good on the inference. And my understanding also is because of the total I mean, doing the inference is really ending on the token performance per second, like how much token you can generate in seconds, and that's really depend we never had a chance to try on the Qualcomm chip. So I can't really tell what the difference between the I and the Qualcomm in terms of the inference, right? So my I can just tell you that if, if there's a chance to try it, and if there's a chance to see what exactly the output for the token performance, then I can tell if that's suitable for doing all the interest. But my to tell you this, like deep sick, you don't necessarily have to go very high performance GPU, like in terms of each 100 you can run deep sea without any problems, right? And some of the video model might require a little bit higher performance on the GPU, but text to text, typically you just utilize single card or two cards should be fine, yeah, so it's very hard for me. So yeah, okay,
I think that's the price showing. Here is the token price, right? So it depending on the models. My understanding is Qualcomm chip is doing very good on the inference. And my understanding also is because of the total I mean, doing the inference is really ending on the token performance per second, like how much token you can generate in seconds, and that's really depend we never had a chance to try on the Qualcomm chip. So I can't really tell what the difference between the I and the Qualcomm in terms of the inference, right? So my I can just tell you that if, if there's a chance to try it, and if there's a chance to see what exactly the output for the token performance, then I can tell if that's suitable for doing all the interest. But my to tell you this, like deep sick, you don't necessarily have to go very high performance GPU, like in terms of each 100 you can run deep sea without any problems, right? And some of the video model might require a little bit higher performance on the GPU, but text to text, typically you just utilize single card or two cards should be fine, yeah, so it's very hard for me. So yeah, okay,
I think that's the price showing. Here is the token price, right? So it depending on the models. My understanding is Qualcomm chip is doing very good on the inference. And my understanding also is because of the total I mean, doing the inference is really ending on the token performance per second, like how much token you can generate in seconds, and that's really depend we never had a chance to try on the Qualcomm chip. So I can't really tell what the difference between the I and the Qualcomm in terms of the inference, right? So my I can just tell you that if, if there's a chance to try it, and if there's a chance to see what exactly the output for the token performance, then I can tell if that's suitable for doing all the interest. But my to tell you this, like deep sick, you don't necessarily have to go very high performance GPU, like in terms of each 100 you can run deep sea without any problems, right? And some of the video model might require a little bit higher performance on the GPU, but text to text, typically you just utilize single card or two cards should be fine, yeah, so it's very hard for me. So yeah, okay,
S Speaker 522:06yeah, just, just additional things that I guess the techniques that we use is we can break an mle model into smaller chunks, like I was mentioning that we split into multiple experts, so throwing the model weights into typical if it's just like larger models, then we can only put it in under one server of h2 100, but we can break that into smaller chunks, into h1 100. So this is also another feature that we could do which would be interesting for like objects, but like what I was mentioning with Andy, especially, we don't know the performance of the quantum chips. So happy to take a look evaluate how we can bestly expect, right?
yeah, just, just additional things that I guess the techniques that we use is we can break an mle model into smaller chunks, like I was mentioning that we split into multiple experts, so throwing the model weights into typical if it's just like larger models, then we can only put it in under one server of h2 100, but we can break that into smaller chunks, into h1 100. So this is also another feature that we could do which would be interesting for like objects, but like what I was mentioning with Andy, especially, we don't know the performance of the quantum chips. So happy to take a look evaluate how we can bestly expect, right?
yeah, just, just additional things that I guess the techniques that we use is we can break an mle model into smaller chunks, like I was mentioning that we split into multiple experts, so throwing the model weights into typical if it's just like larger models, then we can only put it in under one server of h2 100, but we can break that into smaller chunks, into h1 100. So this is also another feature that we could do which would be interesting for like objects, but like what I was mentioning with Andy, especially, we don't know the performance of the quantum chips. So happy to take a look evaluate how we can bestly expect, right?
yeah, just, just additional things that I guess the techniques that we use is we can break an mle model into smaller chunks, like I was mentioning that we split into multiple experts, so throwing the model weights into typical if it's just like larger models, then we can only put it in under one server of h2 100, but we can break that into smaller chunks, into h1 100. So this is also another feature that we could do which would be interesting for like objects, but like what I was mentioning with Andy, especially, we don't know the performance of the quantum chips. So happy to take a look evaluate how we can bestly expect, right?
S Speaker 322:51I think the question we're trying to get a sense of is, you know, if I go to your website and I'm looking at your llms, there's a price you've published, obviously, this price includes your margin and so on. How should we be thinking in terms of you take, let's say, a deep seat, 70 billion model, what performance with what configuration would you need to see from a third option? Right? Doesn't matter which one for it to make sense. Like, for instance, on your website, you know, llama 8 billion looks like it's free. Lama 70 billion, there's some price to it. We just because we know our both numbers. But I don't know what PL, GL you've used underlying so on, right? So how do we get a sense of this? And then another question, you said your customers are all the way from startups to Fortune 500 companies. What's a typical customer, like on the higher end, how many GPUs are they renting at a time?
I think the question we're trying to get a sense of is, you know, if I go to your website and I'm looking at your llms, there's a price you've published, obviously, this price includes your margin and so on. How should we be thinking in terms of you take, let's say, a deep seat, 70 billion model, what performance with what configuration would you need to see from a third option? Right? Doesn't matter which one for it to make sense. Like, for instance, on your website, you know, llama 8 billion looks like it's free. Lama 70 billion, there's some price to it. We just because we know our both numbers. But I don't know what PL, GL you've used underlying so on, right? So how do we get a sense of this? And then another question, you said your customers are all the way from startups to Fortune 500 companies. What's a typical customer, like on the higher end, how many GPUs are they renting at a time?
I think the question we're trying to get a sense of is, you know, if I go to your website and I'm looking at your llms, there's a price you've published, obviously, this price includes your margin and so on. How should we be thinking in terms of you take, let's say, a deep seat, 70 billion model, what performance with what configuration would you need to see from a third option? Right? Doesn't matter which one for it to make sense. Like, for instance, on your website, you know, llama 8 billion looks like it's free. Lama 70 billion, there's some price to it. We just because we know our both numbers. But I don't know what PL, GL you've used underlying so on, right? So how do we get a sense of this? And then another question, you said your customers are all the way from startups to Fortune 500 companies. What's a typical customer, like on the higher end, how many GPUs are they renting at a time?
I think the question we're trying to get a sense of is, you know, if I go to your website and I'm looking at your llms, there's a price you've published, obviously, this price includes your margin and so on. How should we be thinking in terms of you take, let's say, a deep seat, 70 billion model, what performance with what configuration would you need to see from a third option? Right? Doesn't matter which one for it to make sense. Like, for instance, on your website, you know, llama 8 billion looks like it's free. Lama 70 billion, there's some price to it. We just because we know our both numbers. But I don't know what PL, GL you've used underlying so on, right? So how do we get a sense of this? And then another question, you said your customers are all the way from startups to Fortune 500 companies. What's a typical customer, like on the higher end, how many GPUs are they renting at a time?
S Speaker 524:08The largest we have is a 2k customer. It's a fortune 50 customer,
The largest we have is a 2k customer. It's a fortune 50 customer,
The largest we have is a 2k customer. It's a fortune 50 customer,
The largest we have is a 2k customer. It's a fortune 50 customer,
24:14okay, sorry, 2000
24:17Yeah, 2000 GPUs. 2000 GPU cards. Okay,
Yeah, 2000 GPUs. 2000 GPU cards. Okay,
Yeah, 2000 GPUs. 2000 GPU cards. Okay,
Yeah, 2000 GPUs. 2000 GPU cards. Okay,
24:23lower end theology,
S Speaker 224:27that's almost a third of your capacity, right, correct? That's like those, yeah, is that fortune? One customer?
that's almost a third of your capacity, right, correct? That's like those, yeah, is that fortune? One customer?
that's almost a third of your capacity, right, correct? That's like those, yeah, is that fortune? One customer?
that's almost a third of your capacity, right, correct? That's like those, yeah, is that fortune? One customer?
24:39Not yet, not yet. So that yet, so I'm talking
Not yet, not yet. So that yet, so I'm talking
Not yet, not yet. So that yet, so I'm talking
Not yet, not yet. So that yet, so I'm talking
24:57customer's product, right?
customer's product, right?
customer's product, right?
customer's product, right?
S Speaker 224:59No, no. But so the plan, so, which is, we've got an MBA, so which is the plan is, so they're launching and Alex, you can talk about it. Which is, for next year, they're launching this. In Taiwan, they're launching this AI factory, yeah. And in that AI factory fortune, one customer would be the largest customer, I believe, right for that?
No, no. But so the plan, so, which is, we've got an MBA, so which is the plan is, so they're launching and Alex, you can talk about it. Which is, for next year, they're launching this. In Taiwan, they're launching this AI factory, yeah. And in that AI factory fortune, one customer would be the largest customer, I believe, right for that?
No, no. But so the plan, so, which is, we've got an MBA, so which is the plan is, so they're launching and Alex, you can talk about it. Which is, for next year, they're launching this. In Taiwan, they're launching this AI factory, yeah. And in that AI factory fortune, one customer would be the largest customer, I believe, right for that?
No, no. But so the plan, so, which is, we've got an MBA, so which is the plan is, so they're launching and Alex, you can talk about it. Which is, for next year, they're launching this. In Taiwan, they're launching this AI factory, yeah. And in that AI factory fortune, one customer would be the largest customer, I believe, right for that?
25:21Yes, we will have
S Speaker 525:24the largest OEMs chip company, cybersecurity company, also, of course, your wine company, utilizing the GPUs that we provide. Yeah, so
the largest OEMs chip company, cybersecurity company, also, of course, your wine company, utilizing the GPUs that we provide. Yeah, so
the largest OEMs chip company, cybersecurity company, also, of course, your wine company, utilizing the GPUs that we provide. Yeah, so
the largest OEMs chip company, cybersecurity company, also, of course, your wine company, utilizing the GPUs that we provide. Yeah, so
S Speaker 225:40I think so. The plan by AI factory is when next year, Alex, that was q1 or q2
I think so. The plan by AI factory is when next year, Alex, that was q1 or q2
I think so. The plan by AI factory is when next year, Alex, that was q1 or q2
I think so. The plan by AI factory is when next year, Alex, that was q1 or q2
S Speaker 325:51so just to understand that the one way we could work together, there's multiple ways, but one way we could work together is we provide our hardware,
so just to understand that the one way we could work together, there's multiple ways, but one way we could work together is we provide our hardware,
so just to understand that the one way we could work together, there's multiple ways, but one way we could work together is we provide our hardware,
so just to understand that the one way we could work together, there's multiple ways, but one way we could work together is we provide our hardware,
26:04its performances per dollar is,
its performances per dollar is,
its performances per dollar is,
its performances per dollar is,
S Speaker 326:08you know, somehow, somewhat better than this for some of the models. And then, if any customer wants these models you use, they would use our hardware. That's sort of one, one option, the customers that are doing inference as a serve like the bare metal customers, your 2k type customer. Is that a bare metal customer? Or it's a bare metal, right? Okay, I expected that. What kind of are they using this for inference or training or inference? Okay, and then they bring their entire they bring the entire AI stack,
you know, somehow, somewhat better than this for some of the models. And then, if any customer wants these models you use, they would use our hardware. That's sort of one, one option, the customers that are doing inference as a serve like the bare metal customers, your 2k type customer. Is that a bare metal customer? Or it's a bare metal, right? Okay, I expected that. What kind of are they using this for inference or training or inference? Okay, and then they bring their entire they bring the entire AI stack,
you know, somehow, somewhat better than this for some of the models. And then, if any customer wants these models you use, they would use our hardware. That's sort of one, one option, the customers that are doing inference as a serve like the bare metal customers, your 2k type customer. Is that a bare metal customer? Or it's a bare metal, right? Okay, I expected that. What kind of are they using this for inference or training or inference? Okay, and then they bring their entire they bring the entire AI stack,
you know, somehow, somewhat better than this for some of the models. And then, if any customer wants these models you use, they would use our hardware. That's sort of one, one option, the customers that are doing inference as a serve like the bare metal customers, your 2k type customer. Is that a bare metal customer? Or it's a bare metal, right? Okay, I expected that. What kind of are they using this for inference or training or inference? Okay, and then they bring their entire they bring the entire AI stack,
26:59and are those class of customers interested in alternate
and are those class of customers interested in alternate
and are those class of customers interested in alternate
and are those class of customers interested in alternate
S Speaker 327:05options, and what would their Motivation? Again, would be caused by zoom,
options, and what would their Motivation? Again, would be caused by zoom,
options, and what would their Motivation? Again, would be caused by zoom,
options, and what would their Motivation? Again, would be caused by zoom,
27:22The if you would
27:25introduce our hardware
introduce our hardware
introduce our hardware
introduce our hardware
S Speaker 427:30hypothetical question, right? Let's assume for a second that for a given model, GDP or SAS, or whatever we can provide a CCO, which is better than the other that you have now, and therefore you can, you can sell that from a token for per, I mean, for $1 per 1 million token perspective, you can sell that at a lower price, right? Would you simply advertise that as your price versus would you advertise two prices, one with the hardware and the other with immediate hardware? What's your view on that?
hypothetical question, right? Let's assume for a second that for a given model, GDP or SAS, or whatever we can provide a CCO, which is better than the other that you have now, and therefore you can, you can sell that from a token for per, I mean, for $1 per 1 million token perspective, you can sell that at a lower price, right? Would you simply advertise that as your price versus would you advertise two prices, one with the hardware and the other with immediate hardware? What's your view on that?
hypothetical question, right? Let's assume for a second that for a given model, GDP or SAS, or whatever we can provide a CCO, which is better than the other that you have now, and therefore you can, you can sell that from a token for per, I mean, for $1 per 1 million token perspective, you can sell that at a lower price, right? Would you simply advertise that as your price versus would you advertise two prices, one with the hardware and the other with immediate hardware? What's your view on that?
hypothetical question, right? Let's assume for a second that for a given model, GDP or SAS, or whatever we can provide a CCO, which is better than the other that you have now, and therefore you can, you can sell that from a token for per, I mean, for $1 per 1 million token perspective, you can sell that at a lower price, right? Would you simply advertise that as your price versus would you advertise two prices, one with the hardware and the other with immediate hardware? What's your view on that?
28:15That's an Alex question. If
That's an Alex question. If
That's an Alex question. If
That's an Alex question. If
28:18you have a view, maybe you haven't thought about it.
you have a view, maybe you haven't thought about it.
you have a view, maybe you haven't thought about it.
you have a view, maybe you haven't thought about it.
S Speaker 528:21Sorry, sorry, I will try to look far over here. Repeat that again. My bad.
Sorry, sorry, I will try to look far over here. Repeat that again. My bad.
Sorry, sorry, I will try to look far over here. Repeat that again. My bad.
Sorry, sorry, I will try to look far over here. Repeat that again. My bad.
S Speaker 428:27But you want me to repeat, yeah, yeah. So I was saying, let's assume we provide our hardware and our hardware as a better inference PCO for specific model
But you want me to repeat, yeah, yeah. So I was saying, let's assume we provide our hardware and our hardware as a better inference PCO for specific model
But you want me to repeat, yeah, yeah. So I was saying, let's assume we provide our hardware and our hardware as a better inference PCO for specific model
But you want me to repeat, yeah, yeah. So I was saying, let's assume we provide our hardware and our hardware as a better inference PCO for specific model
S Speaker 428:44which one. And so with that, you can have 1 million token per dollar better than what you have now your website,
which one. And so with that, you can have 1 million token per dollar better than what you have now your website,
which one. And so with that, you can have 1 million token per dollar better than what you have now your website,
which one. And so with that, you can have 1 million token per dollar better than what you have now your website,
S Speaker 428:58would you advertise that price your only solution? Or would you advertise those prices Qualcomm hardware versus a different price with Nvidia hardware? Meaning, do you expose to your customer which hardware is used or not?
would you advertise that price your only solution? Or would you advertise those prices Qualcomm hardware versus a different price with Nvidia hardware? Meaning, do you expose to your customer which hardware is used or not?
would you advertise that price your only solution? Or would you advertise those prices Qualcomm hardware versus a different price with Nvidia hardware? Meaning, do you expose to your customer which hardware is used or not?
would you advertise that price your only solution? Or would you advertise those prices Qualcomm hardware versus a different price with Nvidia hardware? Meaning, do you expose to your customer which hardware is used or not?
S Speaker 529:16For APIs? No, for API, the same thing, but for a dedicated points 100%
For APIs? No, for API, the same thing, but for a dedicated points 100%
For APIs? No, for API, the same thing, but for a dedicated points 100%
For APIs? No, for API, the same thing, but for a dedicated points 100%
S Speaker 429:23okay, but if somebody just wants to run GDP, OSS,
okay, but if somebody just wants to run GDP, OSS,
okay, but if somebody just wants to run GDP, OSS,
okay, but if somebody just wants to run GDP, OSS,
S Speaker 529:29yeah, they would, they wouldn't know. They wouldn't know, okay, you wouldn't know, okay.
yeah, they would, they wouldn't know. They wouldn't know, okay, you wouldn't know, okay.
yeah, they would, they wouldn't know. They wouldn't know, okay, you wouldn't know, okay.
yeah, they would, they wouldn't know. They wouldn't know, okay, you wouldn't know, okay.
29:40Rashid, should we? Don't
Rashid, should we? Don't
Rashid, should we? Don't
Rashid, should we? Don't
S Speaker 429:44think we have these slides right now to present on that, but
think we have these slides right now to present on that, but
think we have these slides right now to present on that, but
think we have these slides right now to present on that, but
S Speaker 329:53at the first 10 minutes, oh, yeah, you work on that. So a little bit of what we have, and then we can
at the first 10 minutes, oh, yeah, you work on that. So a little bit of what we have, and then we can
at the first 10 minutes, oh, yeah, you work on that. So a little bit of what we have, and then we can
at the first 10 minutes, oh, yeah, you work on that. So a little bit of what we have, and then we can
S Speaker 430:02I was more thinking about our maybe, I don't know if you added in the slide our analysis, so where we can be from a cost perspective and so forth,
I was more thinking about our maybe, I don't know if you added in the slide our analysis, so where we can be from a cost perspective and so forth,
I was more thinking about our maybe, I don't know if you added in the slide our analysis, so where we can be from a cost perspective and so forth,
I was more thinking about our maybe, I don't know if you added in the slide our analysis, so where we can be from a cost perspective and so forth,
S Speaker 330:11that I haven't we can talk about it a little bit. I don't. This was just to be, I just have a few slides to kind of give you a sense of what product do we have? Yeah, so let's just do that for a little bit, and we can, you know, ask questions as we go.
that I haven't we can talk about it a little bit. I don't. This was just to be, I just have a few slides to kind of give you a sense of what product do we have? Yeah, so let's just do that for a little bit, and we can, you know, ask questions as we go.
that I haven't we can talk about it a little bit. I don't. This was just to be, I just have a few slides to kind of give you a sense of what product do we have? Yeah, so let's just do that for a little bit, and we can, you know, ask questions as we go.
that I haven't we can talk about it a little bit. I don't. This was just to be, I just have a few slides to kind of give you a sense of what product do we have? Yeah, so let's just do that for a little bit, and we can, you know, ask questions as we go.
30:33Yeah, sounds good. What did I do? Okay,
Yeah, sounds good. What did I do? Okay,
Yeah, sounds good. What did I do? Okay,
Yeah, sounds good. What did I do? Okay,
S Speaker 330:42something. Yeah, okay, so we, we have, and this is the current generation, the next one is work in progress, and maybe that would be, and I have a couple of slides on that appropriate for next year for your AI factory, if it all makes sense. So we have we think of our product in some sense, in sort of two buckets. There is an inference accelerator. Our claim is that it can deliver amongst the best per dollar. One interesting dimension of it is we have a lot of memory on the card. So we can on a single card. We can fit large models. For example, you know, either 128 gigabyte of memory. So 109 like a scout model, fits really well and runs pretty fast. A one 20 billion OSS model can run on it as well and so on. You can put 16 of those cards in a single server, so you can get pretty dense systems. You'll see more in the coming slides. We are purely inference, because you're purely interns, you have to work pretty hard to be able to take a model that was trained anywhere then and deploy it on, on, on AI 100 we also provide, and this may not be of as much interest, but of course, we provide the, you know, open AI APIs, so you can deliver API, everything needed for API as a service, but we also provide along. We have what we call our AI inference suite. You get with it, a bunch of enterprise use cases. You get fine tuning support. You can do bring your own model. You can get ready to run model binaries, sort of what you do. But we can have a much bigger list of instead of 30 something models, we can put, you know, hundreds of models in there, if we wish. And then it's it has full support for vllm and then Kubernetes and so on. For scaling, I'll show you that in a second. This is the card we have today. This is the one that's commercially available today. So it's called ai 100 ultra, 150 watt. Pretty good amount of tops and T flops, lots of on chip memory. So there's two interesting things we have, architecturally, we have a bunch of, ton of, bunch of memory, which allows us some models to run out of SRAM, but not a lot, because it's not that much. But we have a ton of on card memory. So if you were to take a model and make it run across four cards or eight cards, you can do pretty large models in a very small amount of power and cost footprint. Okay,
something. Yeah, okay, so we, we have, and this is the current generation, the next one is work in progress, and maybe that would be, and I have a couple of slides on that appropriate for next year for your AI factory, if it all makes sense. So we have we think of our product in some sense, in sort of two buckets. There is an inference accelerator. Our claim is that it can deliver amongst the best per dollar. One interesting dimension of it is we have a lot of memory on the card. So we can on a single card. We can fit large models. For example, you know, either 128 gigabyte of memory. So 109 like a scout model, fits really well and runs pretty fast. A one 20 billion OSS model can run on it as well and so on. You can put 16 of those cards in a single server, so you can get pretty dense systems. You'll see more in the coming slides. We are purely inference, because you're purely interns, you have to work pretty hard to be able to take a model that was trained anywhere then and deploy it on, on, on AI 100 we also provide, and this may not be of as much interest, but of course, we provide the, you know, open AI APIs, so you can deliver API, everything needed for API as a service, but we also provide along. We have what we call our AI inference suite. You get with it, a bunch of enterprise use cases. You get fine tuning support. You can do bring your own model. You can get ready to run model binaries, sort of what you do. But we can have a much bigger list of instead of 30 something models, we can put, you know, hundreds of models in there, if we wish. And then it's it has full support for vllm and then Kubernetes and so on. For scaling, I'll show you that in a second. This is the card we have today. This is the one that's commercially available today. So it's called ai 100 ultra, 150 watt. Pretty good amount of tops and T flops, lots of on chip memory. So there's two interesting things we have, architecturally, we have a bunch of, ton of, bunch of memory, which allows us some models to run out of SRAM, but not a lot, because it's not that much. But we have a ton of on card memory. So if you were to take a model and make it run across four cards or eight cards, you can do pretty large models in a very small amount of power and cost footprint. Okay,
something. Yeah, okay, so we, we have, and this is the current generation, the next one is work in progress, and maybe that would be, and I have a couple of slides on that appropriate for next year for your AI factory, if it all makes sense. So we have we think of our product in some sense, in sort of two buckets. There is an inference accelerator. Our claim is that it can deliver amongst the best per dollar. One interesting dimension of it is we have a lot of memory on the card. So we can on a single card. We can fit large models. For example, you know, either 128 gigabyte of memory. So 109 like a scout model, fits really well and runs pretty fast. A one 20 billion OSS model can run on it as well and so on. You can put 16 of those cards in a single server, so you can get pretty dense systems. You'll see more in the coming slides. We are purely inference, because you're purely interns, you have to work pretty hard to be able to take a model that was trained anywhere then and deploy it on, on, on AI 100 we also provide, and this may not be of as much interest, but of course, we provide the, you know, open AI APIs, so you can deliver API, everything needed for API as a service, but we also provide along. We have what we call our AI inference suite. You get with it, a bunch of enterprise use cases. You get fine tuning support. You can do bring your own model. You can get ready to run model binaries, sort of what you do. But we can have a much bigger list of instead of 30 something models, we can put, you know, hundreds of models in there, if we wish. And then it's it has full support for vllm and then Kubernetes and so on. For scaling, I'll show you that in a second. This is the card we have today. This is the one that's commercially available today. So it's called ai 100 ultra, 150 watt. Pretty good amount of tops and T flops, lots of on chip memory. So there's two interesting things we have, architecturally, we have a bunch of, ton of, bunch of memory, which allows us some models to run out of SRAM, but not a lot, because it's not that much. But we have a ton of on card memory. So if you were to take a model and make it run across four cards or eight cards, you can do pretty large models in a very small amount of power and cost footprint. Okay,
something. Yeah, okay, so we, we have, and this is the current generation, the next one is work in progress, and maybe that would be, and I have a couple of slides on that appropriate for next year for your AI factory, if it all makes sense. So we have we think of our product in some sense, in sort of two buckets. There is an inference accelerator. Our claim is that it can deliver amongst the best per dollar. One interesting dimension of it is we have a lot of memory on the card. So we can on a single card. We can fit large models. For example, you know, either 128 gigabyte of memory. So 109 like a scout model, fits really well and runs pretty fast. A one 20 billion OSS model can run on it as well and so on. You can put 16 of those cards in a single server, so you can get pretty dense systems. You'll see more in the coming slides. We are purely inference, because you're purely interns, you have to work pretty hard to be able to take a model that was trained anywhere then and deploy it on, on, on AI 100 we also provide, and this may not be of as much interest, but of course, we provide the, you know, open AI APIs, so you can deliver API, everything needed for API as a service, but we also provide along. We have what we call our AI inference suite. You get with it, a bunch of enterprise use cases. You get fine tuning support. You can do bring your own model. You can get ready to run model binaries, sort of what you do. But we can have a much bigger list of instead of 30 something models, we can put, you know, hundreds of models in there, if we wish. And then it's it has full support for vllm and then Kubernetes and so on. For scaling, I'll show you that in a second. This is the card we have today. This is the one that's commercially available today. So it's called ai 100 ultra, 150 watt. Pretty good amount of tops and T flops, lots of on chip memory. So there's two interesting things we have, architecturally, we have a bunch of, ton of, bunch of memory, which allows us some models to run out of SRAM, but not a lot, because it's not that much. But we have a ton of on card memory. So if you were to take a model and make it run across four cards or eight cards, you can do pretty large models in a very small amount of power and cost footprint. Okay,
33:52this is how they
S Speaker 336:28to a very high level with DAX, sort of super fast introduction to the product we have today.
to a very high level with DAX, sort of super fast introduction to the product we have today.
to a very high level with DAX, sort of super fast introduction to the product we have today.
to a very high level with DAX, sort of super fast introduction to the product we have today.