Meeting: Staris AI
Wed, Jun 19, 2024
11:02 AM
28 min
Priyesh P
Investing in AI and deep tech startups at Qual
URL: https://otter.ai/u/8BRys_A6B0KtF04Y1L17Vwwn2VI
Downloaded: 2025-12-22T15:10:10.396647
Method: text_extraction
============================================================

S Speaker 10:00Product Management camera computer vision background at Qualcomm ran a few businesses before Qualcomm and then ventures I spend most of my time so the last seven years has been with Qualcomm ventures and I invest in AI and deep tech at Qualcomm ventures Qualcomm Ventures has been around for 2424 years
Product Management camera computer vision background at Qualcomm ran a few businesses before Qualcomm and then ventures I spend most of my time so the last seven years has been with Qualcomm ventures and I invest in AI and deep tech at Qualcomm ventures Qualcomm Ventures has been around for 2424 years
Product Management camera computer vision background at Qualcomm ran a few businesses before Qualcomm and then ventures I spend most of my time so the last seven years has been with Qualcomm ventures and I invest in AI and deep tech at Qualcomm ventures Qualcomm Ventures has been around for 2424 years
Product Management camera computer vision background at Qualcomm ran a few businesses before Qualcomm and then ventures I spend most of my time so the last seven years has been with Qualcomm ventures and I invest in AI and deep tech at Qualcomm ventures Qualcomm Ventures has been around for 2424 years
S Speaker 10:30and we invest in areas that are strategically relevant for Qualcomm. And the nine years to you know, just learn and partner with startups that are building that are pushing the boundaries of compute and connectivity. So across mobile IoT automotive cloud, of course, AI is a theme across the board and working in AI PCs and Macs are devices, parables and heroes. And so it's a really big portfolio planning. So we work with a lot of startups that are pushing the boundaries of computer connectivity in any one of these areas. So we invest in you know, which series agnostic series and we tend to be a fixed paths, but we do some see some Series C and D as well. And then check sizes are to do $10 million initial check size. Yeah, that's a high level overview. Pretty much you can to choose pace as the newest member of our team.
and we invest in areas that are strategically relevant for Qualcomm. And the nine years to you know, just learn and partner with startups that are building that are pushing the boundaries of compute and connectivity. So across mobile IoT automotive cloud, of course, AI is a theme across the board and working in AI PCs and Macs are devices, parables and heroes. And so it's a really big portfolio planning. So we work with a lot of startups that are pushing the boundaries of computer connectivity in any one of these areas. So we invest in you know, which series agnostic series and we tend to be a fixed paths, but we do some see some Series C and D as well. And then check sizes are to do $10 million initial check size. Yeah, that's a high level overview. Pretty much you can to choose pace as the newest member of our team.
and we invest in areas that are strategically relevant for Qualcomm. And the nine years to you know, just learn and partner with startups that are building that are pushing the boundaries of compute and connectivity. So across mobile IoT automotive cloud, of course, AI is a theme across the board and working in AI PCs and Macs are devices, parables and heroes. And so it's a really big portfolio planning. So we work with a lot of startups that are pushing the boundaries of computer connectivity in any one of these areas. So we invest in you know, which series agnostic series and we tend to be a fixed paths, but we do some see some Series C and D as well. And then check sizes are to do $10 million initial check size. Yeah, that's a high level overview. Pretty much you can to choose pace as the newest member of our team.
and we invest in areas that are strategically relevant for Qualcomm. And the nine years to you know, just learn and partner with startups that are building that are pushing the boundaries of compute and connectivity. So across mobile IoT automotive cloud, of course, AI is a theme across the board and working in AI PCs and Macs are devices, parables and heroes. And so it's a really big portfolio planning. So we work with a lot of startups that are pushing the boundaries of computer connectivity in any one of these areas. So we invest in you know, which series agnostic series and we tend to be a fixed paths, but we do some see some Series C and D as well. And then check sizes are to do $10 million initial check size. Yeah, that's a high level overview. Pretty much you can to choose pace as the newest member of our team.
S Speaker 21:33Awesome. Yeah. So a brief about me. I've spent about four years of my life working as an operator at different startups, mostly taking roles across product and growth. More recently, over the last couple of years, I've worked in venture with different funds, including Menlo and Morpheus recently completed my MBA from UCLA and I've joined Qualcomm for the last three months or so now. Yep. So one of the newest members of the team
Awesome. Yeah. So a brief about me. I've spent about four years of my life working as an operator at different startups, mostly taking roles across product and growth. More recently, over the last couple of years, I've worked in venture with different funds, including Menlo and Morpheus recently completed my MBA from UCLA and I've joined Qualcomm for the last three months or so now. Yep. So one of the newest members of the team
Awesome. Yeah. So a brief about me. I've spent about four years of my life working as an operator at different startups, mostly taking roles across product and growth. More recently, over the last couple of years, I've worked in venture with different funds, including Menlo and Morpheus recently completed my MBA from UCLA and I've joined Qualcomm for the last three months or so now. Yep. So one of the newest members of the team
Awesome. Yeah. So a brief about me. I've spent about four years of my life working as an operator at different startups, mostly taking roles across product and growth. More recently, over the last couple of years, I've worked in venture with different funds, including Menlo and Morpheus recently completed my MBA from UCLA and I've joined Qualcomm for the last three months or so now. Yep. So one of the newest members of the team
S Speaker 32:01but a little bit about myself in the background, and then I can just kind of jump into it happy to do whatever it is usage of your time. Keep your conversational slides.
but a little bit about myself in the background, and then I can just kind of jump into it happy to do whatever it is usage of your time. Keep your conversational slides.
but a little bit about myself in the background, and then I can just kind of jump into it happy to do whatever it is usage of your time. Keep your conversational slides.
but a little bit about myself in the background, and then I can just kind of jump into it happy to do whatever it is usage of your time. Keep your conversational slides.
S Speaker 14:24a you haven't accomplished much and I don't feel like
a you haven't accomplished much and I don't feel like
a you haven't accomplished much and I don't feel like
a you haven't accomplished much and I don't feel like
S Speaker 17:31I have a few but I'm guessing it will get answered. So I'm gonna wait.
I have a few but I'm guessing it will get answered. So I'm gonna wait.
I have a few but I'm guessing it will get answered. So I'm gonna wait.
I have a few but I'm guessing it will get answered. So I'm gonna wait.
S Speaker 37:38Absolutely. And so if you think about this, though, like everything from your series, zero precede startup to these large organizations have to buy a pen test they have to say, Okay, I put this on the internet, and somehow I have to protect it. And the way you do this normally is like whether it's by a regulation of sock to or whether it's by mandatory by company policies. You say, I have to go hire some hackers. And so you call up your buddy and you're like, hey, who did you hire when you got a pen test? And they said, Well, here's a firm you can trust. There's a firm, you can't trust this firm's downs from scaling up like and so eventually you make a call, and then the friction starts 20 emails back and forth. It's anyone can send it and say you do sign a contract on day one. It's eight to 12 weeks before someone's available to even show up to do that test. And let's say you get the perfect person. They know your tech stack. They are you know, they're fresh. They're not coming off of like, you know, a vacation or a family event. They're not exhausted from being burned out and do 50 of these in a row. Let's say it's just the perfect pen test right? At the end of that you just get problems. You get a PDF that says I have to go do more work, and you never know exactly how much is covered. So you never know how much they did. How much of the application is still insecure. And it's a point in time thing. So developer go ships wanting new features. You have to do it all over again. It's the reasons it's the reason I call my previous company deja vu securities, we kept doing the same thing over and over and over again. So it's hard to buy. It's really difficult to measure, it's almost impossible to get consistency because let's say you do have big pockets, you're like alright, I'm gonna do this once a quarter or once a year, whatever it is, or maybe once a week, you know, that same person is not available. So the person that had all that context about what your business was doing and what it is, you know, it's it's been eight months since they've seen it again. You know, they've seen 20 Other things between here and there, but you lose all of that effort and energy every single time. So, let's say you're a big company, let's say you're trillion dollar organization, you have $100 million budget to do this, right. And that's what some of these companies spend every year on external consultants to do this. You can't buy your way out of it, because there's simply no one to hire and there's no one to buy. There's 700,000 Missing security consultants in the United States alone and four and a half million missing worldwide. So let's say you are Qualcomm or T Mobile or Amazon, you're like, I'm just gonna throw money at the problem. There's no one to throw money at and as I mentioned, it's a shame because this is where everything important about our lives on the internet closer like you know, the applications are really where businesses are built. There were products are built, there's how people, you know, bridge the air gap between their life and the internet. Now, we have this new thing. Well, you have the Star Trek computer last year, we got it. It's really awesome, right? Like, yeah, GPT came out, I asked for a picture of a cat that gives me a picture of a cat as good for code. It gives me code, and that's awesome because it's fun to hear the thing I've wanted my entire life, but it exacerbates this problem because now every single word is putting 20 Different co pilots in and they have you know their million plugins and they're saying, hey, let's hit the accelerator pedal, and code and ship as fast as we can. There's nothing to keep up with that from a security perspective. We as humans will never keep up with that again. And once AI starts recoating things for itself, that that iterative loop is usually a faster and faster. Candidly, we need something to keep up from it is as fast as AI systems are going to allow us to accelerate to.
Absolutely. And so if you think about this, though, like everything from your series, zero precede startup to these large organizations have to buy a pen test they have to say, Okay, I put this on the internet, and somehow I have to protect it. And the way you do this normally is like whether it's by a regulation of sock to or whether it's by mandatory by company policies. You say, I have to go hire some hackers. And so you call up your buddy and you're like, hey, who did you hire when you got a pen test? And they said, Well, here's a firm you can trust. There's a firm, you can't trust this firm's downs from scaling up like and so eventually you make a call, and then the friction starts 20 emails back and forth. It's anyone can send it and say you do sign a contract on day one. It's eight to 12 weeks before someone's available to even show up to do that test. And let's say you get the perfect person. They know your tech stack. They are you know, they're fresh. They're not coming off of like, you know, a vacation or a family event. They're not exhausted from being burned out and do 50 of these in a row. Let's say it's just the perfect pen test right? At the end of that you just get problems. You get a PDF that says I have to go do more work, and you never know exactly how much is covered. So you never know how much they did. How much of the application is still insecure. And it's a point in time thing. So developer go ships wanting new features. You have to do it all over again. It's the reasons it's the reason I call my previous company deja vu securities, we kept doing the same thing over and over and over again. So it's hard to buy. It's really difficult to measure, it's almost impossible to get consistency because let's say you do have big pockets, you're like alright, I'm gonna do this once a quarter or once a year, whatever it is, or maybe once a week, you know, that same person is not available. So the person that had all that context about what your business was doing and what it is, you know, it's it's been eight months since they've seen it again. You know, they've seen 20 Other things between here and there, but you lose all of that effort and energy every single time. So, let's say you're a big company, let's say you're trillion dollar organization, you have $100 million budget to do this, right. And that's what some of these companies spend every year on external consultants to do this. You can't buy your way out of it, because there's simply no one to hire and there's no one to buy. There's 700,000 Missing security consultants in the United States alone and four and a half million missing worldwide. So let's say you are Qualcomm or T Mobile or Amazon, you're like, I'm just gonna throw money at the problem. There's no one to throw money at and as I mentioned, it's a shame because this is where everything important about our lives on the internet closer like you know, the applications are really where businesses are built. There were products are built, there's how people, you know, bridge the air gap between their life and the internet. Now, we have this new thing. Well, you have the Star Trek computer last year, we got it. It's really awesome, right? Like, yeah, GPT came out, I asked for a picture of a cat that gives me a picture of a cat as good for code. It gives me code, and that's awesome because it's fun to hear the thing I've wanted my entire life, but it exacerbates this problem because now every single word is putting 20 Different co pilots in and they have you know their million plugins and they're saying, hey, let's hit the accelerator pedal, and code and ship as fast as we can. There's nothing to keep up with that from a security perspective. We as humans will never keep up with that again. And once AI starts recoating things for itself, that that iterative loop is usually a faster and faster. Candidly, we need something to keep up from it is as fast as AI systems are going to allow us to accelerate to.
Absolutely. And so if you think about this, though, like everything from your series, zero precede startup to these large organizations have to buy a pen test they have to say, Okay, I put this on the internet, and somehow I have to protect it. And the way you do this normally is like whether it's by a regulation of sock to or whether it's by mandatory by company policies. You say, I have to go hire some hackers. And so you call up your buddy and you're like, hey, who did you hire when you got a pen test? And they said, Well, here's a firm you can trust. There's a firm, you can't trust this firm's downs from scaling up like and so eventually you make a call, and then the friction starts 20 emails back and forth. It's anyone can send it and say you do sign a contract on day one. It's eight to 12 weeks before someone's available to even show up to do that test. And let's say you get the perfect person. They know your tech stack. They are you know, they're fresh. They're not coming off of like, you know, a vacation or a family event. They're not exhausted from being burned out and do 50 of these in a row. Let's say it's just the perfect pen test right? At the end of that you just get problems. You get a PDF that says I have to go do more work, and you never know exactly how much is covered. So you never know how much they did. How much of the application is still insecure. And it's a point in time thing. So developer go ships wanting new features. You have to do it all over again. It's the reasons it's the reason I call my previous company deja vu securities, we kept doing the same thing over and over and over again. So it's hard to buy. It's really difficult to measure, it's almost impossible to get consistency because let's say you do have big pockets, you're like alright, I'm gonna do this once a quarter or once a year, whatever it is, or maybe once a week, you know, that same person is not available. So the person that had all that context about what your business was doing and what it is, you know, it's it's been eight months since they've seen it again. You know, they've seen 20 Other things between here and there, but you lose all of that effort and energy every single time. So, let's say you're a big company, let's say you're trillion dollar organization, you have $100 million budget to do this, right. And that's what some of these companies spend every year on external consultants to do this. You can't buy your way out of it, because there's simply no one to hire and there's no one to buy. There's 700,000 Missing security consultants in the United States alone and four and a half million missing worldwide. So let's say you are Qualcomm or T Mobile or Amazon, you're like, I'm just gonna throw money at the problem. There's no one to throw money at and as I mentioned, it's a shame because this is where everything important about our lives on the internet closer like you know, the applications are really where businesses are built. There were products are built, there's how people, you know, bridge the air gap between their life and the internet. Now, we have this new thing. Well, you have the Star Trek computer last year, we got it. It's really awesome, right? Like, yeah, GPT came out, I asked for a picture of a cat that gives me a picture of a cat as good for code. It gives me code, and that's awesome because it's fun to hear the thing I've wanted my entire life, but it exacerbates this problem because now every single word is putting 20 Different co pilots in and they have you know their million plugins and they're saying, hey, let's hit the accelerator pedal, and code and ship as fast as we can. There's nothing to keep up with that from a security perspective. We as humans will never keep up with that again. And once AI starts recoating things for itself, that that iterative loop is usually a faster and faster. Candidly, we need something to keep up from it is as fast as AI systems are going to allow us to accelerate to.
Absolutely. And so if you think about this, though, like everything from your series, zero precede startup to these large organizations have to buy a pen test they have to say, Okay, I put this on the internet, and somehow I have to protect it. And the way you do this normally is like whether it's by a regulation of sock to or whether it's by mandatory by company policies. You say, I have to go hire some hackers. And so you call up your buddy and you're like, hey, who did you hire when you got a pen test? And they said, Well, here's a firm you can trust. There's a firm, you can't trust this firm's downs from scaling up like and so eventually you make a call, and then the friction starts 20 emails back and forth. It's anyone can send it and say you do sign a contract on day one. It's eight to 12 weeks before someone's available to even show up to do that test. And let's say you get the perfect person. They know your tech stack. They are you know, they're fresh. They're not coming off of like, you know, a vacation or a family event. They're not exhausted from being burned out and do 50 of these in a row. Let's say it's just the perfect pen test right? At the end of that you just get problems. You get a PDF that says I have to go do more work, and you never know exactly how much is covered. So you never know how much they did. How much of the application is still insecure. And it's a point in time thing. So developer go ships wanting new features. You have to do it all over again. It's the reasons it's the reason I call my previous company deja vu securities, we kept doing the same thing over and over and over again. So it's hard to buy. It's really difficult to measure, it's almost impossible to get consistency because let's say you do have big pockets, you're like alright, I'm gonna do this once a quarter or once a year, whatever it is, or maybe once a week, you know, that same person is not available. So the person that had all that context about what your business was doing and what it is, you know, it's it's been eight months since they've seen it again. You know, they've seen 20 Other things between here and there, but you lose all of that effort and energy every single time. So, let's say you're a big company, let's say you're trillion dollar organization, you have $100 million budget to do this, right. And that's what some of these companies spend every year on external consultants to do this. You can't buy your way out of it, because there's simply no one to hire and there's no one to buy. There's 700,000 Missing security consultants in the United States alone and four and a half million missing worldwide. So let's say you are Qualcomm or T Mobile or Amazon, you're like, I'm just gonna throw money at the problem. There's no one to throw money at and as I mentioned, it's a shame because this is where everything important about our lives on the internet closer like you know, the applications are really where businesses are built. There were products are built, there's how people, you know, bridge the air gap between their life and the internet. Now, we have this new thing. Well, you have the Star Trek computer last year, we got it. It's really awesome, right? Like, yeah, GPT came out, I asked for a picture of a cat that gives me a picture of a cat as good for code. It gives me code, and that's awesome because it's fun to hear the thing I've wanted my entire life, but it exacerbates this problem because now every single word is putting 20 Different co pilots in and they have you know their million plugins and they're saying, hey, let's hit the accelerator pedal, and code and ship as fast as we can. There's nothing to keep up with that from a security perspective. We as humans will never keep up with that again. And once AI starts recoating things for itself, that that iterative loop is usually a faster and faster. Candidly, we need something to keep up from it is as fast as AI systems are going to allow us to accelerate to.
S Speaker 110:57And that's just the design. Curious, and I'm sure you have a good answer. Which is if you use AR code. If you ask me, I do write a very robust and secure code, then would the AI systems not get to a point in a year two years from now where they end up writing a very robust and secure piece of code?
And that's just the design. Curious, and I'm sure you have a good answer. Which is if you use AR code. If you ask me, I do write a very robust and secure code, then would the AI systems not get to a point in a year two years from now where they end up writing a very robust and secure piece of code?
And that's just the design. Curious, and I'm sure you have a good answer. Which is if you use AR code. If you ask me, I do write a very robust and secure code, then would the AI systems not get to a point in a year two years from now where they end up writing a very robust and secure piece of code?
And that's just the design. Curious, and I'm sure you have a good answer. Which is if you use AR code. If you ask me, I do write a very robust and secure code, then would the AI systems not get to a point in a year two years from now where they end up writing a very robust and secure piece of code?
S Speaker 311:27So that's there's a few there's a few basic assumptions there. And I agree with you. I think in a long enough timeline, we get to where enough code being generated by some of these foundational models is quote unquote good enough. However, there's there's two there's 2x exterior thought process on that one is we have 60 years of legacy things that aren't going to immediately disappear overnight that are still gonna ride on the same capital ironed out those all that new code is number two is you cannot read a character smarter than yourself. So if you were to sit down and like write a book, you cannot you cannot imagine someone that is smarter than yourself. And so the reason that a lot of these companies have a third party requirement is they're not the ones looking at a cooler context of I built this and looking at that from the lens of the context of I'm trying to break this. And so being able to understand and have coverage and be able to use a different approach and a different candidate training set and different mindset to be able to attack the code is just step one of that. And so, my my opinion of this is, is that, you know, one, let's say the perfect LM comes out tomorrow to write the perfect secure code. Not everyone's going to use that. So there's gonna be a long tail of people using maybe the perfect coding model and a lot of people not being able to afford it or not being able to use it or not integrating it. That's going to be quite a while before we sunset humans on ready code entirely. The number two is you can never have itself verify and a lot of the regulations a lot of the insurance policy is a lot of the regulatory requirements SOC PCI, etc. Say you need to have a third party specifically for that reason, so I don't I don't see that. Like I don't see the silver bullet. causing all the other things to go. Yeah.
So that's there's a few there's a few basic assumptions there. And I agree with you. I think in a long enough timeline, we get to where enough code being generated by some of these foundational models is quote unquote good enough. However, there's there's two there's 2x exterior thought process on that one is we have 60 years of legacy things that aren't going to immediately disappear overnight that are still gonna ride on the same capital ironed out those all that new code is number two is you cannot read a character smarter than yourself. So if you were to sit down and like write a book, you cannot you cannot imagine someone that is smarter than yourself. And so the reason that a lot of these companies have a third party requirement is they're not the ones looking at a cooler context of I built this and looking at that from the lens of the context of I'm trying to break this. And so being able to understand and have coverage and be able to use a different approach and a different candidate training set and different mindset to be able to attack the code is just step one of that. And so, my my opinion of this is, is that, you know, one, let's say the perfect LM comes out tomorrow to write the perfect secure code. Not everyone's going to use that. So there's gonna be a long tail of people using maybe the perfect coding model and a lot of people not being able to afford it or not being able to use it or not integrating it. That's going to be quite a while before we sunset humans on ready code entirely. The number two is you can never have itself verify and a lot of the regulations a lot of the insurance policy is a lot of the regulatory requirements SOC PCI, etc. Say you need to have a third party specifically for that reason, so I don't I don't see that. Like I don't see the silver bullet. causing all the other things to go. Yeah.
So that's there's a few there's a few basic assumptions there. And I agree with you. I think in a long enough timeline, we get to where enough code being generated by some of these foundational models is quote unquote good enough. However, there's there's two there's 2x exterior thought process on that one is we have 60 years of legacy things that aren't going to immediately disappear overnight that are still gonna ride on the same capital ironed out those all that new code is number two is you cannot read a character smarter than yourself. So if you were to sit down and like write a book, you cannot you cannot imagine someone that is smarter than yourself. And so the reason that a lot of these companies have a third party requirement is they're not the ones looking at a cooler context of I built this and looking at that from the lens of the context of I'm trying to break this. And so being able to understand and have coverage and be able to use a different approach and a different candidate training set and different mindset to be able to attack the code is just step one of that. And so, my my opinion of this is, is that, you know, one, let's say the perfect LM comes out tomorrow to write the perfect secure code. Not everyone's going to use that. So there's gonna be a long tail of people using maybe the perfect coding model and a lot of people not being able to afford it or not being able to use it or not integrating it. That's going to be quite a while before we sunset humans on ready code entirely. The number two is you can never have itself verify and a lot of the regulations a lot of the insurance policy is a lot of the regulatory requirements SOC PCI, etc. Say you need to have a third party specifically for that reason, so I don't I don't see that. Like I don't see the silver bullet. causing all the other things to go. Yeah.
So that's there's a few there's a few basic assumptions there. And I agree with you. I think in a long enough timeline, we get to where enough code being generated by some of these foundational models is quote unquote good enough. However, there's there's two there's 2x exterior thought process on that one is we have 60 years of legacy things that aren't going to immediately disappear overnight that are still gonna ride on the same capital ironed out those all that new code is number two is you cannot read a character smarter than yourself. So if you were to sit down and like write a book, you cannot you cannot imagine someone that is smarter than yourself. And so the reason that a lot of these companies have a third party requirement is they're not the ones looking at a cooler context of I built this and looking at that from the lens of the context of I'm trying to break this. And so being able to understand and have coverage and be able to use a different approach and a different candidate training set and different mindset to be able to attack the code is just step one of that. And so, my my opinion of this is, is that, you know, one, let's say the perfect LM comes out tomorrow to write the perfect secure code. Not everyone's going to use that. So there's gonna be a long tail of people using maybe the perfect coding model and a lot of people not being able to afford it or not being able to use it or not integrating it. That's going to be quite a while before we sunset humans on ready code entirely. The number two is you can never have itself verify and a lot of the regulations a lot of the insurance policy is a lot of the regulatory requirements SOC PCI, etc. Say you need to have a third party specifically for that reason, so I don't I don't see that. Like I don't see the silver bullet. causing all the other things to go. Yeah.
S Speaker 313:17So and then we get to some of the like Skynet, Hollywood movie issues and candidly, we have seen hostile AI and deep fog, phishing fakes and a couple of other use cases we have one known example where someone used AI to rewrite malware on the fly to change it from the environment that it was in the Canada, the hospital AI that's autonomously hacking things is coming, whether it's from nation states
So and then we get to some of the like Skynet, Hollywood movie issues and candidly, we have seen hostile AI and deep fog, phishing fakes and a couple of other use cases we have one known example where someone used AI to rewrite malware on the fly to change it from the environment that it was in the Canada, the hospital AI that's autonomously hacking things is coming, whether it's from nation states
So and then we get to some of the like Skynet, Hollywood movie issues and candidly, we have seen hostile AI and deep fog, phishing fakes and a couple of other use cases we have one known example where someone used AI to rewrite malware on the fly to change it from the environment that it was in the Canada, the hospital AI that's autonomously hacking things is coming, whether it's from nation states
So and then we get to some of the like Skynet, Hollywood movie issues and candidly, we have seen hostile AI and deep fog, phishing fakes and a couple of other use cases we have one known example where someone used AI to rewrite malware on the fly to change it from the environment that it was in the Canada, the hospital AI that's autonomously hacking things is coming, whether it's from nation states
S Speaker 113:40or the NSA. The other part of that was thinking as well which is you're gonna use AI to write more and more robust code, but also that it will be AI that will be used for more and more. And
or the NSA. The other part of that was thinking as well which is you're gonna use AI to write more and more robust code, but also that it will be AI that will be used for more and more. And
or the NSA. The other part of that was thinking as well which is you're gonna use AI to write more and more robust code, but also that it will be AI that will be used for more and more. And
or the NSA. The other part of that was thinking as well which is you're gonna use AI to write more and more robust code, but also that it will be AI that will be used for more and more. And
S Speaker 313:55it's the inverse of our pitch, right? It's like our system never sleeps and never needs to eat and never loses context. And this is the same as an attacker doing the exact same thing on the other side. And it's also the reason that none of the big tech companies we've talked to will build it because of the PR optics that we have built the hacking robot to come hack you 24/7 And there's too much Hollywood Skynet brand reputation on the line. So, you know, it's one of the big reasons that you know, we don't like see Google, Amazon, Microsoft, etc. Like, they may build it, but they're never going to release it publicly. So what's the solution? What are we building and what if we don't? So we built the hacking robot. And the idea is to take our hacking robot and put it in a loop with a virtual engineer. So one we use the thing that makes us different than any of the other like source code analysis tools are these buddies or CO pilots. And they we think GitHub and GitLab it's just going to win the copilot race, like they have all the source code, they have all the CI CD that I just opened as from a they're gonna put fortify and Coverity and a half dozen companies out of business. They're on the source code analysis, buddy thing and so that's not where we're pointing just we're on the same page. So, just like we used to do tests, Asia is the first thing to really break something is you have to understand what it is like why was it built? Is it a financial app? Is it a mobile app? Is it a payment pipeline? Is it a, you know, a back end for a SCADA system? So we use the foundational LM to extract their context. We also use it to extract the bits and bytes, or libraries was used with code, what languages were used, and then we create an attack plan, just like a tester saying, Hi, I really want to break this thing. This is how I had this is the code, I have to look at the tests after around the pools after run. And our pipe platform then kicks off a number of AI enabled actions to do that. So let's look at the code and might run a tool and might get some feedback. But that's where if you were to buy this off the shelf today, from Accenture, or NCC or Bishop Fox or any of the other consultancies, that's where it would stop it would give you just a pile of work to do. Every season. We talked to more than 50 cents at this point and CIOs and they're like, I don't need more bucks. I don't need more than do I actually need less to do because I need to focus my engineers on building the new thing. And so we've taken it a step further and said, Let's try and prove that those bugs are real bugs by writing an exploit. So get rid of a false positive actually see if we can make the bug trigger inside of the app to do something. And if we can do that, now let's produce a fix. So and we're not getting it. And we're not just saying hey, let's be a better grammar checker for your code or saying, we took the business context of why was this app made? The bits and bytes and the bug and said Alright, now let's try and produce a real fix and send that back to a pull request. To grandvision Is this just runs it just sits there. In the background finding and fixing issues 24/7 365 I think we're five years away from that being reality. But today where we're at is we're able to do a source assisted pen test with a human and an Iron Man suit where we supercharged the human to to use all of these stages the pipeline to do these faster and better and cheaper for organizations. So
it's the inverse of our pitch, right? It's like our system never sleeps and never needs to eat and never loses context. And this is the same as an attacker doing the exact same thing on the other side. And it's also the reason that none of the big tech companies we've talked to will build it because of the PR optics that we have built the hacking robot to come hack you 24/7 And there's too much Hollywood Skynet brand reputation on the line. So, you know, it's one of the big reasons that you know, we don't like see Google, Amazon, Microsoft, etc. Like, they may build it, but they're never going to release it publicly. So what's the solution? What are we building and what if we don't? So we built the hacking robot. And the idea is to take our hacking robot and put it in a loop with a virtual engineer. So one we use the thing that makes us different than any of the other like source code analysis tools are these buddies or CO pilots. And they we think GitHub and GitLab it's just going to win the copilot race, like they have all the source code, they have all the CI CD that I just opened as from a they're gonna put fortify and Coverity and a half dozen companies out of business. They're on the source code analysis, buddy thing and so that's not where we're pointing just we're on the same page. So, just like we used to do tests, Asia is the first thing to really break something is you have to understand what it is like why was it built? Is it a financial app? Is it a mobile app? Is it a payment pipeline? Is it a, you know, a back end for a SCADA system? So we use the foundational LM to extract their context. We also use it to extract the bits and bytes, or libraries was used with code, what languages were used, and then we create an attack plan, just like a tester saying, Hi, I really want to break this thing. This is how I had this is the code, I have to look at the tests after around the pools after run. And our pipe platform then kicks off a number of AI enabled actions to do that. So let's look at the code and might run a tool and might get some feedback. But that's where if you were to buy this off the shelf today, from Accenture, or NCC or Bishop Fox or any of the other consultancies, that's where it would stop it would give you just a pile of work to do. Every season. We talked to more than 50 cents at this point and CIOs and they're like, I don't need more bucks. I don't need more than do I actually need less to do because I need to focus my engineers on building the new thing. And so we've taken it a step further and said, Let's try and prove that those bugs are real bugs by writing an exploit. So get rid of a false positive actually see if we can make the bug trigger inside of the app to do something. And if we can do that, now let's produce a fix. So and we're not getting it. And we're not just saying hey, let's be a better grammar checker for your code or saying, we took the business context of why was this app made? The bits and bytes and the bug and said Alright, now let's try and produce a real fix and send that back to a pull request. To grandvision Is this just runs it just sits there. In the background finding and fixing issues 24/7 365 I think we're five years away from that being reality. But today where we're at is we're able to do a source assisted pen test with a human and an Iron Man suit where we supercharged the human to to use all of these stages the pipeline to do these faster and better and cheaper for organizations. So
it's the inverse of our pitch, right? It's like our system never sleeps and never needs to eat and never loses context. And this is the same as an attacker doing the exact same thing on the other side. And it's also the reason that none of the big tech companies we've talked to will build it because of the PR optics that we have built the hacking robot to come hack you 24/7 And there's too much Hollywood Skynet brand reputation on the line. So, you know, it's one of the big reasons that you know, we don't like see Google, Amazon, Microsoft, etc. Like, they may build it, but they're never going to release it publicly. So what's the solution? What are we building and what if we don't? So we built the hacking robot. And the idea is to take our hacking robot and put it in a loop with a virtual engineer. So one we use the thing that makes us different than any of the other like source code analysis tools are these buddies or CO pilots. And they we think GitHub and GitLab it's just going to win the copilot race, like they have all the source code, they have all the CI CD that I just opened as from a they're gonna put fortify and Coverity and a half dozen companies out of business. They're on the source code analysis, buddy thing and so that's not where we're pointing just we're on the same page. So, just like we used to do tests, Asia is the first thing to really break something is you have to understand what it is like why was it built? Is it a financial app? Is it a mobile app? Is it a payment pipeline? Is it a, you know, a back end for a SCADA system? So we use the foundational LM to extract their context. We also use it to extract the bits and bytes, or libraries was used with code, what languages were used, and then we create an attack plan, just like a tester saying, Hi, I really want to break this thing. This is how I had this is the code, I have to look at the tests after around the pools after run. And our pipe platform then kicks off a number of AI enabled actions to do that. So let's look at the code and might run a tool and might get some feedback. But that's where if you were to buy this off the shelf today, from Accenture, or NCC or Bishop Fox or any of the other consultancies, that's where it would stop it would give you just a pile of work to do. Every season. We talked to more than 50 cents at this point and CIOs and they're like, I don't need more bucks. I don't need more than do I actually need less to do because I need to focus my engineers on building the new thing. And so we've taken it a step further and said, Let's try and prove that those bugs are real bugs by writing an exploit. So get rid of a false positive actually see if we can make the bug trigger inside of the app to do something. And if we can do that, now let's produce a fix. So and we're not getting it. And we're not just saying hey, let's be a better grammar checker for your code or saying, we took the business context of why was this app made? The bits and bytes and the bug and said Alright, now let's try and produce a real fix and send that back to a pull request. To grandvision Is this just runs it just sits there. In the background finding and fixing issues 24/7 365 I think we're five years away from that being reality. But today where we're at is we're able to do a source assisted pen test with a human and an Iron Man suit where we supercharged the human to to use all of these stages the pipeline to do these faster and better and cheaper for organizations. So
it's the inverse of our pitch, right? It's like our system never sleeps and never needs to eat and never loses context. And this is the same as an attacker doing the exact same thing on the other side. And it's also the reason that none of the big tech companies we've talked to will build it because of the PR optics that we have built the hacking robot to come hack you 24/7 And there's too much Hollywood Skynet brand reputation on the line. So, you know, it's one of the big reasons that you know, we don't like see Google, Amazon, Microsoft, etc. Like, they may build it, but they're never going to release it publicly. So what's the solution? What are we building and what if we don't? So we built the hacking robot. And the idea is to take our hacking robot and put it in a loop with a virtual engineer. So one we use the thing that makes us different than any of the other like source code analysis tools are these buddies or CO pilots. And they we think GitHub and GitLab it's just going to win the copilot race, like they have all the source code, they have all the CI CD that I just opened as from a they're gonna put fortify and Coverity and a half dozen companies out of business. They're on the source code analysis, buddy thing and so that's not where we're pointing just we're on the same page. So, just like we used to do tests, Asia is the first thing to really break something is you have to understand what it is like why was it built? Is it a financial app? Is it a mobile app? Is it a payment pipeline? Is it a, you know, a back end for a SCADA system? So we use the foundational LM to extract their context. We also use it to extract the bits and bytes, or libraries was used with code, what languages were used, and then we create an attack plan, just like a tester saying, Hi, I really want to break this thing. This is how I had this is the code, I have to look at the tests after around the pools after run. And our pipe platform then kicks off a number of AI enabled actions to do that. So let's look at the code and might run a tool and might get some feedback. But that's where if you were to buy this off the shelf today, from Accenture, or NCC or Bishop Fox or any of the other consultancies, that's where it would stop it would give you just a pile of work to do. Every season. We talked to more than 50 cents at this point and CIOs and they're like, I don't need more bucks. I don't need more than do I actually need less to do because I need to focus my engineers on building the new thing. And so we've taken it a step further and said, Let's try and prove that those bugs are real bugs by writing an exploit. So get rid of a false positive actually see if we can make the bug trigger inside of the app to do something. And if we can do that, now let's produce a fix. So and we're not getting it. And we're not just saying hey, let's be a better grammar checker for your code or saying, we took the business context of why was this app made? The bits and bytes and the bug and said Alright, now let's try and produce a real fix and send that back to a pull request. To grandvision Is this just runs it just sits there. In the background finding and fixing issues 24/7 365 I think we're five years away from that being reality. But today where we're at is we're able to do a source assisted pen test with a human and an Iron Man suit where we supercharged the human to to use all of these stages the pipeline to do these faster and better and cheaper for organizations. So
S Speaker 117:05you, you using GPT four, or are you building your own model to do this?
you, you using GPT four, or are you building your own model to do this?
you, you using GPT four, or are you building your own model to do this?
you, you using GPT four, or are you building your own model to do this?
S Speaker 317:15So we're using we are using the Azure endpoint of GPT four. We're wearing three five forever so we got good enough results with three five but the foundational model we're building on top of is more now. We also train our own models for state specific to security as well as we train a micro model specific to that organization's code that said, hey, we need to make an expert. Many experts just like a consultant would come in and be an expert for you. But the upside for us is that consultant never leaves and it gets smarter every time they interact. So so so there we are training our microphones. Yes.
So we're using we are using the Azure endpoint of GPT four. We're wearing three five forever so we got good enough results with three five but the foundational model we're building on top of is more now. We also train our own models for state specific to security as well as we train a micro model specific to that organization's code that said, hey, we need to make an expert. Many experts just like a consultant would come in and be an expert for you. But the upside for us is that consultant never leaves and it gets smarter every time they interact. So so so there we are training our microphones. Yes.
So we're using we are using the Azure endpoint of GPT four. We're wearing three five forever so we got good enough results with three five but the foundational model we're building on top of is more now. We also train our own models for state specific to security as well as we train a micro model specific to that organization's code that said, hey, we need to make an expert. Many experts just like a consultant would come in and be an expert for you. But the upside for us is that consultant never leaves and it gets smarter every time they interact. So so so there we are training our microphones. Yes.
So we're using we are using the Azure endpoint of GPT four. We're wearing three five forever so we got good enough results with three five but the foundational model we're building on top of is more now. We also train our own models for state specific to security as well as we train a micro model specific to that organization's code that said, hey, we need to make an expert. Many experts just like a consultant would come in and be an expert for you. But the upside for us is that consultant never leaves and it gets smarter every time they interact. So so so there we are training our microphones. Yes.
S Speaker 117:49So there's one part instead of one part is understanding where the gaps are, which is to say that St. Joe's in the world is dumping. And then the other part is building these bots that actually go and hack these systems. And then And then the third part is I'm guessing going in and suggesting fixes. That's correct. So for what part, I'm guessing you're using for understanding you're using DBT for endpoint endpoint for DBT for and then and then the micro models are for the second and third part or which.
So there's one part instead of one part is understanding where the gaps are, which is to say that St. Joe's in the world is dumping. And then the other part is building these bots that actually go and hack these systems. And then And then the third part is I'm guessing going in and suggesting fixes. That's correct. So for what part, I'm guessing you're using for understanding you're using DBT for endpoint endpoint for DBT for and then and then the micro models are for the second and third part or which.
So there's one part instead of one part is understanding where the gaps are, which is to say that St. Joe's in the world is dumping. And then the other part is building these bots that actually go and hack these systems. And then And then the third part is I'm guessing going in and suggesting fixes. That's correct. So for what part, I'm guessing you're using for understanding you're using DBT for endpoint endpoint for DBT for and then and then the micro models are for the second and third part or which.
So there's one part instead of one part is understanding where the gaps are, which is to say that St. Joe's in the world is dumping. And then the other part is building these bots that actually go and hack these systems. And then And then the third part is I'm guessing going in and suggesting fixes. That's correct. So for what part, I'm guessing you're using for understanding you're using DBT for endpoint endpoint for DBT for and then and then the micro models are for the second and third part or which.
S Speaker 318:23So we're using the foundational models, what they're good at is taking structured things and turning them into structured things. So we're using that for understanding, understanding the bits and bytes, getting context out and reading lots of things at scale. So some of these customers send us you know, 100,000 source code and documentation files, and it could redefine the things we wanted in the minutes versus that we'd say with a consultant. We use our own models to do the attack and planning and all of the AI enabled actions in our platform and then the last piece the proposed fixes were using the foundational models for okay, but we see in the long term collecting enough data to be able to train our own fix our models, but right now we're we're using
So we're using the foundational models, what they're good at is taking structured things and turning them into structured things. So we're using that for understanding, understanding the bits and bytes, getting context out and reading lots of things at scale. So some of these customers send us you know, 100,000 source code and documentation files, and it could redefine the things we wanted in the minutes versus that we'd say with a consultant. We use our own models to do the attack and planning and all of the AI enabled actions in our platform and then the last piece the proposed fixes were using the foundational models for okay, but we see in the long term collecting enough data to be able to train our own fix our models, but right now we're we're using
So we're using the foundational models, what they're good at is taking structured things and turning them into structured things. So we're using that for understanding, understanding the bits and bytes, getting context out and reading lots of things at scale. So some of these customers send us you know, 100,000 source code and documentation files, and it could redefine the things we wanted in the minutes versus that we'd say with a consultant. We use our own models to do the attack and planning and all of the AI enabled actions in our platform and then the last piece the proposed fixes were using the foundational models for okay, but we see in the long term collecting enough data to be able to train our own fix our models, but right now we're we're using
So we're using the foundational models, what they're good at is taking structured things and turning them into structured things. So we're using that for understanding, understanding the bits and bytes, getting context out and reading lots of things at scale. So some of these customers send us you know, 100,000 source code and documentation files, and it could redefine the things we wanted in the minutes versus that we'd say with a consultant. We use our own models to do the attack and planning and all of the AI enabled actions in our platform and then the last piece the proposed fixes were using the foundational models for okay, but we see in the long term collecting enough data to be able to train our own fix our models, but right now we're we're using
S Speaker 119:04if you're not in the business of it, you don't necessarily have to do it.
if you're not in the business of it, you don't necessarily have to do it.
if you're not in the business of it, you don't necessarily have to do it.
if you're not in the business of it, you don't necessarily have to do it.
S Speaker 319:07And we get the red on top of gtgt or anthropic but the better they get, the better we get every single add on my
And we get the red on top of gtgt or anthropic but the better they get, the better we get every single add on my
And we get the red on top of gtgt or anthropic but the better they get, the better we get every single add on my
And we get the red on top of gtgt or anthropic but the better they get, the better we get every single add on my
S Speaker 119:13take on the capex unless you see a path to significantly reducing costs,
take on the capex unless you see a path to significantly reducing costs,
take on the capex unless you see a path to significantly reducing costs,
take on the capex unless you see a path to significantly reducing costs,
S Speaker 319:21no. So since some of the go to market here, we'll talk about some of the channels on a second, fourth quarter for channels we're targeting there's financial and government will only do offline. And so yeah, once we get more on prem offline, and so once we get to the ability to have a open source GPT for equivalent, then we'll look at shipping them a rack of things that go offline. But till then, like speed, the development cost engineering, we're not we're not we're not building a foundation. That's it's that's not where our our expertise lies. Yeah, that's good. So talking about market so if you add up the fortune 500 in the United States spend to go higher that NCCD Bishop box Accenture consultant to read their code so they don't end up on the front page of the Washington Post. It's about a billion dollars a year. That's just in services.
no. So since some of the go to market here, we'll talk about some of the channels on a second, fourth quarter for channels we're targeting there's financial and government will only do offline. And so yeah, once we get more on prem offline, and so once we get to the ability to have a open source GPT for equivalent, then we'll look at shipping them a rack of things that go offline. But till then, like speed, the development cost engineering, we're not we're not we're not building a foundation. That's it's that's not where our our expertise lies. Yeah, that's good. So talking about market so if you add up the fortune 500 in the United States spend to go higher that NCCD Bishop box Accenture consultant to read their code so they don't end up on the front page of the Washington Post. It's about a billion dollars a year. That's just in services.
no. So since some of the go to market here, we'll talk about some of the channels on a second, fourth quarter for channels we're targeting there's financial and government will only do offline. And so yeah, once we get more on prem offline, and so once we get to the ability to have a open source GPT for equivalent, then we'll look at shipping them a rack of things that go offline. But till then, like speed, the development cost engineering, we're not we're not we're not building a foundation. That's it's that's not where our our expertise lies. Yeah, that's good. So talking about market so if you add up the fortune 500 in the United States spend to go higher that NCCD Bishop box Accenture consultant to read their code so they don't end up on the front page of the Washington Post. It's about a billion dollars a year. That's just in services.
no. So since some of the go to market here, we'll talk about some of the channels on a second, fourth quarter for channels we're targeting there's financial and government will only do offline. And so yeah, once we get more on prem offline, and so once we get to the ability to have a open source GPT for equivalent, then we'll look at shipping them a rack of things that go offline. But till then, like speed, the development cost engineering, we're not we're not we're not building a foundation. That's it's that's not where our our expertise lies. Yeah, that's good. So talking about market so if you add up the fortune 500 in the United States spend to go higher that NCCD Bishop box Accenture consultant to read their code so they don't end up on the front page of the Washington Post. It's about a billion dollars a year. That's just in services.
S Speaker 320:27does not include the internal spends that these companies do for products, tooling. Team hires all the other things. Go on the reasons I know this is back deep relationships with the fortune 10 tech companies for the last decade have sold into all of these logos here. At one point or another anywhere between 500,000 to millions of dollars. And each logo up here spends anywhere between 10 to $100 million a year on hiring application security consultants to just look at their code as one time iterations. And we estimate they're looking at maybe 20% of what they actually want to be able to be looking at and testing. So and that's in every every company you've ever heard of whether it's a series B startup to one of the big one of these big logos has to buy a pen test from these for either SOC to PCI have some compliance things driving some of this and some of them have proactive bars. So a number of these splitters have toll gates in the city you must have an external pen test before you release an external product. So we want to take that Tollgate and turn it to a fast lane like Nope. Anytime you want to ship let's just go ahead so we're using a known upgrade path. So we're taking what has been possibly a human driven service and companies like Arctic wolf expel party that AI are saying, Hey, you used to buy these, these humans to do this. Now let's slowly upgrade you from we'll do one good pen test for you. We'll sign a contract on budget to our platform and we've had some good traction with that as of today. So just jump to this next one's we have four paying customers today. We actually just signed our fifth and I'm in talks with two of those previous logos that you saw on the slides do a proof of concept and GM onboarding. We're still in that sign the paperwork but you know, they've been begging me for an API for a pen test for a decade and we finally have one we can give to them so I have a proof of concept scheduled Grammarly on July that's on that one as well. And I want to three gap is happening this
does not include the internal spends that these companies do for products, tooling. Team hires all the other things. Go on the reasons I know this is back deep relationships with the fortune 10 tech companies for the last decade have sold into all of these logos here. At one point or another anywhere between 500,000 to millions of dollars. And each logo up here spends anywhere between 10 to $100 million a year on hiring application security consultants to just look at their code as one time iterations. And we estimate they're looking at maybe 20% of what they actually want to be able to be looking at and testing. So and that's in every every company you've ever heard of whether it's a series B startup to one of the big one of these big logos has to buy a pen test from these for either SOC to PCI have some compliance things driving some of this and some of them have proactive bars. So a number of these splitters have toll gates in the city you must have an external pen test before you release an external product. So we want to take that Tollgate and turn it to a fast lane like Nope. Anytime you want to ship let's just go ahead so we're using a known upgrade path. So we're taking what has been possibly a human driven service and companies like Arctic wolf expel party that AI are saying, Hey, you used to buy these, these humans to do this. Now let's slowly upgrade you from we'll do one good pen test for you. We'll sign a contract on budget to our platform and we've had some good traction with that as of today. So just jump to this next one's we have four paying customers today. We actually just signed our fifth and I'm in talks with two of those previous logos that you saw on the slides do a proof of concept and GM onboarding. We're still in that sign the paperwork but you know, they've been begging me for an API for a pen test for a decade and we finally have one we can give to them so I have a proof of concept scheduled Grammarly on July that's on that one as well. And I want to three gap is happening this
does not include the internal spends that these companies do for products, tooling. Team hires all the other things. Go on the reasons I know this is back deep relationships with the fortune 10 tech companies for the last decade have sold into all of these logos here. At one point or another anywhere between 500,000 to millions of dollars. And each logo up here spends anywhere between 10 to $100 million a year on hiring application security consultants to just look at their code as one time iterations. And we estimate they're looking at maybe 20% of what they actually want to be able to be looking at and testing. So and that's in every every company you've ever heard of whether it's a series B startup to one of the big one of these big logos has to buy a pen test from these for either SOC to PCI have some compliance things driving some of this and some of them have proactive bars. So a number of these splitters have toll gates in the city you must have an external pen test before you release an external product. So we want to take that Tollgate and turn it to a fast lane like Nope. Anytime you want to ship let's just go ahead so we're using a known upgrade path. So we're taking what has been possibly a human driven service and companies like Arctic wolf expel party that AI are saying, Hey, you used to buy these, these humans to do this. Now let's slowly upgrade you from we'll do one good pen test for you. We'll sign a contract on budget to our platform and we've had some good traction with that as of today. So just jump to this next one's we have four paying customers today. We actually just signed our fifth and I'm in talks with two of those previous logos that you saw on the slides do a proof of concept and GM onboarding. We're still in that sign the paperwork but you know, they've been begging me for an API for a pen test for a decade and we finally have one we can give to them so I have a proof of concept scheduled Grammarly on July that's on that one as well. And I want to three gap is happening this
does not include the internal spends that these companies do for products, tooling. Team hires all the other things. Go on the reasons I know this is back deep relationships with the fortune 10 tech companies for the last decade have sold into all of these logos here. At one point or another anywhere between 500,000 to millions of dollars. And each logo up here spends anywhere between 10 to $100 million a year on hiring application security consultants to just look at their code as one time iterations. And we estimate they're looking at maybe 20% of what they actually want to be able to be looking at and testing. So and that's in every every company you've ever heard of whether it's a series B startup to one of the big one of these big logos has to buy a pen test from these for either SOC to PCI have some compliance things driving some of this and some of them have proactive bars. So a number of these splitters have toll gates in the city you must have an external pen test before you release an external product. So we want to take that Tollgate and turn it to a fast lane like Nope. Anytime you want to ship let's just go ahead so we're using a known upgrade path. So we're taking what has been possibly a human driven service and companies like Arctic wolf expel party that AI are saying, Hey, you used to buy these, these humans to do this. Now let's slowly upgrade you from we'll do one good pen test for you. We'll sign a contract on budget to our platform and we've had some good traction with that as of today. So just jump to this next one's we have four paying customers today. We actually just signed our fifth and I'm in talks with two of those previous logos that you saw on the slides do a proof of concept and GM onboarding. We're still in that sign the paperwork but you know, they've been begging me for an API for a pen test for a decade and we finally have one we can give to them so I have a proof of concept scheduled Grammarly on July that's on that one as well. And I want to three gap is happening this
S Speaker 322:26I don't know if you think Alexa is they were talking with over their potential quarters but
I don't know if you think Alexa is they were talking with over their potential quarters but
I don't know if you think Alexa is they were talking with over their potential quarters but
I don't know if you think Alexa is they were talking with over their potential quarters but
S Speaker 122:34no, I don't know Alexa with me. We know they're CEOs, but Israel teams.
no, I don't know Alexa with me. We know they're CEOs, but Israel teams.
no, I don't know Alexa with me. We know they're CEOs, but Israel teams.
no, I don't know Alexa with me. We know they're CEOs, but Israel teams.
S Speaker 322:42So So yeah, we're the paying customers are spending anywhere between five and $20,000. A year with us right now. We're going to be upgrading a couple of those to get so on boarded to the platform and we have a sales page continuing. You gotta hear
So So yeah, we're the paying customers are spending anywhere between five and $20,000. A year with us right now. We're going to be upgrading a couple of those to get so on boarded to the platform and we have a sales page continuing. You gotta hear
So So yeah, we're the paying customers are spending anywhere between five and $20,000. A year with us right now. We're going to be upgrading a couple of those to get so on boarded to the platform and we have a sales page continuing. You gotta hear
So So yeah, we're the paying customers are spending anywhere between five and $20,000. A year with us right now. We're going to be upgrading a couple of those to get so on boarded to the platform and we have a sales page continuing. You gotta hear
S Speaker 122:57that, like what's the what's the business model and what's the PR?
that, like what's the what's the business model and what's the PR?
that, like what's the what's the business model and what's the PR?
that, like what's the what's the business model and what's the PR?
S Speaker 323:05So we're business models for different channels. And so right now there's the direct selling channel where we go out we say, Hey, you have a no line item on your budget for during during these application security Pentas. Let us come in do one of those. But then instead of being in your point in time thing, we just do it once or on a quarterly for you. And it's the same price that you would have paid for that once point in time assessment and so effectively, you know, when you add it all up poured about $40,000 So right now, we're only only a year old, we've incorporated a July 14 of 2023. And then some of these larger ones, we're still figuring out proof of concept what it's going to look like for if you know hey, they have 50 applications. Do you have 100 athletic applications. It's an enterprise sale and people will regularly spend half a million dollars on sneakers, another summer tool or other thing and so, we would look at displacing a chunk of the pen testing budget into this black conversation of that in turn the sass
So we're business models for different channels. And so right now there's the direct selling channel where we go out we say, Hey, you have a no line item on your budget for during during these application security Pentas. Let us come in do one of those. But then instead of being in your point in time thing, we just do it once or on a quarterly for you. And it's the same price that you would have paid for that once point in time assessment and so effectively, you know, when you add it all up poured about $40,000 So right now, we're only only a year old, we've incorporated a July 14 of 2023. And then some of these larger ones, we're still figuring out proof of concept what it's going to look like for if you know hey, they have 50 applications. Do you have 100 athletic applications. It's an enterprise sale and people will regularly spend half a million dollars on sneakers, another summer tool or other thing and so, we would look at displacing a chunk of the pen testing budget into this black conversation of that in turn the sass
So we're business models for different channels. And so right now there's the direct selling channel where we go out we say, Hey, you have a no line item on your budget for during during these application security Pentas. Let us come in do one of those. But then instead of being in your point in time thing, we just do it once or on a quarterly for you. And it's the same price that you would have paid for that once point in time assessment and so effectively, you know, when you add it all up poured about $40,000 So right now, we're only only a year old, we've incorporated a July 14 of 2023. And then some of these larger ones, we're still figuring out proof of concept what it's going to look like for if you know hey, they have 50 applications. Do you have 100 athletic applications. It's an enterprise sale and people will regularly spend half a million dollars on sneakers, another summer tool or other thing and so, we would look at displacing a chunk of the pen testing budget into this black conversation of that in turn the sass
So we're business models for different channels. And so right now there's the direct selling channel where we go out we say, Hey, you have a no line item on your budget for during during these application security Pentas. Let us come in do one of those. But then instead of being in your point in time thing, we just do it once or on a quarterly for you. And it's the same price that you would have paid for that once point in time assessment and so effectively, you know, when you add it all up poured about $40,000 So right now, we're only only a year old, we've incorporated a July 14 of 2023. And then some of these larger ones, we're still figuring out proof of concept what it's going to look like for if you know hey, they have 50 applications. Do you have 100 athletic applications. It's an enterprise sale and people will regularly spend half a million dollars on sneakers, another summer tool or other thing and so, we would look at displacing a chunk of the pen testing budget into this black conversation of that in turn the sass
S Speaker 324:10Yeah, we're for people. So it's myself and Austin are two co founders. They have a full full time developer. So let me just talk to the team side. Real quick. So I saw that I was the first of the co founders. We've each other for 20 years, both in the same program at Carnegie Mellon. He's been building large scale systems for commercials and government that a stint at Amazon and their database can not So Patrick is one of my former security engineers. He's our human in the loop right now and the robot gets stuck. We need something to kind of augment what we're doing. It's not that we have so full stack dev with two advisors was Daniel Herrera, he's executive director of JPMorgan Chase for this product selection for them, and he wants to come on full time. Once we raise a seed round and Joe there, Steve knows my former director of operations at both digital and peach. He's our fractional CEO. He wants to come on full time as well. We have three engineers we want to hire. Once we do kind of the next raise that we have here marketing events, kind of initial conversations with but the mostly everything from with this next raise is entirely towards more engineering MBK approved up until you raise today we've raised $1.8 million cheap safes. The first safe was $15 million. Notice count cap safe and the second was 20 million that just kept kept safe and we're trying to we're trying to raise raise seed round right now but expand engineer. Okay.
Yeah, we're for people. So it's myself and Austin are two co founders. They have a full full time developer. So let me just talk to the team side. Real quick. So I saw that I was the first of the co founders. We've each other for 20 years, both in the same program at Carnegie Mellon. He's been building large scale systems for commercials and government that a stint at Amazon and their database can not So Patrick is one of my former security engineers. He's our human in the loop right now and the robot gets stuck. We need something to kind of augment what we're doing. It's not that we have so full stack dev with two advisors was Daniel Herrera, he's executive director of JPMorgan Chase for this product selection for them, and he wants to come on full time. Once we raise a seed round and Joe there, Steve knows my former director of operations at both digital and peach. He's our fractional CEO. He wants to come on full time as well. We have three engineers we want to hire. Once we do kind of the next raise that we have here marketing events, kind of initial conversations with but the mostly everything from with this next raise is entirely towards more engineering MBK approved up until you raise today we've raised $1.8 million cheap safes. The first safe was $15 million. Notice count cap safe and the second was 20 million that just kept kept safe and we're trying to we're trying to raise raise seed round right now but expand engineer. Okay.
Yeah, we're for people. So it's myself and Austin are two co founders. They have a full full time developer. So let me just talk to the team side. Real quick. So I saw that I was the first of the co founders. We've each other for 20 years, both in the same program at Carnegie Mellon. He's been building large scale systems for commercials and government that a stint at Amazon and their database can not So Patrick is one of my former security engineers. He's our human in the loop right now and the robot gets stuck. We need something to kind of augment what we're doing. It's not that we have so full stack dev with two advisors was Daniel Herrera, he's executive director of JPMorgan Chase for this product selection for them, and he wants to come on full time. Once we raise a seed round and Joe there, Steve knows my former director of operations at both digital and peach. He's our fractional CEO. He wants to come on full time as well. We have three engineers we want to hire. Once we do kind of the next raise that we have here marketing events, kind of initial conversations with but the mostly everything from with this next raise is entirely towards more engineering MBK approved up until you raise today we've raised $1.8 million cheap safes. The first safe was $15 million. Notice count cap safe and the second was 20 million that just kept kept safe and we're trying to we're trying to raise raise seed round right now but expand engineer. Okay.
Yeah, we're for people. So it's myself and Austin are two co founders. They have a full full time developer. So let me just talk to the team side. Real quick. So I saw that I was the first of the co founders. We've each other for 20 years, both in the same program at Carnegie Mellon. He's been building large scale systems for commercials and government that a stint at Amazon and their database can not So Patrick is one of my former security engineers. He's our human in the loop right now and the robot gets stuck. We need something to kind of augment what we're doing. It's not that we have so full stack dev with two advisors was Daniel Herrera, he's executive director of JPMorgan Chase for this product selection for them, and he wants to come on full time. Once we raise a seed round and Joe there, Steve knows my former director of operations at both digital and peach. He's our fractional CEO. He wants to come on full time as well. We have three engineers we want to hire. Once we do kind of the next raise that we have here marketing events, kind of initial conversations with but the mostly everything from with this next raise is entirely towards more engineering MBK approved up until you raise today we've raised $1.8 million cheap safes. The first safe was $15 million. Notice count cap safe and the second was 20 million that just kept kept safe and we're trying to we're trying to raise raise seed round right now but expand engineer. Okay.
S Speaker 325:42Look at this one half nine. Let me basically double the team size I almost entirely in engineering. Bring on these two. Gentlemen, Daniel and us about two years of runway. So I should probably talk about how we're thinking about the business from a metrics perspective to kind of factor that a little bit more. So right now we're tracking one KPI just one which is how much human time was spent doing one of these engagements, one of these application tests versus how much AI time and right now we're at a one to one ratio. So every hour that Matt spends on one of these engagements, the AI spending about one hour of human equivalent time or savings of about 50% So we're trying to get to a one to 20 ratio, we charted out what it'll take for us to get there. Which means one hour of human time, or 20 hours of AI time, and that's to basically get us to the series A and the, what we want the metric we want to hit for that 2 million error. I have a slide on this I shouldn't just pop it over here sorry. Truly an error with its programmatic customers, so anybody can settle on a pen test that looks like solvency, I got the coffee. I got the literal coffee mug tonight. I never want to do that again. So getting to those big, big customers on board that have those 25 $200 million dollar spends making sure that we give them the self pen test portal where they can click a button and we can hit the one to 20 ratio. So for the A, we want to have a one to 40 ratio when we hit a one to 40 ratio and we've got true automation where now I can have Matt sit back five tickets occasionally when something gets caught, and I even watch 40 pen tests or 100 pen test depending on how well they're going or not. The way we're doing this and thinking about this and talking to customers just saying look, we're gonna come in and do a type of engagement for you one type of service and AFI that the product ties that platform or test that and then we'll expand to different ones as we go on.
Look at this one half nine. Let me basically double the team size I almost entirely in engineering. Bring on these two. Gentlemen, Daniel and us about two years of runway. So I should probably talk about how we're thinking about the business from a metrics perspective to kind of factor that a little bit more. So right now we're tracking one KPI just one which is how much human time was spent doing one of these engagements, one of these application tests versus how much AI time and right now we're at a one to one ratio. So every hour that Matt spends on one of these engagements, the AI spending about one hour of human equivalent time or savings of about 50% So we're trying to get to a one to 20 ratio, we charted out what it'll take for us to get there. Which means one hour of human time, or 20 hours of AI time, and that's to basically get us to the series A and the, what we want the metric we want to hit for that 2 million error. I have a slide on this I shouldn't just pop it over here sorry. Truly an error with its programmatic customers, so anybody can settle on a pen test that looks like solvency, I got the coffee. I got the literal coffee mug tonight. I never want to do that again. So getting to those big, big customers on board that have those 25 $200 million dollar spends making sure that we give them the self pen test portal where they can click a button and we can hit the one to 20 ratio. So for the A, we want to have a one to 40 ratio when we hit a one to 40 ratio and we've got true automation where now I can have Matt sit back five tickets occasionally when something gets caught, and I even watch 40 pen tests or 100 pen test depending on how well they're going or not. The way we're doing this and thinking about this and talking to customers just saying look, we're gonna come in and do a type of engagement for you one type of service and AFI that the product ties that platform or test that and then we'll expand to different ones as we go on.
Look at this one half nine. Let me basically double the team size I almost entirely in engineering. Bring on these two. Gentlemen, Daniel and us about two years of runway. So I should probably talk about how we're thinking about the business from a metrics perspective to kind of factor that a little bit more. So right now we're tracking one KPI just one which is how much human time was spent doing one of these engagements, one of these application tests versus how much AI time and right now we're at a one to one ratio. So every hour that Matt spends on one of these engagements, the AI spending about one hour of human equivalent time or savings of about 50% So we're trying to get to a one to 20 ratio, we charted out what it'll take for us to get there. Which means one hour of human time, or 20 hours of AI time, and that's to basically get us to the series A and the, what we want the metric we want to hit for that 2 million error. I have a slide on this I shouldn't just pop it over here sorry. Truly an error with its programmatic customers, so anybody can settle on a pen test that looks like solvency, I got the coffee. I got the literal coffee mug tonight. I never want to do that again. So getting to those big, big customers on board that have those 25 $200 million dollar spends making sure that we give them the self pen test portal where they can click a button and we can hit the one to 20 ratio. So for the A, we want to have a one to 40 ratio when we hit a one to 40 ratio and we've got true automation where now I can have Matt sit back five tickets occasionally when something gets caught, and I even watch 40 pen tests or 100 pen test depending on how well they're going or not. The way we're doing this and thinking about this and talking to customers just saying look, we're gonna come in and do a type of engagement for you one type of service and AFI that the product ties that platform or test that and then we'll expand to different ones as we go on.
Look at this one half nine. Let me basically double the team size I almost entirely in engineering. Bring on these two. Gentlemen, Daniel and us about two years of runway. So I should probably talk about how we're thinking about the business from a metrics perspective to kind of factor that a little bit more. So right now we're tracking one KPI just one which is how much human time was spent doing one of these engagements, one of these application tests versus how much AI time and right now we're at a one to one ratio. So every hour that Matt spends on one of these engagements, the AI spending about one hour of human equivalent time or savings of about 50% So we're trying to get to a one to 20 ratio, we charted out what it'll take for us to get there. Which means one hour of human time, or 20 hours of AI time, and that's to basically get us to the series A and the, what we want the metric we want to hit for that 2 million error. I have a slide on this I shouldn't just pop it over here sorry. Truly an error with its programmatic customers, so anybody can settle on a pen test that looks like solvency, I got the coffee. I got the literal coffee mug tonight. I never want to do that again. So getting to those big, big customers on board that have those 25 $200 million dollar spends making sure that we give them the self pen test portal where they can click a button and we can hit the one to 20 ratio. So for the A, we want to have a one to 40 ratio when we hit a one to 40 ratio and we've got true automation where now I can have Matt sit back five tickets occasionally when something gets caught, and I even watch 40 pen tests or 100 pen test depending on how well they're going or not. The way we're doing this and thinking about this and talking to customers just saying look, we're gonna come in and do a type of engagement for you one type of service and AFI that the product ties that platform or test that and then we'll expand to different ones as we go on.
S Speaker 127:44This actually, no we had a short squeeze quite a bit, so that's very good. Let me discuss this team. Exactly. I mean, the problem statement is clear. The solution that you're building, so we might need to understand more. It is possible that might be one round earlier for us. But let me discuss with our team and then I'll get back to you. I think we can get back to you within or we can say if we're interested in would like to learn more. And then if not, then we can continue to build
This actually, no we had a short squeeze quite a bit, so that's very good. Let me discuss this team. Exactly. I mean, the problem statement is clear. The solution that you're building, so we might need to understand more. It is possible that might be one round earlier for us. But let me discuss with our team and then I'll get back to you. I think we can get back to you within or we can say if we're interested in would like to learn more. And then if not, then we can continue to build
This actually, no we had a short squeeze quite a bit, so that's very good. Let me discuss this team. Exactly. I mean, the problem statement is clear. The solution that you're building, so we might need to understand more. It is possible that might be one round earlier for us. But let me discuss with our team and then I'll get back to you. I think we can get back to you within or we can say if we're interested in would like to learn more. And then if not, then we can continue to build
This actually, no we had a short squeeze quite a bit, so that's very good. Let me discuss this team. Exactly. I mean, the problem statement is clear. The solution that you're building, so we might need to understand more. It is possible that might be one round earlier for us. But let me discuss with our team and then I'll get back to you. I think we can get back to you within or we can say if we're interested in would like to learn more. And then if not, then we can continue to build
S Speaker 328:22the relationship for the next one. Sounds great. Yeah. And next time I'm down there, I'd love to grab coffee. That
the relationship for the next one. Sounds great. Yeah. And next time I'm down there, I'd love to grab coffee. That
the relationship for the next one. Sounds great. Yeah. And next time I'm down there, I'd love to grab coffee. That
the relationship for the next one. Sounds great. Yeah. And next time I'm down there, I'd love to grab coffee. That
28:27would be great. If you can ping us Yeah. That would be great. Thank you
would be great. If you can ping us Yeah. That would be great. Thank you
would be great. If you can ping us Yeah. That would be great. Thank you
would be great. If you can ping us Yeah. That would be great. Thank you