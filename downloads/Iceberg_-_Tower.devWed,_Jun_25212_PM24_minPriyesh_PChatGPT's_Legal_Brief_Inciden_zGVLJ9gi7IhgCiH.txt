Meeting: Iceberg - Tower.dev
Wed, Jun 25
2:12 PM
24 min
Priyesh P
ChatGPT's Legal Brief Incident and Its Impl
URL: https://otter.ai/u/zGVLJ9gi7IhgCiHl89D0SYE_sxY
Downloaded: 2025-12-21T21:50:00.593357
Method: text_extraction
============================================================

S Speaker 10:00This age, it didn't work out really well to the inhabitants of Earth, but it's going to be much better. Who of you raise your hand? Remember this case from a couple of years ago when two New York lawyers submitted a chatgpt generated court brief? I see a few of you raising the hand. What happened then? Well, our favorite chat bot generated an entire legal brief. It actually created case citations of non existing legal cases. The court accepted this legal brief. The opposing counsel, of course, did the reference checks and discovered it was entirely fabricated. The lawyers were fined $5,000 the plaintiff lost the case. We ended up in a mess. Why did we end up in a mess? Well, this is a very simple diagram. You probably know how chatbot bots work, or used to work two years ago. They used a large, large language model that was trained on a corpus of training data, and it took us months and months to train this model, so by the time the model was created, it was already dated. And secondly, the chatbots didn't really have the ability to check anything. They would accept your user input, and they would use a probabilistic technique to generate the output, and that was the answer to the user. Interestingly, the lawyers in our case did ask chatgpt, are you sure these cases were ill and the Chatbot didn't do what a human would do, which is going to a legal database like LexisNexis and actually type in the case numbers and check if they existed. Chatbot just applied, yes, they exist, and the lawyers relied on this information. Now, since then, the industry ally realized we need to complement the knowledge in our lamps with factual and recent info. So the new technique and this little bit of a refresher of what you all have heard in the first set of sessions here today, we still have our llms, and they receive user input, the questions that people want to ask, but instead of producing output immediately, they start a reasoning loop. And this reasoning loop begins calling out various tools. Some of them call external APIs, such as LexisNexis. Some of them will call enterprise analytical and operational databases, and the loop will continue until the LLM decides this is the final answer. I have enough information to create a final answer, at which point the output will be generated and stored together with a prompt in a thing called memory. So this is generally how these agents now work using tools. Who am I to be speaking to you so passionately about this topic. I've built data and AI systems previously at Databricks, snowflake and Google Cloud. I now run a company called tower dot Dev, and we build Python platforms, portable Python platforms to build, pipelines, agents and tools. So in this talk, we already motivated why it was a problem not having a reference database to check. Let's talk about solutions. So let's let's talk about enabling agents with enterprise data that they can cross check. We'll talk about trends in the world of analytics. There's a big trend, and it's also the main sake of the stock Apache. Iceberg is this big new trend for analytical storage. We'll look at an architecture of a agentic platform that provides easy access to analytical data to agents, and we'll look into the future as well. So how do we complement llms With recent and factual info? Tim scarle, the host of ml state talk, has recently said something that really resonated with me, and effectively, he said, llms are databases. Now I'm a database gig, and if you show me a break, I will call it a database. I'll find a way to explain why it's a database, but it's actually making sense. Think about the training process of a of a LLM, you'll get a corpus of data, and let's say llama llamas force. Case, it was 30 trillion words, which is approximately 150 terabytes of data. We've got enough metaphors here to confirm or disprove whether it was 50 terabytes or 200 but it took us nine months to train this model, and so by and by the end of this training process, we ended up with a set of files that we are less than a terabyte in size. So it was a 200x compression of our training set into this binary output. And everyone who knows information theory, or at least have read a little bit about information theory, a 200x compression of data immediately leads it's lossy. You cannot recover the information that went into this process from one to one. We are still okay accepting our lens as databases because they believe in they're smart and they can create creative responses. So we're okay with this lossy compression. Now, on the other hand, if you look at the world of analytical data lakes or operational databases, we are dealing with a much larger set of data. Some customers
This age, it didn't work out really well to the inhabitants of Earth, but it's going to be much better. Who of you raise your hand? Remember this case from a couple of years ago when two New York lawyers submitted a chatgpt generated court brief? I see a few of you raising the hand. What happened then? Well, our favorite chat bot generated an entire legal brief. It actually created case citations of non existing legal cases. The court accepted this legal brief. The opposing counsel, of course, did the reference checks and discovered it was entirely fabricated. The lawyers were fined $5,000 the plaintiff lost the case. We ended up in a mess. Why did we end up in a mess? Well, this is a very simple diagram. You probably know how chatbot bots work, or used to work two years ago. They used a large, large language model that was trained on a corpus of training data, and it took us months and months to train this model, so by the time the model was created, it was already dated. And secondly, the chatbots didn't really have the ability to check anything. They would accept your user input, and they would use a probabilistic technique to generate the output, and that was the answer to the user. Interestingly, the lawyers in our case did ask chatgpt, are you sure these cases were ill and the Chatbot didn't do what a human would do, which is going to a legal database like LexisNexis and actually type in the case numbers and check if they existed. Chatbot just applied, yes, they exist, and the lawyers relied on this information. Now, since then, the industry ally realized we need to complement the knowledge in our lamps with factual and recent info. So the new technique and this little bit of a refresher of what you all have heard in the first set of sessions here today, we still have our llms, and they receive user input, the questions that people want to ask, but instead of producing output immediately, they start a reasoning loop. And this reasoning loop begins calling out various tools. Some of them call external APIs, such as LexisNexis. Some of them will call enterprise analytical and operational databases, and the loop will continue until the LLM decides this is the final answer. I have enough information to create a final answer, at which point the output will be generated and stored together with a prompt in a thing called memory. So this is generally how these agents now work using tools. Who am I to be speaking to you so passionately about this topic. I've built data and AI systems previously at Databricks, snowflake and Google Cloud. I now run a company called tower dot Dev, and we build Python platforms, portable Python platforms to build, pipelines, agents and tools. So in this talk, we already motivated why it was a problem not having a reference database to check. Let's talk about solutions. So let's let's talk about enabling agents with enterprise data that they can cross check. We'll talk about trends in the world of analytics. There's a big trend, and it's also the main sake of the stock Apache. Iceberg is this big new trend for analytical storage. We'll look at an architecture of a agentic platform that provides easy access to analytical data to agents, and we'll look into the future as well. So how do we complement llms With recent and factual info? Tim scarle, the host of ml state talk, has recently said something that really resonated with me, and effectively, he said, llms are databases. Now I'm a database gig, and if you show me a break, I will call it a database. I'll find a way to explain why it's a database, but it's actually making sense. Think about the training process of a of a LLM, you'll get a corpus of data, and let's say llama llamas force. Case, it was 30 trillion words, which is approximately 150 terabytes of data. We've got enough metaphors here to confirm or disprove whether it was 50 terabytes or 200 but it took us nine months to train this model, and so by and by the end of this training process, we ended up with a set of files that we are less than a terabyte in size. So it was a 200x compression of our training set into this binary output. And everyone who knows information theory, or at least have read a little bit about information theory, a 200x compression of data immediately leads it's lossy. You cannot recover the information that went into this process from one to one. We are still okay accepting our lens as databases because they believe in they're smart and they can create creative responses. So we're okay with this lossy compression. Now, on the other hand, if you look at the world of analytical data lakes or operational databases, we are dealing with a much larger set of data. Some customers
This age, it didn't work out really well to the inhabitants of Earth, but it's going to be much better. Who of you raise your hand? Remember this case from a couple of years ago when two New York lawyers submitted a chatgpt generated court brief? I see a few of you raising the hand. What happened then? Well, our favorite chat bot generated an entire legal brief. It actually created case citations of non existing legal cases. The court accepted this legal brief. The opposing counsel, of course, did the reference checks and discovered it was entirely fabricated. The lawyers were fined $5,000 the plaintiff lost the case. We ended up in a mess. Why did we end up in a mess? Well, this is a very simple diagram. You probably know how chatbot bots work, or used to work two years ago. They used a large, large language model that was trained on a corpus of training data, and it took us months and months to train this model, so by the time the model was created, it was already dated. And secondly, the chatbots didn't really have the ability to check anything. They would accept your user input, and they would use a probabilistic technique to generate the output, and that was the answer to the user. Interestingly, the lawyers in our case did ask chatgpt, are you sure these cases were ill and the Chatbot didn't do what a human would do, which is going to a legal database like LexisNexis and actually type in the case numbers and check if they existed. Chatbot just applied, yes, they exist, and the lawyers relied on this information. Now, since then, the industry ally realized we need to complement the knowledge in our lamps with factual and recent info. So the new technique and this little bit of a refresher of what you all have heard in the first set of sessions here today, we still have our llms, and they receive user input, the questions that people want to ask, but instead of producing output immediately, they start a reasoning loop. And this reasoning loop begins calling out various tools. Some of them call external APIs, such as LexisNexis. Some of them will call enterprise analytical and operational databases, and the loop will continue until the LLM decides this is the final answer. I have enough information to create a final answer, at which point the output will be generated and stored together with a prompt in a thing called memory. So this is generally how these agents now work using tools. Who am I to be speaking to you so passionately about this topic. I've built data and AI systems previously at Databricks, snowflake and Google Cloud. I now run a company called tower dot Dev, and we build Python platforms, portable Python platforms to build, pipelines, agents and tools. So in this talk, we already motivated why it was a problem not having a reference database to check. Let's talk about solutions. So let's let's talk about enabling agents with enterprise data that they can cross check. We'll talk about trends in the world of analytics. There's a big trend, and it's also the main sake of the stock Apache. Iceberg is this big new trend for analytical storage. We'll look at an architecture of a agentic platform that provides easy access to analytical data to agents, and we'll look into the future as well. So how do we complement llms With recent and factual info? Tim scarle, the host of ml state talk, has recently said something that really resonated with me, and effectively, he said, llms are databases. Now I'm a database gig, and if you show me a break, I will call it a database. I'll find a way to explain why it's a database, but it's actually making sense. Think about the training process of a of a LLM, you'll get a corpus of data, and let's say llama llamas force. Case, it was 30 trillion words, which is approximately 150 terabytes of data. We've got enough metaphors here to confirm or disprove whether it was 50 terabytes or 200 but it took us nine months to train this model, and so by and by the end of this training process, we ended up with a set of files that we are less than a terabyte in size. So it was a 200x compression of our training set into this binary output. And everyone who knows information theory, or at least have read a little bit about information theory, a 200x compression of data immediately leads it's lossy. You cannot recover the information that went into this process from one to one. We are still okay accepting our lens as databases because they believe in they're smart and they can create creative responses. So we're okay with this lossy compression. Now, on the other hand, if you look at the world of analytical data lakes or operational databases, we are dealing with a much larger set of data. Some customers
This age, it didn't work out really well to the inhabitants of Earth, but it's going to be much better. Who of you raise your hand? Remember this case from a couple of years ago when two New York lawyers submitted a chatgpt generated court brief? I see a few of you raising the hand. What happened then? Well, our favorite chat bot generated an entire legal brief. It actually created case citations of non existing legal cases. The court accepted this legal brief. The opposing counsel, of course, did the reference checks and discovered it was entirely fabricated. The lawyers were fined $5,000 the plaintiff lost the case. We ended up in a mess. Why did we end up in a mess? Well, this is a very simple diagram. You probably know how chatbot bots work, or used to work two years ago. They used a large, large language model that was trained on a corpus of training data, and it took us months and months to train this model, so by the time the model was created, it was already dated. And secondly, the chatbots didn't really have the ability to check anything. They would accept your user input, and they would use a probabilistic technique to generate the output, and that was the answer to the user. Interestingly, the lawyers in our case did ask chatgpt, are you sure these cases were ill and the Chatbot didn't do what a human would do, which is going to a legal database like LexisNexis and actually type in the case numbers and check if they existed. Chatbot just applied, yes, they exist, and the lawyers relied on this information. Now, since then, the industry ally realized we need to complement the knowledge in our lamps with factual and recent info. So the new technique and this little bit of a refresher of what you all have heard in the first set of sessions here today, we still have our llms, and they receive user input, the questions that people want to ask, but instead of producing output immediately, they start a reasoning loop. And this reasoning loop begins calling out various tools. Some of them call external APIs, such as LexisNexis. Some of them will call enterprise analytical and operational databases, and the loop will continue until the LLM decides this is the final answer. I have enough information to create a final answer, at which point the output will be generated and stored together with a prompt in a thing called memory. So this is generally how these agents now work using tools. Who am I to be speaking to you so passionately about this topic. I've built data and AI systems previously at Databricks, snowflake and Google Cloud. I now run a company called tower dot Dev, and we build Python platforms, portable Python platforms to build, pipelines, agents and tools. So in this talk, we already motivated why it was a problem not having a reference database to check. Let's talk about solutions. So let's let's talk about enabling agents with enterprise data that they can cross check. We'll talk about trends in the world of analytics. There's a big trend, and it's also the main sake of the stock Apache. Iceberg is this big new trend for analytical storage. We'll look at an architecture of a agentic platform that provides easy access to analytical data to agents, and we'll look into the future as well. So how do we complement llms With recent and factual info? Tim scarle, the host of ml state talk, has recently said something that really resonated with me, and effectively, he said, llms are databases. Now I'm a database gig, and if you show me a break, I will call it a database. I'll find a way to explain why it's a database, but it's actually making sense. Think about the training process of a of a LLM, you'll get a corpus of data, and let's say llama llamas force. Case, it was 30 trillion words, which is approximately 150 terabytes of data. We've got enough metaphors here to confirm or disprove whether it was 50 terabytes or 200 but it took us nine months to train this model, and so by and by the end of this training process, we ended up with a set of files that we are less than a terabyte in size. So it was a 200x compression of our training set into this binary output. And everyone who knows information theory, or at least have read a little bit about information theory, a 200x compression of data immediately leads it's lossy. You cannot recover the information that went into this process from one to one. We are still okay accepting our lens as databases because they believe in they're smart and they can create creative responses. So we're okay with this lossy compression. Now, on the other hand, if you look at the world of analytical data lakes or operational databases, we are dealing with a much larger set of data. Some customers
S Speaker 15:36have 10 annual customers, large banks, we have 1000 petabytes of data, some some of it's still sitting on tapes. And the efficiency of data is measured in an hour to the analytical case, or maybe a couple of milliseconds in the operational database case. So it's very recent data. And the compression, this is pretty much no complete a compression, per se, you but what you get in into your database, you can get out by querying it. So there's compassion, but it's not lossy, that's what I meant to say. And because of that, query results are deterministic and they're functional. So it's kind of makes sense to be able to try to use both our lamps with your operational analytical data source. So some of us have been asking the question, can't be can we store everything in one place? Wouldn't be much more convenient to store everything in one place? Well, disregarding the fact that LMS at the inference time require an entire different set of infrastructure, mostly in memory versus operational analytical stores. Have you ever tried to replace a database, a popular database, anywhere you're looking right now at a chart from DB engines rankings that's inside where database needs to go to brag about popularity of their favorite databases. And so what you notice there is that the top three database technologies for the last 10 years have been Oracle, MySQL and SQL Server. This is something that our parents were using, and they made, sorry, I used to work for Microsoft. It's okay. So, and this, this top three rankings remain pretty much intact over the last 10 years, and the hot new technology hot in quotation marks, Postgres, that just recently received a bunch of investments from both Databricks and snowflake, it took them almost 10 years just to get to position four and the newest kids on the block, the snowflakes and databrickses, they've been growing faster, but this is logarithmic scale, so actually, the popularity between snowflake and an Oracle is still 10x it will take you ages to even consider the idea of replacing a database with something else. So I think this means we have to deal with the fact that llms As a database will have to coexist with analytical and operational databases, and we need to figure out a way to make it work all together for agents. So let's look at what's going on in the analytical world, because this is important. There are trends in the analytical data space that will influence how agents will get access to that data. And the big trend in the analytical world is Apache iceberg, maybe another race of endless quote about Apache iceberg as a Open Table format, some of you, everyone. So iceberg is a standard. It defines a way to store relational tables. And it's actually a pretty simple standard that says, There shall be a tree of files, and there are three types of files, and these files kind of cross link to each other. There's a root of this hierarchy. Every table has a root file, and then it points to a bunch of metadata files until it reaches the data layer. Now Apache iceberg is not a database engine that you can buy. You cannot buy iceberg. You can buy a technology that supports the standard but it describes or specifies a very important concept of a metadata catalog, and this will be your single point of failure in Apache, iceberg oriented storage. This is the critical piece that binds everything together, because it serves only one purpose, which is pointing to the root of a meta metadata file, file that initiates the speed for each table so its responsibilities are small, but it's super critical. Now, why are customers, small and large, migrating to iceberg? And to understand this, let's look at what was what was happening. Let's say five years ago. Five years ago, customers would use a combination of different database technologies. They would use Databricks, a snowflake and a fabric and some other database engines. And these database engines were, of course, all cloud based, all distributed processing, all separation of data and compute, shuffle, everywhere, awesome stuff, but they were storing tables in their proprietary table format. Now underneath, they might have used open file formats such as Parque and Avro, but the this middle layer that defined which data files corresponded to which tables, they were all proprietary. So I used to work with snowflake, and part of my job was to create storage optimization technologies to make search really fast on that metadata layer. It was very sophisticated, very smartly written, but it was proprietary. And if you wanted to move data between silos, and let's submit, we used to use these database engines for different tasks. We use snowflake for analytical processing, and we use Databricks for ML Engine and data engineering, and we used fabric if you were a Microsoft shop. So to move data around, we had to employ ETL processes, which were super expensive. There are some estimates that 40% of data processing costs in companies was actually spent on ETL. Well, customers are not entirely naive and down so they be built. And one of these large customers, Netflix, open sourced a technology that later became known as a Bucha iceberg. And since then, this tech actually received a very significant adoption across both enterprises and startups and mid sized companies. You'll see in this graph, the iceberg is represented by the rat chart. It is experiencing a hockey stick growth pattern. Now, what's important for agents when they access analytical data stores? Hopefully, I was able to motivate agents need to access analytical data stores. It's important. It makes ungrounded improved. But what do we need to do to make it work for agents? And I would say there are two important considerations, and I have to really I started a long list of considerations, I ended up with these two most important ones. I think throughput will be one critical consideration. And to understand this, think about you typing in a question or tasking an agent to do some research and find some anomalies in your sales data. So what will this agent do? They will run a bunch of analytical queries, slicing your data by different dimensions. Now, a human analyst. If you gave this task to a human analyst, they would actually stop after a while, because they need to go home and have dinner, they don't have the mental capacity to run 1000 quiz. Agents have unlimited time and hopefully resources. They can continue slicing, dicing the data, looking for these values. Therefore they will execute 100 times more queries. And also an important reason agents don't have yet that knowledge, that institutional knowledge, of interesting insights in your enterprise data, as human analysts do, so they don't really know how to stop and when to stop until they find an interesting anomaly. Because of these factors, I think throughput is a huge consideration for analytical workloads when they're used by agents, and we need to find the technology that enables high skill building the other important consideration is data security. Because if you also think about this, if you were a human user of an adjusting system, you gave the agent the task, you don't want the agent to be using AI information if they didn't necessarily need to accomplish this task. You don't want them to access data sets that are not necessary for accomplishing the goal. So things like column masking and raw filters will become important in a genetic world when they start using analytical data source. How does iceberg measure up for these requirements? Read throughput is amazing in iceberg. It relies on the huge scalability of underlying storage systems such as speed and Azure Blob and GCP or GCS. The right throughput is not great. The community is working on it, but your agents will probably also not be writing transactional data directly into your analytical data store. That's a not very important. The right throughput in terms of data security, it's also work in progress. There are some catalogs that allow you to manage better your column masking policies and your raw filter policies. I will call out maybe a few catalogs here, leak keeper and unity catalog, and are some emerging standards in the world of open, fine grained access controls, open FGA and role based security, RBAC.
have 10 annual customers, large banks, we have 1000 petabytes of data, some some of it's still sitting on tapes. And the efficiency of data is measured in an hour to the analytical case, or maybe a couple of milliseconds in the operational database case. So it's very recent data. And the compression, this is pretty much no complete a compression, per se, you but what you get in into your database, you can get out by querying it. So there's compassion, but it's not lossy, that's what I meant to say. And because of that, query results are deterministic and they're functional. So it's kind of makes sense to be able to try to use both our lamps with your operational analytical data source. So some of us have been asking the question, can't be can we store everything in one place? Wouldn't be much more convenient to store everything in one place? Well, disregarding the fact that LMS at the inference time require an entire different set of infrastructure, mostly in memory versus operational analytical stores. Have you ever tried to replace a database, a popular database, anywhere you're looking right now at a chart from DB engines rankings that's inside where database needs to go to brag about popularity of their favorite databases. And so what you notice there is that the top three database technologies for the last 10 years have been Oracle, MySQL and SQL Server. This is something that our parents were using, and they made, sorry, I used to work for Microsoft. It's okay. So, and this, this top three rankings remain pretty much intact over the last 10 years, and the hot new technology hot in quotation marks, Postgres, that just recently received a bunch of investments from both Databricks and snowflake, it took them almost 10 years just to get to position four and the newest kids on the block, the snowflakes and databrickses, they've been growing faster, but this is logarithmic scale, so actually, the popularity between snowflake and an Oracle is still 10x it will take you ages to even consider the idea of replacing a database with something else. So I think this means we have to deal with the fact that llms As a database will have to coexist with analytical and operational databases, and we need to figure out a way to make it work all together for agents. So let's look at what's going on in the analytical world, because this is important. There are trends in the analytical data space that will influence how agents will get access to that data. And the big trend in the analytical world is Apache iceberg, maybe another race of endless quote about Apache iceberg as a Open Table format, some of you, everyone. So iceberg is a standard. It defines a way to store relational tables. And it's actually a pretty simple standard that says, There shall be a tree of files, and there are three types of files, and these files kind of cross link to each other. There's a root of this hierarchy. Every table has a root file, and then it points to a bunch of metadata files until it reaches the data layer. Now Apache iceberg is not a database engine that you can buy. You cannot buy iceberg. You can buy a technology that supports the standard but it describes or specifies a very important concept of a metadata catalog, and this will be your single point of failure in Apache, iceberg oriented storage. This is the critical piece that binds everything together, because it serves only one purpose, which is pointing to the root of a meta metadata file, file that initiates the speed for each table so its responsibilities are small, but it's super critical. Now, why are customers, small and large, migrating to iceberg? And to understand this, let's look at what was what was happening. Let's say five years ago. Five years ago, customers would use a combination of different database technologies. They would use Databricks, a snowflake and a fabric and some other database engines. And these database engines were, of course, all cloud based, all distributed processing, all separation of data and compute, shuffle, everywhere, awesome stuff, but they were storing tables in their proprietary table format. Now underneath, they might have used open file formats such as Parque and Avro, but the this middle layer that defined which data files corresponded to which tables, they were all proprietary. So I used to work with snowflake, and part of my job was to create storage optimization technologies to make search really fast on that metadata layer. It was very sophisticated, very smartly written, but it was proprietary. And if you wanted to move data between silos, and let's submit, we used to use these database engines for different tasks. We use snowflake for analytical processing, and we use Databricks for ML Engine and data engineering, and we used fabric if you were a Microsoft shop. So to move data around, we had to employ ETL processes, which were super expensive. There are some estimates that 40% of data processing costs in companies was actually spent on ETL. Well, customers are not entirely naive and down so they be built. And one of these large customers, Netflix, open sourced a technology that later became known as a Bucha iceberg. And since then, this tech actually received a very significant adoption across both enterprises and startups and mid sized companies. You'll see in this graph, the iceberg is represented by the rat chart. It is experiencing a hockey stick growth pattern. Now, what's important for agents when they access analytical data stores? Hopefully, I was able to motivate agents need to access analytical data stores. It's important. It makes ungrounded improved. But what do we need to do to make it work for agents? And I would say there are two important considerations, and I have to really I started a long list of considerations, I ended up with these two most important ones. I think throughput will be one critical consideration. And to understand this, think about you typing in a question or tasking an agent to do some research and find some anomalies in your sales data. So what will this agent do? They will run a bunch of analytical queries, slicing your data by different dimensions. Now, a human analyst. If you gave this task to a human analyst, they would actually stop after a while, because they need to go home and have dinner, they don't have the mental capacity to run 1000 quiz. Agents have unlimited time and hopefully resources. They can continue slicing, dicing the data, looking for these values. Therefore they will execute 100 times more queries. And also an important reason agents don't have yet that knowledge, that institutional knowledge, of interesting insights in your enterprise data, as human analysts do, so they don't really know how to stop and when to stop until they find an interesting anomaly. Because of these factors, I think throughput is a huge consideration for analytical workloads when they're used by agents, and we need to find the technology that enables high skill building the other important consideration is data security. Because if you also think about this, if you were a human user of an adjusting system, you gave the agent the task, you don't want the agent to be using AI information if they didn't necessarily need to accomplish this task. You don't want them to access data sets that are not necessary for accomplishing the goal. So things like column masking and raw filters will become important in a genetic world when they start using analytical data source. How does iceberg measure up for these requirements? Read throughput is amazing in iceberg. It relies on the huge scalability of underlying storage systems such as speed and Azure Blob and GCP or GCS. The right throughput is not great. The community is working on it, but your agents will probably also not be writing transactional data directly into your analytical data store. That's a not very important. The right throughput in terms of data security, it's also work in progress. There are some catalogs that allow you to manage better your column masking policies and your raw filter policies. I will call out maybe a few catalogs here, leak keeper and unity catalog, and are some emerging standards in the world of open, fine grained access controls, open FGA and role based security, RBAC.
have 10 annual customers, large banks, we have 1000 petabytes of data, some some of it's still sitting on tapes. And the efficiency of data is measured in an hour to the analytical case, or maybe a couple of milliseconds in the operational database case. So it's very recent data. And the compression, this is pretty much no complete a compression, per se, you but what you get in into your database, you can get out by querying it. So there's compassion, but it's not lossy, that's what I meant to say. And because of that, query results are deterministic and they're functional. So it's kind of makes sense to be able to try to use both our lamps with your operational analytical data source. So some of us have been asking the question, can't be can we store everything in one place? Wouldn't be much more convenient to store everything in one place? Well, disregarding the fact that LMS at the inference time require an entire different set of infrastructure, mostly in memory versus operational analytical stores. Have you ever tried to replace a database, a popular database, anywhere you're looking right now at a chart from DB engines rankings that's inside where database needs to go to brag about popularity of their favorite databases. And so what you notice there is that the top three database technologies for the last 10 years have been Oracle, MySQL and SQL Server. This is something that our parents were using, and they made, sorry, I used to work for Microsoft. It's okay. So, and this, this top three rankings remain pretty much intact over the last 10 years, and the hot new technology hot in quotation marks, Postgres, that just recently received a bunch of investments from both Databricks and snowflake, it took them almost 10 years just to get to position four and the newest kids on the block, the snowflakes and databrickses, they've been growing faster, but this is logarithmic scale, so actually, the popularity between snowflake and an Oracle is still 10x it will take you ages to even consider the idea of replacing a database with something else. So I think this means we have to deal with the fact that llms As a database will have to coexist with analytical and operational databases, and we need to figure out a way to make it work all together for agents. So let's look at what's going on in the analytical world, because this is important. There are trends in the analytical data space that will influence how agents will get access to that data. And the big trend in the analytical world is Apache iceberg, maybe another race of endless quote about Apache iceberg as a Open Table format, some of you, everyone. So iceberg is a standard. It defines a way to store relational tables. And it's actually a pretty simple standard that says, There shall be a tree of files, and there are three types of files, and these files kind of cross link to each other. There's a root of this hierarchy. Every table has a root file, and then it points to a bunch of metadata files until it reaches the data layer. Now Apache iceberg is not a database engine that you can buy. You cannot buy iceberg. You can buy a technology that supports the standard but it describes or specifies a very important concept of a metadata catalog, and this will be your single point of failure in Apache, iceberg oriented storage. This is the critical piece that binds everything together, because it serves only one purpose, which is pointing to the root of a meta metadata file, file that initiates the speed for each table so its responsibilities are small, but it's super critical. Now, why are customers, small and large, migrating to iceberg? And to understand this, let's look at what was what was happening. Let's say five years ago. Five years ago, customers would use a combination of different database technologies. They would use Databricks, a snowflake and a fabric and some other database engines. And these database engines were, of course, all cloud based, all distributed processing, all separation of data and compute, shuffle, everywhere, awesome stuff, but they were storing tables in their proprietary table format. Now underneath, they might have used open file formats such as Parque and Avro, but the this middle layer that defined which data files corresponded to which tables, they were all proprietary. So I used to work with snowflake, and part of my job was to create storage optimization technologies to make search really fast on that metadata layer. It was very sophisticated, very smartly written, but it was proprietary. And if you wanted to move data between silos, and let's submit, we used to use these database engines for different tasks. We use snowflake for analytical processing, and we use Databricks for ML Engine and data engineering, and we used fabric if you were a Microsoft shop. So to move data around, we had to employ ETL processes, which were super expensive. There are some estimates that 40% of data processing costs in companies was actually spent on ETL. Well, customers are not entirely naive and down so they be built. And one of these large customers, Netflix, open sourced a technology that later became known as a Bucha iceberg. And since then, this tech actually received a very significant adoption across both enterprises and startups and mid sized companies. You'll see in this graph, the iceberg is represented by the rat chart. It is experiencing a hockey stick growth pattern. Now, what's important for agents when they access analytical data stores? Hopefully, I was able to motivate agents need to access analytical data stores. It's important. It makes ungrounded improved. But what do we need to do to make it work for agents? And I would say there are two important considerations, and I have to really I started a long list of considerations, I ended up with these two most important ones. I think throughput will be one critical consideration. And to understand this, think about you typing in a question or tasking an agent to do some research and find some anomalies in your sales data. So what will this agent do? They will run a bunch of analytical queries, slicing your data by different dimensions. Now, a human analyst. If you gave this task to a human analyst, they would actually stop after a while, because they need to go home and have dinner, they don't have the mental capacity to run 1000 quiz. Agents have unlimited time and hopefully resources. They can continue slicing, dicing the data, looking for these values. Therefore they will execute 100 times more queries. And also an important reason agents don't have yet that knowledge, that institutional knowledge, of interesting insights in your enterprise data, as human analysts do, so they don't really know how to stop and when to stop until they find an interesting anomaly. Because of these factors, I think throughput is a huge consideration for analytical workloads when they're used by agents, and we need to find the technology that enables high skill building the other important consideration is data security. Because if you also think about this, if you were a human user of an adjusting system, you gave the agent the task, you don't want the agent to be using AI information if they didn't necessarily need to accomplish this task. You don't want them to access data sets that are not necessary for accomplishing the goal. So things like column masking and raw filters will become important in a genetic world when they start using analytical data source. How does iceberg measure up for these requirements? Read throughput is amazing in iceberg. It relies on the huge scalability of underlying storage systems such as speed and Azure Blob and GCP or GCS. The right throughput is not great. The community is working on it, but your agents will probably also not be writing transactional data directly into your analytical data store. That's a not very important. The right throughput in terms of data security, it's also work in progress. There are some catalogs that allow you to manage better your column masking policies and your raw filter policies. I will call out maybe a few catalogs here, leak keeper and unity catalog, and are some emerging standards in the world of open, fine grained access controls, open FGA and role based security, RBAC.
have 10 annual customers, large banks, we have 1000 petabytes of data, some some of it's still sitting on tapes. And the efficiency of data is measured in an hour to the analytical case, or maybe a couple of milliseconds in the operational database case. So it's very recent data. And the compression, this is pretty much no complete a compression, per se, you but what you get in into your database, you can get out by querying it. So there's compassion, but it's not lossy, that's what I meant to say. And because of that, query results are deterministic and they're functional. So it's kind of makes sense to be able to try to use both our lamps with your operational analytical data source. So some of us have been asking the question, can't be can we store everything in one place? Wouldn't be much more convenient to store everything in one place? Well, disregarding the fact that LMS at the inference time require an entire different set of infrastructure, mostly in memory versus operational analytical stores. Have you ever tried to replace a database, a popular database, anywhere you're looking right now at a chart from DB engines rankings that's inside where database needs to go to brag about popularity of their favorite databases. And so what you notice there is that the top three database technologies for the last 10 years have been Oracle, MySQL and SQL Server. This is something that our parents were using, and they made, sorry, I used to work for Microsoft. It's okay. So, and this, this top three rankings remain pretty much intact over the last 10 years, and the hot new technology hot in quotation marks, Postgres, that just recently received a bunch of investments from both Databricks and snowflake, it took them almost 10 years just to get to position four and the newest kids on the block, the snowflakes and databrickses, they've been growing faster, but this is logarithmic scale, so actually, the popularity between snowflake and an Oracle is still 10x it will take you ages to even consider the idea of replacing a database with something else. So I think this means we have to deal with the fact that llms As a database will have to coexist with analytical and operational databases, and we need to figure out a way to make it work all together for agents. So let's look at what's going on in the analytical world, because this is important. There are trends in the analytical data space that will influence how agents will get access to that data. And the big trend in the analytical world is Apache iceberg, maybe another race of endless quote about Apache iceberg as a Open Table format, some of you, everyone. So iceberg is a standard. It defines a way to store relational tables. And it's actually a pretty simple standard that says, There shall be a tree of files, and there are three types of files, and these files kind of cross link to each other. There's a root of this hierarchy. Every table has a root file, and then it points to a bunch of metadata files until it reaches the data layer. Now Apache iceberg is not a database engine that you can buy. You cannot buy iceberg. You can buy a technology that supports the standard but it describes or specifies a very important concept of a metadata catalog, and this will be your single point of failure in Apache, iceberg oriented storage. This is the critical piece that binds everything together, because it serves only one purpose, which is pointing to the root of a meta metadata file, file that initiates the speed for each table so its responsibilities are small, but it's super critical. Now, why are customers, small and large, migrating to iceberg? And to understand this, let's look at what was what was happening. Let's say five years ago. Five years ago, customers would use a combination of different database technologies. They would use Databricks, a snowflake and a fabric and some other database engines. And these database engines were, of course, all cloud based, all distributed processing, all separation of data and compute, shuffle, everywhere, awesome stuff, but they were storing tables in their proprietary table format. Now underneath, they might have used open file formats such as Parque and Avro, but the this middle layer that defined which data files corresponded to which tables, they were all proprietary. So I used to work with snowflake, and part of my job was to create storage optimization technologies to make search really fast on that metadata layer. It was very sophisticated, very smartly written, but it was proprietary. And if you wanted to move data between silos, and let's submit, we used to use these database engines for different tasks. We use snowflake for analytical processing, and we use Databricks for ML Engine and data engineering, and we used fabric if you were a Microsoft shop. So to move data around, we had to employ ETL processes, which were super expensive. There are some estimates that 40% of data processing costs in companies was actually spent on ETL. Well, customers are not entirely naive and down so they be built. And one of these large customers, Netflix, open sourced a technology that later became known as a Bucha iceberg. And since then, this tech actually received a very significant adoption across both enterprises and startups and mid sized companies. You'll see in this graph, the iceberg is represented by the rat chart. It is experiencing a hockey stick growth pattern. Now, what's important for agents when they access analytical data stores? Hopefully, I was able to motivate agents need to access analytical data stores. It's important. It makes ungrounded improved. But what do we need to do to make it work for agents? And I would say there are two important considerations, and I have to really I started a long list of considerations, I ended up with these two most important ones. I think throughput will be one critical consideration. And to understand this, think about you typing in a question or tasking an agent to do some research and find some anomalies in your sales data. So what will this agent do? They will run a bunch of analytical queries, slicing your data by different dimensions. Now, a human analyst. If you gave this task to a human analyst, they would actually stop after a while, because they need to go home and have dinner, they don't have the mental capacity to run 1000 quiz. Agents have unlimited time and hopefully resources. They can continue slicing, dicing the data, looking for these values. Therefore they will execute 100 times more queries. And also an important reason agents don't have yet that knowledge, that institutional knowledge, of interesting insights in your enterprise data, as human analysts do, so they don't really know how to stop and when to stop until they find an interesting anomaly. Because of these factors, I think throughput is a huge consideration for analytical workloads when they're used by agents, and we need to find the technology that enables high skill building the other important consideration is data security. Because if you also think about this, if you were a human user of an adjusting system, you gave the agent the task, you don't want the agent to be using AI information if they didn't necessarily need to accomplish this task. You don't want them to access data sets that are not necessary for accomplishing the goal. So things like column masking and raw filters will become important in a genetic world when they start using analytical data source. How does iceberg measure up for these requirements? Read throughput is amazing in iceberg. It relies on the huge scalability of underlying storage systems such as speed and Azure Blob and GCP or GCS. The right throughput is not great. The community is working on it, but your agents will probably also not be writing transactional data directly into your analytical data store. That's a not very important. The right throughput in terms of data security, it's also work in progress. There are some catalogs that allow you to manage better your column masking policies and your raw filter policies. I will call out maybe a few catalogs here, leak keeper and unity catalog, and are some emerging standards in the world of open, fine grained access controls, open FGA and role based security, RBAC.
S Speaker 115:01Now let's look at a platform that allows agents to easily access analytical data. I'm working on tower. And tower is a serverless platform for building pipelines, agents and tools used by these agents. We provide a couple of building blocks, tables, models, catalogs. We also have horizontal services. We call them beams, collaborative, team, collaboration, orchestration, observability. And in Tumblr, everything revolves around applications or apps. And you can build three types of apps. You can build a traditional transformation pipeline, or you can build tools and agents that use these tools. We rely on partnerships with open source technology to leverage such as DLT hub and bug DB and Polaris to implement these functionalities. So when you when you want to build a platform or a agent with power, we I would recommend using a great agentic framework. I personally like Lang chain, and when you use Lang chain with building a agent on tower, you typically go through the steps of declaring tools, and the textual descriptions will become super important of these tools, because this is how the LLM will know whether to call your tool or not. You define things like memory. You choose a model that you want to use for in the new agent, you pick a prompt. The prompt is also super important. There are some standard prompts that you can use, such as react. Then you create the agent, and then you collect input from the user, and you invoke the agent on this input. I have a demo that will show you how all of this works together. In this demo, I will actually build a data engineering system agent. It's not something that will interface with users directly. You give this agent the task of maintaining a lake house table so the table is stored in iceberg in a lake house, and its purpose is to store data about stock tickers. You'll see it in the demo. And so this agent has two tools available to it. It has a tool that is able to check if data is already available in the table, and it has a tool that is calling an external API and downloads the data and stores it into the table. So using these two tools, the agent can reason whether they need to retrieve data or if they need to, if they can skip a particular stock ticker. What's critical? And I wanted to show you how all of this works, how agents actually are able to reason is, here's an example of a system prompt. It follows the React architecture. You'll see the placeholder in this system prompt tools and tool names. This is how you pass the descriptions of your tools to the LLM to start the reasoning process. There's a template to which the agent will run the reasoning through. And here are some descriptions of the two tools I mentioned before, the one that checks availability of the data and the one that pulls the data from an external API. Now let's look at the demo of an agent developed with Lang chain and running in Tower. And so we'll start by importing tower and some additional libraries, such as followers and one chain will define the function that the tool will use to this one will actually pull the data and it starts another tower application to performance. Task will define our data gather, our data Downloader. Give it a good description. We'll then define a new function that checks the availability of data, and it uses the towers, iceberg tables API to do this quickly and will again define the purpose of this tool so that we can pass it to our agent. And then we'll reach the stage where we'll create the tools, the LLM, the system prompt, create the agent, pass all of the parameters to it, and then we'll collect a user prompt, the description of the goal of the agent, and we'll initiate the invocation. Now there's a concept in Tower of a tower file. This is a basically the metadata about the application. You have to specify. Tell us five things, what is the name of this description of this app, what are the parameters? And here I will add another stock ticker for the agent to process. So we'll now deal with Amazon, both with Amazon and Microsoft. I will deploy this tool to tower. I will start the run of the agent. And so now they are in the web UI. The agent is running. Let's quickly look at the logs of this particular agent run. After the installation of libraries, we'll start getting the reasoning flow of the agents of the first line entering the chain. So it will call the first tool check the data availability for one ticker, and it will figure out I already have data for Amazon, so let's not pull it again. Now, let's, let's call the Availability Checker for Microsoft. Oh, I actually do need to pull data for Microsoft because I don't have it in my lake house table. So I will get my downloader tool and execute it, and this will run as a separate application in Tower. And at the end, the final answer will arrive, and the agent is happy. It synchronized the data in my Lakehouse table with the task that I gave it. So with this, I wanted to maybe talk about a few additional things you could build on tower. We have a customer, for example, cogni they're building a knowledge graph system that implements AI memory, and they use tower for ETL tasks, onboarding documents into Lakehouse table, as well as for serving search requests. Again, the knowledge graph, let's maybe summarize what we learned today. Hopefully I was able to show you how by doing three things, by complementing our lamps with up to date and factual data in our lamps, and by paying attention to the trans analytical world and finding ways to integrate with iceberg and by using agenting frameworks, we can address the fails chatbot Fails of the past. What do we think will happen in the future? We think the future is bright for agents. We think agents will eventually run everywhere, in public clouds, in your Cloud account on your laptop, and the information blockers that they used used to be true in the past and also being addressed. OSS models are becoming as accurate, or if not, if not better than proprietary models. Model sizes are being addressed you can get now through various techniques such as quantization distillation, you can get model sizes down to a size that runs on your local hardware, and local hardware itself is improving apple and video, releasing desktop based machines that you can basically run a full, full scale deep sea car, one on them, the one remaining adoption blocker. This is something that we are working on, are the differences in GPU stacks. There's still differences locally on premises in the cloud. Folks have tried different techniques to address these differences. They tried to package things into Docker containers. Reasons why it doesn't really work on, let's say, Mac, GPUs and Docker and pytorch, that combination just doesn't work. They try to put consumer hardware in the clouds and build inference servers out of them. Also not super scalable. Power is working on a different solution. I think it's a little bit more scalable. We are working on portable Python runtimes that give you virtual Python environments and actually work speed. Well, if you would like to get in touch with me after this talk, we'd love to chat with you with any questions. We do actually have a couple of minutes for questions if you would like to ask me, and the first talk is doing a live Q and A after the talk. So if you would like to ask me questions, go ahead.
Now let's look at a platform that allows agents to easily access analytical data. I'm working on tower. And tower is a serverless platform for building pipelines, agents and tools used by these agents. We provide a couple of building blocks, tables, models, catalogs. We also have horizontal services. We call them beams, collaborative, team, collaboration, orchestration, observability. And in Tumblr, everything revolves around applications or apps. And you can build three types of apps. You can build a traditional transformation pipeline, or you can build tools and agents that use these tools. We rely on partnerships with open source technology to leverage such as DLT hub and bug DB and Polaris to implement these functionalities. So when you when you want to build a platform or a agent with power, we I would recommend using a great agentic framework. I personally like Lang chain, and when you use Lang chain with building a agent on tower, you typically go through the steps of declaring tools, and the textual descriptions will become super important of these tools, because this is how the LLM will know whether to call your tool or not. You define things like memory. You choose a model that you want to use for in the new agent, you pick a prompt. The prompt is also super important. There are some standard prompts that you can use, such as react. Then you create the agent, and then you collect input from the user, and you invoke the agent on this input. I have a demo that will show you how all of this works together. In this demo, I will actually build a data engineering system agent. It's not something that will interface with users directly. You give this agent the task of maintaining a lake house table so the table is stored in iceberg in a lake house, and its purpose is to store data about stock tickers. You'll see it in the demo. And so this agent has two tools available to it. It has a tool that is able to check if data is already available in the table, and it has a tool that is calling an external API and downloads the data and stores it into the table. So using these two tools, the agent can reason whether they need to retrieve data or if they need to, if they can skip a particular stock ticker. What's critical? And I wanted to show you how all of this works, how agents actually are able to reason is, here's an example of a system prompt. It follows the React architecture. You'll see the placeholder in this system prompt tools and tool names. This is how you pass the descriptions of your tools to the LLM to start the reasoning process. There's a template to which the agent will run the reasoning through. And here are some descriptions of the two tools I mentioned before, the one that checks availability of the data and the one that pulls the data from an external API. Now let's look at the demo of an agent developed with Lang chain and running in Tower. And so we'll start by importing tower and some additional libraries, such as followers and one chain will define the function that the tool will use to this one will actually pull the data and it starts another tower application to performance. Task will define our data gather, our data Downloader. Give it a good description. We'll then define a new function that checks the availability of data, and it uses the towers, iceberg tables API to do this quickly and will again define the purpose of this tool so that we can pass it to our agent. And then we'll reach the stage where we'll create the tools, the LLM, the system prompt, create the agent, pass all of the parameters to it, and then we'll collect a user prompt, the description of the goal of the agent, and we'll initiate the invocation. Now there's a concept in Tower of a tower file. This is a basically the metadata about the application. You have to specify. Tell us five things, what is the name of this description of this app, what are the parameters? And here I will add another stock ticker for the agent to process. So we'll now deal with Amazon, both with Amazon and Microsoft. I will deploy this tool to tower. I will start the run of the agent. And so now they are in the web UI. The agent is running. Let's quickly look at the logs of this particular agent run. After the installation of libraries, we'll start getting the reasoning flow of the agents of the first line entering the chain. So it will call the first tool check the data availability for one ticker, and it will figure out I already have data for Amazon, so let's not pull it again. Now, let's, let's call the Availability Checker for Microsoft. Oh, I actually do need to pull data for Microsoft because I don't have it in my lake house table. So I will get my downloader tool and execute it, and this will run as a separate application in Tower. And at the end, the final answer will arrive, and the agent is happy. It synchronized the data in my Lakehouse table with the task that I gave it. So with this, I wanted to maybe talk about a few additional things you could build on tower. We have a customer, for example, cogni they're building a knowledge graph system that implements AI memory, and they use tower for ETL tasks, onboarding documents into Lakehouse table, as well as for serving search requests. Again, the knowledge graph, let's maybe summarize what we learned today. Hopefully I was able to show you how by doing three things, by complementing our lamps with up to date and factual data in our lamps, and by paying attention to the trans analytical world and finding ways to integrate with iceberg and by using agenting frameworks, we can address the fails chatbot Fails of the past. What do we think will happen in the future? We think the future is bright for agents. We think agents will eventually run everywhere, in public clouds, in your Cloud account on your laptop, and the information blockers that they used used to be true in the past and also being addressed. OSS models are becoming as accurate, or if not, if not better than proprietary models. Model sizes are being addressed you can get now through various techniques such as quantization distillation, you can get model sizes down to a size that runs on your local hardware, and local hardware itself is improving apple and video, releasing desktop based machines that you can basically run a full, full scale deep sea car, one on them, the one remaining adoption blocker. This is something that we are working on, are the differences in GPU stacks. There's still differences locally on premises in the cloud. Folks have tried different techniques to address these differences. They tried to package things into Docker containers. Reasons why it doesn't really work on, let's say, Mac, GPUs and Docker and pytorch, that combination just doesn't work. They try to put consumer hardware in the clouds and build inference servers out of them. Also not super scalable. Power is working on a different solution. I think it's a little bit more scalable. We are working on portable Python runtimes that give you virtual Python environments and actually work speed. Well, if you would like to get in touch with me after this talk, we'd love to chat with you with any questions. We do actually have a couple of minutes for questions if you would like to ask me, and the first talk is doing a live Q and A after the talk. So if you would like to ask me questions, go ahead.
Now let's look at a platform that allows agents to easily access analytical data. I'm working on tower. And tower is a serverless platform for building pipelines, agents and tools used by these agents. We provide a couple of building blocks, tables, models, catalogs. We also have horizontal services. We call them beams, collaborative, team, collaboration, orchestration, observability. And in Tumblr, everything revolves around applications or apps. And you can build three types of apps. You can build a traditional transformation pipeline, or you can build tools and agents that use these tools. We rely on partnerships with open source technology to leverage such as DLT hub and bug DB and Polaris to implement these functionalities. So when you when you want to build a platform or a agent with power, we I would recommend using a great agentic framework. I personally like Lang chain, and when you use Lang chain with building a agent on tower, you typically go through the steps of declaring tools, and the textual descriptions will become super important of these tools, because this is how the LLM will know whether to call your tool or not. You define things like memory. You choose a model that you want to use for in the new agent, you pick a prompt. The prompt is also super important. There are some standard prompts that you can use, such as react. Then you create the agent, and then you collect input from the user, and you invoke the agent on this input. I have a demo that will show you how all of this works together. In this demo, I will actually build a data engineering system agent. It's not something that will interface with users directly. You give this agent the task of maintaining a lake house table so the table is stored in iceberg in a lake house, and its purpose is to store data about stock tickers. You'll see it in the demo. And so this agent has two tools available to it. It has a tool that is able to check if data is already available in the table, and it has a tool that is calling an external API and downloads the data and stores it into the table. So using these two tools, the agent can reason whether they need to retrieve data or if they need to, if they can skip a particular stock ticker. What's critical? And I wanted to show you how all of this works, how agents actually are able to reason is, here's an example of a system prompt. It follows the React architecture. You'll see the placeholder in this system prompt tools and tool names. This is how you pass the descriptions of your tools to the LLM to start the reasoning process. There's a template to which the agent will run the reasoning through. And here are some descriptions of the two tools I mentioned before, the one that checks availability of the data and the one that pulls the data from an external API. Now let's look at the demo of an agent developed with Lang chain and running in Tower. And so we'll start by importing tower and some additional libraries, such as followers and one chain will define the function that the tool will use to this one will actually pull the data and it starts another tower application to performance. Task will define our data gather, our data Downloader. Give it a good description. We'll then define a new function that checks the availability of data, and it uses the towers, iceberg tables API to do this quickly and will again define the purpose of this tool so that we can pass it to our agent. And then we'll reach the stage where we'll create the tools, the LLM, the system prompt, create the agent, pass all of the parameters to it, and then we'll collect a user prompt, the description of the goal of the agent, and we'll initiate the invocation. Now there's a concept in Tower of a tower file. This is a basically the metadata about the application. You have to specify. Tell us five things, what is the name of this description of this app, what are the parameters? And here I will add another stock ticker for the agent to process. So we'll now deal with Amazon, both with Amazon and Microsoft. I will deploy this tool to tower. I will start the run of the agent. And so now they are in the web UI. The agent is running. Let's quickly look at the logs of this particular agent run. After the installation of libraries, we'll start getting the reasoning flow of the agents of the first line entering the chain. So it will call the first tool check the data availability for one ticker, and it will figure out I already have data for Amazon, so let's not pull it again. Now, let's, let's call the Availability Checker for Microsoft. Oh, I actually do need to pull data for Microsoft because I don't have it in my lake house table. So I will get my downloader tool and execute it, and this will run as a separate application in Tower. And at the end, the final answer will arrive, and the agent is happy. It synchronized the data in my Lakehouse table with the task that I gave it. So with this, I wanted to maybe talk about a few additional things you could build on tower. We have a customer, for example, cogni they're building a knowledge graph system that implements AI memory, and they use tower for ETL tasks, onboarding documents into Lakehouse table, as well as for serving search requests. Again, the knowledge graph, let's maybe summarize what we learned today. Hopefully I was able to show you how by doing three things, by complementing our lamps with up to date and factual data in our lamps, and by paying attention to the trans analytical world and finding ways to integrate with iceberg and by using agenting frameworks, we can address the fails chatbot Fails of the past. What do we think will happen in the future? We think the future is bright for agents. We think agents will eventually run everywhere, in public clouds, in your Cloud account on your laptop, and the information blockers that they used used to be true in the past and also being addressed. OSS models are becoming as accurate, or if not, if not better than proprietary models. Model sizes are being addressed you can get now through various techniques such as quantization distillation, you can get model sizes down to a size that runs on your local hardware, and local hardware itself is improving apple and video, releasing desktop based machines that you can basically run a full, full scale deep sea car, one on them, the one remaining adoption blocker. This is something that we are working on, are the differences in GPU stacks. There's still differences locally on premises in the cloud. Folks have tried different techniques to address these differences. They tried to package things into Docker containers. Reasons why it doesn't really work on, let's say, Mac, GPUs and Docker and pytorch, that combination just doesn't work. They try to put consumer hardware in the clouds and build inference servers out of them. Also not super scalable. Power is working on a different solution. I think it's a little bit more scalable. We are working on portable Python runtimes that give you virtual Python environments and actually work speed. Well, if you would like to get in touch with me after this talk, we'd love to chat with you with any questions. We do actually have a couple of minutes for questions if you would like to ask me, and the first talk is doing a live Q and A after the talk. So if you would like to ask me questions, go ahead.
Now let's look at a platform that allows agents to easily access analytical data. I'm working on tower. And tower is a serverless platform for building pipelines, agents and tools used by these agents. We provide a couple of building blocks, tables, models, catalogs. We also have horizontal services. We call them beams, collaborative, team, collaboration, orchestration, observability. And in Tumblr, everything revolves around applications or apps. And you can build three types of apps. You can build a traditional transformation pipeline, or you can build tools and agents that use these tools. We rely on partnerships with open source technology to leverage such as DLT hub and bug DB and Polaris to implement these functionalities. So when you when you want to build a platform or a agent with power, we I would recommend using a great agentic framework. I personally like Lang chain, and when you use Lang chain with building a agent on tower, you typically go through the steps of declaring tools, and the textual descriptions will become super important of these tools, because this is how the LLM will know whether to call your tool or not. You define things like memory. You choose a model that you want to use for in the new agent, you pick a prompt. The prompt is also super important. There are some standard prompts that you can use, such as react. Then you create the agent, and then you collect input from the user, and you invoke the agent on this input. I have a demo that will show you how all of this works together. In this demo, I will actually build a data engineering system agent. It's not something that will interface with users directly. You give this agent the task of maintaining a lake house table so the table is stored in iceberg in a lake house, and its purpose is to store data about stock tickers. You'll see it in the demo. And so this agent has two tools available to it. It has a tool that is able to check if data is already available in the table, and it has a tool that is calling an external API and downloads the data and stores it into the table. So using these two tools, the agent can reason whether they need to retrieve data or if they need to, if they can skip a particular stock ticker. What's critical? And I wanted to show you how all of this works, how agents actually are able to reason is, here's an example of a system prompt. It follows the React architecture. You'll see the placeholder in this system prompt tools and tool names. This is how you pass the descriptions of your tools to the LLM to start the reasoning process. There's a template to which the agent will run the reasoning through. And here are some descriptions of the two tools I mentioned before, the one that checks availability of the data and the one that pulls the data from an external API. Now let's look at the demo of an agent developed with Lang chain and running in Tower. And so we'll start by importing tower and some additional libraries, such as followers and one chain will define the function that the tool will use to this one will actually pull the data and it starts another tower application to performance. Task will define our data gather, our data Downloader. Give it a good description. We'll then define a new function that checks the availability of data, and it uses the towers, iceberg tables API to do this quickly and will again define the purpose of this tool so that we can pass it to our agent. And then we'll reach the stage where we'll create the tools, the LLM, the system prompt, create the agent, pass all of the parameters to it, and then we'll collect a user prompt, the description of the goal of the agent, and we'll initiate the invocation. Now there's a concept in Tower of a tower file. This is a basically the metadata about the application. You have to specify. Tell us five things, what is the name of this description of this app, what are the parameters? And here I will add another stock ticker for the agent to process. So we'll now deal with Amazon, both with Amazon and Microsoft. I will deploy this tool to tower. I will start the run of the agent. And so now they are in the web UI. The agent is running. Let's quickly look at the logs of this particular agent run. After the installation of libraries, we'll start getting the reasoning flow of the agents of the first line entering the chain. So it will call the first tool check the data availability for one ticker, and it will figure out I already have data for Amazon, so let's not pull it again. Now, let's, let's call the Availability Checker for Microsoft. Oh, I actually do need to pull data for Microsoft because I don't have it in my lake house table. So I will get my downloader tool and execute it, and this will run as a separate application in Tower. And at the end, the final answer will arrive, and the agent is happy. It synchronized the data in my Lakehouse table with the task that I gave it. So with this, I wanted to maybe talk about a few additional things you could build on tower. We have a customer, for example, cogni they're building a knowledge graph system that implements AI memory, and they use tower for ETL tasks, onboarding documents into Lakehouse table, as well as for serving search requests. Again, the knowledge graph, let's maybe summarize what we learned today. Hopefully I was able to show you how by doing three things, by complementing our lamps with up to date and factual data in our lamps, and by paying attention to the trans analytical world and finding ways to integrate with iceberg and by using agenting frameworks, we can address the fails chatbot Fails of the past. What do we think will happen in the future? We think the future is bright for agents. We think agents will eventually run everywhere, in public clouds, in your Cloud account on your laptop, and the information blockers that they used used to be true in the past and also being addressed. OSS models are becoming as accurate, or if not, if not better than proprietary models. Model sizes are being addressed you can get now through various techniques such as quantization distillation, you can get model sizes down to a size that runs on your local hardware, and local hardware itself is improving apple and video, releasing desktop based machines that you can basically run a full, full scale deep sea car, one on them, the one remaining adoption blocker. This is something that we are working on, are the differences in GPU stacks. There's still differences locally on premises in the cloud. Folks have tried different techniques to address these differences. They tried to package things into Docker containers. Reasons why it doesn't really work on, let's say, Mac, GPUs and Docker and pytorch, that combination just doesn't work. They try to put consumer hardware in the clouds and build inference servers out of them. Also not super scalable. Power is working on a different solution. I think it's a little bit more scalable. We are working on portable Python runtimes that give you virtual Python environments and actually work speed. Well, if you would like to get in touch with me after this talk, we'd love to chat with you with any questions. We do actually have a couple of minutes for questions if you would like to ask me, and the first talk is doing a live Q and A after the talk. So if you would like to ask me questions, go ahead.
S Speaker 223:13Hi there. Survey. Thank you all for submitting questions through the QR code. Unfortunately, we only have time for one question because of time constraints today, and somebody wanted to ask this question, can agents learn from previous queries and optimize their reasoning across time?
Hi there. Survey. Thank you all for submitting questions through the QR code. Unfortunately, we only have time for one question because of time constraints today, and somebody wanted to ask this question, can agents learn from previous queries and optimize their reasoning across time?
Hi there. Survey. Thank you all for submitting questions through the QR code. Unfortunately, we only have time for one question because of time constraints today, and somebody wanted to ask this question, can agents learn from previous queries and optimize their reasoning across time?
Hi there. Survey. Thank you all for submitting questions through the QR code. Unfortunately, we only have time for one question because of time constraints today, and somebody wanted to ask this question, can agents learn from previous queries and optimize their reasoning across time?
S Speaker 123:32They definitely can remember the one of the first slides I had where there was a memory system where you store both the prompt and the output. This is how agents can learn from previous interactions. If you want to learn more about this, happy to answer after the after the talk. All
They definitely can remember the one of the first slides I had where there was a memory system where you store both the prompt and the output. This is how agents can learn from previous interactions. If you want to learn more about this, happy to answer after the after the talk. All
They definitely can remember the one of the first slides I had where there was a memory system where you store both the prompt and the output. This is how agents can learn from previous interactions. If you want to learn more about this, happy to answer after the after the talk. All
They definitely can remember the one of the first slides I had where there was a memory system where you store both the prompt and the output. This is how agents can learn from previous interactions. If you want to learn more about this, happy to answer after the after the talk. All
S Speaker 323:55please welcome to the stage Conlin and bude Ramesh savaka ankar from meta the Ta
please welcome to the stage Conlin and bude Ramesh savaka ankar from meta the Ta
please welcome to the stage Conlin and bude Ramesh savaka ankar from meta the Ta
please welcome to the stage Conlin and bude Ramesh savaka ankar from meta the Ta
S Speaker 424:12operates in large scale data warehouse, in the era of AI agents, how do we evolve.
operates in large scale data warehouse, in the era of AI agents, how do we evolve.
operates in large scale data warehouse, in the era of AI agents, how do we evolve.
operates in large scale data warehouse, in the era of AI agents, how do we evolve.
S Speaker 10:00This age, it didn't work out really well to the inhabitants of Earth, but it's going to be much better. Who of you raise your hand? Remember this case from a couple of years ago when two New York lawyers submitted a chatgpt generated court brief? I see a few of you raising the hand. What happened then? Well, our favorite chat bot generated an entire legal brief. It actually created case citations of non existing legal cases. The court accepted this legal brief. The opposing counsel, of course, did the reference checks and discovered it was entirely fabricated. The lawyers were fined $5,000 the plaintiff lost the case. We ended up in a mess. Why did we end up in a mess? Well, this is a very simple diagram. You probably know how chatbot bots work, or used to work two years ago. They used a large, large language model that was trained on a corpus of training data, and it took us months and months to train this model, so by the time the model was created, it was already dated. And secondly, the chatbots didn't really have the ability to check anything. They would accept your user input, and they would use a probabilistic technique to generate the output, and that was the answer to the user. Interestingly, the lawyers in our case did ask chatgpt, are you sure these cases were ill and the Chatbot didn't do what a human would do, which is going to a legal database like LexisNexis and actually type in the case numbers and check if they existed. Chatbot just applied, yes, they exist, and the lawyers relied on this information. Now, since then, the industry ally realized we need to complement the knowledge in our lamps with factual and recent info. So the new technique and this little bit of a refresher of what you all have heard in the first set of sessions here today, we still have our llms, and they receive user input, the questions that people want to ask, but instead of producing output immediately, they start a reasoning loop. And this reasoning loop begins calling out various tools. Some of them call external APIs, such as LexisNexis. Some of them will call enterprise analytical and operational databases, and the loop will continue until the LLM decides this is the final answer. I have enough information to create a final answer, at which point the output will be generated and stored together with a prompt in a thing called memory. So this is generally how these agents now work using tools. Who am I to be speaking to you so passionately about this topic. I've built data and AI systems previously at Databricks, snowflake and Google Cloud. I now run a company called tower dot Dev, and we build Python platforms, portable Python platforms to build, pipelines, agents and tools. So in this talk, we already motivated why it was a problem not having a reference database to check. Let's talk about solutions. So let's let's talk about enabling agents with enterprise data that they can cross check. We'll talk about trends in the world of analytics. There's a big trend, and it's also the main sake of the stock Apache. Iceberg is this big new trend for analytical storage. We'll look at an architecture of a agentic platform that provides easy access to analytical data to agents, and we'll look into the future as well. So how do we complement llms With recent and factual info? Tim scarle, the host of ml state talk, has recently said something that really resonated with me, and effectively, he said, llms are databases. Now I'm a database gig, and if you show me a break, I will call it a database. I'll find a way to explain why it's a database, but it's actually making sense. Think about the training process of a of a LLM, you'll get a corpus of data, and let's say llama llamas force. Case, it was 30 trillion words, which is approximately 150 terabytes of data. We've got enough metaphors here to confirm or disprove whether it was 50 terabytes or 200 but it took us nine months to train this model, and so by and by the end of this training process, we ended up with a set of files that we are less than a terabyte in size. So it was a 200x compression of our training set into this binary output. And everyone who knows information theory, or at least have read a little bit about information theory, a 200x compression of data immediately leads it's lossy. You cannot recover the information that went into this process from one to one. We are still okay accepting our lens as databases because they believe in they're smart and they can create creative responses. So we're okay with this lossy compression. Now, on the other hand, if you look at the world of analytical data lakes or operational databases, we are dealing with a much larger set of data. Some customersS Speaker 15:36have 10 annual customers, large banks, we have 1000 petabytes of data, some some of it's still sitting on tapes. And the efficiency of data is measured in an hour to the analytical case, or maybe a couple of milliseconds in the operational database case. So it's very recent data. And the compression, this is pretty much no complete a compression, per se, you but what you get in into your database, you can get out by querying it. So there's compassion, but it's not lossy, that's what I meant to say. And because of that, query results are deterministic and they're functional. So it's kind of makes sense to be able to try to use both our lamps with your operational analytical data source. So some of us have been asking the question, can't be can we store everything in one place? Wouldn't be much more convenient to store everything in one place? Well, disregarding the fact that LMS at the inference time require an entire different set of infrastructure, mostly in memory versus operational analytical stores. Have you ever tried to replace a database, a popular database, anywhere you're looking right now at a chart from DB engines rankings that's inside where database needs to go to brag about popularity of their favorite databases. And so what you notice there is that the top three database technologies for the last 10 years have been Oracle, MySQL and SQL Server. This is something that our parents were using, and they made, sorry, I used to work for Microsoft. It's okay. So, and this, this top three rankings remain pretty much intact over the last 10 years, and the hot new technology hot in quotation marks, Postgres, that just recently received a bunch of investments from both Databricks and snowflake, it took them almost 10 years just to get to position four and the newest kids on the block, the snowflakes and databrickses, they've been growing faster, but this is logarithmic scale, so actually, the popularity between snowflake and an Oracle is still 10x it will take you ages to even consider the idea of replacing a database with something else. So I think this means we have to deal with the fact that llms As a database will have to coexist with analytical and operational databases, and we need to figure out a way to make it work all together for agents. So let's look at what's going on in the analytical world, because this is important. There are trends in the analytical data space that will influence how agents will get access to that data. And the big trend in the analytical world is Apache iceberg, maybe another race of endless quote about Apache iceberg as a Open Table format, some of you, everyone. So iceberg is a standard. It defines a way to store relational tables. And it's actually a pretty simple standard that says, There shall be a tree of files, and there are three types of files, and these files kind of cross link to each other. There's a root of this hierarchy. Every table has a root file, and then it points to a bunch of metadata files until it reaches the data layer. Now Apache iceberg is not a database engine that you can buy. You cannot buy iceberg. You can buy a technology that supports the standard but it describes or specifies a very important concept of a metadata catalog, and this will be your single point of failure in Apache, iceberg oriented storage. This is the critical piece that binds everything together, because it serves only one purpose, which is pointing to the root of a meta metadata file, file that initiates the speed for each table so its responsibilities are small, but it's super critical. Now, why are customers, small and large, migrating to iceberg? And to understand this, let's look at what was what was happening. Let's say five years ago. Five years ago, customers would use a combination of different database technologies. They would use Databricks, a snowflake and a fabric and some other database engines. And these database engines were, of course, all cloud based, all distributed processing, all separation of data and compute, shuffle, everywhere, awesome stuff, but they were storing tables in their proprietary table format. Now underneath, they might have used open file formats such as Parque and Avro, but the this middle layer that defined which data files corresponded to which tables, they were all proprietary. So I used to work with snowflake, and part of my job was to create storage optimization technologies to make search really fast on that metadata layer. It was very sophisticated, very smartly written, but it was proprietary. And if you wanted to move data between silos, and let's submit, we used to use these database engines for different tasks. We use snowflake for analytical processing, and we use Databricks for ML Engine and data engineering, and we used fabric if you were a Microsoft shop. So to move data around, we had to employ ETL processes, which were super expensive. There are some estimates that 40% of data processing costs in companies was actually spent on ETL. Well, customers are not entirely naive and down so they be built. And one of these large customers, Netflix, open sourced a technology that later became known as a Bucha iceberg. And since then, this tech actually received a very significant adoption across both enterprises and startups and mid sized companies. You'll see in this graph, the iceberg is represented by the rat chart. It is experiencing a hockey stick growth pattern. Now, what's important for agents when they access analytical data stores? Hopefully, I was able to motivate agents need to access analytical data stores. It's important. It makes ungrounded improved. But what do we need to do to make it work for agents? And I would say there are two important considerations, and I have to really I started a long list of considerations, I ended up with these two most important ones. I think throughput will be one critical consideration. And to understand this, think about you typing in a question or tasking an agent to do some research and find some anomalies in your sales data. So what will this agent do? They will run a bunch of analytical queries, slicing your data by different dimensions. Now, a human analyst. If you gave this task to a human analyst, they would actually stop after a while, because they need to go home and have dinner, they don't have the mental capacity to run 1000 quiz. Agents have unlimited time and hopefully resources. They can continue slicing, dicing the data, looking for these values. Therefore they will execute 100 times more queries. And also an important reason agents don't have yet that knowledge, that institutional knowledge, of interesting insights in your enterprise data, as human analysts do, so they don't really know how to stop and when to stop until they find an interesting anomaly. Because of these factors, I think throughput is a huge consideration for analytical workloads when they're used by agents, and we need to find the technology that enables high skill building the other important consideration is data security. Because if you also think about this, if you were a human user of an adjusting system, you gave the agent the task, you don't want the agent to be using AI information if they didn't necessarily need to accomplish this task. You don't want them to access data sets that are not necessary for accomplishing the goal. So things like column masking and raw filters will become important in a genetic world when they start using analytical data source. How does iceberg measure up for these requirements? Read throughput is amazing in iceberg. It relies on the huge scalability of underlying storage systems such as speed and Azure Blob and GCP or GCS. The right throughput is not great. The community is working on it, but your agents will probably also not be writing transactional data directly into your analytical data store. That's a not very important. The right throughput in terms of data security, it's also work in progress. There are some catalogs that allow you to manage better your column masking policies and your raw filter policies. I will call out maybe a few catalogs here, leak keeper and unity catalog, and are some emerging standards in the world of open, fine grained access controls, open FGA and role based security, RBAC.S Speaker 115:01Now let's look at a platform that allows agents to easily access analytical data. I'm working on tower. And tower is a serverless platform for building pipelines, agents and tools used by these agents. We provide a couple of building blocks, tables, models, catalogs. We also have horizontal services. We call them beams, collaborative, team, collaboration, orchestration, observability. And in Tumblr, everything revolves around applications or apps. And you can build three types of apps. You can build a traditional transformation pipeline, or you can build tools and agents that use these tools. We rely on partnerships with open source technology to leverage such as DLT hub and bug DB and Polaris to implement these functionalities. So when you when you want to build a platform or a agent with power, we I would recommend using a great agentic framework. I personally like Lang chain, and when you use Lang chain with building a agent on tower, you typically go through the steps of declaring tools, and the textual descriptions will become super important of these tools, because this is how the LLM will know whether to call your tool or not. You define things like memory. You choose a model that you want to use for in the new agent, you pick a prompt. The prompt is also super important. There are some standard prompts that you can use, such as react. Then you create the agent, and then you collect input from the user, and you invoke the agent on this input. I have a demo that will show you how all of this works together. In this demo, I will actually build a data engineering system agent. It's not something that will interface with users directly. You give this agent the task of maintaining a lake house table so the table is stored in iceberg in a lake house, and its purpose is to store data about stock tickers. You'll see it in the demo. And so this agent has two tools available to it. It has a tool that is able to check if data is already available in the table, and it has a tool that is calling an external API and downloads the data and stores it into the table. So using these two tools, the agent can reason whether they need to retrieve data or if they need to, if they can skip a particular stock ticker. What's critical? And I wanted to show you how all of this works, how agents actually are able to reason is, here's an example of a system prompt. It follows the React architecture. You'll see the placeholder in this system prompt tools and tool names. This is how you pass the descriptions of your tools to the LLM to start the reasoning process. There's a template to which the agent will run the reasoning through. And here are some descriptions of the two tools I mentioned before, the one that checks availability of the data and the one that pulls the data from an external API. Now let's look at the demo of an agent developed with Lang chain and running in Tower. And so we'll start by importing tower and some additional libraries, such as followers and one chain will define the function that the tool will use to this one will actually pull the data and it starts another tower application to performance. Task will define our data gather, our data Downloader. Give it a good description. We'll then define a new function that checks the availability of data, and it uses the towers, iceberg tables API to do this quickly and will again define the purpose of this tool so that we can pass it to our agent. And then we'll reach the stage where we'll create the tools, the LLM, the system prompt, create the agent, pass all of the parameters to it, and then we'll collect a user prompt, the description of the goal of the agent, and we'll initiate the invocation. Now there's a concept in Tower of a tower file. This is a basically the metadata about the application. You have to specify. Tell us five things, what is the name of this description of this app, what are the parameters? And here I will add another stock ticker for the agent to process. So we'll now deal with Amazon, both with Amazon and Microsoft. I will deploy this tool to tower. I will start the run of the agent. And so now they are in the web UI. The agent is running. Let's quickly look at the logs of this particular agent run. After the installation of libraries, we'll start getting the reasoning flow of the agents of the first line entering the chain. So it will call the first tool check the data availability for one ticker, and it will figure out I already have data for Amazon, so let's not pull it again. Now, let's, let's call the Availability Checker for Microsoft. Oh, I actually do need to pull data for Microsoft because I don't have it in my lake house table. So I will get my downloader tool and execute it, and this will run as a separate application in Tower. And at the end, the final answer will arrive, and the agent is happy. It synchronized the data in my Lakehouse table with the task that I gave it. So with this, I wanted to maybe talk about a few additional things you could build on tower. We have a customer, for example, cogni they're building a knowledge graph system that implements AI memory, and they use tower for ETL tasks, onboarding documents into Lakehouse table, as well as for serving search requests. Again, the knowledge graph, let's maybe summarize what we learned today. Hopefully I was able to show you how by doing three things, by complementing our lamps with up to date and factual data in our lamps, and by paying attention to the trans analytical world and finding ways to integrate with iceberg and by using agenting frameworks, we can address the fails chatbot Fails of the past. What do we think will happen in the future? We think the future is bright for agents. We think agents will eventually run everywhere, in public clouds, in your Cloud account on your laptop, and the information blockers that they used used to be true in the past and also being addressed. OSS models are becoming as accurate, or if not, if not better than proprietary models. Model sizes are being addressed you can get now through various techniques such as quantization distillation, you can get model sizes down to a size that runs on your local hardware, and local hardware itself is improving apple and video, releasing desktop based machines that you can basically run a full, full scale deep sea car, one on them, the one remaining adoption blocker. This is something that we are working on, are the differences in GPU stacks. There's still differences locally on premises in the cloud. Folks have tried different techniques to address these differences. They tried to package things into Docker containers. Reasons why it doesn't really work on, let's say, Mac, GPUs and Docker and pytorch, that combination just doesn't work. They try to put consumer hardware in the clouds and build inference servers out of them. Also not super scalable. Power is working on a different solution. I think it's a little bit more scalable. We are working on portable Python runtimes that give you virtual Python environments and actually work speed. Well, if you would like to get in touch with me after this talk, we'd love to chat with you with any questions. We do actually have a couple of minutes for questions if you would like to ask me, and the first talk is doing a live Q and A after the talk. So if you would like to ask me questions, go ahead.S Speaker 223:13Hi there. Survey. Thank you all for submitting questions through the QR code. Unfortunately, we only have time for one question because of time constraints today, and somebody wanted to ask this question, can agents learn from previous queries and optimize their reasoning across time?S Speaker 123:32They definitely can remember the one of the first slides I had where there was a memory system where you store both the prompt and the output. This is how agents can learn from previous interactions. If you want to learn more about this, happy to answer after the after the talk. All23:49right, I think,S Speaker 323:55please welcome to the stage Conlin and bude Ramesh savaka ankar from meta the TaS Speaker 424:12operates in large scale data warehouse, in the era of AI agents, how do we evolve.How accurate was this transcription?