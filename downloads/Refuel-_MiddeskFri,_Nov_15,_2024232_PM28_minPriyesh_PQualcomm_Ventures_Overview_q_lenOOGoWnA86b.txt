Meeting: Refuel- Middesk
Fri, Nov 15, 2024
2:32 PM
28 min
Priyesh P
Qualcomm Ventures Overview and Investment
URL: https://otter.ai/u/q_lenOOGoWnA86b5OETKAeIm18s
Downloaded: 2025-12-22T13:41:52.922431
Method: text_extraction
============================================================

S Speaker 10:00Hi Priyesh, hi again. Good to meet you. Well, no, thank you to for doing this. So
Hi Priyesh, hi again. Good to meet you. Well, no, thank you to for doing this. So
Hi Priyesh, hi again. Good to meet you. Well, no, thank you to for doing this. So
Hi Priyesh, hi again. Good to meet you. Well, no, thank you to for doing this. So
S Speaker 10:15it's our pleasure that we're able to speak
it's our pleasure that we're able to speak
it's our pleasure that we're able to speak
it's our pleasure that we're able to speak
0:18with you about this, then on Friday.
with you about this, then on Friday.
with you about this, then on Friday.
with you about this, then on Friday.
0:24So are you based in the Bay?
So are you based in the Bay?
So are you based in the Bay?
So are you based in the Bay?
0:25I'm based in San Francisco.
I'm based in San Francisco.
I'm based in San Francisco.
I'm based in San Francisco.
0:27That's right, yeah, okay,
That's right, yeah, okay,
That's right, yeah, okay,
That's right, yeah, okay,
0:28yeah. We're also in panic, yeah.
yeah. We're also in panic, yeah.
yeah. We're also in panic, yeah.
yeah. We're also in panic, yeah.
S Speaker 10:38I'll jump right into it, and then maybe, I mean, I can give you a little bit of an overview Qualcomm ventures. I mean, we are in investment arm for Qualcomm. So we invest in areas that are broadly speaking strategic to Qualcomm. Now you would think that, okay, what's, what's
I'll jump right into it, and then maybe, I mean, I can give you a little bit of an overview Qualcomm ventures. I mean, we are in investment arm for Qualcomm. So we invest in areas that are broadly speaking strategic to Qualcomm. Now you would think that, okay, what's, what's
I'll jump right into it, and then maybe, I mean, I can give you a little bit of an overview Qualcomm ventures. I mean, we are in investment arm for Qualcomm. So we invest in areas that are broadly speaking strategic to Qualcomm. Now you would think that, okay, what's, what's
I'll jump right into it, and then maybe, I mean, I can give you a little bit of an overview Qualcomm ventures. I mean, we are in investment arm for Qualcomm. So we invest in areas that are broadly speaking strategic to Qualcomm. Now you would think that, okay, what's, what's
0:58AI strategic for everything? Exactly,
AI strategic for everything? Exactly,
AI strategic for everything? Exactly,
AI strategic for everything? Exactly,
1:02that's the gravy, right? Like that that
that's the gravy, right? Like that that
that's the gravy, right? Like that that
that's the gravy, right? Like that that
1:04can grow and go on everything.
can grow and go on everything.
can grow and go on everything.
can grow and go on everything.
S Speaker 11:07So the AI gravy is goodness here. But having said that, I mean, there's, there's other things that we're working on where, like every single one of our platforms, needs enterprise, some form of enterprise, AI, workflow automation to be deployed. And then so there's potential to work with, refuel on that. So hence, we're looking at this base in general
So the AI gravy is goodness here. But having said that, I mean, there's, there's other things that we're working on where, like every single one of our platforms, needs enterprise, some form of enterprise, AI, workflow automation to be deployed. And then so there's potential to work with, refuel on that. So hence, we're looking at this base in general
So the AI gravy is goodness here. But having said that, I mean, there's, there's other things that we're working on where, like every single one of our platforms, needs enterprise, some form of enterprise, AI, workflow automation to be deployed. And then so there's potential to work with, refuel on that. So hence, we're looking at this base in general
So the AI gravy is goodness here. But having said that, I mean, there's, there's other things that we're working on where, like every single one of our platforms, needs enterprise, some form of enterprise, AI, workflow automation to be deployed. And then so there's potential to work with, refuel on that. So hence, we're looking at this base in general
S Speaker 11:35what I mean the we're trying to figure out a few things from an investment standpoint for refuel. The big question for us on refuel is, is this a feature, or is this a product that's from pure investment standpoint, right? Like, is it like you're replacing traditional ETL pipelines? You know, will data breaks and snowflake also start providing something like this so, so and and what is the value of a standalone product like refuel to companies like yours, so and then are there build versus buy decisions in companies like mid desk, even for something like this, right? So, so that's the background, and then so, so hence, you know, we're trying to build our thesis in this space and figure out if, if we can invest now. So far, you know, we've been, we've liked Richard a lot, so we'd like conversations with them. And so thanks for taking the time, of course. And would love to understand, you know, your perspective given this background, and then then also maybe talk about your experience with them, right? For
what I mean the we're trying to figure out a few things from an investment standpoint for refuel. The big question for us on refuel is, is this a feature, or is this a product that's from pure investment standpoint, right? Like, is it like you're replacing traditional ETL pipelines? You know, will data breaks and snowflake also start providing something like this so, so and and what is the value of a standalone product like refuel to companies like yours, so and then are there build versus buy decisions in companies like mid desk, even for something like this, right? So, so that's the background, and then so, so hence, you know, we're trying to build our thesis in this space and figure out if, if we can invest now. So far, you know, we've been, we've liked Richard a lot, so we'd like conversations with them. And so thanks for taking the time, of course. And would love to understand, you know, your perspective given this background, and then then also maybe talk about your experience with them, right? For
what I mean the we're trying to figure out a few things from an investment standpoint for refuel. The big question for us on refuel is, is this a feature, or is this a product that's from pure investment standpoint, right? Like, is it like you're replacing traditional ETL pipelines? You know, will data breaks and snowflake also start providing something like this so, so and and what is the value of a standalone product like refuel to companies like yours, so and then are there build versus buy decisions in companies like mid desk, even for something like this, right? So, so that's the background, and then so, so hence, you know, we're trying to build our thesis in this space and figure out if, if we can invest now. So far, you know, we've been, we've liked Richard a lot, so we'd like conversations with them. And so thanks for taking the time, of course. And would love to understand, you know, your perspective given this background, and then then also maybe talk about your experience with them, right? For
what I mean the we're trying to figure out a few things from an investment standpoint for refuel. The big question for us on refuel is, is this a feature, or is this a product that's from pure investment standpoint, right? Like, is it like you're replacing traditional ETL pipelines? You know, will data breaks and snowflake also start providing something like this so, so and and what is the value of a standalone product like refuel to companies like yours, so and then are there build versus buy decisions in companies like mid desk, even for something like this, right? So, so that's the background, and then so, so hence, you know, we're trying to build our thesis in this space and figure out if, if we can invest now. So far, you know, we've been, we've liked Richard a lot, so we'd like conversations with them. And so thanks for taking the time, of course. And would love to understand, you know, your perspective given this background, and then then also maybe talk about your experience with them, right? For
S Speaker 22:55sure. Yeah, I just want to be up front. I think I have a hot stop at 3pm if you guys I
sure. Yeah, I just want to be up front. I think I have a hot stop at 3pm if you guys I
sure. Yeah, I just want to be up front. I think I have a hot stop at 3pm if you guys I
sure. Yeah, I just want to be up front. I think I have a hot stop at 3pm if you guys I
S Speaker 23:01do too. So good, great, great, great. I think a lot of the questions they're asking are super relevant. I think I can speak a little bit more clearly to the build versus buy position, and then I think you might need to guide me a little bit on behavior versus product, and how you guys think about that. I think on the build versus buy, it has made a lot of sense for us to leverage refuel when it comes to testing out new LLM capabilities and see how we can deliver customer facing products with that they have, just like, reduced the time to production tremendously because of the infrastructure that they have built. However, taking a step back, are you familiar with what middesk broadly does?
do too. So good, great, great, great. I think a lot of the questions they're asking are super relevant. I think I can speak a little bit more clearly to the build versus buy position, and then I think you might need to guide me a little bit on behavior versus product, and how you guys think about that. I think on the build versus buy, it has made a lot of sense for us to leverage refuel when it comes to testing out new LLM capabilities and see how we can deliver customer facing products with that they have, just like, reduced the time to production tremendously because of the infrastructure that they have built. However, taking a step back, are you familiar with what middesk broadly does?
do too. So good, great, great, great. I think a lot of the questions they're asking are super relevant. I think I can speak a little bit more clearly to the build versus buy position, and then I think you might need to guide me a little bit on behavior versus product, and how you guys think about that. I think on the build versus buy, it has made a lot of sense for us to leverage refuel when it comes to testing out new LLM capabilities and see how we can deliver customer facing products with that they have, just like, reduced the time to production tremendously because of the infrastructure that they have built. However, taking a step back, are you familiar with what middesk broadly does?
do too. So good, great, great, great. I think a lot of the questions they're asking are super relevant. I think I can speak a little bit more clearly to the build versus buy position, and then I think you might need to guide me a little bit on behavior versus product, and how you guys think about that. I think on the build versus buy, it has made a lot of sense for us to leverage refuel when it comes to testing out new LLM capabilities and see how we can deliver customer facing products with that they have, just like, reduced the time to production tremendously because of the infrastructure that they have built. However, taking a step back, are you familiar with what middesk broadly does?
S Speaker 13:45Besides, I think it'll be good for you to tell us, because we went to the website, we tried to understand, happy
Besides, I think it'll be good for you to tell us, because we went to the website, we tried to understand, happy
Besides, I think it'll be good for you to tell us, because we went to the website, we tried to understand, happy
Besides, I think it'll be good for you to tell us, because we went to the website, we tried to understand, happy
S Speaker 16:33just one question, which is this industry? Which is categorization of industry. So this data was coming also from a structured input, or was this data coming from the web? And which is one of their integrations that they have, that
just one question, which is this industry? Which is categorization of industry. So this data was coming also from a structured input, or was this data coming from the web? And which is one of their integrations that they have, that
just one question, which is this industry? Which is categorization of industry. So this data was coming also from a structured input, or was this data coming from the web? And which is one of their integrations that they have, that
just one question, which is this industry? Which is categorization of industry. So this data was coming also from a structured input, or was this data coming from the web? And which is one of their integrations that they have, that
S Speaker 26:48is, that's one of the things that made it a lot easier for us. Previously we we would scrape the business website that customers submitted to us and then run the interpretation of what an industry would be based on the scraped text. What we were able to do with refuel, because we thought about this conceptually, but they made it easy for us to implement it, and that they have built out like web scraper functionality and made the pipeline to feed it into the model like seamless. And therefore what we could do is like, we no longer need to scrape the website ourselves. We could put in terms, search terms on that business, hey, the business name, the business officer, the city and state that they act in, yeah. And then they could, they could run interpretation just based on the Google search results. If there's higher, high enough confidence on that. Once again, it's like, I think what the magic the most of the magic here is like the llms have been capable, but they have made it. They have built all the infrastructure around it to make this, like time to production way easier for us. Say that if we were to build it in house, I would take maybe, like, one and a half months to two months. And for them, because they have already built it like, the time to production was like, one and a half two weeks. And so that was, like, very good for us. I've taken this approach, like, hey, if there's something that we want to power our land, so let's test it with brief your thoughts and then figure out if we can scale it up. If we cannot, or like, Hey, this is not a good fit, then we'll try and explore something this. This our process of
is, that's one of the things that made it a lot easier for us. Previously we we would scrape the business website that customers submitted to us and then run the interpretation of what an industry would be based on the scraped text. What we were able to do with refuel, because we thought about this conceptually, but they made it easy for us to implement it, and that they have built out like web scraper functionality and made the pipeline to feed it into the model like seamless. And therefore what we could do is like, we no longer need to scrape the website ourselves. We could put in terms, search terms on that business, hey, the business name, the business officer, the city and state that they act in, yeah. And then they could, they could run interpretation just based on the Google search results. If there's higher, high enough confidence on that. Once again, it's like, I think what the magic the most of the magic here is like the llms have been capable, but they have made it. They have built all the infrastructure around it to make this, like time to production way easier for us. Say that if we were to build it in house, I would take maybe, like, one and a half months to two months. And for them, because they have already built it like, the time to production was like, one and a half two weeks. And so that was, like, very good for us. I've taken this approach, like, hey, if there's something that we want to power our land, so let's test it with brief your thoughts and then figure out if we can scale it up. If we cannot, or like, Hey, this is not a good fit, then we'll try and explore something this. This our process of
is, that's one of the things that made it a lot easier for us. Previously we we would scrape the business website that customers submitted to us and then run the interpretation of what an industry would be based on the scraped text. What we were able to do with refuel, because we thought about this conceptually, but they made it easy for us to implement it, and that they have built out like web scraper functionality and made the pipeline to feed it into the model like seamless. And therefore what we could do is like, we no longer need to scrape the website ourselves. We could put in terms, search terms on that business, hey, the business name, the business officer, the city and state that they act in, yeah. And then they could, they could run interpretation just based on the Google search results. If there's higher, high enough confidence on that. Once again, it's like, I think what the magic the most of the magic here is like the llms have been capable, but they have made it. They have built all the infrastructure around it to make this, like time to production way easier for us. Say that if we were to build it in house, I would take maybe, like, one and a half months to two months. And for them, because they have already built it like, the time to production was like, one and a half two weeks. And so that was, like, very good for us. I've taken this approach, like, hey, if there's something that we want to power our land, so let's test it with brief your thoughts and then figure out if we can scale it up. If we cannot, or like, Hey, this is not a good fit, then we'll try and explore something this. This our process of
is, that's one of the things that made it a lot easier for us. Previously we we would scrape the business website that customers submitted to us and then run the interpretation of what an industry would be based on the scraped text. What we were able to do with refuel, because we thought about this conceptually, but they made it easy for us to implement it, and that they have built out like web scraper functionality and made the pipeline to feed it into the model like seamless. And therefore what we could do is like, we no longer need to scrape the website ourselves. We could put in terms, search terms on that business, hey, the business name, the business officer, the city and state that they act in, yeah. And then they could, they could run interpretation just based on the Google search results. If there's higher, high enough confidence on that. Once again, it's like, I think what the magic the most of the magic here is like the llms have been capable, but they have made it. They have built all the infrastructure around it to make this, like time to production way easier for us. Say that if we were to build it in house, I would take maybe, like, one and a half months to two months. And for them, because they have already built it like, the time to production was like, one and a half two weeks. And so that was, like, very good for us. I've taken this approach, like, hey, if there's something that we want to power our land, so let's test it with brief your thoughts and then figure out if we can scale it up. If we cannot, or like, Hey, this is not a good fit, then we'll try and explore something this. This our process of
S Speaker 18:28but you said it would. Did you ever like try to build this yourself with like off the shelf? Because llms Foundation models exist.
but you said it would. Did you ever like try to build this yourself with like off the shelf? Because llms Foundation models exist.
but you said it would. Did you ever like try to build this yourself with like off the shelf? Because llms Foundation models exist.
but you said it would. Did you ever like try to build this yourself with like off the shelf? Because llms Foundation models exist.
8:42Did you ever try to build this yourself?
Did you ever try to build this yourself?
Did you ever try to build this yourself?
Did you ever try to build this yourself?
S Speaker 28:45I think we started building it ourselves when GPT three was the thing. So it's like so many generations ago right at this point. And I would say, once again, a lot of the magic is in the development of the models itself. And at that point when GPT three was still around and we tested, it was just like a little clunky for us to type things in, and it was like slow and everything. So it was, it didn't really make sense today to be transparent. We are also exploring, like open source models. There are platforms like olama, and we definitely are exploring all of that because, like, it's important for the data science team to know what is out there, right? Yeah. And I think where it's like, challenging for us is that ulama requires us to deploy resources, and we still have to host them. We have to ensure that they have made it simpler. But it's there's still some degree of, like, resource management reliability, and that's on us, rather than a managed service reviewer is. So that's one thing I wanted to call out. And I think like, there's always going to be that build versus buy decision, how, like, once again, right? How I frame it is, how love to prototype as quickly as possible to see if the customers even want it, if there's even value being delivered first. And I'm not looking for the simplest I'm not looking for the cheapest solution when I'm prototyping to understand, yeah. So it made sense for me to use the simplest solution for me, which is, like, we start with refuel to like, test their market value before we try and scale it or find cheaper options. So that's how we think about it.
I think we started building it ourselves when GPT three was the thing. So it's like so many generations ago right at this point. And I would say, once again, a lot of the magic is in the development of the models itself. And at that point when GPT three was still around and we tested, it was just like a little clunky for us to type things in, and it was like slow and everything. So it was, it didn't really make sense today to be transparent. We are also exploring, like open source models. There are platforms like olama, and we definitely are exploring all of that because, like, it's important for the data science team to know what is out there, right? Yeah. And I think where it's like, challenging for us is that ulama requires us to deploy resources, and we still have to host them. We have to ensure that they have made it simpler. But it's there's still some degree of, like, resource management reliability, and that's on us, rather than a managed service reviewer is. So that's one thing I wanted to call out. And I think like, there's always going to be that build versus buy decision, how, like, once again, right? How I frame it is, how love to prototype as quickly as possible to see if the customers even want it, if there's even value being delivered first. And I'm not looking for the simplest I'm not looking for the cheapest solution when I'm prototyping to understand, yeah. So it made sense for me to use the simplest solution for me, which is, like, we start with refuel to like, test their market value before we try and scale it or find cheaper options. So that's how we think about it.
I think we started building it ourselves when GPT three was the thing. So it's like so many generations ago right at this point. And I would say, once again, a lot of the magic is in the development of the models itself. And at that point when GPT three was still around and we tested, it was just like a little clunky for us to type things in, and it was like slow and everything. So it was, it didn't really make sense today to be transparent. We are also exploring, like open source models. There are platforms like olama, and we definitely are exploring all of that because, like, it's important for the data science team to know what is out there, right? Yeah. And I think where it's like, challenging for us is that ulama requires us to deploy resources, and we still have to host them. We have to ensure that they have made it simpler. But it's there's still some degree of, like, resource management reliability, and that's on us, rather than a managed service reviewer is. So that's one thing I wanted to call out. And I think like, there's always going to be that build versus buy decision, how, like, once again, right? How I frame it is, how love to prototype as quickly as possible to see if the customers even want it, if there's even value being delivered first. And I'm not looking for the simplest I'm not looking for the cheapest solution when I'm prototyping to understand, yeah. So it made sense for me to use the simplest solution for me, which is, like, we start with refuel to like, test their market value before we try and scale it or find cheaper options. So that's how we think about it.
I think we started building it ourselves when GPT three was the thing. So it's like so many generations ago right at this point. And I would say, once again, a lot of the magic is in the development of the models itself. And at that point when GPT three was still around and we tested, it was just like a little clunky for us to type things in, and it was like slow and everything. So it was, it didn't really make sense today to be transparent. We are also exploring, like open source models. There are platforms like olama, and we definitely are exploring all of that because, like, it's important for the data science team to know what is out there, right? Yeah. And I think where it's like, challenging for us is that ulama requires us to deploy resources, and we still have to host them. We have to ensure that they have made it simpler. But it's there's still some degree of, like, resource management reliability, and that's on us, rather than a managed service reviewer is. So that's one thing I wanted to call out. And I think like, there's always going to be that build versus buy decision, how, like, once again, right? How I frame it is, how love to prototype as quickly as possible to see if the customers even want it, if there's even value being delivered first. And I'm not looking for the simplest I'm not looking for the cheapest solution when I'm prototyping to understand, yeah. So it made sense for me to use the simplest solution for me, which is, like, we start with refuel to like, test their market value before we try and scale it or find cheaper options. So that's how we think about it.
S Speaker 110:26Okay? And then, but the cheaper option, so eventually, when you go into production, so let's say you scale right. Like it sounds like a large part of your product actually could have dependency on something like a refuel, right? I think
Okay? And then, but the cheaper option, so eventually, when you go into production, so let's say you scale right. Like it sounds like a large part of your product actually could have dependency on something like a refuel, right? I think
Okay? And then, but the cheaper option, so eventually, when you go into production, so let's say you scale right. Like it sounds like a large part of your product actually could have dependency on something like a refuel, right? I think
Okay? And then, but the cheaper option, so eventually, when you go into production, so let's say you scale right. Like it sounds like a large part of your product actually could have dependency on something like a refuel, right? I think
S Speaker 210:41what we are trying to do here is, and this is a great question, right? I think mid Das is value prop is in having access to hard to get data Yes, and then marrying that with web data Yes, and building on top of that. So yes, I think llms can play a big part of it, and how, like maybe can play the stitching together, yes, and we believe that it will be a bigger and bigger part of our product stack. So, but from a cost perspective, I think we are also exploring what the trade offs are going direct to, let's say, open AI or anthropic or open source models. And I think
what we are trying to do here is, and this is a great question, right? I think mid Das is value prop is in having access to hard to get data Yes, and then marrying that with web data Yes, and building on top of that. So yes, I think llms can play a big part of it, and how, like maybe can play the stitching together, yes, and we believe that it will be a bigger and bigger part of our product stack. So, but from a cost perspective, I think we are also exploring what the trade offs are going direct to, let's say, open AI or anthropic or open source models. And I think
what we are trying to do here is, and this is a great question, right? I think mid Das is value prop is in having access to hard to get data Yes, and then marrying that with web data Yes, and building on top of that. So yes, I think llms can play a big part of it, and how, like maybe can play the stitching together, yes, and we believe that it will be a bigger and bigger part of our product stack. So, but from a cost perspective, I think we are also exploring what the trade offs are going direct to, let's say, open AI or anthropic or open source models. And I think
what we are trying to do here is, and this is a great question, right? I think mid Das is value prop is in having access to hard to get data Yes, and then marrying that with web data Yes, and building on top of that. So yes, I think llms can play a big part of it, and how, like maybe can play the stitching together, yes, and we believe that it will be a bigger and bigger part of our product stack. So, but from a cost perspective, I think we are also exploring what the trade offs are going direct to, let's say, open AI or anthropic or open source models. And I think
11:27we have been spending around, I would say, $10,000
we have been spending around, I would say, $10,000
we have been spending around, I would say, $10,000
we have been spending around, I would say, $10,000
S Speaker 211:31on a monthly basis, plus minus. And then we are growing with refuel. But I think we are going to move more towards a platform, the kind of platform usage with the tokens as a pass through cost and like, from my perspective, it makes a lot of sense. If like, what we are benefiting from is not a per usage kind of but rather, hey, we are leveraging this platform to celebrate productivity for us that said, there's one that's one like unique spin. And I think Rich was very smart. Rich and his team is very smart in that they are also providing their own models, own, like fine tuned, large language models. And that is a case where I think it makes sense for us to use both the platform and the model, and we pay them for the per usage. And we already starting. We're already starting to do that when we think about leveraging us a hyper large model all the time, it's like not just from a cost perspective, but turnaround time. Our customers are looking for us to return things in a matter of seconds. And if you're if you have a very large model that might not be necessary, like, the cost trade off is not worth it for us. So we are always looking for like options to like, make it more optimized and refuels own. Our land actually has, has been like, good performance and like quite like, time wise, also quite performant.
on a monthly basis, plus minus. And then we are growing with refuel. But I think we are going to move more towards a platform, the kind of platform usage with the tokens as a pass through cost and like, from my perspective, it makes a lot of sense. If like, what we are benefiting from is not a per usage kind of but rather, hey, we are leveraging this platform to celebrate productivity for us that said, there's one that's one like unique spin. And I think Rich was very smart. Rich and his team is very smart in that they are also providing their own models, own, like fine tuned, large language models. And that is a case where I think it makes sense for us to use both the platform and the model, and we pay them for the per usage. And we already starting. We're already starting to do that when we think about leveraging us a hyper large model all the time, it's like not just from a cost perspective, but turnaround time. Our customers are looking for us to return things in a matter of seconds. And if you're if you have a very large model that might not be necessary, like, the cost trade off is not worth it for us. So we are always looking for like options to like, make it more optimized and refuels own. Our land actually has, has been like, good performance and like quite like, time wise, also quite performant.
on a monthly basis, plus minus. And then we are growing with refuel. But I think we are going to move more towards a platform, the kind of platform usage with the tokens as a pass through cost and like, from my perspective, it makes a lot of sense. If like, what we are benefiting from is not a per usage kind of but rather, hey, we are leveraging this platform to celebrate productivity for us that said, there's one that's one like unique spin. And I think Rich was very smart. Rich and his team is very smart in that they are also providing their own models, own, like fine tuned, large language models. And that is a case where I think it makes sense for us to use both the platform and the model, and we pay them for the per usage. And we already starting. We're already starting to do that when we think about leveraging us a hyper large model all the time, it's like not just from a cost perspective, but turnaround time. Our customers are looking for us to return things in a matter of seconds. And if you're if you have a very large model that might not be necessary, like, the cost trade off is not worth it for us. So we are always looking for like options to like, make it more optimized and refuels own. Our land actually has, has been like, good performance and like quite like, time wise, also quite performant.
on a monthly basis, plus minus. And then we are growing with refuel. But I think we are going to move more towards a platform, the kind of platform usage with the tokens as a pass through cost and like, from my perspective, it makes a lot of sense. If like, what we are benefiting from is not a per usage kind of but rather, hey, we are leveraging this platform to celebrate productivity for us that said, there's one that's one like unique spin. And I think Rich was very smart. Rich and his team is very smart in that they are also providing their own models, own, like fine tuned, large language models. And that is a case where I think it makes sense for us to use both the platform and the model, and we pay them for the per usage. And we already starting. We're already starting to do that when we think about leveraging us a hyper large model all the time, it's like not just from a cost perspective, but turnaround time. Our customers are looking for us to return things in a matter of seconds. And if you're if you have a very large model that might not be necessary, like, the cost trade off is not worth it for us. So we are always looking for like options to like, make it more optimized and refuels own. Our land actually has, has been like, good performance and like quite like, time wise, also quite performant.
S Speaker 113:02So have you which one do you? Do you have the option to choose, refuels, LLM versus, yeah, open AI, you know, GPT, four or something, and then the cost is so the cost is significantly different between the own model and and open AI models or anthropic, yeah,
So have you which one do you? Do you have the option to choose, refuels, LLM versus, yeah, open AI, you know, GPT, four or something, and then the cost is so the cost is significantly different between the own model and and open AI models or anthropic, yeah,
So have you which one do you? Do you have the option to choose, refuels, LLM versus, yeah, open AI, you know, GPT, four or something, and then the cost is so the cost is significantly different between the own model and and open AI models or anthropic, yeah,
So have you which one do you? Do you have the option to choose, refuels, LLM versus, yeah, open AI, you know, GPT, four or something, and then the cost is so the cost is significantly different between the own model and and open AI models or anthropic, yeah,
S Speaker 213:23because it's a smaller model, right? And I think because refio is able to host it themselves, of course, I then the next question is, like, how reliable it is? And, yeah, putting that aside, I think because they're hosting it themselves, and the model is a lot smaller, I think the cost is pretty effective, yeah.
because it's a smaller model, right? And I think because refio is able to host it themselves, of course, I then the next question is, like, how reliable it is? And, yeah, putting that aside, I think because they're hosting it themselves, and the model is a lot smaller, I think the cost is pretty effective, yeah.
because it's a smaller model, right? And I think because refio is able to host it themselves, of course, I then the next question is, like, how reliable it is? And, yeah, putting that aside, I think because they're hosting it themselves, and the model is a lot smaller, I think the cost is pretty effective, yeah.
because it's a smaller model, right? And I think because refio is able to host it themselves, of course, I then the next question is, like, how reliable it is? And, yeah, putting that aside, I think because they're hosting it themselves, and the model is a lot smaller, I think the cost is pretty effective, yeah.
13:42So which one are you using? Yeah,
So which one are you using? Yeah,
So which one are you using? Yeah,
So which one are you using? Yeah,
S Speaker 213:44for the production use case, we actually have, we started off with GPT four. Oh, and we shifted to their model, yeah. Okay,
for the production use case, we actually have, we started off with GPT four. Oh, and we shifted to their model, yeah. Okay,
for the production use case, we actually have, we started off with GPT four. Oh, and we shifted to their model, yeah. Okay,
for the production use case, we actually have, we started off with GPT four. Oh, and we shifted to their model, yeah. Okay,
13:51so, so the performance is good enough for you,
so, so the performance is good enough for you,
so, so the performance is good enough for you,
so, so the performance is good enough for you,
13:54exactly for this? Use case, yes.
exactly for this? Use case, yes.
exactly for this? Use case, yes.
exactly for this? Use case, yes.
S Speaker 113:57Use Case, yeah. I mean, you don't need the knowledge of the world you
Use Case, yeah. I mean, you don't need the knowledge of the world you
Use Case, yeah. I mean, you don't need the knowledge of the world you
Use Case, yeah. I mean, you don't need the knowledge of the world you
S Speaker 214:01Yeah, not all the time, right? Yeah, okay.
Yeah, not all the time, right? Yeah, okay.
Yeah, not all the time, right? Yeah, okay.
Yeah, not all the time, right? Yeah, okay.
S Speaker 114:07And their model is also able to surf the web and and get information from
And their model is also able to surf the web and and get information from
And their model is also able to surf the web and and get information from
And their model is also able to surf the web and and get information from
S Speaker 214:13the web. That's where I think they have done it differently, in that they are doing the scraping, not using the web, they have done like, more traditional scrapers first, and of course, there's a lot of agentic frameworks to like with anthropic computer use, and you empower them to assume, but I don't think it's as mature yet, and we are able to get what we need from just like serve API requests to Google, and that's been good enough for us, at least for now. Okay,
the web. That's where I think they have done it differently, in that they are doing the scraping, not using the web, they have done like, more traditional scrapers first, and of course, there's a lot of agentic frameworks to like with anthropic computer use, and you empower them to assume, but I don't think it's as mature yet, and we are able to get what we need from just like serve API requests to Google, and that's been good enough for us, at least for now. Okay,
the web. That's where I think they have done it differently, in that they are doing the scraping, not using the web, they have done like, more traditional scrapers first, and of course, there's a lot of agentic frameworks to like with anthropic computer use, and you empower them to assume, but I don't think it's as mature yet, and we are able to get what we need from just like serve API requests to Google, and that's been good enough for us, at least for now. Okay,
the web. That's where I think they have done it differently, in that they are doing the scraping, not using the web, they have done like, more traditional scrapers first, and of course, there's a lot of agentic frameworks to like with anthropic computer use, and you empower them to assume, but I don't think it's as mature yet, and we are able to get what we need from just like serve API requests to Google, and that's been good enough for us, at least for now. Okay,
S Speaker 114:42so when did you start using Ken?
so when did you start using Ken?
so when did you start using Ken?
so when did you start using Ken?
14:48Oh, sorry, start using review.
Oh, sorry, start using review.
Oh, sorry, start using review.
Oh, sorry, start using review.
14:52let me check at least more than a year.
let me check at least more than a year.
let me check at least more than a year.
let me check at least more than a year.
14:56I think we started late December. 2022
I think we started late December. 2022
I think we started late December. 2022
I think we started late December. 2022
S Speaker 215:04Hey, sorry, I'm messing up my dates a little bit. I think it's like coming to a year. So date around December, 2023 and then it's been close to a year. Yeah. Okay,
Hey, sorry, I'm messing up my dates a little bit. I think it's like coming to a year. So date around December, 2023 and then it's been close to a year. Yeah. Okay,
Hey, sorry, I'm messing up my dates a little bit. I think it's like coming to a year. So date around December, 2023 and then it's been close to a year. Yeah. Okay,
Hey, sorry, I'm messing up my dates a little bit. I think it's like coming to a year. So date around December, 2023 and then it's been close to a year. Yeah. Okay,
S Speaker 115:17so I guess. And did you engage with them, and they had the previous version of the
so I guess. And did you engage with them, and they had the previous version of the
so I guess. And did you engage with them, and they had the previous version of the
so I guess. And did you engage with them, and they had the previous version of the
15:23product, I guess not.
product, I guess not.
product, I guess not.
product, I guess not.
S Speaker 215:27Yeah, we started off with this retail platform. Okay, got it, got
Yeah, we started off with this retail platform. Okay, got it, got
Yeah, we started off with this retail platform. Okay, got it, got
Yeah, we started off with this retail platform. Okay, got it, got
S Speaker 115:31it and, and that was, like, when you started using to production. How long was it?
it and, and that was, like, when you started using to production. How long was it?
it and, and that was, like, when you started using to production. How long was it?
it and, and that was, like, when you started using to production. How long was it?
S Speaker 215:39It was, oh, it was very tasked, as in, like, I think, what was interface of presenting all the data in a table form? Yeah. And then they also made it easy to deploy the model for that specific task as an endpoint, and that is pretty much what we need to just plug it into our production stack, right? Like, hey, there's an airport you can hit and then input, output, etc. Yeah, I will say once again, we didn't start off with a production use case, right? So our initial style was like, Hey, can we generate just like, large data sets or training data, and we can download that in a tabular format and then we just ingest that for our other trading purposes. But yeah, so that was like very fast, because we didn't even need to integrate with our production system for that initial interesting okay. And
It was, oh, it was very tasked, as in, like, I think, what was interface of presenting all the data in a table form? Yeah. And then they also made it easy to deploy the model for that specific task as an endpoint, and that is pretty much what we need to just plug it into our production stack, right? Like, hey, there's an airport you can hit and then input, output, etc. Yeah, I will say once again, we didn't start off with a production use case, right? So our initial style was like, Hey, can we generate just like, large data sets or training data, and we can download that in a tabular format and then we just ingest that for our other trading purposes. But yeah, so that was like very fast, because we didn't even need to integrate with our production system for that initial interesting okay. And
It was, oh, it was very tasked, as in, like, I think, what was interface of presenting all the data in a table form? Yeah. And then they also made it easy to deploy the model for that specific task as an endpoint, and that is pretty much what we need to just plug it into our production stack, right? Like, hey, there's an airport you can hit and then input, output, etc. Yeah, I will say once again, we didn't start off with a production use case, right? So our initial style was like, Hey, can we generate just like, large data sets or training data, and we can download that in a tabular format and then we just ingest that for our other trading purposes. But yeah, so that was like very fast, because we didn't even need to integrate with our production system for that initial interesting okay. And
It was, oh, it was very tasked, as in, like, I think, what was interface of presenting all the data in a table form? Yeah. And then they also made it easy to deploy the model for that specific task as an endpoint, and that is pretty much what we need to just plug it into our production stack, right? Like, hey, there's an airport you can hit and then input, output, etc. Yeah, I will say once again, we didn't start off with a production use case, right? So our initial style was like, Hey, can we generate just like, large data sets or training data, and we can download that in a tabular format and then we just ingest that for our other trading purposes. But yeah, so that was like very fast, because we didn't even need to integrate with our production system for that initial interesting okay. And
S Speaker 116:35did you evaluate snorkel flow or data soar or any other platform?
did you evaluate snorkel flow or data soar or any other platform?
did you evaluate snorkel flow or data soar or any other platform?
did you evaluate snorkel flow or data soar or any other platform?
S Speaker 216:42No, I like on balance, yeah, I have not explored other vendors. Okay, that by a, let's say, an interface for you to orchestrate any other calls yet. Okay, we I think the calculation, the calculus for us is like, Hey, what is the if we have something that moves fast, something
No, I like on balance, yeah, I have not explored other vendors. Okay, that by a, let's say, an interface for you to orchestrate any other calls yet. Okay, we I think the calculation, the calculus for us is like, Hey, what is the if we have something that moves fast, something
No, I like on balance, yeah, I have not explored other vendors. Okay, that by a, let's say, an interface for you to orchestrate any other calls yet. Okay, we I think the calculation, the calculus for us is like, Hey, what is the if we have something that moves fast, something
No, I like on balance, yeah, I have not explored other vendors. Okay, that by a, let's say, an interface for you to orchestrate any other calls yet. Okay, we I think the calculation, the calculus for us is like, Hey, what is the if we have something that moves fast, something
S Speaker 117:05that works, then you don't have any incentive. I was cute,
that works, then you don't have any incentive. I was cute,
that works, then you don't have any incentive. I was cute,
that works, then you don't have any incentive. I was cute,
S Speaker 217:15and I didn't it was like, Hey, someone is working in the data science space. It was cool. That's chat. And then I realized, oh, okay, actually, that's something that tried and see if it's useful. So it was, like, an opportunistic thing. I was not on the market looking for an element,
and I didn't it was like, Hey, someone is working in the data science space. It was cool. That's chat. And then I realized, oh, okay, actually, that's something that tried and see if it's useful. So it was, like, an opportunistic thing. I was not on the market looking for an element,
and I didn't it was like, Hey, someone is working in the data science space. It was cool. That's chat. And then I realized, oh, okay, actually, that's something that tried and see if it's useful. So it was, like, an opportunistic thing. I was not on the market looking for an element,
and I didn't it was like, Hey, someone is working in the data science space. It was cool. That's chat. And then I realized, oh, okay, actually, that's something that tried and see if it's useful. So it was, like, an opportunistic thing. I was not on the market looking for an element,
S Speaker 117:31yeah, oh, okay, so now and then, now you think it's, like, it's, it's an important part of your product, right? Like, yeah, I
yeah, oh, okay, so now and then, now you think it's, like, it's, it's an important part of your product, right? Like, yeah, I
yeah, oh, okay, so now and then, now you think it's, like, it's, it's an important part of your product, right? Like, yeah, I
yeah, oh, okay, so now and then, now you think it's, like, it's, it's an important part of your product, right? Like, yeah, I
S Speaker 217:39think, I think, of course, I models are going to get more capable. More capable. Yes, and with that, I would say, like, how do you continue to to deliver value, differentiated value? I think that is something that the team needs to figure out. I cannot project out for the next two years. Hey, is refuel going to provide different shaded value,
think, I think, of course, I models are going to get more capable. More capable. Yes, and with that, I would say, like, how do you continue to to deliver value, differentiated value? I think that is something that the team needs to figure out. I cannot project out for the next two years. Hey, is refuel going to provide different shaded value,
think, I think, of course, I models are going to get more capable. More capable. Yes, and with that, I would say, like, how do you continue to to deliver value, differentiated value? I think that is something that the team needs to figure out. I cannot project out for the next two years. Hey, is refuel going to provide different shaded value,
think, I think, of course, I models are going to get more capable. More capable. Yes, and with that, I would say, like, how do you continue to to deliver value, differentiated value? I think that is something that the team needs to figure out. I cannot project out for the next two years. Hey, is refuel going to provide different shaded value,
S Speaker 118:03but I just think that what like, like, the value is like, you already know the like, the llms, the large llms, provide enough value, but, but for you, it was more the managed service, and then just time to production is production was the key, and eventually, I mean, for your particular use case, turns out that refuels on LM is cheaper faster.
but I just think that what like, like, the value is like, you already know the like, the llms, the large llms, provide enough value, but, but for you, it was more the managed service, and then just time to production is production was the key, and eventually, I mean, for your particular use case, turns out that refuels on LM is cheaper faster.
but I just think that what like, like, the value is like, you already know the like, the llms, the large llms, provide enough value, but, but for you, it was more the managed service, and then just time to production is production was the key, and eventually, I mean, for your particular use case, turns out that refuels on LM is cheaper faster.
but I just think that what like, like, the value is like, you already know the like, the llms, the large llms, provide enough value, but, but for you, it was more the managed service, and then just time to production is production was the key, and eventually, I mean, for your particular use case, turns out that refuels on LM is cheaper faster.
S Speaker 218:30I think that's, that's the piece, which is like, cool, is cheaper, faster. But the thing is, what if there's, like, the next version of, let's say anthropic Claude four. And then they have a small model that is pretty good. So, and then the question happened,
I think that's, that's the piece, which is like, cool, is cheaper, faster. But the thing is, what if there's, like, the next version of, let's say anthropic Claude four. And then they have a small model that is pretty good. So, and then the question happened,
I think that's, that's the piece, which is like, cool, is cheaper, faster. But the thing is, what if there's, like, the next version of, let's say anthropic Claude four. And then they have a small model that is pretty good. So, and then the question happened,
I think that's, that's the piece, which is like, cool, is cheaper, faster. But the thing is, what if there's, like, the next version of, let's say anthropic Claude four. And then they have a small model that is pretty good. So, and then the question happened,
S Speaker 118:47but then, if that happens, then, but still, you have to fine
but then, if that happens, then, but still, you have to fine
but then, if that happens, then, but still, you have to fine
but then, if that happens, then, but still, you have to fine
S Speaker 120:23someone who gave it a shot to build clean house, because I know several examples of, you know, ml teams at different companies who try to build different types of automation, not in this space. But then some gave up, or most gave up, because, turns out, building a rag plus fine tuning system is not as easy as you think for to achieve the accuracy.
someone who gave it a shot to build clean house, because I know several examples of, you know, ml teams at different companies who try to build different types of automation, not in this space. But then some gave up, or most gave up, because, turns out, building a rag plus fine tuning system is not as easy as you think for to achieve the accuracy.
someone who gave it a shot to build clean house, because I know several examples of, you know, ml teams at different companies who try to build different types of automation, not in this space. But then some gave up, or most gave up, because, turns out, building a rag plus fine tuning system is not as easy as you think for to achieve the accuracy.
someone who gave it a shot to build clean house, because I know several examples of, you know, ml teams at different companies who try to build different types of automation, not in this space. But then some gave up, or most gave up, because, turns out, building a rag plus fine tuning system is not as easy as you think for to achieve the accuracy.
S Speaker 220:47Yeah, we haven't. We didn't try that piece. But what we what we did try, and more recently, is agentic flows. And I don't think, and there's something that rich also told me, I don't think the way they have, like, focus on that as much, yeah, and I don't think the interface that is built is that bad, I would say at least that tailored for Office rate, multiple agent flows or, like, calls. Yet they do have something like, Oh, you're able to chain tasks together. That's it. I I'm just gonna put it out there, like we tested our building our own agent flows. And there's a lot of providers out there. I don't think refuel is in a pole position for that use case. However, for replacing traditional ml models with our lands. That's fine tuned for the use case. I think, like, Rico is a good fit, like, it has been a good fit for us. Yeah, yeah.
Yeah, we haven't. We didn't try that piece. But what we what we did try, and more recently, is agentic flows. And I don't think, and there's something that rich also told me, I don't think the way they have, like, focus on that as much, yeah, and I don't think the interface that is built is that bad, I would say at least that tailored for Office rate, multiple agent flows or, like, calls. Yet they do have something like, Oh, you're able to chain tasks together. That's it. I I'm just gonna put it out there, like we tested our building our own agent flows. And there's a lot of providers out there. I don't think refuel is in a pole position for that use case. However, for replacing traditional ml models with our lands. That's fine tuned for the use case. I think, like, Rico is a good fit, like, it has been a good fit for us. Yeah, yeah.
Yeah, we haven't. We didn't try that piece. But what we what we did try, and more recently, is agentic flows. And I don't think, and there's something that rich also told me, I don't think the way they have, like, focus on that as much, yeah, and I don't think the interface that is built is that bad, I would say at least that tailored for Office rate, multiple agent flows or, like, calls. Yet they do have something like, Oh, you're able to chain tasks together. That's it. I I'm just gonna put it out there, like we tested our building our own agent flows. And there's a lot of providers out there. I don't think refuel is in a pole position for that use case. However, for replacing traditional ml models with our lands. That's fine tuned for the use case. I think, like, Rico is a good fit, like, it has been a good fit for us. Yeah, yeah.
Yeah, we haven't. We didn't try that piece. But what we what we did try, and more recently, is agentic flows. And I don't think, and there's something that rich also told me, I don't think the way they have, like, focus on that as much, yeah, and I don't think the interface that is built is that bad, I would say at least that tailored for Office rate, multiple agent flows or, like, calls. Yet they do have something like, Oh, you're able to chain tasks together. That's it. I I'm just gonna put it out there, like we tested our building our own agent flows. And there's a lot of providers out there. I don't think refuel is in a pole position for that use case. However, for replacing traditional ml models with our lands. That's fine tuned for the use case. I think, like, Rico is a good fit, like, it has been a good fit for us. Yeah, yeah.
S Speaker 121:52And there's that your type of use case is typically, like, without llms. What did you use? What
And there's that your type of use case is typically, like, without llms. What did you use? What
And there's that your type of use case is typically, like, without llms. What did you use? What
And there's that your type of use case is typically, like, without llms. What did you use? What
S Speaker 222:03our own models? You know, like we have our we are a Google Cloud Platform shop, so they have their own vertex, AI, yeah, model hosting platform. So what we do is we train our own models. We deploy it like, as a pipeline to
our own models? You know, like we have our we are a Google Cloud Platform shop, so they have their own vertex, AI, yeah, model hosting platform. So what we do is we train our own models. We deploy it like, as a pipeline to
our own models? You know, like we have our we are a Google Cloud Platform shop, so they have their own vertex, AI, yeah, model hosting platform. So what we do is we train our own models. We deploy it like, as a pipeline to
our own models? You know, like we have our we are a Google Cloud Platform shop, so they have their own vertex, AI, yeah, model hosting platform. So what we do is we train our own models. We deploy it like, as a pipeline to
S Speaker 122:19like, without LLM use, without training. What was the like? Wow. People using,
like, without LLM use, without training. What was the like? Wow. People using,
like, without LLM use, without training. What was the like? Wow. People using,
like, without LLM use, without training. What was the like? Wow. People using,
S Speaker 222:26sorry, as in, if we were training traditional ml models or,
sorry, as in, if we were training traditional ml models or,
sorry, as in, if we were training traditional ml models or,
sorry, as in, if we were training traditional ml models or,
S Speaker 122:29yeah, like traditional ml models. Yeah, exactly.
yeah, like traditional ml models. Yeah, exactly.
yeah, like traditional ml models. Yeah, exactly.
yeah, like traditional ml models. Yeah, exactly.
S Speaker 222:33So vertex AI is a more traditional ML platform. It allows you to, like, let's say, use a Scikit, learn skate, learn kind of model we train ourselves, and then using their platform to, like, deploy it, and then they will have an end point for us, and then we can call the end point again. So that is what Google provides. And there's many ways in which you can train the model and deploy it, but yeah, that's what we used previously. Okay,
So vertex AI is a more traditional ML platform. It allows you to, like, let's say, use a Scikit, learn skate, learn kind of model we train ourselves, and then using their platform to, like, deploy it, and then they will have an end point for us, and then we can call the end point again. So that is what Google provides. And there's many ways in which you can train the model and deploy it, but yeah, that's what we used previously. Okay,
So vertex AI is a more traditional ML platform. It allows you to, like, let's say, use a Scikit, learn skate, learn kind of model we train ourselves, and then using their platform to, like, deploy it, and then they will have an end point for us, and then we can call the end point again. So that is what Google provides. And there's many ways in which you can train the model and deploy it, but yeah, that's what we used previously. Okay,
So vertex AI is a more traditional ML platform. It allows you to, like, let's say, use a Scikit, learn skate, learn kind of model we train ourselves, and then using their platform to, like, deploy it, and then they will have an end point for us, and then we can call the end point again. So that is what Google provides. And there's many ways in which you can train the model and deploy it, but yeah, that's what we used previously. Okay,
23:52I interact with error. This
I interact with error. This
I interact with error. This
I interact with error. This
S Speaker 124:00is also from one of my portfolio companies, which provides these ml tools to startups. The big shift this year has been people started training less and less of their own models, which is based training, right? So, so now you can use either some you know, people go and use open source models and fine tune them for their use cases, or they use companies like refuel, right? And then we're serving up lot of different types of models, whether it could be open source or it could be their own, or could be anthropic or so that's a big, big shift that we're seeing. The other shift that we're seeing is actually in enterprise. If you think about enterprise, in enterprise, especially within the last two quarters, we've seen small language models, which is like, you know, like the models that are getting deployed for, you know, particular use cases, they're getting smaller. And then these smaller models that these are less than, like 30 billion parameter models even, and they are getting really good at solving specific tasks, and also have intelligence, enough intelligence about so what I now, if you were to project that for a year from now, the progress, you know, like in a year, you already have a 30 billion parameter model that is performing much better than a 500 billion parameter model from last year, right? So, if you, if you were to think about so, what that tells you is that the cost, like cost, is not going to be a problem. A year from now, the cost is going to be significantly lower. Like, within a year you can easily, I think, I think it's very hard to predict what the frontier of these llms is going to be, because there's a lot of talk about, oh, pre training has hit a wall, and then Sam tweets, No, it hasn't already hit a wall. And then Dario says, I don't know, and all of that, but I think you can ignore that. I don't know what's going to happen on the frontier, pushing the frontier of llms, but just the models today, you can assume that they will definitely get slightly better, if not really better, compared to the performance today, which is already pretty good, they will definitely get significantly cheaper, even from today. So what that means is you're going to see a lot more in production. You're going to see a lot more, I think, the agentic workflows I have not seen a reliable agentic workflow yet. And I think maybe for agentic workflows, you need a further pushing of I don't know, actually you would know as a practitioner, which is whether it's a better systems design, or whether it's because most of the agentic workflows that I see that are working in production, they're actually not true agentic workflows. They're they're rules. There's like, literally, they define five steps, and then they're rules based, and they call them agentic workflows, for true agentic workflows, such that the model is able to reason on the fly at very high accuracies and very high reliability. I think you would probably need more pushing of the frontier of these large line and then that it's TBD. Who knows? Okay,
is also from one of my portfolio companies, which provides these ml tools to startups. The big shift this year has been people started training less and less of their own models, which is based training, right? So, so now you can use either some you know, people go and use open source models and fine tune them for their use cases, or they use companies like refuel, right? And then we're serving up lot of different types of models, whether it could be open source or it could be their own, or could be anthropic or so that's a big, big shift that we're seeing. The other shift that we're seeing is actually in enterprise. If you think about enterprise, in enterprise, especially within the last two quarters, we've seen small language models, which is like, you know, like the models that are getting deployed for, you know, particular use cases, they're getting smaller. And then these smaller models that these are less than, like 30 billion parameter models even, and they are getting really good at solving specific tasks, and also have intelligence, enough intelligence about so what I now, if you were to project that for a year from now, the progress, you know, like in a year, you already have a 30 billion parameter model that is performing much better than a 500 billion parameter model from last year, right? So, if you, if you were to think about so, what that tells you is that the cost, like cost, is not going to be a problem. A year from now, the cost is going to be significantly lower. Like, within a year you can easily, I think, I think it's very hard to predict what the frontier of these llms is going to be, because there's a lot of talk about, oh, pre training has hit a wall, and then Sam tweets, No, it hasn't already hit a wall. And then Dario says, I don't know, and all of that, but I think you can ignore that. I don't know what's going to happen on the frontier, pushing the frontier of llms, but just the models today, you can assume that they will definitely get slightly better, if not really better, compared to the performance today, which is already pretty good, they will definitely get significantly cheaper, even from today. So what that means is you're going to see a lot more in production. You're going to see a lot more, I think, the agentic workflows I have not seen a reliable agentic workflow yet. And I think maybe for agentic workflows, you need a further pushing of I don't know, actually you would know as a practitioner, which is whether it's a better systems design, or whether it's because most of the agentic workflows that I see that are working in production, they're actually not true agentic workflows. They're they're rules. There's like, literally, they define five steps, and then they're rules based, and they call them agentic workflows, for true agentic workflows, such that the model is able to reason on the fly at very high accuracies and very high reliability. I think you would probably need more pushing of the frontier of these large line and then that it's TBD. Who knows? Okay,
is also from one of my portfolio companies, which provides these ml tools to startups. The big shift this year has been people started training less and less of their own models, which is based training, right? So, so now you can use either some you know, people go and use open source models and fine tune them for their use cases, or they use companies like refuel, right? And then we're serving up lot of different types of models, whether it could be open source or it could be their own, or could be anthropic or so that's a big, big shift that we're seeing. The other shift that we're seeing is actually in enterprise. If you think about enterprise, in enterprise, especially within the last two quarters, we've seen small language models, which is like, you know, like the models that are getting deployed for, you know, particular use cases, they're getting smaller. And then these smaller models that these are less than, like 30 billion parameter models even, and they are getting really good at solving specific tasks, and also have intelligence, enough intelligence about so what I now, if you were to project that for a year from now, the progress, you know, like in a year, you already have a 30 billion parameter model that is performing much better than a 500 billion parameter model from last year, right? So, if you, if you were to think about so, what that tells you is that the cost, like cost, is not going to be a problem. A year from now, the cost is going to be significantly lower. Like, within a year you can easily, I think, I think it's very hard to predict what the frontier of these llms is going to be, because there's a lot of talk about, oh, pre training has hit a wall, and then Sam tweets, No, it hasn't already hit a wall. And then Dario says, I don't know, and all of that, but I think you can ignore that. I don't know what's going to happen on the frontier, pushing the frontier of llms, but just the models today, you can assume that they will definitely get slightly better, if not really better, compared to the performance today, which is already pretty good, they will definitely get significantly cheaper, even from today. So what that means is you're going to see a lot more in production. You're going to see a lot more, I think, the agentic workflows I have not seen a reliable agentic workflow yet. And I think maybe for agentic workflows, you need a further pushing of I don't know, actually you would know as a practitioner, which is whether it's a better systems design, or whether it's because most of the agentic workflows that I see that are working in production, they're actually not true agentic workflows. They're they're rules. There's like, literally, they define five steps, and then they're rules based, and they call them agentic workflows, for true agentic workflows, such that the model is able to reason on the fly at very high accuracies and very high reliability. I think you would probably need more pushing of the frontier of these large line and then that it's TBD. Who knows? Okay,
is also from one of my portfolio companies, which provides these ml tools to startups. The big shift this year has been people started training less and less of their own models, which is based training, right? So, so now you can use either some you know, people go and use open source models and fine tune them for their use cases, or they use companies like refuel, right? And then we're serving up lot of different types of models, whether it could be open source or it could be their own, or could be anthropic or so that's a big, big shift that we're seeing. The other shift that we're seeing is actually in enterprise. If you think about enterprise, in enterprise, especially within the last two quarters, we've seen small language models, which is like, you know, like the models that are getting deployed for, you know, particular use cases, they're getting smaller. And then these smaller models that these are less than, like 30 billion parameter models even, and they are getting really good at solving specific tasks, and also have intelligence, enough intelligence about so what I now, if you were to project that for a year from now, the progress, you know, like in a year, you already have a 30 billion parameter model that is performing much better than a 500 billion parameter model from last year, right? So, if you, if you were to think about so, what that tells you is that the cost, like cost, is not going to be a problem. A year from now, the cost is going to be significantly lower. Like, within a year you can easily, I think, I think it's very hard to predict what the frontier of these llms is going to be, because there's a lot of talk about, oh, pre training has hit a wall, and then Sam tweets, No, it hasn't already hit a wall. And then Dario says, I don't know, and all of that, but I think you can ignore that. I don't know what's going to happen on the frontier, pushing the frontier of llms, but just the models today, you can assume that they will definitely get slightly better, if not really better, compared to the performance today, which is already pretty good, they will definitely get significantly cheaper, even from today. So what that means is you're going to see a lot more in production. You're going to see a lot more, I think, the agentic workflows I have not seen a reliable agentic workflow yet. And I think maybe for agentic workflows, you need a further pushing of I don't know, actually you would know as a practitioner, which is whether it's a better systems design, or whether it's because most of the agentic workflows that I see that are working in production, they're actually not true agentic workflows. They're they're rules. There's like, literally, they define five steps, and then they're rules based, and they call them agentic workflows, for true agentic workflows, such that the model is able to reason on the fly at very high accuracies and very high reliability. I think you would probably need more pushing of the frontier of these large line and then that it's TBD. Who knows? Okay,
27:37yeah, that's interesting.
yeah, that's interesting.
yeah, that's interesting.
yeah, that's interesting.
27:41But thank you. Ken,
S Speaker 227:52Yeah, I would love to connect with you guys on LinkedIn, if you don't mind.
Yeah, I would love to connect with you guys on LinkedIn, if you don't mind.
Yeah, I would love to connect with you guys on LinkedIn, if you don't mind.
Yeah, I would love to connect with you guys on LinkedIn, if you don't mind.
S Speaker 127:55Yeah, let's do that. Absolutely. We're here, sir. Yeah, awesome. Okay, if we can do anything for you, please let us know. Really
Yeah, let's do that. Absolutely. We're here, sir. Yeah, awesome. Okay, if we can do anything for you, please let us know. Really
Yeah, let's do that. Absolutely. We're here, sir. Yeah, awesome. Okay, if we can do anything for you, please let us know. Really
Yeah, let's do that. Absolutely. We're here, sir. Yeah, awesome. Okay, if we can do anything for you, please let us know. Really
28:04happy to learn from you guys too. Okay.
happy to learn from you guys too. Okay.
happy to learn from you guys too. Okay.
happy to learn from you guys too. Okay.