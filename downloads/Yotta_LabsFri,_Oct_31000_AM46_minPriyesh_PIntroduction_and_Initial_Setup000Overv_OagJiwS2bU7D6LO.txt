Meeting: Yotta Labs
Fri, Oct 3
10:00 AM
46 min
Priyesh P
Introduction and Initial Setup
0:00
Overview of Qual
URL: https://otter.ai/u/OagJiwS2bU7D6LOx17_-GSJ7l9A
Downloaded: 2025-12-21T20:11:31.490037
Method: text_extraction
============================================================

0:00You I Hey, Hi, Daniel, hi, Johnny. Can you hear me?
You I Hey, Hi, Daniel, hi, Johnny. Can you hear me?
You I Hey, Hi, Daniel, hi, Johnny. Can you hear me?
You I Hey, Hi, Daniel, hi, Johnny. Can you hear me?
0:50Yeah. Hi, Priyesh, how are you?
Yeah. Hi, Priyesh, how are you?
Yeah. Hi, Priyesh, how are you?
Yeah. Hi, Priyesh, how are you?
0:53Hi, I'm doing great. Where are you calling in from?
Hi, I'm doing great. Where are you calling in from?
Hi, I'm doing great. Where are you calling in from?
Hi, I'm doing great. Where are you calling in from?
S Speaker 10:57I'm calling from palato. Okay, okay. We are based here as well the Qualcomm ventures team, we are in the peninsula, and then some members are in San Diego, where the headquarter is. So maybe the next meeting, we can do it in person.
I'm calling from palato. Okay, okay. We are based here as well the Qualcomm ventures team, we are in the peninsula, and then some members are in San Diego, where the headquarter is. So maybe the next meeting, we can do it in person.
I'm calling from palato. Okay, okay. We are based here as well the Qualcomm ventures team, we are in the peninsula, and then some members are in San Diego, where the headquarter is. So maybe the next meeting, we can do it in person.
I'm calling from palato. Okay, okay. We are based here as well the Qualcomm ventures team, we are in the peninsula, and then some members are in San Diego, where the headquarter is. So maybe the next meeting, we can do it in person.
S Speaker 21:10Yeah, that would be great. So your background is real or virtual?
Yeah, that would be great. So your background is real or virtual?
Yeah, that would be great. So your background is real or virtual?
Yeah, that would be great. So your background is real or virtual?
S Speaker 11:14It's a virtual background. I hope I was working in something like this. It's a virtual background. I use Gemini to create it.
It's a virtual background. I hope I was working in something like this. It's a virtual background. I use Gemini to create it.
It's a virtual background. I hope I was working in something like this. It's a virtual background. I use Gemini to create it.
It's a virtual background. I hope I was working in something like this. It's a virtual background. I use Gemini to create it.
S Speaker 21:23This is a great one, especially the window part. I feel like it's very real somewhere in Santa.
This is a great one, especially the window part. I feel like it's very real somewhere in Santa.
This is a great one, especially the window part. I feel like it's very real somewhere in Santa.
This is a great one, especially the window part. I feel like it's very real somewhere in Santa.
S Speaker 11:31Clara, yeah, looks nice. But, well, Johnny, Daniel, sure, sure, Daniel, we'll just start with some rounds of introduction while you rejoin. So join Daniel pleasure meeting today. Very interesting product I went through the deck so super interesting. I will let you both sort of lead the conversation today. We can keep it more introductory for now, and then I can loop in a few more members of my team who have been very actively looking at AI inferencing. Companies closely so we can we can do an in person meeting with them as well, but we'll keep. meeting today introductory. Would love to understand the team background. How have you been approaching it? What are you building? What is the on what's on the product roadmap, things like that. And then maybe, before we start, happy to give you an intro about Qualcomm ventures, how we work, and even a quick background about myself and so and let me know if you have any specific questions around that, right? So I'm Priyesh. I'm a Senior Associate here with the team. Been with the team for around two years now, actively investing in AI applications and infrastructure. I haven't personally, sort of looked at the inferencing space myself. I've made six investments in the last couple of years, mostly in applications and some in infrastructure, but it's a very interesting space for us, particularly because Qualcomm is also expanding its silicon across different platforms. So we started out as a mobile player. Now we have a chip which goes into PCs. We have chips which go into XR IoT, and then we also now have a cloud inferencing platform called ai 100 so that was launched last year. So very actively looking at the space we are in we are stage agnostic, so we invest anywhere from seed to we have even done pre IPO rounds. And our check sizes are anywhere from two to $15 million depending on the stage and the size of the round. We wouldn't go anywhere below to for seed rounds. I don't think we will do that. But other than that, we try to operate very financially. So we for all of our decisions, we try to look at it from a financial investment perspective. And then at the same time, we don't really need to have a partnership with Qualcomm to make an investment, but we very actively look at any go to market partnerships, any CO product development partnerships, sometimes Qualcomm even becomes a customer. Sometimes the investment becomes a customer for Qualcomm. So we explore all of these paths and try to add value to the investments that we make. Anything specific that I can answer, I think I gave you a quick rundown, but
Clara, yeah, looks nice. But, well, Johnny, Daniel, sure, sure, Daniel, we'll just start with some rounds of introduction while you rejoin. So join Daniel pleasure meeting today. Very interesting product I went through the deck so super interesting. I will let you both sort of lead the conversation today. We can keep it more introductory for now, and then I can loop in a few more members of my team who have been very actively looking at AI inferencing. Companies closely so we can we can do an in person meeting with them as well, but we'll keep. meeting today introductory. Would love to understand the team background. How have you been approaching it? What are you building? What is the on what's on the product roadmap, things like that. And then maybe, before we start, happy to give you an intro about Qualcomm ventures, how we work, and even a quick background about myself and so and let me know if you have any specific questions around that, right? So I'm Priyesh. I'm a Senior Associate here with the team. Been with the team for around two years now, actively investing in AI applications and infrastructure. I haven't personally, sort of looked at the inferencing space myself. I've made six investments in the last couple of years, mostly in applications and some in infrastructure, but it's a very interesting space for us, particularly because Qualcomm is also expanding its silicon across different platforms. So we started out as a mobile player. Now we have a chip which goes into PCs. We have chips which go into XR IoT, and then we also now have a cloud inferencing platform called ai 100 so that was launched last year. So very actively looking at the space we are in we are stage agnostic, so we invest anywhere from seed to we have even done pre IPO rounds. And our check sizes are anywhere from two to $15 million depending on the stage and the size of the round. We wouldn't go anywhere below to for seed rounds. I don't think we will do that. But other than that, we try to operate very financially. So we for all of our decisions, we try to look at it from a financial investment perspective. And then at the same time, we don't really need to have a partnership with Qualcomm to make an investment, but we very actively look at any go to market partnerships, any CO product development partnerships, sometimes Qualcomm even becomes a customer. Sometimes the investment becomes a customer for Qualcomm. So we explore all of these paths and try to add value to the investments that we make. Anything specific that I can answer, I think I gave you a quick rundown, but
Clara, yeah, looks nice. But, well, Johnny, Daniel, sure, sure, Daniel, we'll just start with some rounds of introduction while you rejoin. So join Daniel pleasure meeting today. Very interesting product I went through the deck so super interesting. I will let you both sort of lead the conversation today. We can keep it more introductory for now, and then I can loop in a few more members of my team who have been very actively looking at AI inferencing. Companies closely so we can we can do an in person meeting with them as well, but we'll keep. meeting today introductory. Would love to understand the team background. How have you been approaching it? What are you building? What is the on what's on the product roadmap, things like that. And then maybe, before we start, happy to give you an intro about Qualcomm ventures, how we work, and even a quick background about myself and so and let me know if you have any specific questions around that, right? So I'm Priyesh. I'm a Senior Associate here with the team. Been with the team for around two years now, actively investing in AI applications and infrastructure. I haven't personally, sort of looked at the inferencing space myself. I've made six investments in the last couple of years, mostly in applications and some in infrastructure, but it's a very interesting space for us, particularly because Qualcomm is also expanding its silicon across different platforms. So we started out as a mobile player. Now we have a chip which goes into PCs. We have chips which go into XR IoT, and then we also now have a cloud inferencing platform called ai 100 so that was launched last year. So very actively looking at the space we are in we are stage agnostic, so we invest anywhere from seed to we have even done pre IPO rounds. And our check sizes are anywhere from two to $15 million depending on the stage and the size of the round. We wouldn't go anywhere below to for seed rounds. I don't think we will do that. But other than that, we try to operate very financially. So we for all of our decisions, we try to look at it from a financial investment perspective. And then at the same time, we don't really need to have a partnership with Qualcomm to make an investment, but we very actively look at any go to market partnerships, any CO product development partnerships, sometimes Qualcomm even becomes a customer. Sometimes the investment becomes a customer for Qualcomm. So we explore all of these paths and try to add value to the investments that we make. Anything specific that I can answer, I think I gave you a quick rundown, but
Clara, yeah, looks nice. But, well, Johnny, Daniel, sure, sure, Daniel, we'll just start with some rounds of introduction while you rejoin. So join Daniel pleasure meeting today. Very interesting product I went through the deck so super interesting. I will let you both sort of lead the conversation today. We can keep it more introductory for now, and then I can loop in a few more members of my team who have been very actively looking at AI inferencing. Companies closely so we can we can do an in person meeting with them as well, but we'll keep. meeting today introductory. Would love to understand the team background. How have you been approaching it? What are you building? What is the on what's on the product roadmap, things like that. And then maybe, before we start, happy to give you an intro about Qualcomm ventures, how we work, and even a quick background about myself and so and let me know if you have any specific questions around that, right? So I'm Priyesh. I'm a Senior Associate here with the team. Been with the team for around two years now, actively investing in AI applications and infrastructure. I haven't personally, sort of looked at the inferencing space myself. I've made six investments in the last couple of years, mostly in applications and some in infrastructure, but it's a very interesting space for us, particularly because Qualcomm is also expanding its silicon across different platforms. So we started out as a mobile player. Now we have a chip which goes into PCs. We have chips which go into XR IoT, and then we also now have a cloud inferencing platform called ai 100 so that was launched last year. So very actively looking at the space we are in we are stage agnostic, so we invest anywhere from seed to we have even done pre IPO rounds. And our check sizes are anywhere from two to $15 million depending on the stage and the size of the round. We wouldn't go anywhere below to for seed rounds. I don't think we will do that. But other than that, we try to operate very financially. So we for all of our decisions, we try to look at it from a financial investment perspective. And then at the same time, we don't really need to have a partnership with Qualcomm to make an investment, but we very actively look at any go to market partnerships, any CO product development partnerships, sometimes Qualcomm even becomes a customer. Sometimes the investment becomes a customer for Qualcomm. So we explore all of these paths and try to add value to the investments that we make. Anything specific that I can answer, I think I gave you a quick rundown, but
S Speaker 24:34I think that's very interesting. Maybe one specific question is, do you go What kind of a chip that is in the market that especially whether it's in cloud or on the edge that people use for like, a different workload.
I think that's very interesting. Maybe one specific question is, do you go What kind of a chip that is in the market that especially whether it's in cloud or on the edge that people use for like, a different workload.
I think that's very interesting. Maybe one specific question is, do you go What kind of a chip that is in the market that especially whether it's in cloud or on the edge that people use for like, a different workload.
I think that's very interesting. Maybe one specific question is, do you go What kind of a chip that is in the market that especially whether it's in cloud or on the edge that people use for like, a different workload.
S Speaker 26:09I see, I think that's very interesting. Yeah, as we go into the deck, I think we can see, like, their good potential. Lot of like overlap,
I see, I think that's very interesting. Yeah, as we go into the deck, I think we can see, like, their good potential. Lot of like overlap,
I see, I think that's very interesting. Yeah, as we go into the deck, I think we can see, like, their good potential. Lot of like overlap,
I see, I think that's very interesting. Yeah, as we go into the deck, I think we can see, like, their good potential. Lot of like overlap,
S Speaker 16:19absolutely, absolutely Johnny. And in next call, I will loop in probably Tushar from my team. Tushar is the managing director at Qualcomm ventures, and he is actively working with some of the business units internally building AI chips, so he understands the business units perspective as well, so we can have discussions around partnerships in the next call, for sure.
absolutely, absolutely Johnny. And in next call, I will loop in probably Tushar from my team. Tushar is the managing director at Qualcomm ventures, and he is actively working with some of the business units internally building AI chips, so he understands the business units perspective as well, so we can have discussions around partnerships in the next call, for sure.
absolutely, absolutely Johnny. And in next call, I will loop in probably Tushar from my team. Tushar is the managing director at Qualcomm ventures, and he is actively working with some of the business units internally building AI chips, so he understands the business units perspective as well, so we can have discussions around partnerships in the next call, for sure.
absolutely, absolutely Johnny. And in next call, I will loop in probably Tushar from my team. Tushar is the managing director at Qualcomm ventures, and he is actively working with some of the business units internally building AI chips, so he understands the business units perspective as well, so we can have discussions around partnerships in the next call, for sure.
6:45You asked the exact question I would like to ask,
You asked the exact question I would like to ask,
You asked the exact question I would like to ask,
You asked the exact question I would like to ask,
6:49yeah. I also found the link about the
yeah. I also found the link about the
yeah. I also found the link about the
yeah. I also found the link about the
S Speaker 36:53cloud ai 100 from Qualcomm, yeah. So I think maybe you can start with introducing the team. Let me actually bring up slides, Johnny,
cloud ai 100 from Qualcomm, yeah. So I think maybe you can start with introducing the team. Let me actually bring up slides, Johnny,
cloud ai 100 from Qualcomm, yeah. So I think maybe you can start with introducing the team. Let me actually bring up slides, Johnny,
cloud ai 100 from Qualcomm, yeah. So I think maybe you can start with introducing the team. Let me actually bring up slides, Johnny,
7:06maybe you can go ahead.
maybe you can go ahead.
maybe you can go ahead.
maybe you can go ahead.
S Speaker 27:09Sure, sure. Sure. Yeah. Pies, really honored to meet you. I'm Johnny. I turned the CTO of Yoda lab. How? A few years experience in industry and also in national lab, like, basically, I started my career as HPC engineering researcher in Berkeley National Lab, Berkeley, yeah, and spend four years there, working with DOE, working with like UC Berkeley, and then basically building large scale deep learning at that time, like before deep learning, there was a few years focus on big but big data. So basically we scale up with scale up spark on the supercomputer. And then right in the middle, like, I think, at the beginning of 2019 and then we started to scale up pytorch that time. So basically, we were trying to see how we can use AI for science. That was like between 2015 and 2019 and then after that, like I joined, I started a company, IoT, basically building like a edge sensor and security camera, but also try to figure out how to embed some machine learning model. So one example is like, we have some security camera deployed in the shopping mall, and similar to Amazon, go Amazon store, and then we have a cloud. And then basically, the data is streamed from the edge sensor, from the edge camera, all the way to the cloud, and then we build, like a face recognition and behavior analysis and profile analysis model in the cloud, and then, basically after one day. So like activity or analysis, we send, we send the summary within sometime. We also send the real time notification to the store manager to tell them, hey, you have some customer that they happy about, what kind of product, something like that. So that was like 2019 Yeah. Then, and then pandemic came. And then I, I left the company. I make a joint tick tock. So I spent two years in tick tock, yes, and three years at two to three years at Amazon. So in tick tock, I was the founder of the RE RTC, basically using re, the RE is product from any skill from Berkeley. So I adopted the RE. And then, initially, initially the kuberry, which is the re on Kubernetes, and now it's the most, the most popular open source projecting in the RE community. And then in Tik Tok, basically, I built a re for natural language processing, for both distributed training and inference, and they still running in their back end exactly how they're operating their like latest like ads or Tik Tok recommendation, everything. And then Amazon. So I was a founding member of Amazon Rufus, so I like the team, starting from pre training, like, we build the foundation model from scratch, and then part of the way to, like, evaluation, scaling now and then post training and inference. So before I left, as I'm the roof has already reached 100 million user that's really the foundation model application in the shopping domain, so probably the first and then so far. But there are a lot of competitive open AI, like, if they want to ship them all kinds of, like a shocking feature, right?
Sure, sure. Sure. Yeah. Pies, really honored to meet you. I'm Johnny. I turned the CTO of Yoda lab. How? A few years experience in industry and also in national lab, like, basically, I started my career as HPC engineering researcher in Berkeley National Lab, Berkeley, yeah, and spend four years there, working with DOE, working with like UC Berkeley, and then basically building large scale deep learning at that time, like before deep learning, there was a few years focus on big but big data. So basically we scale up with scale up spark on the supercomputer. And then right in the middle, like, I think, at the beginning of 2019 and then we started to scale up pytorch that time. So basically, we were trying to see how we can use AI for science. That was like between 2015 and 2019 and then after that, like I joined, I started a company, IoT, basically building like a edge sensor and security camera, but also try to figure out how to embed some machine learning model. So one example is like, we have some security camera deployed in the shopping mall, and similar to Amazon, go Amazon store, and then we have a cloud. And then basically, the data is streamed from the edge sensor, from the edge camera, all the way to the cloud, and then we build, like a face recognition and behavior analysis and profile analysis model in the cloud, and then, basically after one day. So like activity or analysis, we send, we send the summary within sometime. We also send the real time notification to the store manager to tell them, hey, you have some customer that they happy about, what kind of product, something like that. So that was like 2019 Yeah. Then, and then pandemic came. And then I, I left the company. I make a joint tick tock. So I spent two years in tick tock, yes, and three years at two to three years at Amazon. So in tick tock, I was the founder of the RE RTC, basically using re, the RE is product from any skill from Berkeley. So I adopted the RE. And then, initially, initially the kuberry, which is the re on Kubernetes, and now it's the most, the most popular open source projecting in the RE community. And then in Tik Tok, basically, I built a re for natural language processing, for both distributed training and inference, and they still running in their back end exactly how they're operating their like latest like ads or Tik Tok recommendation, everything. And then Amazon. So I was a founding member of Amazon Rufus, so I like the team, starting from pre training, like, we build the foundation model from scratch, and then part of the way to, like, evaluation, scaling now and then post training and inference. So before I left, as I'm the roof has already reached 100 million user that's really the foundation model application in the shopping domain, so probably the first and then so far. But there are a lot of competitive open AI, like, if they want to ship them all kinds of, like a shocking feature, right?
Sure, sure. Sure. Yeah. Pies, really honored to meet you. I'm Johnny. I turned the CTO of Yoda lab. How? A few years experience in industry and also in national lab, like, basically, I started my career as HPC engineering researcher in Berkeley National Lab, Berkeley, yeah, and spend four years there, working with DOE, working with like UC Berkeley, and then basically building large scale deep learning at that time, like before deep learning, there was a few years focus on big but big data. So basically we scale up with scale up spark on the supercomputer. And then right in the middle, like, I think, at the beginning of 2019 and then we started to scale up pytorch that time. So basically, we were trying to see how we can use AI for science. That was like between 2015 and 2019 and then after that, like I joined, I started a company, IoT, basically building like a edge sensor and security camera, but also try to figure out how to embed some machine learning model. So one example is like, we have some security camera deployed in the shopping mall, and similar to Amazon, go Amazon store, and then we have a cloud. And then basically, the data is streamed from the edge sensor, from the edge camera, all the way to the cloud, and then we build, like a face recognition and behavior analysis and profile analysis model in the cloud, and then, basically after one day. So like activity or analysis, we send, we send the summary within sometime. We also send the real time notification to the store manager to tell them, hey, you have some customer that they happy about, what kind of product, something like that. So that was like 2019 Yeah. Then, and then pandemic came. And then I, I left the company. I make a joint tick tock. So I spent two years in tick tock, yes, and three years at two to three years at Amazon. So in tick tock, I was the founder of the RE RTC, basically using re, the RE is product from any skill from Berkeley. So I adopted the RE. And then, initially, initially the kuberry, which is the re on Kubernetes, and now it's the most, the most popular open source projecting in the RE community. And then in Tik Tok, basically, I built a re for natural language processing, for both distributed training and inference, and they still running in their back end exactly how they're operating their like latest like ads or Tik Tok recommendation, everything. And then Amazon. So I was a founding member of Amazon Rufus, so I like the team, starting from pre training, like, we build the foundation model from scratch, and then part of the way to, like, evaluation, scaling now and then post training and inference. So before I left, as I'm the roof has already reached 100 million user that's really the foundation model application in the shopping domain, so probably the first and then so far. But there are a lot of competitive open AI, like, if they want to ship them all kinds of, like a shocking feature, right?
Sure, sure. Sure. Yeah. Pies, really honored to meet you. I'm Johnny. I turned the CTO of Yoda lab. How? A few years experience in industry and also in national lab, like, basically, I started my career as HPC engineering researcher in Berkeley National Lab, Berkeley, yeah, and spend four years there, working with DOE, working with like UC Berkeley, and then basically building large scale deep learning at that time, like before deep learning, there was a few years focus on big but big data. So basically we scale up with scale up spark on the supercomputer. And then right in the middle, like, I think, at the beginning of 2019 and then we started to scale up pytorch that time. So basically, we were trying to see how we can use AI for science. That was like between 2015 and 2019 and then after that, like I joined, I started a company, IoT, basically building like a edge sensor and security camera, but also try to figure out how to embed some machine learning model. So one example is like, we have some security camera deployed in the shopping mall, and similar to Amazon, go Amazon store, and then we have a cloud. And then basically, the data is streamed from the edge sensor, from the edge camera, all the way to the cloud, and then we build, like a face recognition and behavior analysis and profile analysis model in the cloud, and then, basically after one day. So like activity or analysis, we send, we send the summary within sometime. We also send the real time notification to the store manager to tell them, hey, you have some customer that they happy about, what kind of product, something like that. So that was like 2019 Yeah. Then, and then pandemic came. And then I, I left the company. I make a joint tick tock. So I spent two years in tick tock, yes, and three years at two to three years at Amazon. So in tick tock, I was the founder of the RE RTC, basically using re, the RE is product from any skill from Berkeley. So I adopted the RE. And then, initially, initially the kuberry, which is the re on Kubernetes, and now it's the most, the most popular open source projecting in the RE community. And then in Tik Tok, basically, I built a re for natural language processing, for both distributed training and inference, and they still running in their back end exactly how they're operating their like latest like ads or Tik Tok recommendation, everything. And then Amazon. So I was a founding member of Amazon Rufus, so I like the team, starting from pre training, like, we build the foundation model from scratch, and then part of the way to, like, evaluation, scaling now and then post training and inference. So before I left, as I'm the roof has already reached 100 million user that's really the foundation model application in the shopping domain, so probably the first and then so far. But there are a lot of competitive open AI, like, if they want to ship them all kinds of, like a shocking feature, right?
S Speaker 110:44It's a gold mine. So I'm sure a lot of companies would get that. Yes, yeah. Do?
It's a gold mine. So I'm sure a lot of companies would get that. Yes, yeah. Do?
It's a gold mine. So I'm sure a lot of companies would get that. Yes, yeah. Do?
It's a gold mine. So I'm sure a lot of companies would get that. Yes, yeah. Do?
10:48Priyesh, thank you. Thank you, Johnny.
Priyesh, thank you. Thank you, Johnny.
Priyesh, thank you. Thank you, Johnny.
Priyesh, thank you. Thank you, Johnny.
13:40Yeah, the right time to be in this exactly.
Yeah, the right time to be in this exactly.
Yeah, the right time to be in this exactly.
Yeah, the right time to be in this exactly.
S Speaker 313:46He's a professor at the UC Merced, he have done a lot of research in AI system and high performance computing. I think he was also research staff at Oak Ridge National Lab. So yeah, so we, like team is actually quite technical, yeah, and I'm probably the least technical people on the team, so they want me to be the CEO. The result of, like, non technical stuff on the business side, we have Jeff joined us. He's a very seasoned GTM sales and with many years of experience. But the most interesting thing is about his plastic service. He used to work at wrong AI and left AI, which for both acquired by a media
He's a professor at the UC Merced, he have done a lot of research in AI system and high performance computing. I think he was also research staff at Oak Ridge National Lab. So yeah, so we, like team is actually quite technical, yeah, and I'm probably the least technical people on the team, so they want me to be the CEO. The result of, like, non technical stuff on the business side, we have Jeff joined us. He's a very seasoned GTM sales and with many years of experience. But the most interesting thing is about his plastic service. He used to work at wrong AI and left AI, which for both acquired by a media
He's a professor at the UC Merced, he have done a lot of research in AI system and high performance computing. I think he was also research staff at Oak Ridge National Lab. So yeah, so we, like team is actually quite technical, yeah, and I'm probably the least technical people on the team, so they want me to be the CEO. The result of, like, non technical stuff on the business side, we have Jeff joined us. He's a very seasoned GTM sales and with many years of experience. But the most interesting thing is about his plastic service. He used to work at wrong AI and left AI, which for both acquired by a media
He's a professor at the UC Merced, he have done a lot of research in AI system and high performance computing. I think he was also research staff at Oak Ridge National Lab. So yeah, so we, like team is actually quite technical, yeah, and I'm probably the least technical people on the team, so they want me to be the CEO. The result of, like, non technical stuff on the business side, we have Jeff joined us. He's a very seasoned GTM sales and with many years of experience. But the most interesting thing is about his plastic service. He used to work at wrong AI and left AI, which for both acquired by a media
S Speaker 114:36so, so, Daniel, a quick question here before we move ahead. So, so how are you thinking about the company? Do you see an acquisition anytime soon? Because I feel the technology you've built. There are multiple acquirers in the in the market today. Do you how are you looking at it? Do you see an acquisition path? Do you see yourself building a very big company, probably targeting an IPO? How are you thinking about it in general?
so, so, Daniel, a quick question here before we move ahead. So, so how are you thinking about the company? Do you see an acquisition anytime soon? Because I feel the technology you've built. There are multiple acquirers in the in the market today. Do you how are you looking at it? Do you see an acquisition path? Do you see yourself building a very big company, probably targeting an IPO? How are you thinking about it in general?
so, so, Daniel, a quick question here before we move ahead. So, so how are you thinking about the company? Do you see an acquisition anytime soon? Because I feel the technology you've built. There are multiple acquirers in the in the market today. Do you how are you looking at it? Do you see an acquisition path? Do you see yourself building a very big company, probably targeting an IPO? How are you thinking about it in general?
so, so, Daniel, a quick question here before we move ahead. So, so how are you thinking about the company? Do you see an acquisition anytime soon? Because I feel the technology you've built. There are multiple acquirers in the in the market today. Do you how are you looking at it? Do you see an acquisition path? Do you see yourself building a very big company, probably targeting an IPO? How are you thinking about it in general?
S Speaker 315:04That's a good question. I personally prefer to not being acquired and listened to earlier, because, like again, as we go through pitch deck, you will see like our vision is that they want to build a solution, the orchestration layer that can be as elastic as possible to support efficient machine learning. And those overalls, like emphasize for us would be on multi silicon, multi cluster, and also multi cloud. So in that case, like we would like to stay as independent, we would like to stay independent as long as possible.
That's a good question. I personally prefer to not being acquired and listened to earlier, because, like again, as we go through pitch deck, you will see like our vision is that they want to build a solution, the orchestration layer that can be as elastic as possible to support efficient machine learning. And those overalls, like emphasize for us would be on multi silicon, multi cluster, and also multi cloud. So in that case, like we would like to stay as independent, we would like to stay independent as long as possible.
That's a good question. I personally prefer to not being acquired and listened to earlier, because, like again, as we go through pitch deck, you will see like our vision is that they want to build a solution, the orchestration layer that can be as elastic as possible to support efficient machine learning. And those overalls, like emphasize for us would be on multi silicon, multi cluster, and also multi cloud. So in that case, like we would like to stay as independent, we would like to stay independent as long as possible.
That's a good question. I personally prefer to not being acquired and listened to earlier, because, like again, as we go through pitch deck, you will see like our vision is that they want to build a solution, the orchestration layer that can be as elastic as possible to support efficient machine learning. And those overalls, like emphasize for us would be on multi silicon, multi cluster, and also multi cloud. So in that case, like we would like to stay as independent, we would like to stay independent as long as possible.
S Speaker 115:39Absolutely get that and and have you raised before? Or is this the first institution around you?
Absolutely get that and and have you raised before? Or is this the first institution around you?
Absolutely get that and and have you raised before? Or is this the first institution around you?
Absolutely get that and and have you raised before? Or is this the first institution around you?
S Speaker 315:47Yeah, we actually, yeah, priyes. Previously, like, we resolved about 3.5
Yeah, we actually, yeah, priyes. Previously, like, we resolved about 3.5
Yeah, we actually, yeah, priyes. Previously, like, we resolved about 3.5
Yeah, we actually, yeah, priyes. Previously, like, we resolved about 3.5
15:54million, and who were the investors for
million, and who were the investors for
million, and who were the investors for
million, and who were the investors for
S Speaker 315:58that deep brain holding and Eaton block at the Mr. Labs. They were actually quite interesting, like decentralized AI. So yeah, they coded this round. They coded the Priyesh round.
that deep brain holding and Eaton block at the Mr. Labs. They were actually quite interesting, like decentralized AI. So yeah, they coded this round. They coded the Priyesh round.
that deep brain holding and Eaton block at the Mr. Labs. They were actually quite interesting, like decentralized AI. So yeah, they coded this round. They coded the Priyesh round.
that deep brain holding and Eaton block at the Mr. Labs. They were actually quite interesting, like decentralized AI. So yeah, they coded this round. They coded the Priyesh round.
16:12Is good, okay, yeah.
S Speaker 316:15So we are, like, we just start actually try to the pitch deck and then try to raise the next round.
So we are, like, we just start actually try to the pitch deck and then try to raise the next round.
So we are, like, we just start actually try to the pitch deck and then try to raise the next round.
So we are, like, we just start actually try to the pitch deck and then try to raise the next round.
16:23Got it? Got it? Daniel,
Got it? Got it? Daniel,
Got it? Got it? Daniel,
Got it? Got it? Daniel,
18:28I think as Qualcomm, we believe in that vision for sure.
I think as Qualcomm, we believe in that vision for sure.
I think as Qualcomm, we believe in that vision for sure.
I think as Qualcomm, we believe in that vision for sure.
S Speaker 318:32Yeah, we also, we also believe so. That's why, like, when you ask any question, like, my answer about, like, whether we are seeking for a choir or going IPO, like, I have to say, like we would try to stay as independent as long
Yeah, we also, we also believe so. That's why, like, when you ask any question, like, my answer about, like, whether we are seeking for a choir or going IPO, like, I have to say, like we would try to stay as independent as long
Yeah, we also, we also believe so. That's why, like, when you ask any question, like, my answer about, like, whether we are seeking for a choir or going IPO, like, I have to say, like we would try to stay as independent as long
Yeah, we also, we also believe so. That's why, like, when you ask any question, like, my answer about, like, whether we are seeking for a choir or going IPO, like, I have to say, like we would try to stay as independent as long
S Speaker 118:48as possible. Make sense? Totally. Yeah.
as possible. Make sense? Totally. Yeah.
as possible. Make sense? Totally. Yeah.
as possible. Make sense? Totally. Yeah.
S Speaker 318:53So in this slides, I think we see, actually, there's a huge paradigm shift in terms of, like, what type of workload that the is the people are caring about in the AI industry, and because of workload, that also because the change of application, which bring actually new opportunity for the infrastructure layer. So one thing is that the pre training is that that's kind of like already, like we already see, like many people actually start focusing on, like doing the reinforcement learning, and then also we see, like, the booming of inference related workload, because of, because of, like, those two changes we actually see we can, we can see that now people, in terms of a application, people studying, care more about the reliability and how they also care about How they can reduce the operational complexity, because, like, they now are focusing on how they can actually deploy the model and leverage model at a scale. At the same time, people started caring about the cost, because the customer see the real value of AI and would like to use them more. But if economically, it doesn't make sense, like the company is basically billions. Won't be sustainable if you basically spend like $1 on inference, but you can only make like 50 cents. Like it's getting very tough for business. So that's why we also see, like, a lot of companies and business, they start like, paying attention about how they can actually reduce insurance costs so that they can make the unique economics actually more reasonable. Yeah. And then the last piece is like we are seeing, like the overall cloud infrastructure actually quite fragmented. And then like companies are paying attention about like, as I having concern about the vendor locking, risk of vendor locking and and the the in general, like, if they can actually easily support multi cloud strategy, it can, it can actually bring them more flexibility in terms of, like, where they can get the GPU to run the workload. It could also give them, actually, the more backing power when they try to negotiate the price with different cloud vendor. Yes, yes. So that's where, why, like, we think, like there's actually huge opportunity that, if we can actually build the orchestration there that can meet the needs for this actually new, trending application from application layer. Yeah. So this is this. In this slide, we would like to quickly talk about the market size of AI compute.
So in this slides, I think we see, actually, there's a huge paradigm shift in terms of, like, what type of workload that the is the people are caring about in the AI industry, and because of workload, that also because the change of application, which bring actually new opportunity for the infrastructure layer. So one thing is that the pre training is that that's kind of like already, like we already see, like many people actually start focusing on, like doing the reinforcement learning, and then also we see, like, the booming of inference related workload, because of, because of, like, those two changes we actually see we can, we can see that now people, in terms of a application, people studying, care more about the reliability and how they also care about How they can reduce the operational complexity, because, like, they now are focusing on how they can actually deploy the model and leverage model at a scale. At the same time, people started caring about the cost, because the customer see the real value of AI and would like to use them more. But if economically, it doesn't make sense, like the company is basically billions. Won't be sustainable if you basically spend like $1 on inference, but you can only make like 50 cents. Like it's getting very tough for business. So that's why we also see, like, a lot of companies and business, they start like, paying attention about how they can actually reduce insurance costs so that they can make the unique economics actually more reasonable. Yeah. And then the last piece is like we are seeing, like the overall cloud infrastructure actually quite fragmented. And then like companies are paying attention about like, as I having concern about the vendor locking, risk of vendor locking and and the the in general, like, if they can actually easily support multi cloud strategy, it can, it can actually bring them more flexibility in terms of, like, where they can get the GPU to run the workload. It could also give them, actually, the more backing power when they try to negotiate the price with different cloud vendor. Yes, yes. So that's where, why, like, we think, like there's actually huge opportunity that, if we can actually build the orchestration there that can meet the needs for this actually new, trending application from application layer. Yeah. So this is this. In this slide, we would like to quickly talk about the market size of AI compute.
So in this slides, I think we see, actually, there's a huge paradigm shift in terms of, like, what type of workload that the is the people are caring about in the AI industry, and because of workload, that also because the change of application, which bring actually new opportunity for the infrastructure layer. So one thing is that the pre training is that that's kind of like already, like we already see, like many people actually start focusing on, like doing the reinforcement learning, and then also we see, like, the booming of inference related workload, because of, because of, like, those two changes we actually see we can, we can see that now people, in terms of a application, people studying, care more about the reliability and how they also care about How they can reduce the operational complexity, because, like, they now are focusing on how they can actually deploy the model and leverage model at a scale. At the same time, people started caring about the cost, because the customer see the real value of AI and would like to use them more. But if economically, it doesn't make sense, like the company is basically billions. Won't be sustainable if you basically spend like $1 on inference, but you can only make like 50 cents. Like it's getting very tough for business. So that's why we also see, like, a lot of companies and business, they start like, paying attention about how they can actually reduce insurance costs so that they can make the unique economics actually more reasonable. Yeah. And then the last piece is like we are seeing, like the overall cloud infrastructure actually quite fragmented. And then like companies are paying attention about like, as I having concern about the vendor locking, risk of vendor locking and and the the in general, like, if they can actually easily support multi cloud strategy, it can, it can actually bring them more flexibility in terms of, like, where they can get the GPU to run the workload. It could also give them, actually, the more backing power when they try to negotiate the price with different cloud vendor. Yes, yes. So that's where, why, like, we think, like there's actually huge opportunity that, if we can actually build the orchestration there that can meet the needs for this actually new, trending application from application layer. Yeah. So this is this. In this slide, we would like to quickly talk about the market size of AI compute.
So in this slides, I think we see, actually, there's a huge paradigm shift in terms of, like, what type of workload that the is the people are caring about in the AI industry, and because of workload, that also because the change of application, which bring actually new opportunity for the infrastructure layer. So one thing is that the pre training is that that's kind of like already, like we already see, like many people actually start focusing on, like doing the reinforcement learning, and then also we see, like, the booming of inference related workload, because of, because of, like, those two changes we actually see we can, we can see that now people, in terms of a application, people studying, care more about the reliability and how they also care about How they can reduce the operational complexity, because, like, they now are focusing on how they can actually deploy the model and leverage model at a scale. At the same time, people started caring about the cost, because the customer see the real value of AI and would like to use them more. But if economically, it doesn't make sense, like the company is basically billions. Won't be sustainable if you basically spend like $1 on inference, but you can only make like 50 cents. Like it's getting very tough for business. So that's why we also see, like, a lot of companies and business, they start like, paying attention about how they can actually reduce insurance costs so that they can make the unique economics actually more reasonable. Yeah. And then the last piece is like we are seeing, like the overall cloud infrastructure actually quite fragmented. And then like companies are paying attention about like, as I having concern about the vendor locking, risk of vendor locking and and the the in general, like, if they can actually easily support multi cloud strategy, it can, it can actually bring them more flexibility in terms of, like, where they can get the GPU to run the workload. It could also give them, actually, the more backing power when they try to negotiate the price with different cloud vendor. Yes, yes. So that's where, why, like, we think, like there's actually huge opportunity that, if we can actually build the orchestration there that can meet the needs for this actually new, trending application from application layer. Yeah. So this is this. In this slide, we would like to quickly talk about the market size of AI compute.
S Speaker 121:39I think we can skip this. Daniel, I agree with this. Everything on the slide for sure. Okay, yeah, even, even this one, I think. Yeah, okay, would love to understand this for sure. Okay, so me to go through these slides. Yeah, this one, this one for sure, if you can dig deeper into it, yes.
I think we can skip this. Daniel, I agree with this. Everything on the slide for sure. Okay, yeah, even, even this one, I think. Yeah, okay, would love to understand this for sure. Okay, so me to go through these slides. Yeah, this one, this one for sure, if you can dig deeper into it, yes.
I think we can skip this. Daniel, I agree with this. Everything on the slide for sure. Okay, yeah, even, even this one, I think. Yeah, okay, would love to understand this for sure. Okay, so me to go through these slides. Yeah, this one, this one for sure, if you can dig deeper into it, yes.
I think we can skip this. Daniel, I agree with this. Everything on the slide for sure. Okay, yeah, even, even this one, I think. Yeah, okay, would love to understand this for sure. Okay, so me to go through these slides. Yeah, this one, this one for sure, if you can dig deeper into it, yes.
S Speaker 322:01So this is, like the vision of what we're actually trying to build, which is a Utah platform at the top level, like they at least actually three type of business model revenue. One is pass where we built the product, including quality, elastic deployment and other products, and we sell both the software and the hardware together. So in that case, like we basically charge my GPU hours. The second business actually is more interesting where we actually allow people to bring their own GPU, or actually use whatever cloud GPUs they have on different cloud, but we would like to use our server solution. So we basically charge them based on how many GPUs. It's under managed by
So this is, like the vision of what we're actually trying to build, which is a Utah platform at the top level, like they at least actually three type of business model revenue. One is pass where we built the product, including quality, elastic deployment and other products, and we sell both the software and the hardware together. So in that case, like we basically charge my GPU hours. The second business actually is more interesting where we actually allow people to bring their own GPU, or actually use whatever cloud GPUs they have on different cloud, but we would like to use our server solution. So we basically charge them based on how many GPUs. It's under managed by
So this is, like the vision of what we're actually trying to build, which is a Utah platform at the top level, like they at least actually three type of business model revenue. One is pass where we built the product, including quality, elastic deployment and other products, and we sell both the software and the hardware together. So in that case, like we basically charge my GPU hours. The second business actually is more interesting where we actually allow people to bring their own GPU, or actually use whatever cloud GPUs they have on different cloud, but we would like to use our server solution. So we basically charge them based on how many GPUs. It's under managed by
So this is, like the vision of what we're actually trying to build, which is a Utah platform at the top level, like they at least actually three type of business model revenue. One is pass where we built the product, including quality, elastic deployment and other products, and we sell both the software and the hardware together. So in that case, like we basically charge my GPU hours. The second business actually is more interesting where we actually allow people to bring their own GPU, or actually use whatever cloud GPUs they have on different cloud, but we would like to use our server solution. So we basically charge them based on how many GPUs. It's under managed by
S Speaker 122:54us. And in this in SaaS Daniel, you can also support on prem GPUs.
us. And in this in SaaS Daniel, you can also support on prem GPUs.
us. And in this in SaaS Daniel, you can also support on prem GPUs.
us. And in this in SaaS Daniel, you can also support on prem GPUs.
S Speaker 125:04And Daniel you, I think the deck mentions that you currently have hosted 1000s of GPUs. What are these models, and where are these hosted?
And Daniel you, I think the deck mentions that you currently have hosted 1000s of GPUs. What are these models, and where are these hosted?
And Daniel you, I think the deck mentions that you currently have hosted 1000s of GPUs. What are these models, and where are these hosted?
And Daniel you, I think the deck mentions that you currently have hosted 1000s of GPUs. What are these models, and where are these hosted?
S Speaker 325:15It's all actually like, it's all like under secure, secure cloud. So we work directly with the data center vendor, including hydro and voltage part, and also like small hosting team, like they have Co Location. So the data center, like we hosting our machine are in like, North Carolina, Texas, Washington,
It's all actually like, it's all like under secure, secure cloud. So we work directly with the data center vendor, including hydro and voltage part, and also like small hosting team, like they have Co Location. So the data center, like we hosting our machine are in like, North Carolina, Texas, Washington,
It's all actually like, it's all like under secure, secure cloud. So we work directly with the data center vendor, including hydro and voltage part, and also like small hosting team, like they have Co Location. So the data center, like we hosting our machine are in like, North Carolina, Texas, Washington,
It's all actually like, it's all like under secure, secure cloud. So we work directly with the data center vendor, including hydro and voltage part, and also like small hosting team, like they have Co Location. So the data center, like we hosting our machine are in like, North Carolina, Texas, Washington,
25:37all in the US for now, right?
all in the US for now, right?
all in the US for now, right?
all in the US for now, right?
S Speaker 325:40Yeah, all in us for now. We do have a vendor in Japan,
Yeah, all in us for now. We do have a vendor in Japan,
Yeah, all in us for now. We do have a vendor in Japan,
Yeah, all in us for now. We do have a vendor in Japan,
S Speaker 125:44okay, okay, got it. And these are all Nvidia h1 hundreds. Or do you have other GPUs as well?
okay, okay, got it. And these are all Nvidia h1 hundreds. Or do you have other GPUs as well?
okay, okay, got it. And these are all Nvidia h1 hundreds. Or do you have other GPUs as well?
okay, okay, got it. And these are all Nvidia h1 hundreds. Or do you have other GPUs as well?
S Speaker 325:51Today we have a video age 100 h2, 100 b2, 100 as well as 5090 Okay, 4090 Yeah. 50 9040, I think that extremely cost efficient for AICc application. Yeah.
Today we have a video age 100 h2, 100 b2, 100 as well as 5090 Okay, 4090 Yeah. 50 9040, I think that extremely cost efficient for AICc application. Yeah.
Today we have a video age 100 h2, 100 b2, 100 as well as 5090 Okay, 4090 Yeah. 50 9040, I think that extremely cost efficient for AICc application. Yeah.
Today we have a video age 100 h2, 100 b2, 100 as well as 5090 Okay, 4090 Yeah. 50 9040, I think that extremely cost efficient for AICc application. Yeah.
S Speaker 126:08So I think for for some of these 5090, 4090s, Daniel, your technology would really shine. Have do you have any benchmarks per se on how, say, across latency, throughput, how are some models based on your software stack running on some of these low cost hardware versus the other ones,
So I think for for some of these 5090, 4090s, Daniel, your technology would really shine. Have do you have any benchmarks per se on how, say, across latency, throughput, how are some models based on your software stack running on some of these low cost hardware versus the other ones,
So I think for for some of these 5090, 4090s, Daniel, your technology would really shine. Have do you have any benchmarks per se on how, say, across latency, throughput, how are some models based on your software stack running on some of these low cost hardware versus the other ones,
So I think for for some of these 5090, 4090s, Daniel, your technology would really shine. Have do you have any benchmarks per se on how, say, across latency, throughput, how are some models based on your software stack running on some of these low cost hardware versus the other ones,
26:32yeah. So I think
S Speaker 127:04Got it. Got it and and so for this customer, a they are using, you would say, 5090s 4090s are they using those for their inference?
Got it. Got it and and so for this customer, a they are using, you would say, 5090s 4090s are they using those for their inference?
Got it. Got it and and so for this customer, a they are using, you would say, 5090s 4090s are they using those for their inference?
Got it. Got it and and so for this customer, a they are using, you would say, 5090s 4090s are they using those for their inference?
S Speaker 327:17Yeah, they are using that for the interest. Interesting. That's pretty interesting, yeah. So they were, they were previously using extra 100, yeah. We work with them and then identify the opportunity. If we can, they can migrate to the like the the new device, and then work with them, hand by hand, to actually migrate the workloads. We're actually quite happy. For instance, based on their Benchmark Number performance wise, 5090, can achieve 70% of performance against hr 100, without doing without doing any optimization, just actually micro the workload and make sure you can run and then. But the cost actually is the difference is actually huge. So that's where like they can actually reduce a lot of cost.
Yeah, they are using that for the interest. Interesting. That's pretty interesting, yeah. So they were, they were previously using extra 100, yeah. We work with them and then identify the opportunity. If we can, they can migrate to the like the the new device, and then work with them, hand by hand, to actually migrate the workloads. We're actually quite happy. For instance, based on their Benchmark Number performance wise, 5090, can achieve 70% of performance against hr 100, without doing without doing any optimization, just actually micro the workload and make sure you can run and then. But the cost actually is the difference is actually huge. So that's where like they can actually reduce a lot of cost.
Yeah, they are using that for the interest. Interesting. That's pretty interesting, yeah. So they were, they were previously using extra 100, yeah. We work with them and then identify the opportunity. If we can, they can migrate to the like the the new device, and then work with them, hand by hand, to actually migrate the workloads. We're actually quite happy. For instance, based on their Benchmark Number performance wise, 5090, can achieve 70% of performance against hr 100, without doing without doing any optimization, just actually micro the workload and make sure you can run and then. But the cost actually is the difference is actually huge. So that's where like they can actually reduce a lot of cost.
Yeah, they are using that for the interest. Interesting. That's pretty interesting, yeah. So they were, they were previously using extra 100, yeah. We work with them and then identify the opportunity. If we can, they can migrate to the like the the new device, and then work with them, hand by hand, to actually migrate the workloads. We're actually quite happy. For instance, based on their Benchmark Number performance wise, 5090, can achieve 70% of performance against hr 100, without doing without doing any optimization, just actually micro the workload and make sure you can run and then. But the cost actually is the difference is actually huge. So that's where like they can actually reduce a lot of cost.
S Speaker 128:04And do you know which, which models say customer a is running today.
And do you know which, which models say customer a is running today.
And do you know which, which models say customer a is running today.
And do you know which, which models say customer a is running today.
S Speaker 328:09They run their own model.
They run their own model.
They run their own model.
They run their own model.
S Speaker 128:14So I would assume these are video image generation models mostly
So I would assume these are video image generation models mostly
So I would assume these are video image generation models mostly
So I would assume these are video image generation models mostly
S Speaker 328:20correct, more particularly, they are like in the upscaling space for image and video. Upscaling for image is not that challenging, but for video, they are probably the only and the best
correct, more particularly, they are like in the upscaling space for image and video. Upscaling for image is not that challenging, but for video, they are probably the only and the best
correct, more particularly, they are like in the upscaling space for image and video. Upscaling for image is not that challenging, but for video, they are probably the only and the best
correct, more particularly, they are like in the upscaling space for image and video. Upscaling for image is not that challenging, but for video, they are probably the only and the best
S Speaker 128:35one for sure. I totally understand, like video. I've looked at a few video companies. All of them have really poor gross margins today, so I imagine some of them can shift to Yoda and probably improve that.
one for sure. I totally understand, like video. I've looked at a few video companies. All of them have really poor gross margins today, so I imagine some of them can shift to Yoda and probably improve that.
one for sure. I totally understand, like video. I've looked at a few video companies. All of them have really poor gross margins today, so I imagine some of them can shift to Yoda and probably improve that.
one for sure. I totally understand, like video. I've looked at a few video companies. All of them have really poor gross margins today, so I imagine some of them can shift to Yoda and probably improve that.
S Speaker 328:49Yeah, they were actually quite happy, yeah, yeah. They told me, like, the next year, like, their target, AI, is about 100
Yeah, they were actually quite happy, yeah, yeah. They told me, like, the next year, like, their target, AI, is about 100
Yeah, they were actually quite happy, yeah, yeah. They told me, like, the next year, like, their target, AI, is about 100
Yeah, they were actually quite happy, yeah, yeah. They told me, like, the next year, like, their target, AI, is about 100
29:00mil interesting. Do you? Do you know, if you are
mil interesting. Do you? Do you know, if you are
mil interesting. Do you? Do you know, if you are
mil interesting. Do you? Do you know, if you are
29:04interested investing them, like, I
interested investing them, like, I
interested investing them, like, I
interested investing them, like, I
29:16confidential, but the name of the company
confidential, but the name of the company
confidential, but the name of the company
confidential, but the name of the company
S Speaker 129:18is Topas labs, topa slabs. Okay, interesting. Very interesting. Daniel, sorry for breaking your flow. Yes, please go ahead.
is Topas labs, topa slabs. Okay, interesting. Very interesting. Daniel, sorry for breaking your flow. Yes, please go ahead.
is Topas labs, topa slabs. Okay, interesting. Very interesting. Daniel, sorry for breaking your flow. Yes, please go ahead.
is Topas labs, topa slabs. Okay, interesting. Very interesting. Daniel, sorry for breaking your flow. Yes, please go ahead.
S Speaker 329:30So I think we probably the community, community car. The reason, like, I would like to, like, build the community cloud, because, like, we see a lot of idle, idle computer resources across different car, but we also see, like, there's a lot of idle GPUs that owned by gamer on Windows desktop, and those actually commodity GPU they couldn't actually host large language model, but they actually, but they actually quite good for diffusion model or AI GC application. And even some of them like have really tiny, tiny GPU memory, different from like 5090 have 32 gigabytes of memory. But with tensor offloading that technology, we can actually leverage the memory space on CPU side so we can offload some of the model, and can actually fit a much larger model into much smaller GPU memory. And then like, we think, like we can, if we can leverage that device, like we can really bring a lot of value by actually enable some enterprise grade workload to run that and the compared to, like, the secure cloud, we think the price would be only like 20 to 30% but again, many enterprise users would have concern to leverage that is less reliable and it's less secure, but there's a late day, but we would like to leave that as an option for this company. They know their workloads. They know like what type of application they have less security and privacy concern, and they can actually run some of those, actually cloud, with a much cheaper choice,
So I think we probably the community, community car. The reason, like, I would like to, like, build the community cloud, because, like, we see a lot of idle, idle computer resources across different car, but we also see, like, there's a lot of idle GPUs that owned by gamer on Windows desktop, and those actually commodity GPU they couldn't actually host large language model, but they actually, but they actually quite good for diffusion model or AI GC application. And even some of them like have really tiny, tiny GPU memory, different from like 5090 have 32 gigabytes of memory. But with tensor offloading that technology, we can actually leverage the memory space on CPU side so we can offload some of the model, and can actually fit a much larger model into much smaller GPU memory. And then like, we think, like we can, if we can leverage that device, like we can really bring a lot of value by actually enable some enterprise grade workload to run that and the compared to, like, the secure cloud, we think the price would be only like 20 to 30% but again, many enterprise users would have concern to leverage that is less reliable and it's less secure, but there's a late day, but we would like to leave that as an option for this company. They know their workloads. They know like what type of application they have less security and privacy concern, and they can actually run some of those, actually cloud, with a much cheaper choice,
So I think we probably the community, community car. The reason, like, I would like to, like, build the community cloud, because, like, we see a lot of idle, idle computer resources across different car, but we also see, like, there's a lot of idle GPUs that owned by gamer on Windows desktop, and those actually commodity GPU they couldn't actually host large language model, but they actually, but they actually quite good for diffusion model or AI GC application. And even some of them like have really tiny, tiny GPU memory, different from like 5090 have 32 gigabytes of memory. But with tensor offloading that technology, we can actually leverage the memory space on CPU side so we can offload some of the model, and can actually fit a much larger model into much smaller GPU memory. And then like, we think, like we can, if we can leverage that device, like we can really bring a lot of value by actually enable some enterprise grade workload to run that and the compared to, like, the secure cloud, we think the price would be only like 20 to 30% but again, many enterprise users would have concern to leverage that is less reliable and it's less secure, but there's a late day, but we would like to leave that as an option for this company. They know their workloads. They know like what type of application they have less security and privacy concern, and they can actually run some of those, actually cloud, with a much cheaper choice,
So I think we probably the community, community car. The reason, like, I would like to, like, build the community cloud, because, like, we see a lot of idle, idle computer resources across different car, but we also see, like, there's a lot of idle GPUs that owned by gamer on Windows desktop, and those actually commodity GPU they couldn't actually host large language model, but they actually, but they actually quite good for diffusion model or AI GC application. And even some of them like have really tiny, tiny GPU memory, different from like 5090 have 32 gigabytes of memory. But with tensor offloading that technology, we can actually leverage the memory space on CPU side so we can offload some of the model, and can actually fit a much larger model into much smaller GPU memory. And then like, we think, like we can, if we can leverage that device, like we can really bring a lot of value by actually enable some enterprise grade workload to run that and the compared to, like, the secure cloud, we think the price would be only like 20 to 30% but again, many enterprise users would have concern to leverage that is less reliable and it's less secure, but there's a late day, but we would like to leave that as an option for this company. They know their workloads. They know like what type of application they have less security and privacy concern, and they can actually run some of those, actually cloud, with a much cheaper choice,
S Speaker 131:18100% Daniel, I agree to that, and especially say maybe not enterprises, but a lot of AI applications, if you abstract away the hardware and you provide it as a SaaS or a mass model, I think a lot, not a lot of them would worry. So totally agree, yes,
100% Daniel, I agree to that, and especially say maybe not enterprises, but a lot of AI applications, if you abstract away the hardware and you provide it as a SaaS or a mass model, I think a lot, not a lot of them would worry. So totally agree, yes,
100% Daniel, I agree to that, and especially say maybe not enterprises, but a lot of AI applications, if you abstract away the hardware and you provide it as a SaaS or a mass model, I think a lot, not a lot of them would worry. So totally agree, yes,
100% Daniel, I agree to that, and especially say maybe not enterprises, but a lot of AI applications, if you abstract away the hardware and you provide it as a SaaS or a mass model, I think a lot, not a lot of them would worry. So totally agree, yes,
S Speaker 331:34yeah, yeah. So the research cloud is actually like interesting idea, like we discussed internally. So if we can actually build a community cloud, leveraging technology that to offer cheaper option to the business and individual developer, why can't we actually leverage the same technology to build a research cloud by looping in like the idle computer resources from natural lab and the universities, who happens to have extra computer capacity, and they use that to serve for the education research purpose, And this is going to be actually quite important, especially for those underrepresented universities and students who have access to advance the AI compute infrastructure.
yeah, yeah. So the research cloud is actually like interesting idea, like we discussed internally. So if we can actually build a community cloud, leveraging technology that to offer cheaper option to the business and individual developer, why can't we actually leverage the same technology to build a research cloud by looping in like the idle computer resources from natural lab and the universities, who happens to have extra computer capacity, and they use that to serve for the education research purpose, And this is going to be actually quite important, especially for those underrepresented universities and students who have access to advance the AI compute infrastructure.
yeah, yeah. So the research cloud is actually like interesting idea, like we discussed internally. So if we can actually build a community cloud, leveraging technology that to offer cheaper option to the business and individual developer, why can't we actually leverage the same technology to build a research cloud by looping in like the idle computer resources from natural lab and the universities, who happens to have extra computer capacity, and they use that to serve for the education research purpose, And this is going to be actually quite important, especially for those underrepresented universities and students who have access to advance the AI compute infrastructure.
yeah, yeah. So the research cloud is actually like interesting idea, like we discussed internally. So if we can actually build a community cloud, leveraging technology that to offer cheaper option to the business and individual developer, why can't we actually leverage the same technology to build a research cloud by looping in like the idle computer resources from natural lab and the universities, who happens to have extra computer capacity, and they use that to serve for the education research purpose, And this is going to be actually quite important, especially for those underrepresented universities and students who have access to advance the AI compute infrastructure.
32:18Totally Yes, yeah.
S Speaker 332:23So yeah, that's, that's like the whole vision the Utah platform right now, like, we only have, like, a tiny piece of that. So we right now, like, in terms of product offering, we have called, we have elastic deployment. We have very simple, like, one click contactation solution. We are building our manage the control plan and trying to actually expand, like, start and expand the business for the model API, the Yeah, and in terms of like ML ops agent like, we actually quite interested to, maybe to develop an agent so that to handle some MMR ml ops and to actually handle some like GPU optimization as well. Got it for the for the computing structure. Right now we are mainly focused on secure cloud. By the end of this year, we would like to really kick off the Community Cloud and and then, like beginning of next year, we would like to also kick off the effort to start building the research cloud. So some of the fundraising is going to actually go into support those two infrastructure, particularly for the research cloud, like we would like to highlight our recent proposal that has been funded by NSF. It's talking about like, how we can build and decentralized, decentralized AI operating system where we can actually aggregate different compute resources. So I think we, this is, like the Phase One funding. So I think if we can actually build the research cloud, we really would like to work with academic research lab to form academic advisor committee, advisory committee, and then we can actually roll out that program. And then, like with that like we may also consider to we will, for sure, to apply the phase two funding.
So yeah, that's, that's like the whole vision the Utah platform right now, like, we only have, like, a tiny piece of that. So we right now, like, in terms of product offering, we have called, we have elastic deployment. We have very simple, like, one click contactation solution. We are building our manage the control plan and trying to actually expand, like, start and expand the business for the model API, the Yeah, and in terms of like ML ops agent like, we actually quite interested to, maybe to develop an agent so that to handle some MMR ml ops and to actually handle some like GPU optimization as well. Got it for the for the computing structure. Right now we are mainly focused on secure cloud. By the end of this year, we would like to really kick off the Community Cloud and and then, like beginning of next year, we would like to also kick off the effort to start building the research cloud. So some of the fundraising is going to actually go into support those two infrastructure, particularly for the research cloud, like we would like to highlight our recent proposal that has been funded by NSF. It's talking about like, how we can build and decentralized, decentralized AI operating system where we can actually aggregate different compute resources. So I think we, this is, like the Phase One funding. So I think if we can actually build the research cloud, we really would like to work with academic research lab to form academic advisor committee, advisory committee, and then we can actually roll out that program. And then, like with that like we may also consider to we will, for sure, to apply the phase two funding.
So yeah, that's, that's like the whole vision the Utah platform right now, like, we only have, like, a tiny piece of that. So we right now, like, in terms of product offering, we have called, we have elastic deployment. We have very simple, like, one click contactation solution. We are building our manage the control plan and trying to actually expand, like, start and expand the business for the model API, the Yeah, and in terms of like ML ops agent like, we actually quite interested to, maybe to develop an agent so that to handle some MMR ml ops and to actually handle some like GPU optimization as well. Got it for the for the computing structure. Right now we are mainly focused on secure cloud. By the end of this year, we would like to really kick off the Community Cloud and and then, like beginning of next year, we would like to also kick off the effort to start building the research cloud. So some of the fundraising is going to actually go into support those two infrastructure, particularly for the research cloud, like we would like to highlight our recent proposal that has been funded by NSF. It's talking about like, how we can build and decentralized, decentralized AI operating system where we can actually aggregate different compute resources. So I think we, this is, like the Phase One funding. So I think if we can actually build the research cloud, we really would like to work with academic research lab to form academic advisor committee, advisory committee, and then we can actually roll out that program. And then, like with that like we may also consider to we will, for sure, to apply the phase two funding.
So yeah, that's, that's like the whole vision the Utah platform right now, like, we only have, like, a tiny piece of that. So we right now, like, in terms of product offering, we have called, we have elastic deployment. We have very simple, like, one click contactation solution. We are building our manage the control plan and trying to actually expand, like, start and expand the business for the model API, the Yeah, and in terms of like ML ops agent like, we actually quite interested to, maybe to develop an agent so that to handle some MMR ml ops and to actually handle some like GPU optimization as well. Got it for the for the computing structure. Right now we are mainly focused on secure cloud. By the end of this year, we would like to really kick off the Community Cloud and and then, like beginning of next year, we would like to also kick off the effort to start building the research cloud. So some of the fundraising is going to actually go into support those two infrastructure, particularly for the research cloud, like we would like to highlight our recent proposal that has been funded by NSF. It's talking about like, how we can build and decentralized, decentralized AI operating system where we can actually aggregate different compute resources. So I think we, this is, like the Phase One funding. So I think if we can actually build the research cloud, we really would like to work with academic research lab to form academic advisor committee, advisory committee, and then we can actually roll out that program. And then, like with that like we may also consider to we will, for sure, to apply the phase two funding.
S Speaker 134:15That's interesting. So for now, you are supporting mostly Nvidia chips. What's your roadmap in terms of adding more chips onto the platform from different vendors?
That's interesting. So for now, you are supporting mostly Nvidia chips. What's your roadmap in terms of adding more chips onto the platform from different vendors?
That's interesting. So for now, you are supporting mostly Nvidia chips. What's your roadmap in terms of adding more chips onto the platform from different vendors?
That's interesting. So for now, you are supporting mostly Nvidia chips. What's your roadmap in terms of adding more chips onto the platform from different vendors?
S Speaker 334:26Yeah, so we are actually working with AMD right now. Okay, so we have a couple of efforts. One of efforts is to like to leverage AMD GPU for reinforcement learning, and joining me, we can actually cover that a little bit. We are also actually participating AMD kernel optimization challenge on the on the on the use case side. We So, for instance, like for Topaz lab and also some of the application, we are actually working with them to basically migrate the workload, because they may be, they mainly actually on using pytorch. So we're also migrating some workloads on AMD GPU, yeah, so that, if they are just, if you're using the platform, it's going to be actually, GPU agnostic,
Yeah, so we are actually working with AMD right now. Okay, so we have a couple of efforts. One of efforts is to like to leverage AMD GPU for reinforcement learning, and joining me, we can actually cover that a little bit. We are also actually participating AMD kernel optimization challenge on the on the on the use case side. We So, for instance, like for Topaz lab and also some of the application, we are actually working with them to basically migrate the workload, because they may be, they mainly actually on using pytorch. So we're also migrating some workloads on AMD GPU, yeah, so that, if they are just, if you're using the platform, it's going to be actually, GPU agnostic,
Yeah, so we are actually working with AMD right now. Okay, so we have a couple of efforts. One of efforts is to like to leverage AMD GPU for reinforcement learning, and joining me, we can actually cover that a little bit. We are also actually participating AMD kernel optimization challenge on the on the on the use case side. We So, for instance, like for Topaz lab and also some of the application, we are actually working with them to basically migrate the workload, because they may be, they mainly actually on using pytorch. So we're also migrating some workloads on AMD GPU, yeah, so that, if they are just, if you're using the platform, it's going to be actually, GPU agnostic,
Yeah, so we are actually working with AMD right now. Okay, so we have a couple of efforts. One of efforts is to like to leverage AMD GPU for reinforcement learning, and joining me, we can actually cover that a little bit. We are also actually participating AMD kernel optimization challenge on the on the on the use case side. We So, for instance, like for Topaz lab and also some of the application, we are actually working with them to basically migrate the workload, because they may be, they mainly actually on using pytorch. So we're also migrating some workloads on AMD GPU, yeah, so that, if they are just, if you're using the platform, it's going to be actually, GPU agnostic,
35:17interesting. And for
S Speaker 135:20Daniel, for some of your customers, how do they what's the model that they are using today? And do they care if or say, what level of abstraction Are they okay with? Do they care what hardware you use? Or are they sort of just looking at the end metrics?
Daniel, for some of your customers, how do they what's the model that they are using today? And do they care if or say, what level of abstraction Are they okay with? Do they care what hardware you use? Or are they sort of just looking at the end metrics?
Daniel, for some of your customers, how do they what's the model that they are using today? And do they care if or say, what level of abstraction Are they okay with? Do they care what hardware you use? Or are they sort of just looking at the end metrics?
Daniel, for some of your customers, how do they what's the model that they are using today? And do they care if or say, what level of abstraction Are they okay with? Do they care what hardware you use? Or are they sort of just looking at the end metrics?
35:41That's a good question. So
That's a good question. So
That's a good question. So
That's a good question. So
36:54on right now. Understood, understood,
on right now. Understood, understood,
on right now. Understood, understood,
on right now. Understood, understood,
S Speaker 336:55yeah, we are not actually, we haven't actually started actually working on the support for different AI chips, but we would like to actually at least learn some like successful story with AMD, and then maybe pick up a few AI chips that both interestingly, like in terms of business model, and also architecturally interesting to start with,
yeah, we are not actually, we haven't actually started actually working on the support for different AI chips, but we would like to actually at least learn some like successful story with AMD, and then maybe pick up a few AI chips that both interestingly, like in terms of business model, and also architecturally interesting to start with,
yeah, we are not actually, we haven't actually started actually working on the support for different AI chips, but we would like to actually at least learn some like successful story with AMD, and then maybe pick up a few AI chips that both interestingly, like in terms of business model, and also architecturally interesting to start with,
yeah, we are not actually, we haven't actually started actually working on the support for different AI chips, but we would like to actually at least learn some like successful story with AMD, and then maybe pick up a few AI chips that both interestingly, like in terms of business model, and also architecturally interesting to start with,
37:20yeah, yeah. For sure, absolutely, yeah.
yeah, yeah. For sure, absolutely, yeah.
yeah, yeah. For sure, absolutely, yeah.
yeah, yeah. For sure, absolutely, yeah.
S Speaker 337:26So this is, like a product category with some except that this is like traction they have beginning
So this is, like a product category with some except that this is like traction they have beginning
So this is, like a product category with some except that this is like traction they have beginning
So this is, like a product category with some except that this is like traction they have beginning
37:39of this year. Okay, yeah, sorry, sorry, please go ahead.
of this year. Okay, yeah, sorry, sorry, please go ahead.
of this year. Okay, yeah, sorry, sorry, please go ahead.
of this year. Okay, yeah, sorry, sorry, please go ahead.
S Speaker 337:45We start actually reading the platform in the beginning of this year. So this is like what we achieve within six to seven months.
We start actually reading the platform in the beginning of this year. So this is like what we achieve within six to seven months.
We start actually reading the platform in the beginning of this year. So this is like what we achieve within six to seven months.
We start actually reading the platform in the beginning of this year. So this is like what we achieve within six to seven months.
S Speaker 137:55And Daniel here, what do you mean by sales contract versus arr? How are you calculating those
And Daniel here, what do you mean by sales contract versus arr? How are you calculating those
And Daniel here, what do you mean by sales contract versus arr? How are you calculating those
And Daniel here, what do you mean by sales contract versus arr? How are you calculating those
S Speaker 338:01good question? So sales contract that means. So for instance, someone customer, they basically like sign extreme hours contract using our GPU and the solution. So, like the sales contract, that means, like, all the value in each of the contract to add together AI was more of, like, adjusted AI based on the monthly income right for the for the invoice, the amount, it's about a million including both paid and unpaid.
good question? So sales contract that means. So for instance, someone customer, they basically like sign extreme hours contract using our GPU and the solution. So, like the sales contract, that means, like, all the value in each of the contract to add together AI was more of, like, adjusted AI based on the monthly income right for the for the invoice, the amount, it's about a million including both paid and unpaid.
good question? So sales contract that means. So for instance, someone customer, they basically like sign extreme hours contract using our GPU and the solution. So, like the sales contract, that means, like, all the value in each of the contract to add together AI was more of, like, adjusted AI based on the monthly income right for the for the invoice, the amount, it's about a million including both paid and unpaid.
good question? So sales contract that means. So for instance, someone customer, they basically like sign extreme hours contract using our GPU and the solution. So, like the sales contract, that means, like, all the value in each of the contract to add together AI was more of, like, adjusted AI based on the monthly income right for the for the invoice, the amount, it's about a million including both paid and unpaid.
S Speaker 138:30Yeah, yeah, understood. And Daniel, for for your current customers, what's the gross margin you are hitting right now?
Yeah, yeah, understood. And Daniel, for for your current customers, what's the gross margin you are hitting right now?
Yeah, yeah, understood. And Daniel, for for your current customers, what's the gross margin you are hitting right now?
Yeah, yeah, understood. And Daniel, for for your current customers, what's the gross margin you are hitting right now?
S Speaker 338:38Yeah, so that's good question. Depends on the case, because for two bit business, it's always actually case by case. Yeah, so the gross margin right now is 10% to 30% so which is a like, okay, for like pass. That's also why we want to actually move, start, actually build and expanding the SAS business, because that ghost margin is probably much higher. Probably we're about 50 to 60% understood.
Yeah, so that's good question. Depends on the case, because for two bit business, it's always actually case by case. Yeah, so the gross margin right now is 10% to 30% so which is a like, okay, for like pass. That's also why we want to actually move, start, actually build and expanding the SAS business, because that ghost margin is probably much higher. Probably we're about 50 to 60% understood.
Yeah, so that's good question. Depends on the case, because for two bit business, it's always actually case by case. Yeah, so the gross margin right now is 10% to 30% so which is a like, okay, for like pass. That's also why we want to actually move, start, actually build and expanding the SAS business, because that ghost margin is probably much higher. Probably we're about 50 to 60% understood.
Yeah, so that's good question. Depends on the case, because for two bit business, it's always actually case by case. Yeah, so the gross margin right now is 10% to 30% so which is a like, okay, for like pass. That's also why we want to actually move, start, actually build and expanding the SAS business, because that ghost margin is probably much higher. Probably we're about 50 to 60% understood.
S Speaker 139:07Then that's like for some of the cloud inferencing players like together AI fireworks that we have spoken to, gross margin seems to be the biggest limiting factor. I believe, since you have, say, a quantization, quantization play, you have your running AI workloads on different, say, lower end hardware per se, do you see a path of your gross margins expanding in the future? Like you mentioned, from pass to SaaS, you will increase from 30 to 60. And what do you see for mass kind of a model?
Then that's like for some of the cloud inferencing players like together AI fireworks that we have spoken to, gross margin seems to be the biggest limiting factor. I believe, since you have, say, a quantization, quantization play, you have your running AI workloads on different, say, lower end hardware per se, do you see a path of your gross margins expanding in the future? Like you mentioned, from pass to SaaS, you will increase from 30 to 60. And what do you see for mass kind of a model?
Then that's like for some of the cloud inferencing players like together AI fireworks that we have spoken to, gross margin seems to be the biggest limiting factor. I believe, since you have, say, a quantization, quantization play, you have your running AI workloads on different, say, lower end hardware per se, do you see a path of your gross margins expanding in the future? Like you mentioned, from pass to SaaS, you will increase from 30 to 60. And what do you see for mass kind of a model?
Then that's like for some of the cloud inferencing players like together AI fireworks that we have spoken to, gross margin seems to be the biggest limiting factor. I believe, since you have, say, a quantization, quantization play, you have your running AI workloads on different, say, lower end hardware per se, do you see a path of your gross margins expanding in the future? Like you mentioned, from pass to SaaS, you will increase from 30 to 60. And what do you see for mass kind of a model?
S Speaker 339:49So basically, like between past SAS and mass, the range of gross margin are actually different, like SAS and the mass, they are very similar to each other, yeah. But, but for mass, it also depends on whether we are entering a competition with other people by hosting the base model, or we actually have our own priority model that we specifically train and tune for a certain like high frequency cases. But they're actually small model, so those two actually have very different growth margin, right? So, yeah. So for instance, like we noticed, like there are some scenario that, if, especially in video standing there are some, some of tasks are really fundamental, and we are quite interested to actually train some small model. Can can do that and also beat the performance large model. And in that case, like, our growth margin is going to be much higher than just a hosting base model plus a laurel adapter, because that's actually very heated, not kind of a very heated
So basically, like between past SAS and mass, the range of gross margin are actually different, like SAS and the mass, they are very similar to each other, yeah. But, but for mass, it also depends on whether we are entering a competition with other people by hosting the base model, or we actually have our own priority model that we specifically train and tune for a certain like high frequency cases. But they're actually small model, so those two actually have very different growth margin, right? So, yeah. So for instance, like we noticed, like there are some scenario that, if, especially in video standing there are some, some of tasks are really fundamental, and we are quite interested to actually train some small model. Can can do that and also beat the performance large model. And in that case, like, our growth margin is going to be much higher than just a hosting base model plus a laurel adapter, because that's actually very heated, not kind of a very heated
So basically, like between past SAS and mass, the range of gross margin are actually different, like SAS and the mass, they are very similar to each other, yeah. But, but for mass, it also depends on whether we are entering a competition with other people by hosting the base model, or we actually have our own priority model that we specifically train and tune for a certain like high frequency cases. But they're actually small model, so those two actually have very different growth margin, right? So, yeah. So for instance, like we noticed, like there are some scenario that, if, especially in video standing there are some, some of tasks are really fundamental, and we are quite interested to actually train some small model. Can can do that and also beat the performance large model. And in that case, like, our growth margin is going to be much higher than just a hosting base model plus a laurel adapter, because that's actually very heated, not kind of a very heated
So basically, like between past SAS and mass, the range of gross margin are actually different, like SAS and the mass, they are very similar to each other, yeah. But, but for mass, it also depends on whether we are entering a competition with other people by hosting the base model, or we actually have our own priority model that we specifically train and tune for a certain like high frequency cases. But they're actually small model, so those two actually have very different growth margin, right? So, yeah. So for instance, like we noticed, like there are some scenario that, if, especially in video standing there are some, some of tasks are really fundamental, and we are quite interested to actually train some small model. Can can do that and also beat the performance large model. And in that case, like, our growth margin is going to be much higher than just a hosting base model plus a laurel adapter, because that's actually very heated, not kind of a very heated
41:00competition, Understood, understood,
competition, Understood, understood,
competition, Understood, understood,
competition, Understood, understood,
S Speaker 141:51Got it. Got it. Daniel and I know Daniel, we're coming close to our time. A couple of quick questions, and then I'll follow up with a few more questions over email, and then set up something with with Tushar as well by mid next week, where, where are you in the OR, let's, let's first dig into traction. You mentioned you're at 5 million run rate today. Where do you plan to end up this year, and how does your pipeline look like?
Got it. Got it. Daniel and I know Daniel, we're coming close to our time. A couple of quick questions, and then I'll follow up with a few more questions over email, and then set up something with with Tushar as well by mid next week, where, where are you in the OR, let's, let's first dig into traction. You mentioned you're at 5 million run rate today. Where do you plan to end up this year, and how does your pipeline look like?
Got it. Got it. Daniel and I know Daniel, we're coming close to our time. A couple of quick questions, and then I'll follow up with a few more questions over email, and then set up something with with Tushar as well by mid next week, where, where are you in the OR, let's, let's first dig into traction. You mentioned you're at 5 million run rate today. Where do you plan to end up this year, and how does your pipeline look like?
Got it. Got it. Daniel and I know Daniel, we're coming close to our time. A couple of quick questions, and then I'll follow up with a few more questions over email, and then set up something with with Tushar as well by mid next week, where, where are you in the OR, let's, let's first dig into traction. You mentioned you're at 5 million run rate today. Where do you plan to end up this year, and how does your pipeline look like?
S Speaker 342:26Yeah, yeah. So I think by the end of this year, like we hope, we can actually hit, AI wise, hit eight to 10 million. Yeah, we have a couple of customer in pipeline. So some actually from the aigc Tik Tok this, which was very they were in the space, which is very similar to what actually has been just released by open AI, right? Yeah, there's also, we're also onboarding some of the company that who are actually building, like hiring agent, so they're actually doing a lot of PII data, and that's why, like, we're also working on SOC two, we hope, like, once we can talk to it's the conversation with customer will be much easier. Yeah, and also the elastic deployment right now, like, we only support the past model, where you can only deploy on our GPU and data center, we are going to actually enable, like, we're going to actually kick off the partnership with nebis. So like customer who already have a GPU on navios, will be able to leverage that the elastic deployment to deploy the workload on nebulas directly. So we hope, like that would also unblock some of the very interesting business business cases and and then we're going to expand that to support more, ranging from new cloud to AWS and hyperscaler.
Yeah, yeah. So I think by the end of this year, like we hope, we can actually hit, AI wise, hit eight to 10 million. Yeah, we have a couple of customer in pipeline. So some actually from the aigc Tik Tok this, which was very they were in the space, which is very similar to what actually has been just released by open AI, right? Yeah, there's also, we're also onboarding some of the company that who are actually building, like hiring agent, so they're actually doing a lot of PII data, and that's why, like, we're also working on SOC two, we hope, like, once we can talk to it's the conversation with customer will be much easier. Yeah, and also the elastic deployment right now, like, we only support the past model, where you can only deploy on our GPU and data center, we are going to actually enable, like, we're going to actually kick off the partnership with nebis. So like customer who already have a GPU on navios, will be able to leverage that the elastic deployment to deploy the workload on nebulas directly. So we hope, like that would also unblock some of the very interesting business business cases and and then we're going to expand that to support more, ranging from new cloud to AWS and hyperscaler.
Yeah, yeah. So I think by the end of this year, like we hope, we can actually hit, AI wise, hit eight to 10 million. Yeah, we have a couple of customer in pipeline. So some actually from the aigc Tik Tok this, which was very they were in the space, which is very similar to what actually has been just released by open AI, right? Yeah, there's also, we're also onboarding some of the company that who are actually building, like hiring agent, so they're actually doing a lot of PII data, and that's why, like, we're also working on SOC two, we hope, like, once we can talk to it's the conversation with customer will be much easier. Yeah, and also the elastic deployment right now, like, we only support the past model, where you can only deploy on our GPU and data center, we are going to actually enable, like, we're going to actually kick off the partnership with nebis. So like customer who already have a GPU on navios, will be able to leverage that the elastic deployment to deploy the workload on nebulas directly. So we hope, like that would also unblock some of the very interesting business business cases and and then we're going to expand that to support more, ranging from new cloud to AWS and hyperscaler.
Yeah, yeah. So I think by the end of this year, like we hope, we can actually hit, AI wise, hit eight to 10 million. Yeah, we have a couple of customer in pipeline. So some actually from the aigc Tik Tok this, which was very they were in the space, which is very similar to what actually has been just released by open AI, right? Yeah, there's also, we're also onboarding some of the company that who are actually building, like hiring agent, so they're actually doing a lot of PII data, and that's why, like, we're also working on SOC two, we hope, like, once we can talk to it's the conversation with customer will be much easier. Yeah, and also the elastic deployment right now, like, we only support the past model, where you can only deploy on our GPU and data center, we are going to actually enable, like, we're going to actually kick off the partnership with nebis. So like customer who already have a GPU on navios, will be able to leverage that the elastic deployment to deploy the workload on nebulas directly. So we hope, like that would also unblock some of the very interesting business business cases and and then we're going to expand that to support more, ranging from new cloud to AWS and hyperscaler.
S Speaker 143:52Super interesting. Daniel, the video company you mentioned. Is it more of a foundation model company or more of an application company? Just trying to get a sense of the customers you have,
Super interesting. Daniel, the video company you mentioned. Is it more of a foundation model company or more of an application company? Just trying to get a sense of the customers you have,
Super interesting. Daniel, the video company you mentioned. Is it more of a foundation model company or more of an application company? Just trying to get a sense of the customers you have,
Super interesting. Daniel, the video company you mentioned. Is it more of a foundation model company or more of an application company? Just trying to get a sense of the customers you have,
S Speaker 344:05application, application company, okay? Like the AICc, Tik Tok, right?
application, application company, okay? Like the AICc, Tik Tok, right?
application, application company, okay? Like the AICc, Tik Tok, right?
application, application company, okay? Like the AICc, Tik Tok, right?
44:10Yeah. Application company got
Yeah. Application company got
Yeah. Application company got
Yeah. Application company got
S Speaker 144:13it, got it. And where are you in the process of fundraising? Daniel, I believe you started a couple of weeks back. And what's the timeline you're looking at to close this?
it, got it. And where are you in the process of fundraising? Daniel, I believe you started a couple of weeks back. And what's the timeline you're looking at to close this?
it, got it. And where are you in the process of fundraising? Daniel, I believe you started a couple of weeks back. And what's the timeline you're looking at to close this?
it, got it. And where are you in the process of fundraising? Daniel, I believe you started a couple of weeks back. And what's the timeline you're looking at to close this?
S Speaker 344:27That's a good question. We started actually two to three weeks ago, and we're actually talking to investor. So goal is like, maybe close by either end of so basically, before Thanksgiving,
That's a good question. We started actually two to three weeks ago, and we're actually talking to investor. So goal is like, maybe close by either end of so basically, before Thanksgiving,
That's a good question. We started actually two to three weeks ago, and we're actually talking to investor. So goal is like, maybe close by either end of so basically, before Thanksgiving,
That's a good question. We started actually two to three weeks ago, and we're actually talking to investor. So goal is like, maybe close by either end of so basically, before Thanksgiving,
45:10Okay, fantastic. Thank you so much. Thanks.
Okay, fantastic. Thank you so much. Thanks.
Okay, fantastic. Thank you so much. Thanks.
Okay, fantastic. Thank you so much. Thanks.
S Speaker 145:12Thanks, guys, super interesting. Let's stay in touch, and maybe, if I'll also mention, if we can do the next meeting in person, that would be great.
Thanks, guys, super interesting. Let's stay in touch, and maybe, if I'll also mention, if we can do the next meeting in person, that would be great.
Thanks, guys, super interesting. Let's stay in touch, and maybe, if I'll also mention, if we can do the next meeting in person, that would be great.
Thanks, guys, super interesting. Let's stay in touch, and maybe, if I'll also mention, if we can do the next meeting in person, that would be great.
45:22Yeah, I think you can
Yeah, I think you can
Yeah, I think you can
Yeah, I think you can
S Speaker 345:25maybe meet with Johnny in person. And if I can also fly
maybe meet with Johnny in person. And if I can also fly
maybe meet with Johnny in person. And if I can also fly
maybe meet with Johnny in person. And if I can also fly
S Speaker 145:31to the Bay Area. Oh, I didn't know you. So you're based in Seattle, and where, where, where the other team members best based.
to the Bay Area. Oh, I didn't know you. So you're based in Seattle, and where, where, where the other team members best based.
to the Bay Area. Oh, I didn't know you. So you're based in Seattle, and where, where, where the other team members best based.
to the Bay Area. Oh, I didn't know you. So you're based in Seattle, and where, where, where the other team members best based.
S Speaker 345:38So Johnny is in the Bay Area. UC Merced,
So Johnny is in the Bay Area. UC Merced,
So Johnny is in the Bay Area. UC Merced,
So Johnny is in the Bay Area. UC Merced,
S Speaker 145:42okay, okay. Understood. No worries. No worries. Daniel. Sounds good. Thanks a lot for this. I appreciate the discussion today and then let me get back.
okay, okay. Understood. No worries. No worries. Daniel. Sounds good. Thanks a lot for this. I appreciate the discussion today and then let me get back.
okay, okay. Understood. No worries. No worries. Daniel. Sounds good. Thanks a lot for this. I appreciate the discussion today and then let me get back.
okay, okay. Understood. No worries. No worries. Daniel. Sounds good. Thanks a lot for this. I appreciate the discussion today and then let me get back.
45:53Okay, thank you so much. Thanks for
Okay, thank you so much. Thanks for
Okay, thank you so much. Thanks for
Okay, thank you so much. Thanks for
45:57your time. Thanks guys. You.
your time. Thanks guys. You.
your time. Thanks guys. You.
your time. Thanks guys. You.