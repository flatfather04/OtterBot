Meeting: AWS Bedrock Demo Day
Tue, Mar 18
6:04 PM
1 h 9 min
Priyesh P
AWS Bedrock Demo Day Introduction and O
URL: https://otter.ai/u/GxC4q2x4gmxP-bTV_Jm8omTaOx8
Downloaded: 2025-12-22T12:36:44.110313
Method: text_extraction
============================================================

S Speaker 10:00Introduce himself, but we're going to go through this. A lot of this here is all generated with Gen AI. He'll talk about some, I'll talk about another, and then we'll be here afterwards, if you want to ask any questions or dive any deeper on that episode.
Introduce himself, but we're going to go through this. A lot of this here is all generated with Gen AI. He'll talk about some, I'll talk about another, and then we'll be here afterwards, if you want to ask any questions or dive any deeper on that episode.
Introduce himself, but we're going to go through this. A lot of this here is all generated with Gen AI. He'll talk about some, I'll talk about another, and then we'll be here afterwards, if you want to ask any questions or dive any deeper on that episode.
Introduce himself, but we're going to go through this. A lot of this here is all generated with Gen AI. He'll talk about some, I'll talk about another, and then we'll be here afterwards, if you want to ask any questions or dive any deeper on that episode.
S Speaker 20:14Thanks. Pat. Hi everyone. I'm Fred. I'm based in France. I'm a solution architect manager, and I've worked with gaming customers, and many customers too. And we have built this demo for GDC for a solution area in the game tech environment, which is called player acquisition, engagement, monetization, where we have seen when the content is personalized, we have more engagement for players. So let's run the demo. So the idea is
Thanks. Pat. Hi everyone. I'm Fred. I'm based in France. I'm a solution architect manager, and I've worked with gaming customers, and many customers too. And we have built this demo for GDC for a solution area in the game tech environment, which is called player acquisition, engagement, monetization, where we have seen when the content is personalized, we have more engagement for players. So let's run the demo. So the idea is
Thanks. Pat. Hi everyone. I'm Fred. I'm based in France. I'm a solution architect manager, and I've worked with gaming customers, and many customers too. And we have built this demo for GDC for a solution area in the game tech environment, which is called player acquisition, engagement, monetization, where we have seen when the content is personalized, we have more engagement for players. So let's run the demo. So the idea is
Thanks. Pat. Hi everyone. I'm Fred. I'm based in France. I'm a solution architect manager, and I've worked with gaming customers, and many customers too. And we have built this demo for GDC for a solution area in the game tech environment, which is called player acquisition, engagement, monetization, where we have seen when the content is personalized, we have more engagement for players. So let's run the demo. So the idea is
0:47that three things keep in mind.
that three things keep in mind.
that three things keep in mind.
that three things keep in mind.
1:01when you have a playable ad, can drive more engagement
when you have a playable ad, can drive more engagement
when you have a playable ad, can drive more engagement
when you have a playable ad, can drive more engagement
S Speaker 21:06Sorry about that. So the first objective is to drive more engagement, more let's say engagement and click and acquisition. So they are used to create playable ads, which are very small games you can interact with. And it has been shown that it drives more more acquisition conversion. Okay, that's the first thing. And so building those games can be quite costly, and the idea is to use Gen AI to speed up the creation of those playable ads. Okay, so who wants to try the game? Can be here. So just, yeah. So we just create a team, the pure plan writes versus job, very generic. So you click, click on the button. Is it working? So the egg? Oh, no, so let's do it without the pad. So let's start. So the images has been generated with bedrock and Nova the video too. Okay. And then when you create advertising campaign, you need to, well, we want to personalize the ads, and for that, we get some data from the advertising companies, the DSP in generals, and well, here we're going to fake that by choosing the favorite game jar. So let's choose, for example, RPGs. Okay. Want to shoot parallel now. Yeah.
Sorry about that. So the first objective is to drive more engagement, more let's say engagement and click and acquisition. So they are used to create playable ads, which are very small games you can interact with. And it has been shown that it drives more more acquisition conversion. Okay, that's the first thing. And so building those games can be quite costly, and the idea is to use Gen AI to speed up the creation of those playable ads. Okay, so who wants to try the game? Can be here. So just, yeah. So we just create a team, the pure plan writes versus job, very generic. So you click, click on the button. Is it working? So the egg? Oh, no, so let's do it without the pad. So let's start. So the images has been generated with bedrock and Nova the video too. Okay. And then when you create advertising campaign, you need to, well, we want to personalize the ads, and for that, we get some data from the advertising companies, the DSP in generals, and well, here we're going to fake that by choosing the favorite game jar. So let's choose, for example, RPGs. Okay. Want to shoot parallel now. Yeah.
Sorry about that. So the first objective is to drive more engagement, more let's say engagement and click and acquisition. So they are used to create playable ads, which are very small games you can interact with. And it has been shown that it drives more more acquisition conversion. Okay, that's the first thing. And so building those games can be quite costly, and the idea is to use Gen AI to speed up the creation of those playable ads. Okay, so who wants to try the game? Can be here. So just, yeah. So we just create a team, the pure plan writes versus job, very generic. So you click, click on the button. Is it working? So the egg? Oh, no, so let's do it without the pad. So let's start. So the images has been generated with bedrock and Nova the video too. Okay. And then when you create advertising campaign, you need to, well, we want to personalize the ads, and for that, we get some data from the advertising companies, the DSP in generals, and well, here we're going to fake that by choosing the favorite game jar. So let's choose, for example, RPGs. Okay. Want to shoot parallel now. Yeah.
Sorry about that. So the first objective is to drive more engagement, more let's say engagement and click and acquisition. So they are used to create playable ads, which are very small games you can interact with. And it has been shown that it drives more more acquisition conversion. Okay, that's the first thing. And so building those games can be quite costly, and the idea is to use Gen AI to speed up the creation of those playable ads. Okay, so who wants to try the game? Can be here. So just, yeah. So we just create a team, the pure plan writes versus job, very generic. So you click, click on the button. Is it working? So the egg? Oh, no, so let's do it without the pad. So let's start. So the images has been generated with bedrock and Nova the video too. Okay. And then when you create advertising campaign, you need to, well, we want to personalize the ads, and for that, we get some data from the advertising companies, the DSP in generals, and well, here we're going to fake that by choosing the favorite game jar. So let's choose, for example, RPGs. Okay. Want to shoot parallel now. Yeah.
S Speaker 23:13Thanks that. And we have, we have seen customers using that to create in game assistant to to just interact and ask what they could what's the next thing they should do in the game? So, very popular and very useful next. So why don't we choose your very Gucci game? So I have to play,
Thanks that. And we have, we have seen customers using that to create in game assistant to to just interact and ask what they could what's the next thing they should do in the game? So, very popular and very useful next. So why don't we choose your very Gucci game? So I have to play,
Thanks that. And we have, we have seen customers using that to create in game assistant to to just interact and ask what they could what's the next thing they should do in the game? So, very popular and very useful next. So why don't we choose your very Gucci game? So I have to play,
Thanks that. And we have, we have seen customers using that to create in game assistant to to just interact and ask what they could what's the next thing they should do in the game? So, very popular and very useful next. So why don't we choose your very Gucci game? So I have to play,
S Speaker 33:33I have to squash some charts here, and I have a try. I'll tell you. Okay, and
I have to squash some charts here, and I have a try. I'll tell you. Okay, and
I have to squash some charts here, and I have a try. I'll tell you. Okay, and
I have to squash some charts here, and I have a try. I'll tell you. Okay, and
3:40if I'm doing well, the power
if I'm doing well, the power
if I'm doing well, the power
if I'm doing well, the power
S Speaker 33:43to curve me. Now let's go, okay, so you play it just be, then when I die.
to curve me. Now let's go, okay, so you play it just be, then when I die.
to curve me. Now let's go, okay, so you play it just be, then when I die.
to curve me. Now let's go, okay, so you play it just be, then when I die.
S Speaker 13:54The parent that you just saw is also Gen AI. The parent you just saw is also Gen AI. So it's happening in real time. It's taking a leaderboard, it's taking how many, how many sharks you killed, and it's telling you, back in that parrot voice, how many you know you killed it, where you are on that leaderboard. And when the game
The parent that you just saw is also Gen AI. The parent you just saw is also Gen AI. So it's happening in real time. It's taking a leaderboard, it's taking how many, how many sharks you killed, and it's telling you, back in that parrot voice, how many you know you killed it, where you are on that leaderboard. And when the game
The parent that you just saw is also Gen AI. The parent you just saw is also Gen AI. So it's happening in real time. It's taking a leaderboard, it's taking how many, how many sharks you killed, and it's telling you, back in that parrot voice, how many you know you killed it, where you are on that leaderboard. And when the game
The parent that you just saw is also Gen AI. The parent you just saw is also Gen AI. So it's happening in real time. It's taking a leaderboard, it's taking how many, how many sharks you killed, and it's telling you, back in that parrot voice, how many you know you killed it, where you are on that leaderboard. And when the game
S Speaker 24:13happens, you have to choose, it's a test on the demo booth. The player has to choose an ad. Let's choose this one. Okay? And luckily, it shows a personalized app. It has been generated dynamically with some input data. Here we are faking the data, but we can imagine anything you can add to generate something personalized. Okay? And imagine that you have a dozen of dimensions of your Player Profile. You can add that and personalize the app from the region they are coming from anything else, okay, you can imagine. And here, well, we showcase that personalized ad app, they are forming better than the non personalized ad, okay? And the last thing we've done is also generating a small video. Is no very real, okay, that's not dynamically generated is done offline, because it's taking a bit of time to generate that, but in few minutes, you can have something more animated, more engaging, that will drive more conversion on your playable app, for example. And also, here there is a quick test over for voiceover messaging, which also describe what you're gonna have if you click on the I want to try it again, but then stuff like that. Okay, so all the content you've seen has been generated, like Gen AI, even the code, I think that's very important to mention, because you're not seeing it. But for example, the pie chart you've seen with using an game engine, which is called Godo. It's open source, and we don't have a bunch of libraries to display statistics in those games, and here, in few minutes, I generated pie charts animated and without needing to have an extra layer library or to drive the component available on the shelf. So everything generated with Gen AI,
happens, you have to choose, it's a test on the demo booth. The player has to choose an ad. Let's choose this one. Okay? And luckily, it shows a personalized app. It has been generated dynamically with some input data. Here we are faking the data, but we can imagine anything you can add to generate something personalized. Okay? And imagine that you have a dozen of dimensions of your Player Profile. You can add that and personalize the app from the region they are coming from anything else, okay, you can imagine. And here, well, we showcase that personalized ad app, they are forming better than the non personalized ad, okay? And the last thing we've done is also generating a small video. Is no very real, okay, that's not dynamically generated is done offline, because it's taking a bit of time to generate that, but in few minutes, you can have something more animated, more engaging, that will drive more conversion on your playable app, for example. And also, here there is a quick test over for voiceover messaging, which also describe what you're gonna have if you click on the I want to try it again, but then stuff like that. Okay, so all the content you've seen has been generated, like Gen AI, even the code, I think that's very important to mention, because you're not seeing it. But for example, the pie chart you've seen with using an game engine, which is called Godo. It's open source, and we don't have a bunch of libraries to display statistics in those games, and here, in few minutes, I generated pie charts animated and without needing to have an extra layer library or to drive the component available on the shelf. So everything generated with Gen AI,
happens, you have to choose, it's a test on the demo booth. The player has to choose an ad. Let's choose this one. Okay? And luckily, it shows a personalized app. It has been generated dynamically with some input data. Here we are faking the data, but we can imagine anything you can add to generate something personalized. Okay? And imagine that you have a dozen of dimensions of your Player Profile. You can add that and personalize the app from the region they are coming from anything else, okay, you can imagine. And here, well, we showcase that personalized ad app, they are forming better than the non personalized ad, okay? And the last thing we've done is also generating a small video. Is no very real, okay, that's not dynamically generated is done offline, because it's taking a bit of time to generate that, but in few minutes, you can have something more animated, more engaging, that will drive more conversion on your playable app, for example. And also, here there is a quick test over for voiceover messaging, which also describe what you're gonna have if you click on the I want to try it again, but then stuff like that. Okay, so all the content you've seen has been generated, like Gen AI, even the code, I think that's very important to mention, because you're not seeing it. But for example, the pie chart you've seen with using an game engine, which is called Godo. It's open source, and we don't have a bunch of libraries to display statistics in those games, and here, in few minutes, I generated pie charts animated and without needing to have an extra layer library or to drive the component available on the shelf. So everything generated with Gen AI,
happens, you have to choose, it's a test on the demo booth. The player has to choose an ad. Let's choose this one. Okay? And luckily, it shows a personalized app. It has been generated dynamically with some input data. Here we are faking the data, but we can imagine anything you can add to generate something personalized. Okay? And imagine that you have a dozen of dimensions of your Player Profile. You can add that and personalize the app from the region they are coming from anything else, okay, you can imagine. And here, well, we showcase that personalized ad app, they are forming better than the non personalized ad, okay? And the last thing we've done is also generating a small video. Is no very real, okay, that's not dynamically generated is done offline, because it's taking a bit of time to generate that, but in few minutes, you can have something more animated, more engaging, that will drive more conversion on your playable app, for example. And also, here there is a quick test over for voiceover messaging, which also describe what you're gonna have if you click on the I want to try it again, but then stuff like that. Okay, so all the content you've seen has been generated, like Gen AI, even the code, I think that's very important to mention, because you're not seeing it. But for example, the pie chart you've seen with using an game engine, which is called Godo. It's open source, and we don't have a bunch of libraries to display statistics in those games, and here, in few minutes, I generated pie charts animated and without needing to have an extra layer library or to drive the component available on the shelf. So everything generated with Gen AI,
S Speaker 26:40back. But yeah, yeah. Nice, maybe Nice. Sorry. Last thing, the three assets you've seen that been also generated by AI. That's not with network yet, but we have done that in sagemaker with moles on the shelf, taking with hugging face containers. So everything is available in the US, and you can do nearly all the chain of asset for games with gender right now, coincidence for playable right now, but we are seeing a lot of great deals at GDC too. So, yeah, yes, if you have time, anyone have questions?
back. But yeah, yeah. Nice, maybe Nice. Sorry. Last thing, the three assets you've seen that been also generated by AI. That's not with network yet, but we have done that in sagemaker with moles on the shelf, taking with hugging face containers. So everything is available in the US, and you can do nearly all the chain of asset for games with gender right now, coincidence for playable right now, but we are seeing a lot of great deals at GDC too. So, yeah, yes, if you have time, anyone have questions?
back. But yeah, yeah. Nice, maybe Nice. Sorry. Last thing, the three assets you've seen that been also generated by AI. That's not with network yet, but we have done that in sagemaker with moles on the shelf, taking with hugging face containers. So everything is available in the US, and you can do nearly all the chain of asset for games with gender right now, coincidence for playable right now, but we are seeing a lot of great deals at GDC too. So, yeah, yes, if you have time, anyone have questions?
back. But yeah, yeah. Nice, maybe Nice. Sorry. Last thing, the three assets you've seen that been also generated by AI. That's not with network yet, but we have done that in sagemaker with moles on the shelf, taking with hugging face containers. So everything is available in the US, and you can do nearly all the chain of asset for games with gender right now, coincidence for playable right now, but we are seeing a lot of great deals at GDC too. So, yeah, yes, if you have time, anyone have questions?
S Speaker 17:13Yeah, Q and A, anybody have any questions about this question? How are you loading the asset together? Are there any differences for doing that to make sure it doesn't
Yeah, Q and A, anybody have any questions about this question? How are you loading the asset together? Are there any differences for doing that to make sure it doesn't
Yeah, Q and A, anybody have any questions about this question? How are you loading the asset together? Are there any differences for doing that to make sure it doesn't
Yeah, Q and A, anybody have any questions about this question? How are you loading the asset together? Are there any differences for doing that to make sure it doesn't
S Speaker 27:27take long. So, so yeah, you notice that there is a practice time when you have to experiment to test the gameplay. We use that time to generate the images. It's, in fact, it's kind of, we are faking it a little bit, and it takes about 10 seconds to generate that. But imagine that. Well, you can see four, four advising company. Imagine that you want to generate different personalized ads. You can imagine you have a cache somewhere. You can want the cache and generate the well, you won't have an infinite amount of dimensions from your playable files, so you can generate that live and without the need of doing manual stuff and redeploying stuff. It can be done live. And if you change the player profile in engineering, we can do that live and with well tragedy, the cost of producing those that momentum.
take long. So, so yeah, you notice that there is a practice time when you have to experiment to test the gameplay. We use that time to generate the images. It's, in fact, it's kind of, we are faking it a little bit, and it takes about 10 seconds to generate that. But imagine that. Well, you can see four, four advising company. Imagine that you want to generate different personalized ads. You can imagine you have a cache somewhere. You can want the cache and generate the well, you won't have an infinite amount of dimensions from your playable files, so you can generate that live and without the need of doing manual stuff and redeploying stuff. It can be done live. And if you change the player profile in engineering, we can do that live and with well tragedy, the cost of producing those that momentum.
take long. So, so yeah, you notice that there is a practice time when you have to experiment to test the gameplay. We use that time to generate the images. It's, in fact, it's kind of, we are faking it a little bit, and it takes about 10 seconds to generate that. But imagine that. Well, you can see four, four advising company. Imagine that you want to generate different personalized ads. You can imagine you have a cache somewhere. You can want the cache and generate the well, you won't have an infinite amount of dimensions from your playable files, so you can generate that live and without the need of doing manual stuff and redeploying stuff. It can be done live. And if you change the player profile in engineering, we can do that live and with well tragedy, the cost of producing those that momentum.
take long. So, so yeah, you notice that there is a practice time when you have to experiment to test the gameplay. We use that time to generate the images. It's, in fact, it's kind of, we are faking it a little bit, and it takes about 10 seconds to generate that. But imagine that. Well, you can see four, four advising company. Imagine that you want to generate different personalized ads. You can imagine you have a cache somewhere. You can want the cache and generate the well, you won't have an infinite amount of dimensions from your playable files, so you can generate that live and without the need of doing manual stuff and redeploying stuff. It can be done live. And if you change the player profile in engineering, we can do that live and with well tragedy, the cost of producing those that momentum.
8:13Any other questions,
S Speaker 28:20do we have a benchmark for image generation? What do you mean exactly? Latency, quality depends what you want to render, obviously. Well, we have to notice that dopa is having some baud rates to prevent generating some IP protected stuff. Okay. Well, you can be blocked if the content is showing something with the eyepiece productions. But on the on the performance side, I think it's so mobile, about, well, 10 seconds in general, depending on So, on the size you want. So it may vary on the quality. I think we are, well, depends on what you want exactly. So if you have benchmark, I think it's more up to you to benchmark the quality mode and compare what's the output of Nova, real, Nova, sorry, Canvas versus other models.
do we have a benchmark for image generation? What do you mean exactly? Latency, quality depends what you want to render, obviously. Well, we have to notice that dopa is having some baud rates to prevent generating some IP protected stuff. Okay. Well, you can be blocked if the content is showing something with the eyepiece productions. But on the on the performance side, I think it's so mobile, about, well, 10 seconds in general, depending on So, on the size you want. So it may vary on the quality. I think we are, well, depends on what you want exactly. So if you have benchmark, I think it's more up to you to benchmark the quality mode and compare what's the output of Nova, real, Nova, sorry, Canvas versus other models.
do we have a benchmark for image generation? What do you mean exactly? Latency, quality depends what you want to render, obviously. Well, we have to notice that dopa is having some baud rates to prevent generating some IP protected stuff. Okay. Well, you can be blocked if the content is showing something with the eyepiece productions. But on the on the performance side, I think it's so mobile, about, well, 10 seconds in general, depending on So, on the size you want. So it may vary on the quality. I think we are, well, depends on what you want exactly. So if you have benchmark, I think it's more up to you to benchmark the quality mode and compare what's the output of Nova, real, Nova, sorry, Canvas versus other models.
do we have a benchmark for image generation? What do you mean exactly? Latency, quality depends what you want to render, obviously. Well, we have to notice that dopa is having some baud rates to prevent generating some IP protected stuff. Okay. Well, you can be blocked if the content is showing something with the eyepiece productions. But on the on the performance side, I think it's so mobile, about, well, 10 seconds in general, depending on So, on the size you want. So it may vary on the quality. I think we are, well, depends on what you want exactly. So if you have benchmark, I think it's more up to you to benchmark the quality mode and compare what's the output of Nova, real, Nova, sorry, Canvas versus other models.
9:16So for image, Nova Canva,
So for image, Nova Canva,
So for image, Nova Canva,
So for image, Nova Canva,
S Speaker 29:19for video, Nova real and for, for D, we have used the moles on again, face, I think the 10 cent. I don't remember the name of the model, 10 cent model, well, they are available on, I think we can show that technology after the delivery. And no, so what?
for video, Nova real and for, for D, we have used the moles on again, face, I think the 10 cent. I don't remember the name of the model, 10 cent model, well, they are available on, I think we can show that technology after the delivery. And no, so what?
for video, Nova real and for, for D, we have used the moles on again, face, I think the 10 cent. I don't remember the name of the model, 10 cent model, well, they are available on, I think we can show that technology after the delivery. And no, so what?
for video, Nova real and for, for D, we have used the moles on again, face, I think the 10 cent. I don't remember the name of the model, 10 cent model, well, they are available on, I think we can show that technology after the delivery. And no, so what?
9:38Let's get the pad and PETA render plus.
Let's get the pad and PETA render plus.
Let's get the pad and PETA render plus.
Let's get the pad and PETA render plus.
9:48So next up, we have Jeff with codian.
So next up, we have Jeff with codian.
So next up, we have Jeff with codian.
So next up, we have Jeff with codian.
S Speaker 49:58All right, can everyone see this? Let's see, hey, I was told to stop by here and just like, meet with some AWS folks. I had no idea there'd be, like, hundreds of people in the room looking at a demo. But okay, let's, let's go ahead. So my name is Jeff. I'm from Codd. We're the makers of Windsor. How many of you people actually have heard of Windsor or have used Windsor? That's a pretty, pretty good amount. I used to come to these meetings and like, nobody raised their hands. So it's a code editor. I know this is not the most exciting thing. We're going to like write some code together, but in the spirit of gaming and all that, I pulled open a gaming repo. But I really don't know what this repo does, so I'm just going to ask the chat, we call it cascade, what is going on with this repo. So one sec,
All right, can everyone see this? Let's see, hey, I was told to stop by here and just like, meet with some AWS folks. I had no idea there'd be, like, hundreds of people in the room looking at a demo. But okay, let's, let's go ahead. So my name is Jeff. I'm from Codd. We're the makers of Windsor. How many of you people actually have heard of Windsor or have used Windsor? That's a pretty, pretty good amount. I used to come to these meetings and like, nobody raised their hands. So it's a code editor. I know this is not the most exciting thing. We're going to like write some code together, but in the spirit of gaming and all that, I pulled open a gaming repo. But I really don't know what this repo does, so I'm just going to ask the chat, we call it cascade, what is going on with this repo. So one sec,
All right, can everyone see this? Let's see, hey, I was told to stop by here and just like, meet with some AWS folks. I had no idea there'd be, like, hundreds of people in the room looking at a demo. But okay, let's, let's go ahead. So my name is Jeff. I'm from Codd. We're the makers of Windsor. How many of you people actually have heard of Windsor or have used Windsor? That's a pretty, pretty good amount. I used to come to these meetings and like, nobody raised their hands. So it's a code editor. I know this is not the most exciting thing. We're going to like write some code together, but in the spirit of gaming and all that, I pulled open a gaming repo. But I really don't know what this repo does, so I'm just going to ask the chat, we call it cascade, what is going on with this repo. So one sec,
All right, can everyone see this? Let's see, hey, I was told to stop by here and just like, meet with some AWS folks. I had no idea there'd be, like, hundreds of people in the room looking at a demo. But okay, let's, let's go ahead. So my name is Jeff. I'm from Codd. We're the makers of Windsor. How many of you people actually have heard of Windsor or have used Windsor? That's a pretty, pretty good amount. I used to come to these meetings and like, nobody raised their hands. So it's a code editor. I know this is not the most exciting thing. We're going to like write some code together, but in the spirit of gaming and all that, I pulled open a gaming repo. But I really don't know what this repo does, so I'm just going to ask the chat, we call it cascade, what is going on with this repo. So one sec,
S Speaker 410:44typed reopen. Let's see. Well, it still knew what I was talking about. I'll summarize this repository. So what it's doing now is like looking at the folder and reading the reading and actually ripping through the entire code base. So what we have is technology we call Riptide. We actually take the entire code base and run a parallel LLM call across the whole thing. This is very different than the rag and embeddings kind of technology people have done like a year ago, right? If you're still using rag, that is like so 2023 but this is because there's a lot of data that we have to go and a lot of latency that we were trying to prevent, right? So to rip across the entire code base, read through it, run LLM calls on all of it. It takes a lot of effort, a lot of infrastructure expertise, and it's already starting to kind of like, look at the files and give me a whole summary of what is going on, right? So this is a game, again, in the spirit of the GDC happening over here, and explaining what's going on, right? So that's kind of the first kind of demo, which is like, hey, like, explain this entire code base to me, even if it's like hundreds of 1000s or even millions of lines of code, right? And one thing that let's see developers probably don't like to do is, well, who actually likes writing unit tests and debugging all day? Probably no hands are up. OK, that's kind of what I expected. So let's kind of just ask this to do that for us one sec. All right. So I typed, run all js tests, and it's gonna say, Okay, let's run these for you. And you'll notice, actually, like, there is an ability to run commands too. So this is another like difference of our product versus a lot of these other coding agents. I can actually, it actually prompts me if you want to run NPM test, and I run NPM test, and it says, Okay, look at it. There's actually one failed test here, right? And the question is like, well, let's see what's wrong with this, right? So it's now it's analyzing the file to where the test failed, and it's like, maybe, maybe it'll ask me to fix it. Let's see. This is all non deterministic, so let's see what happens here. And it's gone ahead, and it's automatically fixing the file for me, so you see that, right? All that was just from me asking a question, like, Hey, can you run this? The this unit test, and it's already gone through and fixed a bug that it found, all right, so there's a fire alarm in the back. We'll just keep going through this. Let's do a little more of a fun a fun example here with with windsurf. Let's kind of build an app from scratch here. So let's I have this, like, empty, empty folder here. It has the bedrock logo and the codium logo. Let's kind of ask it to just create a web app from scratch. All right. So any, any ideas, by the way, anyone want to throw out a cool idea for this? Maybe, like, partnership was the best demo, maybe tonight. All right, I'll just, I'll just write
typed reopen. Let's see. Well, it still knew what I was talking about. I'll summarize this repository. So what it's doing now is like looking at the folder and reading the reading and actually ripping through the entire code base. So what we have is technology we call Riptide. We actually take the entire code base and run a parallel LLM call across the whole thing. This is very different than the rag and embeddings kind of technology people have done like a year ago, right? If you're still using rag, that is like so 2023 but this is because there's a lot of data that we have to go and a lot of latency that we were trying to prevent, right? So to rip across the entire code base, read through it, run LLM calls on all of it. It takes a lot of effort, a lot of infrastructure expertise, and it's already starting to kind of like, look at the files and give me a whole summary of what is going on, right? So this is a game, again, in the spirit of the GDC happening over here, and explaining what's going on, right? So that's kind of the first kind of demo, which is like, hey, like, explain this entire code base to me, even if it's like hundreds of 1000s or even millions of lines of code, right? And one thing that let's see developers probably don't like to do is, well, who actually likes writing unit tests and debugging all day? Probably no hands are up. OK, that's kind of what I expected. So let's kind of just ask this to do that for us one sec. All right. So I typed, run all js tests, and it's gonna say, Okay, let's run these for you. And you'll notice, actually, like, there is an ability to run commands too. So this is another like difference of our product versus a lot of these other coding agents. I can actually, it actually prompts me if you want to run NPM test, and I run NPM test, and it says, Okay, look at it. There's actually one failed test here, right? And the question is like, well, let's see what's wrong with this, right? So it's now it's analyzing the file to where the test failed, and it's like, maybe, maybe it'll ask me to fix it. Let's see. This is all non deterministic, so let's see what happens here. And it's gone ahead, and it's automatically fixing the file for me, so you see that, right? All that was just from me asking a question, like, Hey, can you run this? The this unit test, and it's already gone through and fixed a bug that it found, all right, so there's a fire alarm in the back. We'll just keep going through this. Let's do a little more of a fun a fun example here with with windsurf. Let's kind of build an app from scratch here. So let's I have this, like, empty, empty folder here. It has the bedrock logo and the codium logo. Let's kind of ask it to just create a web app from scratch. All right. So any, any ideas, by the way, anyone want to throw out a cool idea for this? Maybe, like, partnership was the best demo, maybe tonight. All right, I'll just, I'll just write
typed reopen. Let's see. Well, it still knew what I was talking about. I'll summarize this repository. So what it's doing now is like looking at the folder and reading the reading and actually ripping through the entire code base. So what we have is technology we call Riptide. We actually take the entire code base and run a parallel LLM call across the whole thing. This is very different than the rag and embeddings kind of technology people have done like a year ago, right? If you're still using rag, that is like so 2023 but this is because there's a lot of data that we have to go and a lot of latency that we were trying to prevent, right? So to rip across the entire code base, read through it, run LLM calls on all of it. It takes a lot of effort, a lot of infrastructure expertise, and it's already starting to kind of like, look at the files and give me a whole summary of what is going on, right? So this is a game, again, in the spirit of the GDC happening over here, and explaining what's going on, right? So that's kind of the first kind of demo, which is like, hey, like, explain this entire code base to me, even if it's like hundreds of 1000s or even millions of lines of code, right? And one thing that let's see developers probably don't like to do is, well, who actually likes writing unit tests and debugging all day? Probably no hands are up. OK, that's kind of what I expected. So let's kind of just ask this to do that for us one sec. All right. So I typed, run all js tests, and it's gonna say, Okay, let's run these for you. And you'll notice, actually, like, there is an ability to run commands too. So this is another like difference of our product versus a lot of these other coding agents. I can actually, it actually prompts me if you want to run NPM test, and I run NPM test, and it says, Okay, look at it. There's actually one failed test here, right? And the question is like, well, let's see what's wrong with this, right? So it's now it's analyzing the file to where the test failed, and it's like, maybe, maybe it'll ask me to fix it. Let's see. This is all non deterministic, so let's see what happens here. And it's gone ahead, and it's automatically fixing the file for me, so you see that, right? All that was just from me asking a question, like, Hey, can you run this? The this unit test, and it's already gone through and fixed a bug that it found, all right, so there's a fire alarm in the back. We'll just keep going through this. Let's do a little more of a fun a fun example here with with windsurf. Let's kind of build an app from scratch here. So let's I have this, like, empty, empty folder here. It has the bedrock logo and the codium logo. Let's kind of ask it to just create a web app from scratch. All right. So any, any ideas, by the way, anyone want to throw out a cool idea for this? Maybe, like, partnership was the best demo, maybe tonight. All right, I'll just, I'll just write
typed reopen. Let's see. Well, it still knew what I was talking about. I'll summarize this repository. So what it's doing now is like looking at the folder and reading the reading and actually ripping through the entire code base. So what we have is technology we call Riptide. We actually take the entire code base and run a parallel LLM call across the whole thing. This is very different than the rag and embeddings kind of technology people have done like a year ago, right? If you're still using rag, that is like so 2023 but this is because there's a lot of data that we have to go and a lot of latency that we were trying to prevent, right? So to rip across the entire code base, read through it, run LLM calls on all of it. It takes a lot of effort, a lot of infrastructure expertise, and it's already starting to kind of like, look at the files and give me a whole summary of what is going on, right? So this is a game, again, in the spirit of the GDC happening over here, and explaining what's going on, right? So that's kind of the first kind of demo, which is like, hey, like, explain this entire code base to me, even if it's like hundreds of 1000s or even millions of lines of code, right? And one thing that let's see developers probably don't like to do is, well, who actually likes writing unit tests and debugging all day? Probably no hands are up. OK, that's kind of what I expected. So let's kind of just ask this to do that for us one sec. All right. So I typed, run all js tests, and it's gonna say, Okay, let's run these for you. And you'll notice, actually, like, there is an ability to run commands too. So this is another like difference of our product versus a lot of these other coding agents. I can actually, it actually prompts me if you want to run NPM test, and I run NPM test, and it says, Okay, look at it. There's actually one failed test here, right? And the question is like, well, let's see what's wrong with this, right? So it's now it's analyzing the file to where the test failed, and it's like, maybe, maybe it'll ask me to fix it. Let's see. This is all non deterministic, so let's see what happens here. And it's gone ahead, and it's automatically fixing the file for me, so you see that, right? All that was just from me asking a question, like, Hey, can you run this? The this unit test, and it's already gone through and fixed a bug that it found, all right, so there's a fire alarm in the back. We'll just keep going through this. Let's do a little more of a fun a fun example here with with windsurf. Let's kind of build an app from scratch here. So let's I have this, like, empty, empty folder here. It has the bedrock logo and the codium logo. Let's kind of ask it to just create a web app from scratch. All right. So any, any ideas, by the way, anyone want to throw out a cool idea for this? Maybe, like, partnership was the best demo, maybe tonight. All right, I'll just, I'll just write
13:20out folder partnership
out folder partnership
out folder partnership
out folder partnership
S Speaker 413:39proposal. All right, so I wrote, write a simple web app using the logos in the folder, make a partnership proposal for AWS and codium, and where's Adrian? I think we're actually supposed to write one. So maybe this is going to do it for me. All right. So I think one thing that's interesting is it could do web search. It looks like it kind of skipped that. It went directly to writing the web app directly. But it could actually do research for you go online. Look a little bit more about AWS bedrock. I think it probably has enough data about it already. Maybe you guys have a lot of content already, and it's actually just creating this web app from scratch. So all that code that you see, you saw pop up on the screen, was literally just generated from scratch, just now. And now it's running like the styles. And maybe, actually, I don't like the styles. Maybe I want to do it. Maybe I want to match, like, this style here, right? The windsurve editor, the dark and green. So let me actually just stop this here. Let me just, like, stop it here.
proposal. All right, so I wrote, write a simple web app using the logos in the folder, make a partnership proposal for AWS and codium, and where's Adrian? I think we're actually supposed to write one. So maybe this is going to do it for me. All right. So I think one thing that's interesting is it could do web search. It looks like it kind of skipped that. It went directly to writing the web app directly. But it could actually do research for you go online. Look a little bit more about AWS bedrock. I think it probably has enough data about it already. Maybe you guys have a lot of content already, and it's actually just creating this web app from scratch. So all that code that you see, you saw pop up on the screen, was literally just generated from scratch, just now. And now it's running like the styles. And maybe, actually, I don't like the styles. Maybe I want to do it. Maybe I want to match, like, this style here, right? The windsurve editor, the dark and green. So let me actually just stop this here. Let me just, like, stop it here.
proposal. All right, so I wrote, write a simple web app using the logos in the folder, make a partnership proposal for AWS and codium, and where's Adrian? I think we're actually supposed to write one. So maybe this is going to do it for me. All right. So I think one thing that's interesting is it could do web search. It looks like it kind of skipped that. It went directly to writing the web app directly. But it could actually do research for you go online. Look a little bit more about AWS bedrock. I think it probably has enough data about it already. Maybe you guys have a lot of content already, and it's actually just creating this web app from scratch. So all that code that you see, you saw pop up on the screen, was literally just generated from scratch, just now. And now it's running like the styles. And maybe, actually, I don't like the styles. Maybe I want to do it. Maybe I want to match, like, this style here, right? The windsurve editor, the dark and green. So let me actually just stop this here. Let me just, like, stop it here.
proposal. All right, so I wrote, write a simple web app using the logos in the folder, make a partnership proposal for AWS and codium, and where's Adrian? I think we're actually supposed to write one. So maybe this is going to do it for me. All right. So I think one thing that's interesting is it could do web search. It looks like it kind of skipped that. It went directly to writing the web app directly. But it could actually do research for you go online. Look a little bit more about AWS bedrock. I think it probably has enough data about it already. Maybe you guys have a lot of content already, and it's actually just creating this web app from scratch. So all that code that you see, you saw pop up on the screen, was literally just generated from scratch, just now. And now it's running like the styles. And maybe, actually, I don't like the styles. Maybe I want to do it. Maybe I want to match, like, this style here, right? The windsurve editor, the dark and green. So let me actually just stop this here. Let me just, like, stop it here.
S Speaker 414:38All right, so I just gave it a screenshot of my web page, and you'll notice I stopped the agent midway. Right? A lot of people forget about this with agents like you. If you let an agent go for an objective and it goes off the wrong path, what do you do? Right? So we have this concept called AI flows, which is like putting a human in the loop as the agent is running. And you saw, right now, right there, I just stopped it and said, Wait a second. I don't know what you're going to do. I don't know what style you're going to give me. I'm give me. Let me give you some guidance. Here's a screenshot of what I want to do, and now it's going to go, okay, great. I like this dark theme with teal and turquoise accents it like actually recognizes that it's multimodal, right? And it's creating, it already created the CSS page for me. You can see this over here. It's even showing the color, which is nice. Oh, that's really cool. I didn't know we had that. Had that feature, okay? And of course, everyone forgets documentation. So we've always also added documentation by default as you create these apps from scratch. So if you hand it, hand this off to someone else, they can see what you built. And let's go see here what it looks like as well. Great. It summarizes exactly what's going on here. And you'll notice it's actually asking, like, Hey, do you want to see this? Right? So I run, oh, look, it ran the python script and said, Hey, Python not found. So it has to self correct again. Oh, python three, right? So it automatically kind of readjust. So now we're running the web app locally, checking if it's working. And you could even open the app in the IDE. I can catch up with this one sec, all right, there we go. Here's the web app. And it's, it's in the style of our home page too, right? So, yeah, I was able to guide it midway show what I wanted to get from it. And I could even keep modifying this. I could even point to an element on this, on this page, see, and then say, like, Hey, I don't like this box. Maybe I want circles instead, right? You get the gist, right? I can just keep modifying it further and further further, all right, and I think where's the AWS reps that we're trying to do the partnership? Here's the web page. You can take a look at this, and we'll talk later. Okay, so that is windsurf. Obviously, it's great for existing code bases. It's great for running agents through like debugging and creating code, modifying code, migrating code, all sorts of use cases. Is a total game changer for organizations. I can attest to some of the companies that we're working with already. And yeah, I think Amazon, they're listing a lot of our graphic models, but also we just went FedRAMP High yesterday. We just came up with FedRAMP, but that's also in AWS go cloud. So thank you Amazon for having me here and putting this demo on me. But here we go, open opening the floor now to questions for anybody here. Anyone have questions?
All right, so I just gave it a screenshot of my web page, and you'll notice I stopped the agent midway. Right? A lot of people forget about this with agents like you. If you let an agent go for an objective and it goes off the wrong path, what do you do? Right? So we have this concept called AI flows, which is like putting a human in the loop as the agent is running. And you saw, right now, right there, I just stopped it and said, Wait a second. I don't know what you're going to do. I don't know what style you're going to give me. I'm give me. Let me give you some guidance. Here's a screenshot of what I want to do, and now it's going to go, okay, great. I like this dark theme with teal and turquoise accents it like actually recognizes that it's multimodal, right? And it's creating, it already created the CSS page for me. You can see this over here. It's even showing the color, which is nice. Oh, that's really cool. I didn't know we had that. Had that feature, okay? And of course, everyone forgets documentation. So we've always also added documentation by default as you create these apps from scratch. So if you hand it, hand this off to someone else, they can see what you built. And let's go see here what it looks like as well. Great. It summarizes exactly what's going on here. And you'll notice it's actually asking, like, Hey, do you want to see this? Right? So I run, oh, look, it ran the python script and said, Hey, Python not found. So it has to self correct again. Oh, python three, right? So it automatically kind of readjust. So now we're running the web app locally, checking if it's working. And you could even open the app in the IDE. I can catch up with this one sec, all right, there we go. Here's the web app. And it's, it's in the style of our home page too, right? So, yeah, I was able to guide it midway show what I wanted to get from it. And I could even keep modifying this. I could even point to an element on this, on this page, see, and then say, like, Hey, I don't like this box. Maybe I want circles instead, right? You get the gist, right? I can just keep modifying it further and further further, all right, and I think where's the AWS reps that we're trying to do the partnership? Here's the web page. You can take a look at this, and we'll talk later. Okay, so that is windsurf. Obviously, it's great for existing code bases. It's great for running agents through like debugging and creating code, modifying code, migrating code, all sorts of use cases. Is a total game changer for organizations. I can attest to some of the companies that we're working with already. And yeah, I think Amazon, they're listing a lot of our graphic models, but also we just went FedRAMP High yesterday. We just came up with FedRAMP, but that's also in AWS go cloud. So thank you Amazon for having me here and putting this demo on me. But here we go, open opening the floor now to questions for anybody here. Anyone have questions?
All right, so I just gave it a screenshot of my web page, and you'll notice I stopped the agent midway. Right? A lot of people forget about this with agents like you. If you let an agent go for an objective and it goes off the wrong path, what do you do? Right? So we have this concept called AI flows, which is like putting a human in the loop as the agent is running. And you saw, right now, right there, I just stopped it and said, Wait a second. I don't know what you're going to do. I don't know what style you're going to give me. I'm give me. Let me give you some guidance. Here's a screenshot of what I want to do, and now it's going to go, okay, great. I like this dark theme with teal and turquoise accents it like actually recognizes that it's multimodal, right? And it's creating, it already created the CSS page for me. You can see this over here. It's even showing the color, which is nice. Oh, that's really cool. I didn't know we had that. Had that feature, okay? And of course, everyone forgets documentation. So we've always also added documentation by default as you create these apps from scratch. So if you hand it, hand this off to someone else, they can see what you built. And let's go see here what it looks like as well. Great. It summarizes exactly what's going on here. And you'll notice it's actually asking, like, Hey, do you want to see this? Right? So I run, oh, look, it ran the python script and said, Hey, Python not found. So it has to self correct again. Oh, python three, right? So it automatically kind of readjust. So now we're running the web app locally, checking if it's working. And you could even open the app in the IDE. I can catch up with this one sec, all right, there we go. Here's the web app. And it's, it's in the style of our home page too, right? So, yeah, I was able to guide it midway show what I wanted to get from it. And I could even keep modifying this. I could even point to an element on this, on this page, see, and then say, like, Hey, I don't like this box. Maybe I want circles instead, right? You get the gist, right? I can just keep modifying it further and further further, all right, and I think where's the AWS reps that we're trying to do the partnership? Here's the web page. You can take a look at this, and we'll talk later. Okay, so that is windsurf. Obviously, it's great for existing code bases. It's great for running agents through like debugging and creating code, modifying code, migrating code, all sorts of use cases. Is a total game changer for organizations. I can attest to some of the companies that we're working with already. And yeah, I think Amazon, they're listing a lot of our graphic models, but also we just went FedRAMP High yesterday. We just came up with FedRAMP, but that's also in AWS go cloud. So thank you Amazon for having me here and putting this demo on me. But here we go, open opening the floor now to questions for anybody here. Anyone have questions?
All right, so I just gave it a screenshot of my web page, and you'll notice I stopped the agent midway. Right? A lot of people forget about this with agents like you. If you let an agent go for an objective and it goes off the wrong path, what do you do? Right? So we have this concept called AI flows, which is like putting a human in the loop as the agent is running. And you saw, right now, right there, I just stopped it and said, Wait a second. I don't know what you're going to do. I don't know what style you're going to give me. I'm give me. Let me give you some guidance. Here's a screenshot of what I want to do, and now it's going to go, okay, great. I like this dark theme with teal and turquoise accents it like actually recognizes that it's multimodal, right? And it's creating, it already created the CSS page for me. You can see this over here. It's even showing the color, which is nice. Oh, that's really cool. I didn't know we had that. Had that feature, okay? And of course, everyone forgets documentation. So we've always also added documentation by default as you create these apps from scratch. So if you hand it, hand this off to someone else, they can see what you built. And let's go see here what it looks like as well. Great. It summarizes exactly what's going on here. And you'll notice it's actually asking, like, Hey, do you want to see this? Right? So I run, oh, look, it ran the python script and said, Hey, Python not found. So it has to self correct again. Oh, python three, right? So it automatically kind of readjust. So now we're running the web app locally, checking if it's working. And you could even open the app in the IDE. I can catch up with this one sec, all right, there we go. Here's the web app. And it's, it's in the style of our home page too, right? So, yeah, I was able to guide it midway show what I wanted to get from it. And I could even keep modifying this. I could even point to an element on this, on this page, see, and then say, like, Hey, I don't like this box. Maybe I want circles instead, right? You get the gist, right? I can just keep modifying it further and further further, all right, and I think where's the AWS reps that we're trying to do the partnership? Here's the web page. You can take a look at this, and we'll talk later. Okay, so that is windsurf. Obviously, it's great for existing code bases. It's great for running agents through like debugging and creating code, modifying code, migrating code, all sorts of use cases. Is a total game changer for organizations. I can attest to some of the companies that we're working with already. And yeah, I think Amazon, they're listing a lot of our graphic models, but also we just went FedRAMP High yesterday. We just came up with FedRAMP, but that's also in AWS go cloud. So thank you Amazon for having me here and putting this demo on me. But here we go, open opening the floor now to questions for anybody here. Anyone have questions?
S Speaker 517:22So actually, I was wondering, how good is the for, like, large context, windows, what are the best practices? Because I've been using a little bit of wind service, started like last month, like last week, to resize. So I think it gets a little lost in the agent. It's I like the agentic flow. But what would you say for the best work for practice, or something for an large code base I'm working on, like, a large code base and trying to add
So actually, I was wondering, how good is the for, like, large context, windows, what are the best practices? Because I've been using a little bit of wind service, started like last month, like last week, to resize. So I think it gets a little lost in the agent. It's I like the agentic flow. But what would you say for the best work for practice, or something for an large code base I'm working on, like, a large code base and trying to add
So actually, I was wondering, how good is the for, like, large context, windows, what are the best practices? Because I've been using a little bit of wind service, started like last month, like last week, to resize. So I think it gets a little lost in the agent. It's I like the agentic flow. But what would you say for the best work for practice, or something for an large code base I'm working on, like, a large code base and trying to add
So actually, I was wondering, how good is the for, like, large context, windows, what are the best practices? Because I've been using a little bit of wind service, started like last month, like last week, to resize. So I think it gets a little lost in the agent. It's I like the agentic flow. But what would you say for the best work for practice, or something for an large code base I'm working on, like, a large code base and trying to add
S Speaker 618:27trying to do, right? Thank you. Most use cases is currently impossible in Windsor, but you expected it to be possible, like, near future?
trying to do, right? Thank you. Most use cases is currently impossible in Windsor, but you expected it to be possible, like, near future?
trying to do, right? Thank you. Most use cases is currently impossible in Windsor, but you expected it to be possible, like, near future?
trying to do, right? Thank you. Most use cases is currently impossible in Windsor, but you expected it to be possible, like, near future?
S Speaker 418:37Yeah, there's actually some customers that have, like, proprietary languages as well. I can't there's a very large GDC customer prospect. I should say they have their own language, and they don't know how to use coding assistance with it, because there's just simply not a lot of data. This is also true with like hardware language and COBOL. There's a lot of like, code that's just very ancient, and there's not a lot of data to train these models with, right so I think, like, right now the code is like, is like relying on this train training data. But what we've seen now with, as the models are getting better, even the proprietary languages are getting enhanced too, like it somehow understands the relationship between what you're giving it to what probably the code is trying to do. I think that's one example I can think of.
Yeah, there's actually some customers that have, like, proprietary languages as well. I can't there's a very large GDC customer prospect. I should say they have their own language, and they don't know how to use coding assistance with it, because there's just simply not a lot of data. This is also true with like hardware language and COBOL. There's a lot of like, code that's just very ancient, and there's not a lot of data to train these models with, right so I think, like, right now the code is like, is like relying on this train training data. But what we've seen now with, as the models are getting better, even the proprietary languages are getting enhanced too, like it somehow understands the relationship between what you're giving it to what probably the code is trying to do. I think that's one example I can think of.
Yeah, there's actually some customers that have, like, proprietary languages as well. I can't there's a very large GDC customer prospect. I should say they have their own language, and they don't know how to use coding assistance with it, because there's just simply not a lot of data. This is also true with like hardware language and COBOL. There's a lot of like, code that's just very ancient, and there's not a lot of data to train these models with, right so I think, like, right now the code is like, is like relying on this train training data. But what we've seen now with, as the models are getting better, even the proprietary languages are getting enhanced too, like it somehow understands the relationship between what you're giving it to what probably the code is trying to do. I think that's one example I can think of.
Yeah, there's actually some customers that have, like, proprietary languages as well. I can't there's a very large GDC customer prospect. I should say they have their own language, and they don't know how to use coding assistance with it, because there's just simply not a lot of data. This is also true with like hardware language and COBOL. There's a lot of like, code that's just very ancient, and there's not a lot of data to train these models with, right so I think, like, right now the code is like, is like relying on this train training data. But what we've seen now with, as the models are getting better, even the proprietary languages are getting enhanced too, like it somehow understands the relationship between what you're giving it to what probably the code is trying to do. I think that's one example I can think of.
S Speaker 719:21Hi, great demo. So my question is, there are so many kind of, this kind of code generator, like, how you are going to compete? And then more technical part is like, so you are making LLM call for the whole entire code base. So what is your pricing model, and how can you kind of like a simple code base, if I call send everything to a LLM, so that costs a lot of money. So is it you pass to developer, or is it you consume? So business model and competition?
Hi, great demo. So my question is, there are so many kind of, this kind of code generator, like, how you are going to compete? And then more technical part is like, so you are making LLM call for the whole entire code base. So what is your pricing model, and how can you kind of like a simple code base, if I call send everything to a LLM, so that costs a lot of money. So is it you pass to developer, or is it you consume? So business model and competition?
Hi, great demo. So my question is, there are so many kind of, this kind of code generator, like, how you are going to compete? And then more technical part is like, so you are making LLM call for the whole entire code base. So what is your pricing model, and how can you kind of like a simple code base, if I call send everything to a LLM, so that costs a lot of money. So is it you pass to developer, or is it you consume? So business model and competition?
Hi, great demo. So my question is, there are so many kind of, this kind of code generator, like, how you are going to compete? And then more technical part is like, so you are making LLM call for the whole entire code base. So what is your pricing model, and how can you kind of like a simple code base, if I call send everything to a LLM, so that costs a lot of money. So is it you pass to developer, or is it you consume? So business model and competition?
S Speaker 419:57Yeah, I would say our biggest core competency is infrastructure. For example, we have self hosted deployments. You can host an entire air gap deployment of codium to your organization, and it works with 1000 engineers from a single GPU, right? And if anybody else that we partnered with, they usually come to us with a giant cluster node and say, like, this will support 100 people. And then inside, I'm like, well, they still haven't optimized event enough, right? And I think, like that's one of our biggest strengths, is being able to optimize stuff like the RIP like rip type, and doing these huge LLM calls, because that is simply not scalable for any other company, right? How we differentiate with all these other coding assistants, we try to remain very enterprise ready. So I just talked about FedRAMP just yesterday, or we just got approved yesterday. There's no other AI coding assistant on the market that has that nobody really has a good self hosted deployment either, nobody. Well, I think everybody has talked to now, but like, everybody is kind of like catching up to us in terms of our security and our compliance, and a lot of the things we've thought about, for example, like, cursor actually stores their code, all their customers code on another third party, right? And we don't do that. We actually store it locally or on your own hybrid deployment. We have a hybrid deployment model as well. So like we just think about these things like that we've run into with every single security like CSO and CIO, and then we try to make it that there's no excuse not to use COVID, at least not from the compliance and security perspective. So
Yeah, I would say our biggest core competency is infrastructure. For example, we have self hosted deployments. You can host an entire air gap deployment of codium to your organization, and it works with 1000 engineers from a single GPU, right? And if anybody else that we partnered with, they usually come to us with a giant cluster node and say, like, this will support 100 people. And then inside, I'm like, well, they still haven't optimized event enough, right? And I think, like that's one of our biggest strengths, is being able to optimize stuff like the RIP like rip type, and doing these huge LLM calls, because that is simply not scalable for any other company, right? How we differentiate with all these other coding assistants, we try to remain very enterprise ready. So I just talked about FedRAMP just yesterday, or we just got approved yesterday. There's no other AI coding assistant on the market that has that nobody really has a good self hosted deployment either, nobody. Well, I think everybody has talked to now, but like, everybody is kind of like catching up to us in terms of our security and our compliance, and a lot of the things we've thought about, for example, like, cursor actually stores their code, all their customers code on another third party, right? And we don't do that. We actually store it locally or on your own hybrid deployment. We have a hybrid deployment model as well. So like we just think about these things like that we've run into with every single security like CSO and CIO, and then we try to make it that there's no excuse not to use COVID, at least not from the compliance and security perspective. So
Yeah, I would say our biggest core competency is infrastructure. For example, we have self hosted deployments. You can host an entire air gap deployment of codium to your organization, and it works with 1000 engineers from a single GPU, right? And if anybody else that we partnered with, they usually come to us with a giant cluster node and say, like, this will support 100 people. And then inside, I'm like, well, they still haven't optimized event enough, right? And I think, like that's one of our biggest strengths, is being able to optimize stuff like the RIP like rip type, and doing these huge LLM calls, because that is simply not scalable for any other company, right? How we differentiate with all these other coding assistants, we try to remain very enterprise ready. So I just talked about FedRAMP just yesterday, or we just got approved yesterday. There's no other AI coding assistant on the market that has that nobody really has a good self hosted deployment either, nobody. Well, I think everybody has talked to now, but like, everybody is kind of like catching up to us in terms of our security and our compliance, and a lot of the things we've thought about, for example, like, cursor actually stores their code, all their customers code on another third party, right? And we don't do that. We actually store it locally or on your own hybrid deployment. We have a hybrid deployment model as well. So like we just think about these things like that we've run into with every single security like CSO and CIO, and then we try to make it that there's no excuse not to use COVID, at least not from the compliance and security perspective. So
Yeah, I would say our biggest core competency is infrastructure. For example, we have self hosted deployments. You can host an entire air gap deployment of codium to your organization, and it works with 1000 engineers from a single GPU, right? And if anybody else that we partnered with, they usually come to us with a giant cluster node and say, like, this will support 100 people. And then inside, I'm like, well, they still haven't optimized event enough, right? And I think, like that's one of our biggest strengths, is being able to optimize stuff like the RIP like rip type, and doing these huge LLM calls, because that is simply not scalable for any other company, right? How we differentiate with all these other coding assistants, we try to remain very enterprise ready. So I just talked about FedRAMP just yesterday, or we just got approved yesterday. There's no other AI coding assistant on the market that has that nobody really has a good self hosted deployment either, nobody. Well, I think everybody has talked to now, but like, everybody is kind of like catching up to us in terms of our security and our compliance, and a lot of the things we've thought about, for example, like, cursor actually stores their code, all their customers code on another third party, right? And we don't do that. We actually store it locally or on your own hybrid deployment. We have a hybrid deployment model as well. So like we just think about these things like that we've run into with every single security like CSO and CIO, and then we try to make it that there's no excuse not to use COVID, at least not from the compliance and security perspective. So
S Speaker 821:15let's give Jennifer a round of applause. Everyone. I
let's give Jennifer a round of applause. Everyone. I
let's give Jennifer a round of applause. Everyone. I
let's give Jennifer a round of applause. Everyone. I
21:23said, we have Leah with detainee. Thanks, Artie, all right,
said, we have Leah with detainee. Thanks, Artie, all right,
said, we have Leah with detainee. Thanks, Artie, all right,
said, we have Leah with detainee. Thanks, Artie, all right,
S Speaker 921:44everybody. My name is Leo Ness from defang. Let me pop up that real quick. Actually, perfect to be right after codium With windsurf, because we take it from there so we deploy to the cloud. Defang is a deployment tool.
everybody. My name is Leo Ness from defang. Let me pop up that real quick. Actually, perfect to be right after codium With windsurf, because we take it from there so we deploy to the cloud. Defang is a deployment tool.
everybody. My name is Leo Ness from defang. Let me pop up that real quick. Actually, perfect to be right after codium With windsurf, because we take it from there so we deploy to the cloud. Defang is a deployment tool.
everybody. My name is Leo Ness from defang. Let me pop up that real quick. Actually, perfect to be right after codium With windsurf, because we take it from there so we deploy to the cloud. Defang is a deployment tool.
22:04Quick show of hands. Who is familiar with Docker Compose?
Quick show of hands. Who is familiar with Docker Compose?
Quick show of hands. Who is familiar with Docker Compose?
Quick show of hands. Who is familiar with Docker Compose?
S Speaker 922:09Cool, yeah. Patrick crowd is familiar with Docker Compose. So Docker compose is great for development cycle, but then when it's time to deploy, people use other tools. They use cloud formation, TerraForm, pulumi, these kind of tools. Defang is a tool that takes your Docker compose file and deploys it to the cloud and AWS, of course, we also have auto support, but I'll show AWS today. So this is the website defang.io, it's a command line tool. You can plug it. The developers can use it, right? They don't need a DevOps. They deploy from their own dev machine to a sandbox, or you plug it into your
Cool, yeah. Patrick crowd is familiar with Docker Compose. So Docker compose is great for development cycle, but then when it's time to deploy, people use other tools. They use cloud formation, TerraForm, pulumi, these kind of tools. Defang is a tool that takes your Docker compose file and deploys it to the cloud and AWS, of course, we also have auto support, but I'll show AWS today. So this is the website defang.io, it's a command line tool. You can plug it. The developers can use it, right? They don't need a DevOps. They deploy from their own dev machine to a sandbox, or you plug it into your
Cool, yeah. Patrick crowd is familiar with Docker Compose. So Docker compose is great for development cycle, but then when it's time to deploy, people use other tools. They use cloud formation, TerraForm, pulumi, these kind of tools. Defang is a tool that takes your Docker compose file and deploys it to the cloud and AWS, of course, we also have auto support, but I'll show AWS today. So this is the website defang.io, it's a command line tool. You can plug it. The developers can use it, right? They don't need a DevOps. They deploy from their own dev machine to a sandbox, or you plug it into your
Cool, yeah. Patrick crowd is familiar with Docker Compose. So Docker compose is great for development cycle, but then when it's time to deploy, people use other tools. They use cloud formation, TerraForm, pulumi, these kind of tools. Defang is a tool that takes your Docker compose file and deploys it to the cloud and AWS, of course, we also have auto support, but I'll show AWS today. So this is the website defang.io, it's a command line tool. You can plug it. The developers can use it, right? They don't need a DevOps. They deploy from their own dev machine to a sandbox, or you plug it into your
22:50your GitHub action or your pipeline.
your GitHub action or your pipeline.
your GitHub action or your pipeline.
your GitHub action or your pipeline.
S Speaker 922:53So we're using backlog for to get started. Like, no, I don't have a project now, so I'll start on from scratch. We are not a kind of AI driven development tool. There's other companies that do that, but we do get you started with like, a small project. So I'll start with something simple. And you can see here, we're using the anthropic cloud 3.5 on bedrock. So I'll demo that. I'll put down the microphone for a second.
So we're using backlog for to get started. Like, no, I don't have a project now, so I'll start on from scratch. We are not a kind of AI driven development tool. There's other companies that do that, but we do get you started with like, a small project. So I'll start with something simple. And you can see here, we're using the anthropic cloud 3.5 on bedrock. So I'll demo that. I'll put down the microphone for a second.
So we're using backlog for to get started. Like, no, I don't have a project now, so I'll start on from scratch. We are not a kind of AI driven development tool. There's other companies that do that, but we do get you started with like, a small project. So I'll start with something simple. And you can see here, we're using the anthropic cloud 3.5 on bedrock. So I'll demo that. I'll put down the microphone for a second.
So we're using backlog for to get started. Like, no, I don't have a project now, so I'll start on from scratch. We are not a kind of AI driven development tool. There's other companies that do that, but we do get you started with like, a small project. So I'll start with something simple. And you can see here, we're using the anthropic cloud 3.5 on bedrock. So I'll demo that. I'll put down the microphone for a second.
S Speaker 923:33this lets you browse all our samples, but I'll generate a project from scratch. I'll pick no JS so again, we can do something silly. I'll probably type something that I've done before website that shows an image
this lets you browse all our samples, but I'll generate a project from scratch. I'll pick no JS so again, we can do something silly. I'll probably type something that I've done before website that shows an image
this lets you browse all our samples, but I'll generate a project from scratch. I'll pick no JS so again, we can do something silly. I'll probably type something that I've done before website that shows an image
this lets you browse all our samples, but I'll generate a project from scratch. I'll pick no JS so again, we can do something silly. I'll probably type something that I've done before website that shows an image
S Speaker 923:59of I'll call the folder something out. So I have done this an hour ago, and the reason why I override the same folder is because defang is a tool that deploys to your own cloud account. So we're not a host. We're not hosting your stuff. You deploy your project from your dev machine or from your repository to your cloud. And if I do it from scratch, it'll start creating VPC load balancers, net gateway. It just takes too long, so I'm now overriding the project editor point an hour ago, and it's a all the files are overwritten. So this is now newly generated. It opens the project in your editor. In my case, it opened it in cursor. Have a quick look whether all of this makes sense. No, kill all humans kind of stuff going on looks fine. And so now, people that are familiar with the Docker they would at this point, they would do Docker compose up right? There's a compose file there. You can run it on your own machine. I'm going to skip that. I'm going to do defang compose up. So we'll use the same Docker file, the same compose file. We use the same stack. I'm also already logged into our lab account. Defang compose up. Now everything your project gets uploaded to your cloud account. So in this case, my AWS account and it gets built there. So there's no building happening on the dev machine. It's literally just packaging the files that are needed for this project sends it to a builder that is running in your account and deploying to your account. So it's a bit of a snake eating his own tail situation. So this is going to go out. Hopefully it doesn't take too long, because I deployed to that account earlier. So VPC and everything is already there under the covers. Defang is writing the infrastructure as code for you. Now, I say writing, not generating. This is all rules based. So this is completely predictable, 100% reproducible, reuse pulumi. Anybody familiar with pulumi? All right, so pulumi is an infrastructure as code tool basically define now runs pulumi In your Cloud account and deploys to your cloud account. You can actually switch while this is running. Can switch to the Amazon
of I'll call the folder something out. So I have done this an hour ago, and the reason why I override the same folder is because defang is a tool that deploys to your own cloud account. So we're not a host. We're not hosting your stuff. You deploy your project from your dev machine or from your repository to your cloud. And if I do it from scratch, it'll start creating VPC load balancers, net gateway. It just takes too long, so I'm now overriding the project editor point an hour ago, and it's a all the files are overwritten. So this is now newly generated. It opens the project in your editor. In my case, it opened it in cursor. Have a quick look whether all of this makes sense. No, kill all humans kind of stuff going on looks fine. And so now, people that are familiar with the Docker they would at this point, they would do Docker compose up right? There's a compose file there. You can run it on your own machine. I'm going to skip that. I'm going to do defang compose up. So we'll use the same Docker file, the same compose file. We use the same stack. I'm also already logged into our lab account. Defang compose up. Now everything your project gets uploaded to your cloud account. So in this case, my AWS account and it gets built there. So there's no building happening on the dev machine. It's literally just packaging the files that are needed for this project sends it to a builder that is running in your account and deploying to your account. So it's a bit of a snake eating his own tail situation. So this is going to go out. Hopefully it doesn't take too long, because I deployed to that account earlier. So VPC and everything is already there under the covers. Defang is writing the infrastructure as code for you. Now, I say writing, not generating. This is all rules based. So this is completely predictable, 100% reproducible, reuse pulumi. Anybody familiar with pulumi? All right, so pulumi is an infrastructure as code tool basically define now runs pulumi In your Cloud account and deploys to your cloud account. You can actually switch while this is running. Can switch to the Amazon
of I'll call the folder something out. So I have done this an hour ago, and the reason why I override the same folder is because defang is a tool that deploys to your own cloud account. So we're not a host. We're not hosting your stuff. You deploy your project from your dev machine or from your repository to your cloud. And if I do it from scratch, it'll start creating VPC load balancers, net gateway. It just takes too long, so I'm now overriding the project editor point an hour ago, and it's a all the files are overwritten. So this is now newly generated. It opens the project in your editor. In my case, it opened it in cursor. Have a quick look whether all of this makes sense. No, kill all humans kind of stuff going on looks fine. And so now, people that are familiar with the Docker they would at this point, they would do Docker compose up right? There's a compose file there. You can run it on your own machine. I'm going to skip that. I'm going to do defang compose up. So we'll use the same Docker file, the same compose file. We use the same stack. I'm also already logged into our lab account. Defang compose up. Now everything your project gets uploaded to your cloud account. So in this case, my AWS account and it gets built there. So there's no building happening on the dev machine. It's literally just packaging the files that are needed for this project sends it to a builder that is running in your account and deploying to your account. So it's a bit of a snake eating his own tail situation. So this is going to go out. Hopefully it doesn't take too long, because I deployed to that account earlier. So VPC and everything is already there under the covers. Defang is writing the infrastructure as code for you. Now, I say writing, not generating. This is all rules based. So this is completely predictable, 100% reproducible, reuse pulumi. Anybody familiar with pulumi? All right, so pulumi is an infrastructure as code tool basically define now runs pulumi In your Cloud account and deploys to your cloud account. You can actually switch while this is running. Can switch to the Amazon
of I'll call the folder something out. So I have done this an hour ago, and the reason why I override the same folder is because defang is a tool that deploys to your own cloud account. So we're not a host. We're not hosting your stuff. You deploy your project from your dev machine or from your repository to your cloud. And if I do it from scratch, it'll start creating VPC load balancers, net gateway. It just takes too long, so I'm now overriding the project editor point an hour ago, and it's a all the files are overwritten. So this is now newly generated. It opens the project in your editor. In my case, it opened it in cursor. Have a quick look whether all of this makes sense. No, kill all humans kind of stuff going on looks fine. And so now, people that are familiar with the Docker they would at this point, they would do Docker compose up right? There's a compose file there. You can run it on your own machine. I'm going to skip that. I'm going to do defang compose up. So we'll use the same Docker file, the same compose file. We use the same stack. I'm also already logged into our lab account. Defang compose up. Now everything your project gets uploaded to your cloud account. So in this case, my AWS account and it gets built there. So there's no building happening on the dev machine. It's literally just packaging the files that are needed for this project sends it to a builder that is running in your account and deploying to your account. So it's a bit of a snake eating his own tail situation. So this is going to go out. Hopefully it doesn't take too long, because I deployed to that account earlier. So VPC and everything is already there under the covers. Defang is writing the infrastructure as code for you. Now, I say writing, not generating. This is all rules based. So this is completely predictable, 100% reproducible, reuse pulumi. Anybody familiar with pulumi? All right, so pulumi is an infrastructure as code tool basically define now runs pulumi In your Cloud account and deploys to your cloud account. You can actually switch while this is running. Can switch to the Amazon
26:25dashboard here, and you see,
dashboard here, and you see,
dashboard here, and you see,
dashboard here, and you see,
S Speaker 926:29you see there's, this is our CD task that is running. So there's a bootstrap cluster just for running your deployment, which then deploys your actual application, which I called ASDF for better or worse, and so this is now running in your account, deployed from the Docker compose file. Of course, this is a very simple example. We also support databases, Redis, all of that is decorative in your compose file. So what people often do during development is they add a DB service to their compose file. It looks something like this, right? You'll run an image of the postgres database like this. This is, of course, not great. You'll be running a database in a container. You'll lose data when the container restarts. But what happens? Defam is actually able to, during deployment, hook this up to a managed version of that database. So we have a right now that's all opt in, because there's cost applications for doing that. So if you give it this custom extension in the deployment, your application will now be using a managed database, not a self, self running one. Same goes for Redis. And the feature that I will, you know, hopefully will launch this week, is we do the same for language models. Because we have done this for bedrock. We know how to integrate with bedrock. Our application integrated with bedrock. We were using chatgpt Before we can actually make that same automatic hookup with that rock. So if you have an application that's already talking to open AI doing deployment, it's your choice. You can produce a bedrock sidecar that transparently sits between bedrock and your application, and so you don't have to deal with AWS, API calls, authorization, all of that is set up the application defense sets up your security groups, your target groups, your load balancing, all of that stuff. You see, the server has started. We also provision a domain name for you if you haven't picked one. But similar to all the other stuff, the Compose pack actually has a way for you to specify a domain name. So if you have here a domain name, then we'll hook that up with TLS certificates and all of that, so you don't have to worry about that either. What more to say? There's a knob that you can turn. So by default, things get deployed in a development mode that is kind of cheaper deployments, faster iteration, if you say production mode, now you get extra resiliency. You get three of everything, etc. Of course, there's also cost applications to that, but the idea is you deploy with defang using production mode, and you are following best practices. In fact, we have the Amazon well architected review. We went through that because we went through that. All the code that we emailed goes through that as well. So this is the URL, a bit weird URL because I didn't bring my own. So, yeah, this is the website which rendered me. Of course, this is ridiculously simple, but from here, you use your code editor and go from there, right? So I can now ask cursor to add an endpoint or add a button, or I should have done with Windsor.
you see there's, this is our CD task that is running. So there's a bootstrap cluster just for running your deployment, which then deploys your actual application, which I called ASDF for better or worse, and so this is now running in your account, deployed from the Docker compose file. Of course, this is a very simple example. We also support databases, Redis, all of that is decorative in your compose file. So what people often do during development is they add a DB service to their compose file. It looks something like this, right? You'll run an image of the postgres database like this. This is, of course, not great. You'll be running a database in a container. You'll lose data when the container restarts. But what happens? Defam is actually able to, during deployment, hook this up to a managed version of that database. So we have a right now that's all opt in, because there's cost applications for doing that. So if you give it this custom extension in the deployment, your application will now be using a managed database, not a self, self running one. Same goes for Redis. And the feature that I will, you know, hopefully will launch this week, is we do the same for language models. Because we have done this for bedrock. We know how to integrate with bedrock. Our application integrated with bedrock. We were using chatgpt Before we can actually make that same automatic hookup with that rock. So if you have an application that's already talking to open AI doing deployment, it's your choice. You can produce a bedrock sidecar that transparently sits between bedrock and your application, and so you don't have to deal with AWS, API calls, authorization, all of that is set up the application defense sets up your security groups, your target groups, your load balancing, all of that stuff. You see, the server has started. We also provision a domain name for you if you haven't picked one. But similar to all the other stuff, the Compose pack actually has a way for you to specify a domain name. So if you have here a domain name, then we'll hook that up with TLS certificates and all of that, so you don't have to worry about that either. What more to say? There's a knob that you can turn. So by default, things get deployed in a development mode that is kind of cheaper deployments, faster iteration, if you say production mode, now you get extra resiliency. You get three of everything, etc. Of course, there's also cost applications to that, but the idea is you deploy with defang using production mode, and you are following best practices. In fact, we have the Amazon well architected review. We went through that because we went through that. All the code that we emailed goes through that as well. So this is the URL, a bit weird URL because I didn't bring my own. So, yeah, this is the website which rendered me. Of course, this is ridiculously simple, but from here, you use your code editor and go from there, right? So I can now ask cursor to add an endpoint or add a button, or I should have done with Windsor.
you see there's, this is our CD task that is running. So there's a bootstrap cluster just for running your deployment, which then deploys your actual application, which I called ASDF for better or worse, and so this is now running in your account, deployed from the Docker compose file. Of course, this is a very simple example. We also support databases, Redis, all of that is decorative in your compose file. So what people often do during development is they add a DB service to their compose file. It looks something like this, right? You'll run an image of the postgres database like this. This is, of course, not great. You'll be running a database in a container. You'll lose data when the container restarts. But what happens? Defam is actually able to, during deployment, hook this up to a managed version of that database. So we have a right now that's all opt in, because there's cost applications for doing that. So if you give it this custom extension in the deployment, your application will now be using a managed database, not a self, self running one. Same goes for Redis. And the feature that I will, you know, hopefully will launch this week, is we do the same for language models. Because we have done this for bedrock. We know how to integrate with bedrock. Our application integrated with bedrock. We were using chatgpt Before we can actually make that same automatic hookup with that rock. So if you have an application that's already talking to open AI doing deployment, it's your choice. You can produce a bedrock sidecar that transparently sits between bedrock and your application, and so you don't have to deal with AWS, API calls, authorization, all of that is set up the application defense sets up your security groups, your target groups, your load balancing, all of that stuff. You see, the server has started. We also provision a domain name for you if you haven't picked one. But similar to all the other stuff, the Compose pack actually has a way for you to specify a domain name. So if you have here a domain name, then we'll hook that up with TLS certificates and all of that, so you don't have to worry about that either. What more to say? There's a knob that you can turn. So by default, things get deployed in a development mode that is kind of cheaper deployments, faster iteration, if you say production mode, now you get extra resiliency. You get three of everything, etc. Of course, there's also cost applications to that, but the idea is you deploy with defang using production mode, and you are following best practices. In fact, we have the Amazon well architected review. We went through that because we went through that. All the code that we emailed goes through that as well. So this is the URL, a bit weird URL because I didn't bring my own. So, yeah, this is the website which rendered me. Of course, this is ridiculously simple, but from here, you use your code editor and go from there, right? So I can now ask cursor to add an endpoint or add a button, or I should have done with Windsor.
you see there's, this is our CD task that is running. So there's a bootstrap cluster just for running your deployment, which then deploys your actual application, which I called ASDF for better or worse, and so this is now running in your account, deployed from the Docker compose file. Of course, this is a very simple example. We also support databases, Redis, all of that is decorative in your compose file. So what people often do during development is they add a DB service to their compose file. It looks something like this, right? You'll run an image of the postgres database like this. This is, of course, not great. You'll be running a database in a container. You'll lose data when the container restarts. But what happens? Defam is actually able to, during deployment, hook this up to a managed version of that database. So we have a right now that's all opt in, because there's cost applications for doing that. So if you give it this custom extension in the deployment, your application will now be using a managed database, not a self, self running one. Same goes for Redis. And the feature that I will, you know, hopefully will launch this week, is we do the same for language models. Because we have done this for bedrock. We know how to integrate with bedrock. Our application integrated with bedrock. We were using chatgpt Before we can actually make that same automatic hookup with that rock. So if you have an application that's already talking to open AI doing deployment, it's your choice. You can produce a bedrock sidecar that transparently sits between bedrock and your application, and so you don't have to deal with AWS, API calls, authorization, all of that is set up the application defense sets up your security groups, your target groups, your load balancing, all of that stuff. You see, the server has started. We also provision a domain name for you if you haven't picked one. But similar to all the other stuff, the Compose pack actually has a way for you to specify a domain name. So if you have here a domain name, then we'll hook that up with TLS certificates and all of that, so you don't have to worry about that either. What more to say? There's a knob that you can turn. So by default, things get deployed in a development mode that is kind of cheaper deployments, faster iteration, if you say production mode, now you get extra resiliency. You get three of everything, etc. Of course, there's also cost applications to that, but the idea is you deploy with defang using production mode, and you are following best practices. In fact, we have the Amazon well architected review. We went through that because we went through that. All the code that we emailed goes through that as well. So this is the URL, a bit weird URL because I didn't bring my own. So, yeah, this is the website which rendered me. Of course, this is ridiculously simple, but from here, you use your code editor and go from there, right? So I can now ask cursor to add an endpoint or add a button, or I should have done with Windsor.
29:58Next, next one. Next the demo will be with Windsor.
Next, next one. Next the demo will be with Windsor.
Next, next one. Next the demo will be with Windsor.
Next, next one. Next the demo will be with Windsor.
S Speaker 1030:53Actually, there are many business that have already deployed their applications.
Actually, there are many business that have already deployed their applications.
Actually, there are many business that have already deployed their applications.
Actually, there are many business that have already deployed their applications.
S Speaker 931:05So we don't migrate because we cannot adopt resources that are already in the cloud. We will do a clean deployment from your compose file. So the compose file is kind of the logical description of your app, and define is the tool that decides what the physical description looks like, whether you want to server it or you want to use like containers. All of that is kind of the translation that define does. So we kind of are very opinionated on what the research, the physical resources, look like. So we cannot migrate an existing project, but you can use it for new services, right? You can have new things written
So we don't migrate because we cannot adopt resources that are already in the cloud. We will do a clean deployment from your compose file. So the compose file is kind of the logical description of your app, and define is the tool that decides what the physical description looks like, whether you want to server it or you want to use like containers. All of that is kind of the translation that define does. So we kind of are very opinionated on what the research, the physical resources, look like. So we cannot migrate an existing project, but you can use it for new services, right? You can have new things written
So we don't migrate because we cannot adopt resources that are already in the cloud. We will do a clean deployment from your compose file. So the compose file is kind of the logical description of your app, and define is the tool that decides what the physical description looks like, whether you want to server it or you want to use like containers. All of that is kind of the translation that define does. So we kind of are very opinionated on what the research, the physical resources, look like. So we cannot migrate an existing project, but you can use it for new services, right? You can have new things written
So we don't migrate because we cannot adopt resources that are already in the cloud. We will do a clean deployment from your compose file. So the compose file is kind of the logical description of your app, and define is the tool that decides what the physical description looks like, whether you want to server it or you want to use like containers. All of that is kind of the translation that define does. So we kind of are very opinionated on what the research, the physical resources, look like. So we cannot migrate an existing project, but you can use it for new services, right? You can have new things written
31:48with defense. Next up, we
with defense. Next up, we
with defense. Next up, we
with defense. Next up, we
31:56actually have a great
actually have a great
actually have a great
actually have a great
32:02okay. My train of thought.
okay. My train of thought.
okay. My train of thought.
okay. My train of thought.
32:07Yeah, so Greg with graph and launch notes.
Yeah, so Greg with graph and launch notes.
Yeah, so Greg with graph and launch notes.
Yeah, so Greg with graph and launch notes.
S Speaker 1232:12Thank you, Artie, hi everyone. My name is Greg, and I'm here with graph and the launch notes as I get this thing plugged in. So launch notes, we're kind of two companies under one umbrella as we get set up here. Launch notes, we started a few years ago. It is a product change communication platform, so we're looking at, like, road map planning. Release Notes, that kind of thing. Targeted sales and marketing teams. We wanted to kind of shift and start to target engineering teams. And we thought that, like, you know, the AI agent space was kind of a useful way for us to kind of enter that so then we went about building graph, which is a Slack native AI agent that kind of takes your like, code repository tools, issue tracking, and kind of gives that contextual layer of what your team is working on, if projects are getting off track, that kind of thing. Well, let's see if I can get this going.
Thank you, Artie, hi everyone. My name is Greg, and I'm here with graph and the launch notes as I get this thing plugged in. So launch notes, we're kind of two companies under one umbrella as we get set up here. Launch notes, we started a few years ago. It is a product change communication platform, so we're looking at, like, road map planning. Release Notes, that kind of thing. Targeted sales and marketing teams. We wanted to kind of shift and start to target engineering teams. And we thought that, like, you know, the AI agent space was kind of a useful way for us to kind of enter that so then we went about building graph, which is a Slack native AI agent that kind of takes your like, code repository tools, issue tracking, and kind of gives that contextual layer of what your team is working on, if projects are getting off track, that kind of thing. Well, let's see if I can get this going.
Thank you, Artie, hi everyone. My name is Greg, and I'm here with graph and the launch notes as I get this thing plugged in. So launch notes, we're kind of two companies under one umbrella as we get set up here. Launch notes, we started a few years ago. It is a product change communication platform, so we're looking at, like, road map planning. Release Notes, that kind of thing. Targeted sales and marketing teams. We wanted to kind of shift and start to target engineering teams. And we thought that, like, you know, the AI agent space was kind of a useful way for us to kind of enter that so then we went about building graph, which is a Slack native AI agent that kind of takes your like, code repository tools, issue tracking, and kind of gives that contextual layer of what your team is working on, if projects are getting off track, that kind of thing. Well, let's see if I can get this going.
Thank you, Artie, hi everyone. My name is Greg, and I'm here with graph and the launch notes as I get this thing plugged in. So launch notes, we're kind of two companies under one umbrella as we get set up here. Launch notes, we started a few years ago. It is a product change communication platform, so we're looking at, like, road map planning. Release Notes, that kind of thing. Targeted sales and marketing teams. We wanted to kind of shift and start to target engineering teams. And we thought that, like, you know, the AI agent space was kind of a useful way for us to kind of enter that so then we went about building graph, which is a Slack native AI agent that kind of takes your like, code repository tools, issue tracking, and kind of gives that contextual layer of what your team is working on, if projects are getting off track, that kind of thing. Well, let's see if I can get this going.
33:08It's worked earlier.
33:19I'll try again. I Well,
I'll try again. I Well,
I'll try again. I Well,
I'll try again. I Well,
36:23All right, so, yeah.
S Speaker 1236:35So we want to say, like, Hey, tell me more about this PR kind of interest and see, like, what was merged here. Why is that important as well? That's going let's ask something about a different repo that we have. Like, how do we implement logging across that repo? Let's say, like, you can task, give me some tasks, and you're like, I don't know anything about this. Think of some other scenarios. Let's think of like a subject matter expert kind of question. So I might ask graph like, who knows the most about user authentication across our system? So I should go into linear I should be able to pull up, like, tickets and individuals code examples and say, Okay, if you're being tasked with fixing some bug here, who should you go talk to with your team? And obviously, like a really small team, you may know this. But if you're at a larger organization, you may know that this is a much more complicated thing that you have to go solve.
So we want to say, like, Hey, tell me more about this PR kind of interest and see, like, what was merged here. Why is that important as well? That's going let's ask something about a different repo that we have. Like, how do we implement logging across that repo? Let's say, like, you can task, give me some tasks, and you're like, I don't know anything about this. Think of some other scenarios. Let's think of like a subject matter expert kind of question. So I might ask graph like, who knows the most about user authentication across our system? So I should go into linear I should be able to pull up, like, tickets and individuals code examples and say, Okay, if you're being tasked with fixing some bug here, who should you go talk to with your team? And obviously, like a really small team, you may know this. But if you're at a larger organization, you may know that this is a much more complicated thing that you have to go solve.
So we want to say, like, Hey, tell me more about this PR kind of interest and see, like, what was merged here. Why is that important as well? That's going let's ask something about a different repo that we have. Like, how do we implement logging across that repo? Let's say, like, you can task, give me some tasks, and you're like, I don't know anything about this. Think of some other scenarios. Let's think of like a subject matter expert kind of question. So I might ask graph like, who knows the most about user authentication across our system? So I should go into linear I should be able to pull up, like, tickets and individuals code examples and say, Okay, if you're being tasked with fixing some bug here, who should you go talk to with your team? And obviously, like a really small team, you may know this. But if you're at a larger organization, you may know that this is a much more complicated thing that you have to go solve.
So we want to say, like, Hey, tell me more about this PR kind of interest and see, like, what was merged here. Why is that important as well? That's going let's ask something about a different repo that we have. Like, how do we implement logging across that repo? Let's say, like, you can task, give me some tasks, and you're like, I don't know anything about this. Think of some other scenarios. Let's think of like a subject matter expert kind of question. So I might ask graph like, who knows the most about user authentication across our system? So I should go into linear I should be able to pull up, like, tickets and individuals code examples and say, Okay, if you're being tasked with fixing some bug here, who should you go talk to with your team? And obviously, like a really small team, you may know this. But if you're at a larger organization, you may know that this is a much more complicated thing that you have to go solve.
37:27So looking back in our history here,
So looking back in our history here,
So looking back in our history here,
So looking back in our history here,
S Speaker 1237:31we can see that, like, we've got a little bit more information about this particular PR so we've got, like, the status of it and kind of key insights, and then those things that we've pulled up, the logging implementation here, we can see that, like, Okay, here's an like, overview of it pulls up, you know, these are kind of the key files to look at. These are like, read or PRs that, you know, we've kind of determined are kind of like, valuable or important. And then we can also look at this subject matter expert answer as well. We'll drag this over, make a little larger, so it's actually providing examples of how this is used in the code base. And then we'll provide related PRs and tickets, and then actually suggest who are like the experts that you should go talk to about this. Now, how does this use? Bedrock? We use like this is primarily like a planning agent. So it's building kind of that that every was free move style. So it's like reasoning without observation. Takes a set of tools. We develop a plan based on those tools, then we execute it that uses, like Claude, we use on it for that. But along the way we have, like a pre planning process. We have some like use case, kind of, we call use cases. So like, inference matching of commonly asked questions. So we use, like, haiku for that. And then for a lot of the within the tools, we do a kind of like, you know, if we're chunking data, or we're kind of, like fixing the JQL query or something that is malformed, we use, know, the light for a lot of that. So we can do, like, really fast parallel calls. We can be, like, really, really efficient with how we do that, and we try to save the kind of, like, beefier longer calls for, for some of the anthropic models. And obviously, like, we use bedrock, but we can kind of test out new models as they come across. This is also built using kind of Lang chain, Lang graph. Sorry if that's a dirty term around here, but yeah, so we've been using that as well. So, yeah, we can see, you know, some other questions that are commonly pulled up. You know, we can look at, like, trend analysis kind of things. So let's take a look at this. You know, what's the velocity trend for the Explore teams? We can go back look at past cycles. We can actually generate charts with this. So we have a chart generation tool, and you can say, okay, like, we're going to map these things over time. For engineering managers who are looking at, like, bottlenecks, of like, you know, where their team might be going off, we can kind of pull that information up. And then the last thing I want to show is one thing we've been working on. It's been working on kind of an incident analysis tool where you can provide, like, some basic context about an incident and when it occurred, and then we can go and try to, like, pull up all the PRS around that, analyze each PR based on the question and the kind of error provided, and give you kind of, like a confidence score where we think you should be looking first. We know that when you have incidents like, you know, finding that initial PR is really, really important, and being like, Okay, what might have caused this? So we've been looking at, kind of exploring that, and how we can kind of identify that, like, root cause analysis and like, kind of cut the time to action there. So, you know, we can see that we have, like, a ability to build, like, the confidence score on that, the related ticket, the author, all of those kind of things. And we'll pull up this velocity report here, so you can see here that, like, here's the team's velocity over the last couple weeks. We have, like, you know, breakdowns of each cycle, that kind of thing. And what we've tried to do with a lot of this is a lot of these questions that do match, we will provide kind of like, its own context free grammar per question on the ones that match. So you can provide really, like, reliable and consistent, formatted answers, which we know is really important. And it's not just saying, like, well, this answer looked good, but then the next one looks kind of like off. So we want to have that consistency in a lot of the answers that we provide. We are about that at a time. So questions,
we can see that, like, we've got a little bit more information about this particular PR so we've got, like, the status of it and kind of key insights, and then those things that we've pulled up, the logging implementation here, we can see that, like, Okay, here's an like, overview of it pulls up, you know, these are kind of the key files to look at. These are like, read or PRs that, you know, we've kind of determined are kind of like, valuable or important. And then we can also look at this subject matter expert answer as well. We'll drag this over, make a little larger, so it's actually providing examples of how this is used in the code base. And then we'll provide related PRs and tickets, and then actually suggest who are like the experts that you should go talk to about this. Now, how does this use? Bedrock? We use like this is primarily like a planning agent. So it's building kind of that that every was free move style. So it's like reasoning without observation. Takes a set of tools. We develop a plan based on those tools, then we execute it that uses, like Claude, we use on it for that. But along the way we have, like a pre planning process. We have some like use case, kind of, we call use cases. So like, inference matching of commonly asked questions. So we use, like, haiku for that. And then for a lot of the within the tools, we do a kind of like, you know, if we're chunking data, or we're kind of, like fixing the JQL query or something that is malformed, we use, know, the light for a lot of that. So we can do, like, really fast parallel calls. We can be, like, really, really efficient with how we do that, and we try to save the kind of, like, beefier longer calls for, for some of the anthropic models. And obviously, like, we use bedrock, but we can kind of test out new models as they come across. This is also built using kind of Lang chain, Lang graph. Sorry if that's a dirty term around here, but yeah, so we've been using that as well. So, yeah, we can see, you know, some other questions that are commonly pulled up. You know, we can look at, like, trend analysis kind of things. So let's take a look at this. You know, what's the velocity trend for the Explore teams? We can go back look at past cycles. We can actually generate charts with this. So we have a chart generation tool, and you can say, okay, like, we're going to map these things over time. For engineering managers who are looking at, like, bottlenecks, of like, you know, where their team might be going off, we can kind of pull that information up. And then the last thing I want to show is one thing we've been working on. It's been working on kind of an incident analysis tool where you can provide, like, some basic context about an incident and when it occurred, and then we can go and try to, like, pull up all the PRS around that, analyze each PR based on the question and the kind of error provided, and give you kind of, like a confidence score where we think you should be looking first. We know that when you have incidents like, you know, finding that initial PR is really, really important, and being like, Okay, what might have caused this? So we've been looking at, kind of exploring that, and how we can kind of identify that, like, root cause analysis and like, kind of cut the time to action there. So, you know, we can see that we have, like, a ability to build, like, the confidence score on that, the related ticket, the author, all of those kind of things. And we'll pull up this velocity report here, so you can see here that, like, here's the team's velocity over the last couple weeks. We have, like, you know, breakdowns of each cycle, that kind of thing. And what we've tried to do with a lot of this is a lot of these questions that do match, we will provide kind of like, its own context free grammar per question on the ones that match. So you can provide really, like, reliable and consistent, formatted answers, which we know is really important. And it's not just saying, like, well, this answer looked good, but then the next one looks kind of like off. So we want to have that consistency in a lot of the answers that we provide. We are about that at a time. So questions,
we can see that, like, we've got a little bit more information about this particular PR so we've got, like, the status of it and kind of key insights, and then those things that we've pulled up, the logging implementation here, we can see that, like, Okay, here's an like, overview of it pulls up, you know, these are kind of the key files to look at. These are like, read or PRs that, you know, we've kind of determined are kind of like, valuable or important. And then we can also look at this subject matter expert answer as well. We'll drag this over, make a little larger, so it's actually providing examples of how this is used in the code base. And then we'll provide related PRs and tickets, and then actually suggest who are like the experts that you should go talk to about this. Now, how does this use? Bedrock? We use like this is primarily like a planning agent. So it's building kind of that that every was free move style. So it's like reasoning without observation. Takes a set of tools. We develop a plan based on those tools, then we execute it that uses, like Claude, we use on it for that. But along the way we have, like a pre planning process. We have some like use case, kind of, we call use cases. So like, inference matching of commonly asked questions. So we use, like, haiku for that. And then for a lot of the within the tools, we do a kind of like, you know, if we're chunking data, or we're kind of, like fixing the JQL query or something that is malformed, we use, know, the light for a lot of that. So we can do, like, really fast parallel calls. We can be, like, really, really efficient with how we do that, and we try to save the kind of, like, beefier longer calls for, for some of the anthropic models. And obviously, like, we use bedrock, but we can kind of test out new models as they come across. This is also built using kind of Lang chain, Lang graph. Sorry if that's a dirty term around here, but yeah, so we've been using that as well. So, yeah, we can see, you know, some other questions that are commonly pulled up. You know, we can look at, like, trend analysis kind of things. So let's take a look at this. You know, what's the velocity trend for the Explore teams? We can go back look at past cycles. We can actually generate charts with this. So we have a chart generation tool, and you can say, okay, like, we're going to map these things over time. For engineering managers who are looking at, like, bottlenecks, of like, you know, where their team might be going off, we can kind of pull that information up. And then the last thing I want to show is one thing we've been working on. It's been working on kind of an incident analysis tool where you can provide, like, some basic context about an incident and when it occurred, and then we can go and try to, like, pull up all the PRS around that, analyze each PR based on the question and the kind of error provided, and give you kind of, like a confidence score where we think you should be looking first. We know that when you have incidents like, you know, finding that initial PR is really, really important, and being like, Okay, what might have caused this? So we've been looking at, kind of exploring that, and how we can kind of identify that, like, root cause analysis and like, kind of cut the time to action there. So, you know, we can see that we have, like, a ability to build, like, the confidence score on that, the related ticket, the author, all of those kind of things. And we'll pull up this velocity report here, so you can see here that, like, here's the team's velocity over the last couple weeks. We have, like, you know, breakdowns of each cycle, that kind of thing. And what we've tried to do with a lot of this is a lot of these questions that do match, we will provide kind of like, its own context free grammar per question on the ones that match. So you can provide really, like, reliable and consistent, formatted answers, which we know is really important. And it's not just saying, like, well, this answer looked good, but then the next one looks kind of like off. So we want to have that consistency in a lot of the answers that we provide. We are about that at a time. So questions,
we can see that, like, we've got a little bit more information about this particular PR so we've got, like, the status of it and kind of key insights, and then those things that we've pulled up, the logging implementation here, we can see that, like, Okay, here's an like, overview of it pulls up, you know, these are kind of the key files to look at. These are like, read or PRs that, you know, we've kind of determined are kind of like, valuable or important. And then we can also look at this subject matter expert answer as well. We'll drag this over, make a little larger, so it's actually providing examples of how this is used in the code base. And then we'll provide related PRs and tickets, and then actually suggest who are like the experts that you should go talk to about this. Now, how does this use? Bedrock? We use like this is primarily like a planning agent. So it's building kind of that that every was free move style. So it's like reasoning without observation. Takes a set of tools. We develop a plan based on those tools, then we execute it that uses, like Claude, we use on it for that. But along the way we have, like a pre planning process. We have some like use case, kind of, we call use cases. So like, inference matching of commonly asked questions. So we use, like, haiku for that. And then for a lot of the within the tools, we do a kind of like, you know, if we're chunking data, or we're kind of, like fixing the JQL query or something that is malformed, we use, know, the light for a lot of that. So we can do, like, really fast parallel calls. We can be, like, really, really efficient with how we do that, and we try to save the kind of, like, beefier longer calls for, for some of the anthropic models. And obviously, like, we use bedrock, but we can kind of test out new models as they come across. This is also built using kind of Lang chain, Lang graph. Sorry if that's a dirty term around here, but yeah, so we've been using that as well. So, yeah, we can see, you know, some other questions that are commonly pulled up. You know, we can look at, like, trend analysis kind of things. So let's take a look at this. You know, what's the velocity trend for the Explore teams? We can go back look at past cycles. We can actually generate charts with this. So we have a chart generation tool, and you can say, okay, like, we're going to map these things over time. For engineering managers who are looking at, like, bottlenecks, of like, you know, where their team might be going off, we can kind of pull that information up. And then the last thing I want to show is one thing we've been working on. It's been working on kind of an incident analysis tool where you can provide, like, some basic context about an incident and when it occurred, and then we can go and try to, like, pull up all the PRS around that, analyze each PR based on the question and the kind of error provided, and give you kind of, like a confidence score where we think you should be looking first. We know that when you have incidents like, you know, finding that initial PR is really, really important, and being like, Okay, what might have caused this? So we've been looking at, kind of exploring that, and how we can kind of identify that, like, root cause analysis and like, kind of cut the time to action there. So, you know, we can see that we have, like, a ability to build, like, the confidence score on that, the related ticket, the author, all of those kind of things. And we'll pull up this velocity report here, so you can see here that, like, here's the team's velocity over the last couple weeks. We have, like, you know, breakdowns of each cycle, that kind of thing. And what we've tried to do with a lot of this is a lot of these questions that do match, we will provide kind of like, its own context free grammar per question on the ones that match. So you can provide really, like, reliable and consistent, formatted answers, which we know is really important. And it's not just saying, like, well, this answer looked good, but then the next one looks kind of like off. So we want to have that consistency in a lot of the answers that we provide. We are about that at a time. So questions,
41:20anyone have any questions for Greg?
anyone have any questions for Greg?
anyone have any questions for Greg?
anyone have any questions for Greg?
S Speaker 1341:27So what kind of challenges have you come across with generating the graphs like this?
So what kind of challenges have you come across with generating the graphs like this?
So what kind of challenges have you come across with generating the graphs like this?
So what kind of challenges have you come across with generating the graphs like this?
S Speaker 1241:33I mean, there's obviously the consistency piece of it. Just like making up some data and giving you a graph, we allow the chart tool to decide what kind of chart it should generate. And the prompt engineering aspect of that of like, when is a pie chart appropriate and when is a line chart appropriate, it can sometimes get incredibly wrong, and it's like, I gave you a pie chart for something that is like, makes no sense. It just says, like, here's your team members in a pie chart. Obviously, that's not important.
I mean, there's obviously the consistency piece of it. Just like making up some data and giving you a graph, we allow the chart tool to decide what kind of chart it should generate. And the prompt engineering aspect of that of like, when is a pie chart appropriate and when is a line chart appropriate, it can sometimes get incredibly wrong, and it's like, I gave you a pie chart for something that is like, makes no sense. It just says, like, here's your team members in a pie chart. Obviously, that's not important.
I mean, there's obviously the consistency piece of it. Just like making up some data and giving you a graph, we allow the chart tool to decide what kind of chart it should generate. And the prompt engineering aspect of that of like, when is a pie chart appropriate and when is a line chart appropriate, it can sometimes get incredibly wrong, and it's like, I gave you a pie chart for something that is like, makes no sense. It just says, like, here's your team members in a pie chart. Obviously, that's not important.
I mean, there's obviously the consistency piece of it. Just like making up some data and giving you a graph, we allow the chart tool to decide what kind of chart it should generate. And the prompt engineering aspect of that of like, when is a pie chart appropriate and when is a line chart appropriate, it can sometimes get incredibly wrong, and it's like, I gave you a pie chart for something that is like, makes no sense. It just says, like, here's your team members in a pie chart. Obviously, that's not important.
S Speaker 1342:02So did you get any issues with the numbers? Are you showing? We do have some
So did you get any issues with the numbers? Are you showing? We do have some
So did you get any issues with the numbers? Are you showing? We do have some
So did you get any issues with the numbers? Are you showing? We do have some
S Speaker 1242:06issues with some of like the formatting and numbers, but we've been able to largely, kind of like fight through that. And you know, it's the charts are kind of held to a higher standard than other pieces in the code base, where it'll kind of like attempt to fix it at the end, and if it says, like, hey, this isn't meeting a certain standard. It just will toss out the chart.
issues with some of like the formatting and numbers, but we've been able to largely, kind of like fight through that. And you know, it's the charts are kind of held to a higher standard than other pieces in the code base, where it'll kind of like attempt to fix it at the end, and if it says, like, hey, this isn't meeting a certain standard. It just will toss out the chart.
issues with some of like the formatting and numbers, but we've been able to largely, kind of like fight through that. And you know, it's the charts are kind of held to a higher standard than other pieces in the code base, where it'll kind of like attempt to fix it at the end, and if it says, like, hey, this isn't meeting a certain standard. It just will toss out the chart.
issues with some of like the formatting and numbers, but we've been able to largely, kind of like fight through that. And you know, it's the charts are kind of held to a higher standard than other pieces in the code base, where it'll kind of like attempt to fix it at the end, and if it says, like, hey, this isn't meeting a certain standard. It just will toss out the chart.
S Speaker 1442:26Thank you. So you mentioned that you guys have been working on some, like, specific tools. Do you ground the functionality of kind of like, you know this, Jenny I functionality of asking anything into a specific, limited set of functionalities inside.
Thank you. So you mentioned that you guys have been working on some, like, specific tools. Do you ground the functionality of kind of like, you know this, Jenny I functionality of asking anything into a specific, limited set of functionalities inside.
Thank you. So you mentioned that you guys have been working on some, like, specific tools. Do you ground the functionality of kind of like, you know this, Jenny I functionality of asking anything into a specific, limited set of functionalities inside.
Thank you. So you mentioned that you guys have been working on some, like, specific tools. Do you ground the functionality of kind of like, you know this, Jenny I functionality of asking anything into a specific, limited set of functionalities inside.
S Speaker 1242:42So we actually have, like, a kind of, like a pre planning process where it'll decide, like, hey, given the available tools that I have, and like, the question that's being asked to me, do I, like, make a plan for it, or I just pass it through? And so it'll pass through to to Claude, just like a regular haiku question otherwise. So if I ask, like, you know, what's the population of San Francisco? It's not going to go pull up a JIRA tool and try to answer that. We kind of debated, of, like, whether we even wanted to do that, but we just kind of said, Hey, for now, just like, pass through, especially if you say, like, oh, I want to provide some code snippet and then, like, ask it about a ticket. You should be able to do that in one chat thread.
So we actually have, like, a kind of, like a pre planning process where it'll decide, like, hey, given the available tools that I have, and like, the question that's being asked to me, do I, like, make a plan for it, or I just pass it through? And so it'll pass through to to Claude, just like a regular haiku question otherwise. So if I ask, like, you know, what's the population of San Francisco? It's not going to go pull up a JIRA tool and try to answer that. We kind of debated, of, like, whether we even wanted to do that, but we just kind of said, Hey, for now, just like, pass through, especially if you say, like, oh, I want to provide some code snippet and then, like, ask it about a ticket. You should be able to do that in one chat thread.
So we actually have, like, a kind of, like a pre planning process where it'll decide, like, hey, given the available tools that I have, and like, the question that's being asked to me, do I, like, make a plan for it, or I just pass it through? And so it'll pass through to to Claude, just like a regular haiku question otherwise. So if I ask, like, you know, what's the population of San Francisco? It's not going to go pull up a JIRA tool and try to answer that. We kind of debated, of, like, whether we even wanted to do that, but we just kind of said, Hey, for now, just like, pass through, especially if you say, like, oh, I want to provide some code snippet and then, like, ask it about a ticket. You should be able to do that in one chat thread.
So we actually have, like, a kind of, like a pre planning process where it'll decide, like, hey, given the available tools that I have, and like, the question that's being asked to me, do I, like, make a plan for it, or I just pass it through? And so it'll pass through to to Claude, just like a regular haiku question otherwise. So if I ask, like, you know, what's the population of San Francisco? It's not going to go pull up a JIRA tool and try to answer that. We kind of debated, of, like, whether we even wanted to do that, but we just kind of said, Hey, for now, just like, pass through, especially if you say, like, oh, I want to provide some code snippet and then, like, ask it about a ticket. You should be able to do that in one chat thread.
43:19This get greater medical Fauci Positive.
This get greater medical Fauci Positive.
This get greater medical Fauci Positive.
This get greater medical Fauci Positive.
43:25Sarah with letter.
S Speaker 1543:42App, hi everyone. My name is Sarah from Letta. So today I'm going to talk about the Leda agents framework. Many of you might be familiar with frameworks like career, AI land graph, this is kind of in the same category of that, but Letta is really a framework that's really focused on state and memory and context management as first class principles. The company and also the project was founded by the team behind men GPT, which was probably one of the first projects to introduce the notion of using agents for memory or context management. And so a lot of the challenges that we saw with people developing agents kind of came down to like context management or having like canonical state representation that works across model providers. So Letta is basically an agent framework that does context management and memory for you, and is also provider agnostic, so you can use OpenAI anthropic bedrock, whatever back end that we support, but then you still have all your state and context represented in an agnostic way. So your memories will never be locked into open AI assistance. You can always move the entire state of your agent between different model providers. So yeah, if you're interested, since we are at Amazon, you can use letter with Amazon AWS bedrock by basically just setting up these environment variables and then running our server. So for today, I'm going to basically go over a demo of using letter locally. So so we do have lot of cloud which is currently not generally available, but this is this. What I'm showing here is actually all like you can go and try this today. So this is basically what we call the agent development environment, basically a UI to build and interact with agents. But I do also want to emphasize that everything that I do in this demo is actually also, you can also do all the same operations, like listing agents, creating agents, whatever, through both our APIs, and then we also have a TypeScript in Python SDK,
App, hi everyone. My name is Sarah from Letta. So today I'm going to talk about the Leda agents framework. Many of you might be familiar with frameworks like career, AI land graph, this is kind of in the same category of that, but Letta is really a framework that's really focused on state and memory and context management as first class principles. The company and also the project was founded by the team behind men GPT, which was probably one of the first projects to introduce the notion of using agents for memory or context management. And so a lot of the challenges that we saw with people developing agents kind of came down to like context management or having like canonical state representation that works across model providers. So Letta is basically an agent framework that does context management and memory for you, and is also provider agnostic, so you can use OpenAI anthropic bedrock, whatever back end that we support, but then you still have all your state and context represented in an agnostic way. So your memories will never be locked into open AI assistance. You can always move the entire state of your agent between different model providers. So yeah, if you're interested, since we are at Amazon, you can use letter with Amazon AWS bedrock by basically just setting up these environment variables and then running our server. So for today, I'm going to basically go over a demo of using letter locally. So so we do have lot of cloud which is currently not generally available, but this is this. What I'm showing here is actually all like you can go and try this today. So this is basically what we call the agent development environment, basically a UI to build and interact with agents. But I do also want to emphasize that everything that I do in this demo is actually also, you can also do all the same operations, like listing agents, creating agents, whatever, through both our APIs, and then we also have a TypeScript in Python SDK,
App, hi everyone. My name is Sarah from Letta. So today I'm going to talk about the Leda agents framework. Many of you might be familiar with frameworks like career, AI land graph, this is kind of in the same category of that, but Letta is really a framework that's really focused on state and memory and context management as first class principles. The company and also the project was founded by the team behind men GPT, which was probably one of the first projects to introduce the notion of using agents for memory or context management. And so a lot of the challenges that we saw with people developing agents kind of came down to like context management or having like canonical state representation that works across model providers. So Letta is basically an agent framework that does context management and memory for you, and is also provider agnostic, so you can use OpenAI anthropic bedrock, whatever back end that we support, but then you still have all your state and context represented in an agnostic way. So your memories will never be locked into open AI assistance. You can always move the entire state of your agent between different model providers. So yeah, if you're interested, since we are at Amazon, you can use letter with Amazon AWS bedrock by basically just setting up these environment variables and then running our server. So for today, I'm going to basically go over a demo of using letter locally. So so we do have lot of cloud which is currently not generally available, but this is this. What I'm showing here is actually all like you can go and try this today. So this is basically what we call the agent development environment, basically a UI to build and interact with agents. But I do also want to emphasize that everything that I do in this demo is actually also, you can also do all the same operations, like listing agents, creating agents, whatever, through both our APIs, and then we also have a TypeScript in Python SDK,
App, hi everyone. My name is Sarah from Letta. So today I'm going to talk about the Leda agents framework. Many of you might be familiar with frameworks like career, AI land graph, this is kind of in the same category of that, but Letta is really a framework that's really focused on state and memory and context management as first class principles. The company and also the project was founded by the team behind men GPT, which was probably one of the first projects to introduce the notion of using agents for memory or context management. And so a lot of the challenges that we saw with people developing agents kind of came down to like context management or having like canonical state representation that works across model providers. So Letta is basically an agent framework that does context management and memory for you, and is also provider agnostic, so you can use OpenAI anthropic bedrock, whatever back end that we support, but then you still have all your state and context represented in an agnostic way. So your memories will never be locked into open AI assistance. You can always move the entire state of your agent between different model providers. So yeah, if you're interested, since we are at Amazon, you can use letter with Amazon AWS bedrock by basically just setting up these environment variables and then running our server. So for today, I'm going to basically go over a demo of using letter locally. So so we do have lot of cloud which is currently not generally available, but this is this. What I'm showing here is actually all like you can go and try this today. So this is basically what we call the agent development environment, basically a UI to build and interact with agents. But I do also want to emphasize that everything that I do in this demo is actually also, you can also do all the same operations, like listing agents, creating agents, whatever, through both our APIs, and then we also have a TypeScript in Python SDK,
46:15all right. So one thing that's kind of unique about Lyta
all right. So one thing that's kind of unique about Lyta
all right. So one thing that's kind of unique about Lyta
all right. So one thing that's kind of unique about Lyta
S Speaker 1546:19is that it runs as a service. So to basically start up let out, I need to run docker run. So this is going to get basically start up the letter service with both anthropic and open AI enabled, because I set these keys. And so now if I go and refresh the server, I can see that. I can actually see agents. So these are basically agents running on this service over here.
is that it runs as a service. So to basically start up let out, I need to run docker run. So this is going to get basically start up the letter service with both anthropic and open AI enabled, because I set these keys. And so now if I go and refresh the server, I can see that. I can actually see agents. So these are basically agents running on this service over here.
is that it runs as a service. So to basically start up let out, I need to run docker run. So this is going to get basically start up the letter service with both anthropic and open AI enabled, because I set these keys. And so now if I go and refresh the server, I can see that. I can actually see agents. So these are basically agents running on this service over here.
is that it runs as a service. So to basically start up let out, I need to run docker run. So this is going to get basically start up the letter service with both anthropic and open AI enabled, because I set these keys. And so now if I go and refresh the server, I can see that. I can actually see agents. So these are basically agents running on this service over here.
46:49Also is this thing known for people, all
Also is this thing known for people, all
Also is this thing known for people, all
Also is this thing known for people, all
S Speaker 1551:37Oh yeah, this is a really dummy toy. I'm sorry I didn't come up with something more realistic.
Oh yeah, this is a really dummy toy. I'm sorry I didn't come up with something more realistic.
Oh yeah, this is a really dummy toy. I'm sorry I didn't come up with something more realistic.
Oh yeah, this is a really dummy toy. I'm sorry I didn't come up with something more realistic.
S Speaker 1551:52Yeah. So this is basically like a tool that I can now like create for my agent. And then Are people familiar with like tool calling and JSON schemas, yeah. So this is basically, like the JSON schema of this tool I may have messed up from, okay, I think this might be a little behind, but yeah, I can also kind of like, test out this tool in here. So I can, like, write name Sarah, and then run this, see, like the logs. And then now I can, like, attach this to my agent as well. So I can be like, please look up my Okay, sorry. This name is Sarah, so
Yeah. So this is basically like a tool that I can now like create for my agent. And then Are people familiar with like tool calling and JSON schemas, yeah. So this is basically, like the JSON schema of this tool I may have messed up from, okay, I think this might be a little behind, but yeah, I can also kind of like, test out this tool in here. So I can, like, write name Sarah, and then run this, see, like the logs. And then now I can, like, attach this to my agent as well. So I can be like, please look up my Okay, sorry. This name is Sarah, so
Yeah. So this is basically like a tool that I can now like create for my agent. And then Are people familiar with like tool calling and JSON schemas, yeah. So this is basically, like the JSON schema of this tool I may have messed up from, okay, I think this might be a little behind, but yeah, I can also kind of like, test out this tool in here. So I can, like, write name Sarah, and then run this, see, like the logs. And then now I can, like, attach this to my agent as well. So I can be like, please look up my Okay, sorry. This name is Sarah, so
Yeah. So this is basically like a tool that I can now like create for my agent. And then Are people familiar with like tool calling and JSON schemas, yeah. So this is basically, like the JSON schema of this tool I may have messed up from, okay, I think this might be a little behind, but yeah, I can also kind of like, test out this tool in here. So I can, like, write name Sarah, and then run this, see, like the logs. And then now I can, like, attach this to my agent as well. So I can be like, please look up my Okay, sorry. This name is Sarah, so
52:34this should hopefully trigger it to call a tool.
this should hopefully trigger it to call a tool.
this should hopefully trigger it to call a tool.
this should hopefully trigger it to call a tool.
S Speaker 853:59Anyone have if we have time for one question here,
Anyone have if we have time for one question here,
Anyone have if we have time for one question here,
Anyone have if we have time for one question here,
S Speaker 1654:06I feel bad that you got the one question you have, but yeah, just quick question on memory limit at some point of time, you could eventually hit this ceiling weight. At some point of time, you can eventually not be able to compress it anymore to what sort of the limit, and then the other sort of do barter with no bar. None of the bars actually makes sense compared to the other. But the second one was, does that sort of exist as a layer between your API, like your opening eye API, and yourself? So that's just how it exists, just up there.
I feel bad that you got the one question you have, but yeah, just quick question on memory limit at some point of time, you could eventually hit this ceiling weight. At some point of time, you can eventually not be able to compress it anymore to what sort of the limit, and then the other sort of do barter with no bar. None of the bars actually makes sense compared to the other. But the second one was, does that sort of exist as a layer between your API, like your opening eye API, and yourself? So that's just how it exists, just up there.
I feel bad that you got the one question you have, but yeah, just quick question on memory limit at some point of time, you could eventually hit this ceiling weight. At some point of time, you can eventually not be able to compress it anymore to what sort of the limit, and then the other sort of do barter with no bar. None of the bars actually makes sense compared to the other. But the second one was, does that sort of exist as a layer between your API, like your opening eye API, and yourself? So that's just how it exists, just up there.
I feel bad that you got the one question you have, but yeah, just quick question on memory limit at some point of time, you could eventually hit this ceiling weight. At some point of time, you can eventually not be able to compress it anymore to what sort of the limit, and then the other sort of do barter with no bar. None of the bars actually makes sense compared to the other. But the second one was, does that sort of exist as a layer between your API, like your opening eye API, and yourself? So that's just how it exists, just up there.
S Speaker 1554:39Yeah. So letter is basically like a layer between the LMS that you interact with. So you interact with a stateful Asians API, instead of a stateless track locations API, and in terms of the compaction, so let it actually has like a notion of like a memory hierarchy. So there's in context memory, and then there's external memory. And so external memory, we have both archival memory, which is just kind of like a catch all vector dB, and then we also log all the conversations, and the agent also has the ability to go back and search that conversation. So basically, what happens, like with, yeah, I went through this bit quickly, but like this demo of like spamming the agent with a bunch of messages, is that eventually letter will just evict older messages to come back down the context window, and then just the agent can only access those through searching the database, so it's basically kind of automatically managed over time, rather than just continually compressing. Let's
Yeah. So letter is basically like a layer between the LMS that you interact with. So you interact with a stateful Asians API, instead of a stateless track locations API, and in terms of the compaction, so let it actually has like a notion of like a memory hierarchy. So there's in context memory, and then there's external memory. And so external memory, we have both archival memory, which is just kind of like a catch all vector dB, and then we also log all the conversations, and the agent also has the ability to go back and search that conversation. So basically, what happens, like with, yeah, I went through this bit quickly, but like this demo of like spamming the agent with a bunch of messages, is that eventually letter will just evict older messages to come back down the context window, and then just the agent can only access those through searching the database, so it's basically kind of automatically managed over time, rather than just continually compressing. Let's
Yeah. So letter is basically like a layer between the LMS that you interact with. So you interact with a stateful Asians API, instead of a stateless track locations API, and in terms of the compaction, so let it actually has like a notion of like a memory hierarchy. So there's in context memory, and then there's external memory. And so external memory, we have both archival memory, which is just kind of like a catch all vector dB, and then we also log all the conversations, and the agent also has the ability to go back and search that conversation. So basically, what happens, like with, yeah, I went through this bit quickly, but like this demo of like spamming the agent with a bunch of messages, is that eventually letter will just evict older messages to come back down the context window, and then just the agent can only access those through searching the database, so it's basically kind of automatically managed over time, rather than just continually compressing. Let's
Yeah. So letter is basically like a layer between the LMS that you interact with. So you interact with a stateful Asians API, instead of a stateless track locations API, and in terms of the compaction, so let it actually has like a notion of like a memory hierarchy. So there's in context memory, and then there's external memory. And so external memory, we have both archival memory, which is just kind of like a catch all vector dB, and then we also log all the conversations, and the agent also has the ability to go back and search that conversation. So basically, what happens, like with, yeah, I went through this bit quickly, but like this demo of like spamming the agent with a bunch of messages, is that eventually letter will just evict older messages to come back down the context window, and then just the agent can only access those through searching the database, so it's basically kind of automatically managed over time, rather than just continually compressing. Let's
55:29get started around in
get started around in
get started around in
get started around in
55:37The pause with Ninja tech. Do
The pause with Ninja tech. Do
The pause with Ninja tech. Do
The pause with Ninja tech. Do
56:00the first test I'm
56:22telling failing the first test guys.
telling failing the first test guys.
telling failing the first test guys.
telling failing the first test guys.
S Speaker 1756:27I don't know if you guys can look right. So okay, so, yeah, all right. So it sounds like I'm between food and break, so I'll try to make it quick, but also interesting.
I don't know if you guys can look right. So okay, so, yeah, all right. So it sounds like I'm between food and break, so I'll try to make it quick, but also interesting.
I don't know if you guys can look right. So okay, so, yeah, all right. So it sounds like I'm between food and break, so I'll try to make it quick, but also interesting.
I don't know if you guys can look right. So okay, so, yeah, all right. So it sounds like I'm between food and break, so I'll try to make it quick, but also interesting.
56:42My name is Bobak. I'm the
My name is Bobak. I'm the
My name is Bobak. I'm the
My name is Bobak. I'm the
56:43founder and CEO of ninja tech. Ai. I don't know
founder and CEO of ninja tech. Ai. I don't know
founder and CEO of ninja tech. Ai. I don't know
founder and CEO of ninja tech. Ai. I don't know
S Speaker 1756:47how many of you guys have heard of us. Has anyone heard of ninja four? Yes, just two people. Three. Sounds good. So we are an agentic multi assistant. The vision is to democratize access to agentic AI, but also give everyone a product as well as a platform that's all in one. Essentially, there's a lot of obviously great general products, but each one costs $20 $30 $40 so it won't use everything. It's going to cost 260 essentially, how to use the best of the best. So we set out on vision to just create these things with a combination of open source as well as proprietary models. And I'll walk you guys through sort of a few things. But in a sense, the way to think about us is that we operate at this skill level. We have eight skills in the system. It's you can use it for writing and brainstorming, AI, sort of code generation, image generation, editing, file analysis, image analysis and video Gen and whatnot. So that's one way to think about us, but also our core technologies, about compound AI, and we've been on this quest to get through these agentic flow workflows that we can solve problems step by step, both in this digital world and then soon out in the real world. And we made some advances. We said, I will show you guys a few things, but let's start off small. So you know, typical stuff that we all seen, chat bots and whatnot. So I can just say, so write me snake game in c+ with mouse and keyboard support. So if you're in the ninja mode, it takes certainly one of the models that we've set us to the default for some certain questions. You change that default. This does the right code. It's using deep seat version three, and it's doing something so but you know what I'm going to say, Hey, should we go and I use sonnet and as this COVID, I can say, reflect on reflect on this and make it much faster, and now it's going to feed that into sonnet seamlessly. You don't need to have multiple accounts. You just have one account and have access to 20 models and whatnot. And let's just say, faster, it's
how many of you guys have heard of us. Has anyone heard of ninja four? Yes, just two people. Three. Sounds good. So we are an agentic multi assistant. The vision is to democratize access to agentic AI, but also give everyone a product as well as a platform that's all in one. Essentially, there's a lot of obviously great general products, but each one costs $20 $30 $40 so it won't use everything. It's going to cost 260 essentially, how to use the best of the best. So we set out on vision to just create these things with a combination of open source as well as proprietary models. And I'll walk you guys through sort of a few things. But in a sense, the way to think about us is that we operate at this skill level. We have eight skills in the system. It's you can use it for writing and brainstorming, AI, sort of code generation, image generation, editing, file analysis, image analysis and video Gen and whatnot. So that's one way to think about us, but also our core technologies, about compound AI, and we've been on this quest to get through these agentic flow workflows that we can solve problems step by step, both in this digital world and then soon out in the real world. And we made some advances. We said, I will show you guys a few things, but let's start off small. So you know, typical stuff that we all seen, chat bots and whatnot. So I can just say, so write me snake game in c+ with mouse and keyboard support. So if you're in the ninja mode, it takes certainly one of the models that we've set us to the default for some certain questions. You change that default. This does the right code. It's using deep seat version three, and it's doing something so but you know what I'm going to say, Hey, should we go and I use sonnet and as this COVID, I can say, reflect on reflect on this and make it much faster, and now it's going to feed that into sonnet seamlessly. You don't need to have multiple accounts. You just have one account and have access to 20 models and whatnot. And let's just say, faster, it's
how many of you guys have heard of us. Has anyone heard of ninja four? Yes, just two people. Three. Sounds good. So we are an agentic multi assistant. The vision is to democratize access to agentic AI, but also give everyone a product as well as a platform that's all in one. Essentially, there's a lot of obviously great general products, but each one costs $20 $30 $40 so it won't use everything. It's going to cost 260 essentially, how to use the best of the best. So we set out on vision to just create these things with a combination of open source as well as proprietary models. And I'll walk you guys through sort of a few things. But in a sense, the way to think about us is that we operate at this skill level. We have eight skills in the system. It's you can use it for writing and brainstorming, AI, sort of code generation, image generation, editing, file analysis, image analysis and video Gen and whatnot. So that's one way to think about us, but also our core technologies, about compound AI, and we've been on this quest to get through these agentic flow workflows that we can solve problems step by step, both in this digital world and then soon out in the real world. And we made some advances. We said, I will show you guys a few things, but let's start off small. So you know, typical stuff that we all seen, chat bots and whatnot. So I can just say, so write me snake game in c+ with mouse and keyboard support. So if you're in the ninja mode, it takes certainly one of the models that we've set us to the default for some certain questions. You change that default. This does the right code. It's using deep seat version three, and it's doing something so but you know what I'm going to say, Hey, should we go and I use sonnet and as this COVID, I can say, reflect on reflect on this and make it much faster, and now it's going to feed that into sonnet seamlessly. You don't need to have multiple accounts. You just have one account and have access to 20 models and whatnot. And let's just say, faster, it's
how many of you guys have heard of us. Has anyone heard of ninja four? Yes, just two people. Three. Sounds good. So we are an agentic multi assistant. The vision is to democratize access to agentic AI, but also give everyone a product as well as a platform that's all in one. Essentially, there's a lot of obviously great general products, but each one costs $20 $30 $40 so it won't use everything. It's going to cost 260 essentially, how to use the best of the best. So we set out on vision to just create these things with a combination of open source as well as proprietary models. And I'll walk you guys through sort of a few things. But in a sense, the way to think about us is that we operate at this skill level. We have eight skills in the system. It's you can use it for writing and brainstorming, AI, sort of code generation, image generation, editing, file analysis, image analysis and video Gen and whatnot. So that's one way to think about us, but also our core technologies, about compound AI, and we've been on this quest to get through these agentic flow workflows that we can solve problems step by step, both in this digital world and then soon out in the real world. And we made some advances. We said, I will show you guys a few things, but let's start off small. So you know, typical stuff that we all seen, chat bots and whatnot. So I can just say, so write me snake game in c+ with mouse and keyboard support. So if you're in the ninja mode, it takes certainly one of the models that we've set us to the default for some certain questions. You change that default. This does the right code. It's using deep seat version three, and it's doing something so but you know what I'm going to say, Hey, should we go and I use sonnet and as this COVID, I can say, reflect on reflect on this and make it much faster, and now it's going to feed that into sonnet seamlessly. You don't need to have multiple accounts. You just have one account and have access to 20 models and whatnot. And let's just say, faster, it's
59:22going to execution, and let's see.
going to execution, and let's see.
going to execution, and let's see.
going to execution, and let's see.
S Speaker 1759:29And let's see. And so if I was coding the sonnet, I can say, Hey, can you give me an image to go along with this game? And our system is fully asynchronous during that most of the systems you need to wait for it, you can actually fire away other questions, hey, who created?
And let's see. And so if I was coding the sonnet, I can say, Hey, can you give me an image to go along with this game? And our system is fully asynchronous during that most of the systems you need to wait for it, you can actually fire away other questions, hey, who created?
And let's see. And so if I was coding the sonnet, I can say, Hey, can you give me an image to go along with this game? And our system is fully asynchronous during that most of the systems you need to wait for it, you can actually fire away other questions, hey, who created?
And let's see. And so if I was coding the sonnet, I can say, Hey, can you give me an image to go along with this game? And our system is fully asynchronous during that most of the systems you need to wait for it, you can actually fire away other questions, hey, who created?
59:51Who created the first snake game?
Who created the first snake game?
Who created the first snake game?
Who created the first snake game?
S Speaker 171:00:00As you all know, when you use Cloud, sonnet is not or cloud is not internet connected, our agents will go and search the red and then repeat the context to the model we selected. So now you're seeing dev searches on so this is sort of, this is sort of the beginning. You've enabled GP for all in one product. You access to Gemini GP for all sonnet Nova models and whatnot, and also our own model defined to increase. So that was sort of the start of this, this thing. And it's sort of, it's very versatile. We can choose it. We can even set the defaults for writing encoding. You can choose one model for an image. You can use flux or Nova, or any observed things and or just, let me just choose for you. So that was kind of like the product that's been around for less than a year. So we launched last year, and we originally launched on using training on one. So we were the first companies that use databases, customer chips to do this. But now let so we one of the sort of the compounding that technology developed had to do with this problem we've been having. So in order to get agentic stuff going, as most of you guys know, it turns out, you really need to be good at step by step problem solving. And the best model in the world is actually opening I 01, and this damn thing is too expensive to use, so it's cool to make a demo out of it. But in our calculation, every time we try to make something super full blown agentic style, like a DP search or like order me, you know, in our experience, it would cost anywhere between $2 up to actually $60 per task. So no bueno, that won't work. So we ended up building our own. We ended up creating this. We used inference level optimization heavily. It's and we also learn how to fine tune reasoning models. So there is core right now. It's a deep sea car, one that's fine tuned by us, and we do series of optimizations to further improve it. As you guys know, most reasoning models, if you want to see how good at it, how good is it at the agentic stuff? Actually open AI showcase that if you're really good at Olympia level math, you're damn good at it, essentially. So there are benchmarks. We announced this about a week and a half ago. This is, you know, regular sonnet, 23% is the score, solid, special scaffolding. It's about 80 and then ours is 86 we are actually above all one already. But let me show you that in action what it means to do this. So this is aim. This is one of the problems. You can give it now to our system to solve it. And I kind of waste your time too much, but I didn't solve it. But you actually come here, you choose our super agent or 2.0 and then, once it's selected, this is a thinking model. It has three gears built in. So if it's if you say, Hello, how are you? It doesn't turn into reasoning model. If it becomes and then and then also, if it's none of those, if it's not a chapel banker, it kicks into a reasoning mode as that was the problem solving, and some way through if the problem is too hard. We also developed a technology that actually tries to detect if the problem is going nowhere. Then it switches into high power mode automatically. Because I don't know if you guys know chatgpt, any user you have, there's a whole one, then there's a lot formal and you have to kind of decide, we're trying to do later model that users can decide on these things. So yeah, so as you know, the right answer is, so you know these things. So this is doing its own thing, and can actually watch it in action as it's just trying to sort of me since I started, I moved forward this one. We're not exposing all the thinking tokens it's generating. We're just showing sort of the summary of the thinking as we're working its way through. The answer. The correct answer is 80 for this math problem. This is generalized flash thinking experimental This is the answer that they got for this problem as I was solving it. This is open AI 01 gave you the same problem in a minute and 58 seconds later, you got confidently 60 is the right answer, and the correct answer is actually not that. It's actually, in fact, 80. So that's it. That's the compound AI system. It doesn't cost you $60 per million token. Internally, when we release the API, we run price it between two to $4 per million token, because internally, our cost, thanks to our partners AWS, is less than $1 per million token. So that's what we achieve now. So this is the analysis two weeks ago. You guys are more important to use it, and it doesn't cost $200 a month. Our product, we have been for individuals and businesses. It's to get this super agent. It's $15 a month, unlimited, all you can eat. You know, there's no being counting, even doing it. The subject are used, if you have use the system to be attached like an automation tool, will definitely throttle you. But the two birds, two modes, consumer and business and the business one is only $20 it's also available on AWS Marketplace. So everyone else is 30. Are just 20. And how we can do that is because we're doing a heavy use of open source, you really got to put up fine tuning models. And our costs are pretty damn cheap because we're not carrying the foundation model behind our back so but that was so super agent really has been a big unlock for us. I
As you all know, when you use Cloud, sonnet is not or cloud is not internet connected, our agents will go and search the red and then repeat the context to the model we selected. So now you're seeing dev searches on so this is sort of, this is sort of the beginning. You've enabled GP for all in one product. You access to Gemini GP for all sonnet Nova models and whatnot, and also our own model defined to increase. So that was sort of the start of this, this thing. And it's sort of, it's very versatile. We can choose it. We can even set the defaults for writing encoding. You can choose one model for an image. You can use flux or Nova, or any observed things and or just, let me just choose for you. So that was kind of like the product that's been around for less than a year. So we launched last year, and we originally launched on using training on one. So we were the first companies that use databases, customer chips to do this. But now let so we one of the sort of the compounding that technology developed had to do with this problem we've been having. So in order to get agentic stuff going, as most of you guys know, it turns out, you really need to be good at step by step problem solving. And the best model in the world is actually opening I 01, and this damn thing is too expensive to use, so it's cool to make a demo out of it. But in our calculation, every time we try to make something super full blown agentic style, like a DP search or like order me, you know, in our experience, it would cost anywhere between $2 up to actually $60 per task. So no bueno, that won't work. So we ended up building our own. We ended up creating this. We used inference level optimization heavily. It's and we also learn how to fine tune reasoning models. So there is core right now. It's a deep sea car, one that's fine tuned by us, and we do series of optimizations to further improve it. As you guys know, most reasoning models, if you want to see how good at it, how good is it at the agentic stuff? Actually open AI showcase that if you're really good at Olympia level math, you're damn good at it, essentially. So there are benchmarks. We announced this about a week and a half ago. This is, you know, regular sonnet, 23% is the score, solid, special scaffolding. It's about 80 and then ours is 86 we are actually above all one already. But let me show you that in action what it means to do this. So this is aim. This is one of the problems. You can give it now to our system to solve it. And I kind of waste your time too much, but I didn't solve it. But you actually come here, you choose our super agent or 2.0 and then, once it's selected, this is a thinking model. It has three gears built in. So if it's if you say, Hello, how are you? It doesn't turn into reasoning model. If it becomes and then and then also, if it's none of those, if it's not a chapel banker, it kicks into a reasoning mode as that was the problem solving, and some way through if the problem is too hard. We also developed a technology that actually tries to detect if the problem is going nowhere. Then it switches into high power mode automatically. Because I don't know if you guys know chatgpt, any user you have, there's a whole one, then there's a lot formal and you have to kind of decide, we're trying to do later model that users can decide on these things. So yeah, so as you know, the right answer is, so you know these things. So this is doing its own thing, and can actually watch it in action as it's just trying to sort of me since I started, I moved forward this one. We're not exposing all the thinking tokens it's generating. We're just showing sort of the summary of the thinking as we're working its way through. The answer. The correct answer is 80 for this math problem. This is generalized flash thinking experimental This is the answer that they got for this problem as I was solving it. This is open AI 01 gave you the same problem in a minute and 58 seconds later, you got confidently 60 is the right answer, and the correct answer is actually not that. It's actually, in fact, 80. So that's it. That's the compound AI system. It doesn't cost you $60 per million token. Internally, when we release the API, we run price it between two to $4 per million token, because internally, our cost, thanks to our partners AWS, is less than $1 per million token. So that's what we achieve now. So this is the analysis two weeks ago. You guys are more important to use it, and it doesn't cost $200 a month. Our product, we have been for individuals and businesses. It's to get this super agent. It's $15 a month, unlimited, all you can eat. You know, there's no being counting, even doing it. The subject are used, if you have use the system to be attached like an automation tool, will definitely throttle you. But the two birds, two modes, consumer and business and the business one is only $20 it's also available on AWS Marketplace. So everyone else is 30. Are just 20. And how we can do that is because we're doing a heavy use of open source, you really got to put up fine tuning models. And our costs are pretty damn cheap because we're not carrying the foundation model behind our back so but that was so super agent really has been a big unlock for us. I
As you all know, when you use Cloud, sonnet is not or cloud is not internet connected, our agents will go and search the red and then repeat the context to the model we selected. So now you're seeing dev searches on so this is sort of, this is sort of the beginning. You've enabled GP for all in one product. You access to Gemini GP for all sonnet Nova models and whatnot, and also our own model defined to increase. So that was sort of the start of this, this thing. And it's sort of, it's very versatile. We can choose it. We can even set the defaults for writing encoding. You can choose one model for an image. You can use flux or Nova, or any observed things and or just, let me just choose for you. So that was kind of like the product that's been around for less than a year. So we launched last year, and we originally launched on using training on one. So we were the first companies that use databases, customer chips to do this. But now let so we one of the sort of the compounding that technology developed had to do with this problem we've been having. So in order to get agentic stuff going, as most of you guys know, it turns out, you really need to be good at step by step problem solving. And the best model in the world is actually opening I 01, and this damn thing is too expensive to use, so it's cool to make a demo out of it. But in our calculation, every time we try to make something super full blown agentic style, like a DP search or like order me, you know, in our experience, it would cost anywhere between $2 up to actually $60 per task. So no bueno, that won't work. So we ended up building our own. We ended up creating this. We used inference level optimization heavily. It's and we also learn how to fine tune reasoning models. So there is core right now. It's a deep sea car, one that's fine tuned by us, and we do series of optimizations to further improve it. As you guys know, most reasoning models, if you want to see how good at it, how good is it at the agentic stuff? Actually open AI showcase that if you're really good at Olympia level math, you're damn good at it, essentially. So there are benchmarks. We announced this about a week and a half ago. This is, you know, regular sonnet, 23% is the score, solid, special scaffolding. It's about 80 and then ours is 86 we are actually above all one already. But let me show you that in action what it means to do this. So this is aim. This is one of the problems. You can give it now to our system to solve it. And I kind of waste your time too much, but I didn't solve it. But you actually come here, you choose our super agent or 2.0 and then, once it's selected, this is a thinking model. It has three gears built in. So if it's if you say, Hello, how are you? It doesn't turn into reasoning model. If it becomes and then and then also, if it's none of those, if it's not a chapel banker, it kicks into a reasoning mode as that was the problem solving, and some way through if the problem is too hard. We also developed a technology that actually tries to detect if the problem is going nowhere. Then it switches into high power mode automatically. Because I don't know if you guys know chatgpt, any user you have, there's a whole one, then there's a lot formal and you have to kind of decide, we're trying to do later model that users can decide on these things. So yeah, so as you know, the right answer is, so you know these things. So this is doing its own thing, and can actually watch it in action as it's just trying to sort of me since I started, I moved forward this one. We're not exposing all the thinking tokens it's generating. We're just showing sort of the summary of the thinking as we're working its way through. The answer. The correct answer is 80 for this math problem. This is generalized flash thinking experimental This is the answer that they got for this problem as I was solving it. This is open AI 01 gave you the same problem in a minute and 58 seconds later, you got confidently 60 is the right answer, and the correct answer is actually not that. It's actually, in fact, 80. So that's it. That's the compound AI system. It doesn't cost you $60 per million token. Internally, when we release the API, we run price it between two to $4 per million token, because internally, our cost, thanks to our partners AWS, is less than $1 per million token. So that's what we achieve now. So this is the analysis two weeks ago. You guys are more important to use it, and it doesn't cost $200 a month. Our product, we have been for individuals and businesses. It's to get this super agent. It's $15 a month, unlimited, all you can eat. You know, there's no being counting, even doing it. The subject are used, if you have use the system to be attached like an automation tool, will definitely throttle you. But the two birds, two modes, consumer and business and the business one is only $20 it's also available on AWS Marketplace. So everyone else is 30. Are just 20. And how we can do that is because we're doing a heavy use of open source, you really got to put up fine tuning models. And our costs are pretty damn cheap because we're not carrying the foundation model behind our back so but that was so super agent really has been a big unlock for us. I
As you all know, when you use Cloud, sonnet is not or cloud is not internet connected, our agents will go and search the red and then repeat the context to the model we selected. So now you're seeing dev searches on so this is sort of, this is sort of the beginning. You've enabled GP for all in one product. You access to Gemini GP for all sonnet Nova models and whatnot, and also our own model defined to increase. So that was sort of the start of this, this thing. And it's sort of, it's very versatile. We can choose it. We can even set the defaults for writing encoding. You can choose one model for an image. You can use flux or Nova, or any observed things and or just, let me just choose for you. So that was kind of like the product that's been around for less than a year. So we launched last year, and we originally launched on using training on one. So we were the first companies that use databases, customer chips to do this. But now let so we one of the sort of the compounding that technology developed had to do with this problem we've been having. So in order to get agentic stuff going, as most of you guys know, it turns out, you really need to be good at step by step problem solving. And the best model in the world is actually opening I 01, and this damn thing is too expensive to use, so it's cool to make a demo out of it. But in our calculation, every time we try to make something super full blown agentic style, like a DP search or like order me, you know, in our experience, it would cost anywhere between $2 up to actually $60 per task. So no bueno, that won't work. So we ended up building our own. We ended up creating this. We used inference level optimization heavily. It's and we also learn how to fine tune reasoning models. So there is core right now. It's a deep sea car, one that's fine tuned by us, and we do series of optimizations to further improve it. As you guys know, most reasoning models, if you want to see how good at it, how good is it at the agentic stuff? Actually open AI showcase that if you're really good at Olympia level math, you're damn good at it, essentially. So there are benchmarks. We announced this about a week and a half ago. This is, you know, regular sonnet, 23% is the score, solid, special scaffolding. It's about 80 and then ours is 86 we are actually above all one already. But let me show you that in action what it means to do this. So this is aim. This is one of the problems. You can give it now to our system to solve it. And I kind of waste your time too much, but I didn't solve it. But you actually come here, you choose our super agent or 2.0 and then, once it's selected, this is a thinking model. It has three gears built in. So if it's if you say, Hello, how are you? It doesn't turn into reasoning model. If it becomes and then and then also, if it's none of those, if it's not a chapel banker, it kicks into a reasoning mode as that was the problem solving, and some way through if the problem is too hard. We also developed a technology that actually tries to detect if the problem is going nowhere. Then it switches into high power mode automatically. Because I don't know if you guys know chatgpt, any user you have, there's a whole one, then there's a lot formal and you have to kind of decide, we're trying to do later model that users can decide on these things. So yeah, so as you know, the right answer is, so you know these things. So this is doing its own thing, and can actually watch it in action as it's just trying to sort of me since I started, I moved forward this one. We're not exposing all the thinking tokens it's generating. We're just showing sort of the summary of the thinking as we're working its way through. The answer. The correct answer is 80 for this math problem. This is generalized flash thinking experimental This is the answer that they got for this problem as I was solving it. This is open AI 01 gave you the same problem in a minute and 58 seconds later, you got confidently 60 is the right answer, and the correct answer is actually not that. It's actually, in fact, 80. So that's it. That's the compound AI system. It doesn't cost you $60 per million token. Internally, when we release the API, we run price it between two to $4 per million token, because internally, our cost, thanks to our partners AWS, is less than $1 per million token. So that's what we achieve now. So this is the analysis two weeks ago. You guys are more important to use it, and it doesn't cost $200 a month. Our product, we have been for individuals and businesses. It's to get this super agent. It's $15 a month, unlimited, all you can eat. You know, there's no being counting, even doing it. The subject are used, if you have use the system to be attached like an automation tool, will definitely throttle you. But the two birds, two modes, consumer and business and the business one is only $20 it's also available on AWS Marketplace. So everyone else is 30. Are just 20. And how we can do that is because we're doing a heavy use of open source, you really got to put up fine tuning models. And our costs are pretty damn cheap because we're not carrying the foundation model behind our back so but that was so super agent really has been a big unlock for us. I
1:05:37have 40 seconds to search to show you something cool.
have 40 seconds to search to show you something cool.
have 40 seconds to search to show you something cool.
have 40 seconds to search to show you something cool.
S Speaker 171:05:40Artists like time for questions. Oh, yeah, you guys want to see something we're about to launch next week already. So we are about to launch our next gen deep researcher based on the same technology to build. And this is coming out. We're trying to we will have a quiet release about that Friday. If you use our product, you can proceed in the product, and then we'll do LinkedIn post about it before next week or something. This thing is the model that we've been after, essentially. So you can give it a difficult so any difficult question, travel, shopping, you name it, or deep research of any sort. I'm trying to I'm trying to find tickets from San Jose to Seattle for one person, ideally business class in the next four weeks now for three nights, find me dates that ideally are not raining, and keep the price of the trip under 5k but make it fun and give me cool things to do at nights per day. So I don't know how many of you guys do business travel and stuff. This stuff usually takes a couple hours to land right. Remember, I said this reasoning stuff is too damn expensive. We think we craft the code because this thing is now it's going to come with a plan, and it uses the same reasoning model. But also, we do use code ads. One of the technologies we built back then was to solve very difficult questions, to combat hallucination and to achieve accuracy. For every step, we actually use Python. We generate Python code if you run through the problem, we debug the Python code, and then off it goes. I ran a farm query, actually. So this is going to stop looking in a second. So there is a, there is a there's a query that's very interesting, actually. So one of our good friends, I don't know if you guys know Alexa from people,
Artists like time for questions. Oh, yeah, you guys want to see something we're about to launch next week already. So we are about to launch our next gen deep researcher based on the same technology to build. And this is coming out. We're trying to we will have a quiet release about that Friday. If you use our product, you can proceed in the product, and then we'll do LinkedIn post about it before next week or something. This thing is the model that we've been after, essentially. So you can give it a difficult so any difficult question, travel, shopping, you name it, or deep research of any sort. I'm trying to I'm trying to find tickets from San Jose to Seattle for one person, ideally business class in the next four weeks now for three nights, find me dates that ideally are not raining, and keep the price of the trip under 5k but make it fun and give me cool things to do at nights per day. So I don't know how many of you guys do business travel and stuff. This stuff usually takes a couple hours to land right. Remember, I said this reasoning stuff is too damn expensive. We think we craft the code because this thing is now it's going to come with a plan, and it uses the same reasoning model. But also, we do use code ads. One of the technologies we built back then was to solve very difficult questions, to combat hallucination and to achieve accuracy. For every step, we actually use Python. We generate Python code if you run through the problem, we debug the Python code, and then off it goes. I ran a farm query, actually. So this is going to stop looking in a second. So there is a, there is a there's a query that's very interesting, actually. So one of our good friends, I don't know if you guys know Alexa from people,
Artists like time for questions. Oh, yeah, you guys want to see something we're about to launch next week already. So we are about to launch our next gen deep researcher based on the same technology to build. And this is coming out. We're trying to we will have a quiet release about that Friday. If you use our product, you can proceed in the product, and then we'll do LinkedIn post about it before next week or something. This thing is the model that we've been after, essentially. So you can give it a difficult so any difficult question, travel, shopping, you name it, or deep research of any sort. I'm trying to I'm trying to find tickets from San Jose to Seattle for one person, ideally business class in the next four weeks now for three nights, find me dates that ideally are not raining, and keep the price of the trip under 5k but make it fun and give me cool things to do at nights per day. So I don't know how many of you guys do business travel and stuff. This stuff usually takes a couple hours to land right. Remember, I said this reasoning stuff is too damn expensive. We think we craft the code because this thing is now it's going to come with a plan, and it uses the same reasoning model. But also, we do use code ads. One of the technologies we built back then was to solve very difficult questions, to combat hallucination and to achieve accuracy. For every step, we actually use Python. We generate Python code if you run through the problem, we debug the Python code, and then off it goes. I ran a farm query, actually. So this is going to stop looking in a second. So there is a, there is a there's a query that's very interesting, actually. So one of our good friends, I don't know if you guys know Alexa from people,
Artists like time for questions. Oh, yeah, you guys want to see something we're about to launch next week already. So we are about to launch our next gen deep researcher based on the same technology to build. And this is coming out. We're trying to we will have a quiet release about that Friday. If you use our product, you can proceed in the product, and then we'll do LinkedIn post about it before next week or something. This thing is the model that we've been after, essentially. So you can give it a difficult so any difficult question, travel, shopping, you name it, or deep research of any sort. I'm trying to I'm trying to find tickets from San Jose to Seattle for one person, ideally business class in the next four weeks now for three nights, find me dates that ideally are not raining, and keep the price of the trip under 5k but make it fun and give me cool things to do at nights per day. So I don't know how many of you guys do business travel and stuff. This stuff usually takes a couple hours to land right. Remember, I said this reasoning stuff is too damn expensive. We think we craft the code because this thing is now it's going to come with a plan, and it uses the same reasoning model. But also, we do use code ads. One of the technologies we built back then was to solve very difficult questions, to combat hallucination and to achieve accuracy. For every step, we actually use Python. We generate Python code if you run through the problem, we debug the Python code, and then off it goes. I ran a farm query, actually. So this is going to stop looking in a second. So there is a, there is a there's a query that's very interesting, actually. So one of our good friends, I don't know if you guys know Alexa from people,
1:08:05just maybe, like, one
just maybe, like, one
just maybe, like, one
just maybe, like, one
S Speaker 171:08:08more seconds, a friend of mine, his name is Danny. He works at pork. I forgot his last name. I know he works at Alexa. He has the title business, and they can find out what that is, right? So I use perplexity, and it says, So, found Danny. And then it works out for and then it says, confidently, Danny's is a Dallas Texas thingy and whatever. That's not true. OpenAI doesn't do it. And Gemini goes into this investigative report about Danny something, something which is complete hallucination. And then ours found Danny's side hustle. It turns out it's actually a textile business. I forgot it was just genuinely, actually happened to me. And it finds the entire sort of answer, and how it does it is we are step by step reasoning as well as collapse. So we are using use code generation to fix all our issues. And this is not viable as a technology, as a product. It doesn't cost us 60 bucks this query and use opening attribute that cost us about $11 the dollars is paying. What's it?
more seconds, a friend of mine, his name is Danny. He works at pork. I forgot his last name. I know he works at Alexa. He has the title business, and they can find out what that is, right? So I use perplexity, and it says, So, found Danny. And then it works out for and then it says, confidently, Danny's is a Dallas Texas thingy and whatever. That's not true. OpenAI doesn't do it. And Gemini goes into this investigative report about Danny something, something which is complete hallucination. And then ours found Danny's side hustle. It turns out it's actually a textile business. I forgot it was just genuinely, actually happened to me. And it finds the entire sort of answer, and how it does it is we are step by step reasoning as well as collapse. So we are using use code generation to fix all our issues. And this is not viable as a technology, as a product. It doesn't cost us 60 bucks this query and use opening attribute that cost us about $11 the dollars is paying. What's it?
more seconds, a friend of mine, his name is Danny. He works at pork. I forgot his last name. I know he works at Alexa. He has the title business, and they can find out what that is, right? So I use perplexity, and it says, So, found Danny. And then it works out for and then it says, confidently, Danny's is a Dallas Texas thingy and whatever. That's not true. OpenAI doesn't do it. And Gemini goes into this investigative report about Danny something, something which is complete hallucination. And then ours found Danny's side hustle. It turns out it's actually a textile business. I forgot it was just genuinely, actually happened to me. And it finds the entire sort of answer, and how it does it is we are step by step reasoning as well as collapse. So we are using use code generation to fix all our issues. And this is not viable as a technology, as a product. It doesn't cost us 60 bucks this query and use opening attribute that cost us about $11 the dollars is paying. What's it?
more seconds, a friend of mine, his name is Danny. He works at pork. I forgot his last name. I know he works at Alexa. He has the title business, and they can find out what that is, right? So I use perplexity, and it says, So, found Danny. And then it works out for and then it says, confidently, Danny's is a Dallas Texas thingy and whatever. That's not true. OpenAI doesn't do it. And Gemini goes into this investigative report about Danny something, something which is complete hallucination. And then ours found Danny's side hustle. It turns out it's actually a textile business. I forgot it was just genuinely, actually happened to me. And it finds the entire sort of answer, and how it does it is we are step by step reasoning as well as collapse. So we are using use code generation to fix all our issues. And this is not viable as a technology, as a product. It doesn't cost us 60 bucks this query and use opening attribute that cost us about $11 the dollars is paying. What's it?
S Speaker 181:09:22Thanks? I appreciate
S Speaker 81:09:28everyone coming out. We have, we still have food and drinks, I think, left, or at least some drinks and maybe some snacks.
everyone coming out. We have, we still have food and drinks, I think, left, or at least some drinks and maybe some snacks.
everyone coming out. We have, we still have food and drinks, I think, left, or at least some drinks and maybe some snacks.
everyone coming out. We have, we still have food and drinks, I think, left, or at least some drinks and maybe some snacks.
1:09:42a survey here.
S Speaker 81:09:48So, two things we have an event, the next event is April 16.
So, two things we have an event, the next event is April 16.
So, two things we have an event, the next event is April 16.
So, two things we have an event, the next event is April 16.