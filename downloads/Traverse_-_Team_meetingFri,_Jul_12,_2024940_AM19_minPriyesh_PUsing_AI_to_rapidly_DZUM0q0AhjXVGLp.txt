Meeting: Traverse - Team meeting
Fri, Jul 12, 2024
9:40 AM
19 min
Priyesh P
Using AI to rapidly understand 3D
URL: https://otter.ai/u/DZUM0q0AhjXVGLp8jVzP7GIzPtk
Downloaded: 2025-12-22T15:06:03.479157
Method: text_extraction
============================================================

S Speaker 10:00anything or granding Dyno if you're familiar with those with those models, and then once you have that we convert that into a scene tree where this is the this is a reconstruction of that scene. And then you can actually feed that into another model to get to answer questions about seen. So when we talked about how, you know, when you upload, you scan a construction site and then you upload it to the model and ask it to show you the the safety violations. This is kind of the underlying foundation of that how it would actually answer some of those questions and there's different methods to do this. All these are all very state of the art and they're very new. And they are there. This to answer your question from just a moment ago. These are all very data intensive where they tokenize huge amounts of 3d data and they take months and months and months to train. And ours doesn't need that. Ours just needs the ability to take that Gaussian spotting seen cluster ID and then explicitly write out the scene tree for its understanding. And we want to build on top of this as well and there's there's different hybrid approaches that can be extremely performance. But this is just an example of like the underlying direction that we're moving in, that is focused on rapidly understanding 3d Seems for customers and being able to build additional applications on top of that,
anything or granding Dyno if you're familiar with those with those models, and then once you have that we convert that into a scene tree where this is the this is a reconstruction of that scene. And then you can actually feed that into another model to get to answer questions about seen. So when we talked about how, you know, when you upload, you scan a construction site and then you upload it to the model and ask it to show you the the safety violations. This is kind of the underlying foundation of that how it would actually answer some of those questions and there's different methods to do this. All these are all very state of the art and they're very new. And they are there. This to answer your question from just a moment ago. These are all very data intensive where they tokenize huge amounts of 3d data and they take months and months and months to train. And ours doesn't need that. Ours just needs the ability to take that Gaussian spotting seen cluster ID and then explicitly write out the scene tree for its understanding. And we want to build on top of this as well and there's there's different hybrid approaches that can be extremely performance. But this is just an example of like the underlying direction that we're moving in, that is focused on rapidly understanding 3d Seems for customers and being able to build additional applications on top of that,
anything or granding Dyno if you're familiar with those with those models, and then once you have that we convert that into a scene tree where this is the this is a reconstruction of that scene. And then you can actually feed that into another model to get to answer questions about seen. So when we talked about how, you know, when you upload, you scan a construction site and then you upload it to the model and ask it to show you the the safety violations. This is kind of the underlying foundation of that how it would actually answer some of those questions and there's different methods to do this. All these are all very state of the art and they're very new. And they are there. This to answer your question from just a moment ago. These are all very data intensive where they tokenize huge amounts of 3d data and they take months and months and months to train. And ours doesn't need that. Ours just needs the ability to take that Gaussian spotting seen cluster ID and then explicitly write out the scene tree for its understanding. And we want to build on top of this as well and there's there's different hybrid approaches that can be extremely performance. But this is just an example of like the underlying direction that we're moving in, that is focused on rapidly understanding 3d Seems for customers and being able to build additional applications on top of that,
anything or granding Dyno if you're familiar with those with those models, and then once you have that we convert that into a scene tree where this is the this is a reconstruction of that scene. And then you can actually feed that into another model to get to answer questions about seen. So when we talked about how, you know, when you upload, you scan a construction site and then you upload it to the model and ask it to show you the the safety violations. This is kind of the underlying foundation of that how it would actually answer some of those questions and there's different methods to do this. All these are all very state of the art and they're very new. And they are there. This to answer your question from just a moment ago. These are all very data intensive where they tokenize huge amounts of 3d data and they take months and months and months to train. And ours doesn't need that. Ours just needs the ability to take that Gaussian spotting seen cluster ID and then explicitly write out the scene tree for its understanding. And we want to build on top of this as well and there's there's different hybrid approaches that can be extremely performance. But this is just an example of like the underlying direction that we're moving in, that is focused on rapidly understanding 3d Seems for customers and being able to build additional applications on top of that,
S Speaker 21:28but this is not applying to the rendering, right? This is only explaining what's in the scene.
but this is not applying to the rendering, right? This is only explaining what's in the scene.
but this is not applying to the rendering, right? This is only explaining what's in the scene.
but this is not applying to the rendering, right? This is only explaining what's in the scene.
S Speaker 11:35So while you go through this process, you do apply it to the scene as well. So when you when you run it through when you get the language embeddings and you cluster there's language embeddings that is being applied to the scene directly as well. And so that's where it's a really good point that you made. So the first one is understanding cognitively the scene so it can answer your questions about it. And the second piece of that, which is everything we're currently building right now is is that is the call and response between the user in the scene as you're typing into the chat like Hey, I like tires, it has to know and interact with that scene, like ping those like language embeddings of the tire and then highlight those tires for them to be displayed to the to the customer. So and this is all the at the application layer that we're really trying to build out. But none of this exists yet. So it's slow going to some degree.
So while you go through this process, you do apply it to the scene as well. So when you when you run it through when you get the language embeddings and you cluster there's language embeddings that is being applied to the scene directly as well. And so that's where it's a really good point that you made. So the first one is understanding cognitively the scene so it can answer your questions about it. And the second piece of that, which is everything we're currently building right now is is that is the call and response between the user in the scene as you're typing into the chat like Hey, I like tires, it has to know and interact with that scene, like ping those like language embeddings of the tire and then highlight those tires for them to be displayed to the to the customer. So and this is all the at the application layer that we're really trying to build out. But none of this exists yet. So it's slow going to some degree.
So while you go through this process, you do apply it to the scene as well. So when you when you run it through when you get the language embeddings and you cluster there's language embeddings that is being applied to the scene directly as well. And so that's where it's a really good point that you made. So the first one is understanding cognitively the scene so it can answer your questions about it. And the second piece of that, which is everything we're currently building right now is is that is the call and response between the user in the scene as you're typing into the chat like Hey, I like tires, it has to know and interact with that scene, like ping those like language embeddings of the tire and then highlight those tires for them to be displayed to the to the customer. So and this is all the at the application layer that we're really trying to build out. But none of this exists yet. So it's slow going to some degree.
So while you go through this process, you do apply it to the scene as well. So when you when you run it through when you get the language embeddings and you cluster there's language embeddings that is being applied to the scene directly as well. And so that's where it's a really good point that you made. So the first one is understanding cognitively the scene so it can answer your questions about it. And the second piece of that, which is everything we're currently building right now is is that is the call and response between the user in the scene as you're typing into the chat like Hey, I like tires, it has to know and interact with that scene, like ping those like language embeddings of the tire and then highlight those tires for them to be displayed to the to the customer. So and this is all the at the application layer that we're really trying to build out. But none of this exists yet. So it's slow going to some degree.
S Speaker 22:32right on the 3d rendering, or do you have that it goes through extensive training process to build on the neural network in order to do that?
right on the 3d rendering, or do you have that it goes through extensive training process to build on the neural network in order to do that?
right on the 3d rendering, or do you have that it goes through extensive training process to build on the neural network in order to do that?
right on the 3d rendering, or do you have that it goes through extensive training process to build on the neural network in order to do that?
S Speaker 12:45No, I did. Go ahead. Yeah. I mean, this, okay, so you still have to go through a structure for motion. pipeline. And you still have to, you have to go through that process of neural rendering 3d scene segmentation that we just showed you. And then 3d scene understanding and 3d scene retrieval augmented generation. For the when you're when you're asking about this for the neural rendering and 3d scene segmentation portion. This is what we hope to have in that 30 minute window.
No, I did. Go ahead. Yeah. I mean, this, okay, so you still have to go through a structure for motion. pipeline. And you still have to, you have to go through that process of neural rendering 3d scene segmentation that we just showed you. And then 3d scene understanding and 3d scene retrieval augmented generation. For the when you're when you're asking about this for the neural rendering and 3d scene segmentation portion. This is what we hope to have in that 30 minute window.
No, I did. Go ahead. Yeah. I mean, this, okay, so you still have to go through a structure for motion. pipeline. And you still have to, you have to go through that process of neural rendering 3d scene segmentation that we just showed you. And then 3d scene understanding and 3d scene retrieval augmented generation. For the when you're when you're asking about this for the neural rendering and 3d scene segmentation portion. This is what we hope to have in that 30 minute window.
No, I did. Go ahead. Yeah. I mean, this, okay, so you still have to go through a structure for motion. pipeline. And you still have to, you have to go through that process of neural rendering 3d scene segmentation that we just showed you. And then 3d scene understanding and 3d scene retrieval augmented generation. For the when you're when you're asking about this for the neural rendering and 3d scene segmentation portion. This is what we hope to have in that 30 minute window.
S Speaker 23:25Are you training your own neural rendering model in order to generate a 3d scenery from to the inputs?
Are you training your own neural rendering model in order to generate a 3d scenery from to the inputs?
Are you training your own neural rendering model in order to generate a 3d scenery from to the inputs?
Are you training your own neural rendering model in order to generate a 3d scenery from to the inputs?
S Speaker 13:38I'm sorry, what was the what? Can you could you rephrase the question?
I'm sorry, what was the what? Can you could you rephrase the question?
I'm sorry, what was the what? Can you could you rephrase the question?
I'm sorry, what was the what? Can you could you rephrase the question?
S Speaker 23:41So my question is, are you building your own model, neural network model to tape it to the inputs and create a 3d scenario?
So my question is, are you building your own model, neural network model to tape it to the inputs and create a 3d scenario?
So my question is, are you building your own model, neural network model to tape it to the inputs and create a 3d scenario?
So my question is, are you building your own model, neural network model to tape it to the inputs and create a 3d scenario?
S Speaker 13:52Now right now we have like made a few adjustments to the, to the current models that are out there, but we don't necessarily want to make just the 3d reconstruction model will will make it better we have made it better. But that is something that we hope to do in the future, but we want to move as quickly as we can and just use the neural rendering models that are already out there, and then focus more on what you can do with that neural render once you're on the Berkeley research. Yes, exactly. Yeah.
Now right now we have like made a few adjustments to the, to the current models that are out there, but we don't necessarily want to make just the 3d reconstruction model will will make it better we have made it better. But that is something that we hope to do in the future, but we want to move as quickly as we can and just use the neural rendering models that are already out there, and then focus more on what you can do with that neural render once you're on the Berkeley research. Yes, exactly. Yeah.
Now right now we have like made a few adjustments to the, to the current models that are out there, but we don't necessarily want to make just the 3d reconstruction model will will make it better we have made it better. But that is something that we hope to do in the future, but we want to move as quickly as we can and just use the neural rendering models that are already out there, and then focus more on what you can do with that neural render once you're on the Berkeley research. Yes, exactly. Yeah.
Now right now we have like made a few adjustments to the, to the current models that are out there, but we don't necessarily want to make just the 3d reconstruction model will will make it better we have made it better. But that is something that we hope to do in the future, but we want to move as quickly as we can and just use the neural rendering models that are already out there, and then focus more on what you can do with that neural render once you're on the Berkeley research. Yes, exactly. Yeah.
4:27And if we don't know if you got anything to add to that as well.
And if we don't know if you got anything to add to that as well.
And if we don't know if you got anything to add to that as well.
And if we don't know if you got anything to add to that as well.
S Speaker 35:16impart language embeddings onto the different frames of video and then send that over into the 3d scene. And so that kind of was where our core IP is not is. We're using the open source rendering techniques, but we're trying to innovate on the ways to spatially and semantically embed these reconstructions with better intelligence. Are those models open source? They're gonna be good. 3d renders.
impart language embeddings onto the different frames of video and then send that over into the 3d scene. And so that kind of was where our core IP is not is. We're using the open source rendering techniques, but we're trying to innovate on the ways to spatially and semantically embed these reconstructions with better intelligence. Are those models open source? They're gonna be good. 3d renders.
impart language embeddings onto the different frames of video and then send that over into the 3d scene. And so that kind of was where our core IP is not is. We're using the open source rendering techniques, but we're trying to innovate on the ways to spatially and semantically embed these reconstructions with better intelligence. Are those models open source? They're gonna be good. 3d renders.
impart language embeddings onto the different frames of video and then send that over into the 3d scene. And so that kind of was where our core IP is not is. We're using the open source rendering techniques, but we're trying to innovate on the ways to spatially and semantically embed these reconstructions with better intelligence. Are those models open source? They're gonna be good. 3d renders.
5:47Are you seeing rendering or the semantic models
S Speaker 35:54Yes. Like the the Gaussian sliding models are pretty openly available and I think they're good enough that I think creating our own cluster IP we would like we wouldn't be able to take advantage of the research that is happening is because there's so many reasons papers and labs are working in this that we think it's probably foolish to just like create our IP while everyone has created this research property. Right. And so I think, at the point where like the research has stabilized and will clear on how stem might be around the right area as well.
Yes. Like the the Gaussian sliding models are pretty openly available and I think they're good enough that I think creating our own cluster IP we would like we wouldn't be able to take advantage of the research that is happening is because there's so many reasons papers and labs are working in this that we think it's probably foolish to just like create our IP while everyone has created this research property. Right. And so I think, at the point where like the research has stabilized and will clear on how stem might be around the right area as well.
Yes. Like the the Gaussian sliding models are pretty openly available and I think they're good enough that I think creating our own cluster IP we would like we wouldn't be able to take advantage of the research that is happening is because there's so many reasons papers and labs are working in this that we think it's probably foolish to just like create our IP while everyone has created this research property. Right. And so I think, at the point where like the research has stabilized and will clear on how stem might be around the right area as well.
Yes. Like the the Gaussian sliding models are pretty openly available and I think they're good enough that I think creating our own cluster IP we would like we wouldn't be able to take advantage of the research that is happening is because there's so many reasons papers and labs are working in this that we think it's probably foolish to just like create our IP while everyone has created this research property. Right. And so I think, at the point where like the research has stabilized and will clear on how stem might be around the right area as well.
S Speaker 26:31Professor UCSD a few months ago, and the USA humanitarian visa to do 3d, right models, and as we call them limitation bargain was you know, a lot of these things 3d cartoonish objects in this what a what a
Professor UCSD a few months ago, and the USA humanitarian visa to do 3d, right models, and as we call them limitation bargain was you know, a lot of these things 3d cartoonish objects in this what a what a
Professor UCSD a few months ago, and the USA humanitarian visa to do 3d, right models, and as we call them limitation bargain was you know, a lot of these things 3d cartoonish objects in this what a what a
Professor UCSD a few months ago, and the USA humanitarian visa to do 3d, right models, and as we call them limitation bargain was you know, a lot of these things 3d cartoonish objects in this what a what a
S Speaker 26:48but not necessarily do some real life scenarios. Where do you encounter that problem? Is all these use cases? I guess.
but not necessarily do some real life scenarios. Where do you encounter that problem? Is all these use cases? I guess.
but not necessarily do some real life scenarios. Where do you encounter that problem? Is all these use cases? I guess.
but not necessarily do some real life scenarios. Where do you encounter that problem? Is all these use cases? I guess.
S Speaker 37:00Yeah, I guess that brings up two points. I think what you're referring to are like generative text to 3d models, and that is given the text create a 3d model
Yeah, I guess that brings up two points. I think what you're referring to are like generative text to 3d models, and that is given the text create a 3d model
Yeah, I guess that brings up two points. I think what you're referring to are like generative text to 3d models, and that is given the text create a 3d model
Yeah, I guess that brings up two points. I think what you're referring to are like generative text to 3d models, and that is given the text create a 3d model
S Speaker 37:15and 3d. That is also kind of the same paradigm as well. Right? Or gender attributed 3d as well. This Yeah, like, I think it could be cartoonish but however, I think the rebuttal to that is like Sora, or all these texts, videos that have been coming out. And you can actually see that there's different tweets, actually, that these videos that these really good video models are coming out are actually really consistent. And so, like whatever you see in those videos, can become like a 3d nerd or a Gaussian slide. If that answered your question, right, like, in terms of, yeah, I think mainly the models were limited by the data, and now that people have trained it on such large datasets, the problem of it leaking cartoonish is like no no, no longer an issue. Okay.
and 3d. That is also kind of the same paradigm as well. Right? Or gender attributed 3d as well. This Yeah, like, I think it could be cartoonish but however, I think the rebuttal to that is like Sora, or all these texts, videos that have been coming out. And you can actually see that there's different tweets, actually, that these videos that these really good video models are coming out are actually really consistent. And so, like whatever you see in those videos, can become like a 3d nerd or a Gaussian slide. If that answered your question, right, like, in terms of, yeah, I think mainly the models were limited by the data, and now that people have trained it on such large datasets, the problem of it leaking cartoonish is like no no, no longer an issue. Okay.
and 3d. That is also kind of the same paradigm as well. Right? Or gender attributed 3d as well. This Yeah, like, I think it could be cartoonish but however, I think the rebuttal to that is like Sora, or all these texts, videos that have been coming out. And you can actually see that there's different tweets, actually, that these videos that these really good video models are coming out are actually really consistent. And so, like whatever you see in those videos, can become like a 3d nerd or a Gaussian slide. If that answered your question, right, like, in terms of, yeah, I think mainly the models were limited by the data, and now that people have trained it on such large datasets, the problem of it leaking cartoonish is like no no, no longer an issue. Okay.
and 3d. That is also kind of the same paradigm as well. Right? Or gender attributed 3d as well. This Yeah, like, I think it could be cartoonish but however, I think the rebuttal to that is like Sora, or all these texts, videos that have been coming out. And you can actually see that there's different tweets, actually, that these videos that these really good video models are coming out are actually really consistent. And so, like whatever you see in those videos, can become like a 3d nerd or a Gaussian slide. If that answered your question, right, like, in terms of, yeah, I think mainly the models were limited by the data, and now that people have trained it on such large datasets, the problem of it leaking cartoonish is like no no, no longer an issue. Okay.
S Speaker 18:09There's, so there's different ways of combining this with other centers as well. So we don't do this as of yet but this is a team out of out of China actually, that is working on something similar and they combine it with LIDAR.
There's, so there's different ways of combining this with other centers as well. So we don't do this as of yet but this is a team out of out of China actually, that is working on something similar and they combine it with LIDAR.
There's, so there's different ways of combining this with other centers as well. So we don't do this as of yet but this is a team out of out of China actually, that is working on something similar and they combine it with LIDAR.
There's, so there's different ways of combining this with other centers as well. So we don't do this as of yet but this is a team out of out of China actually, that is working on something similar and they combine it with LIDAR.
S Speaker 18:30And they get just absolutely stunning scenes. From this trying to get one of the videos and it goes to show that like in terms of like cartoonish or inaccurate scans. It's, it's rapidly getting to the point that technology is advancing so quickly, and there's different ways different techniques to this, that you can get extremely high quality survey grade renders that are acceptable for enterprise and military use very quickly. But we we are trying to build on the on the reconstructions to get people to be able to use these models more effectively. Because if you put this in the hands of if I were on the intelligence watch floor, if I just had a scan of Ukraine from overhead drone footage or some ground center LIDAR footage or something like that, the first question that I'm going to have it, what do I do with this? Like, what do I actually take this and go with it? And if you're telling me that I could interact with this using natural queries, and I could just say, show me where I think enemy might be because these are our intelligence reports. Or you actually give the models that the military is going to be using the ability to understand the spatial world through these reconstructions. That's kind of the area that we want to be and the quality of these reconstructions is going up rapidly. I even wear what is for, I would say last year is almost already obsolete, which is pretty wild.
And they get just absolutely stunning scenes. From this trying to get one of the videos and it goes to show that like in terms of like cartoonish or inaccurate scans. It's, it's rapidly getting to the point that technology is advancing so quickly, and there's different ways different techniques to this, that you can get extremely high quality survey grade renders that are acceptable for enterprise and military use very quickly. But we we are trying to build on the on the reconstructions to get people to be able to use these models more effectively. Because if you put this in the hands of if I were on the intelligence watch floor, if I just had a scan of Ukraine from overhead drone footage or some ground center LIDAR footage or something like that, the first question that I'm going to have it, what do I do with this? Like, what do I actually take this and go with it? And if you're telling me that I could interact with this using natural queries, and I could just say, show me where I think enemy might be because these are our intelligence reports. Or you actually give the models that the military is going to be using the ability to understand the spatial world through these reconstructions. That's kind of the area that we want to be and the quality of these reconstructions is going up rapidly. I even wear what is for, I would say last year is almost already obsolete, which is pretty wild.
And they get just absolutely stunning scenes. From this trying to get one of the videos and it goes to show that like in terms of like cartoonish or inaccurate scans. It's, it's rapidly getting to the point that technology is advancing so quickly, and there's different ways different techniques to this, that you can get extremely high quality survey grade renders that are acceptable for enterprise and military use very quickly. But we we are trying to build on the on the reconstructions to get people to be able to use these models more effectively. Because if you put this in the hands of if I were on the intelligence watch floor, if I just had a scan of Ukraine from overhead drone footage or some ground center LIDAR footage or something like that, the first question that I'm going to have it, what do I do with this? Like, what do I actually take this and go with it? And if you're telling me that I could interact with this using natural queries, and I could just say, show me where I think enemy might be because these are our intelligence reports. Or you actually give the models that the military is going to be using the ability to understand the spatial world through these reconstructions. That's kind of the area that we want to be and the quality of these reconstructions is going up rapidly. I even wear what is for, I would say last year is almost already obsolete, which is pretty wild.
And they get just absolutely stunning scenes. From this trying to get one of the videos and it goes to show that like in terms of like cartoonish or inaccurate scans. It's, it's rapidly getting to the point that technology is advancing so quickly, and there's different ways different techniques to this, that you can get extremely high quality survey grade renders that are acceptable for enterprise and military use very quickly. But we we are trying to build on the on the reconstructions to get people to be able to use these models more effectively. Because if you put this in the hands of if I were on the intelligence watch floor, if I just had a scan of Ukraine from overhead drone footage or some ground center LIDAR footage or something like that, the first question that I'm going to have it, what do I do with this? Like, what do I actually take this and go with it? And if you're telling me that I could interact with this using natural queries, and I could just say, show me where I think enemy might be because these are our intelligence reports. Or you actually give the models that the military is going to be using the ability to understand the spatial world through these reconstructions. That's kind of the area that we want to be and the quality of these reconstructions is going up rapidly. I even wear what is for, I would say last year is almost already obsolete, which is pretty wild.
S Speaker 410:15to you plus Company X groups that you just showed James skepticism. So they've got LIDAR. This is LIDAR data only or RGB plus Lidar and they put it on a drone and those those point clouds look very high fidelity.
to you plus Company X groups that you just showed James skepticism. So they've got LIDAR. This is LIDAR data only or RGB plus Lidar and they put it on a drone and those those point clouds look very high fidelity.
to you plus Company X groups that you just showed James skepticism. So they've got LIDAR. This is LIDAR data only or RGB plus Lidar and they put it on a drone and those those point clouds look very high fidelity.
to you plus Company X groups that you just showed James skepticism. So they've got LIDAR. This is LIDAR data only or RGB plus Lidar and they put it on a drone and those those point clouds look very high fidelity.
10:29Yeah, they're extremely high fidelity. So this
S Speaker 410:32is this is why they are also using Gaussians planning or no, yes,
is this is why they are also using Gaussians planning or no, yes,
is this is why they are also using Gaussians planning or no, yes,
is this is why they are also using Gaussians planning or no, yes,
S Speaker 511:01But I recall from the top of our call that you said that any regular RGB RGB camera does that, you know, you work with any, you know, any RGB camera, right. Don't need anything special.
But I recall from the top of our call that you said that any regular RGB RGB camera does that, you know, you work with any, you know, any RGB camera, right. Don't need anything special.
But I recall from the top of our call that you said that any regular RGB RGB camera does that, you know, you work with any, you know, any RGB camera, right. Don't need anything special.
But I recall from the top of our call that you said that any regular RGB RGB camera does that, you know, you work with any, you know, any RGB camera, right. Don't need anything special.
S Speaker 111:13Yeah, this is so I'm trying to make a connection between where we're going and where we currently are now. So we do not use LIDAR, but I'm saying that there's there are new techniques that are coming out that prevent any of this looking cartoonish. So the quality is increasing rapidly and have different techniques to do that Lidar is one of them. Another example maybe as a rebuttal to that is, this is the same team that put out the initial gumption splatting paper, they are doing huge, large scale reconstructions using nothing but RGB video Gaussian splatting. That's what you see here. And this is this is I would say the quality of what's coming on the horizon in the next couple of months and the quality we've seen just an explosion of it we've been following for the last two years now just in our in our personal research now as a company. And it's moving so rapidly because the quality is now tied to compute. It's not it's not it's not like traditional photogrammetry at all, so it's only going to get better over the over the coming years.
Yeah, this is so I'm trying to make a connection between where we're going and where we currently are now. So we do not use LIDAR, but I'm saying that there's there are new techniques that are coming out that prevent any of this looking cartoonish. So the quality is increasing rapidly and have different techniques to do that Lidar is one of them. Another example maybe as a rebuttal to that is, this is the same team that put out the initial gumption splatting paper, they are doing huge, large scale reconstructions using nothing but RGB video Gaussian splatting. That's what you see here. And this is this is I would say the quality of what's coming on the horizon in the next couple of months and the quality we've seen just an explosion of it we've been following for the last two years now just in our in our personal research now as a company. And it's moving so rapidly because the quality is now tied to compute. It's not it's not it's not like traditional photogrammetry at all, so it's only going to get better over the over the coming years.
Yeah, this is so I'm trying to make a connection between where we're going and where we currently are now. So we do not use LIDAR, but I'm saying that there's there are new techniques that are coming out that prevent any of this looking cartoonish. So the quality is increasing rapidly and have different techniques to do that Lidar is one of them. Another example maybe as a rebuttal to that is, this is the same team that put out the initial gumption splatting paper, they are doing huge, large scale reconstructions using nothing but RGB video Gaussian splatting. That's what you see here. And this is this is I would say the quality of what's coming on the horizon in the next couple of months and the quality we've seen just an explosion of it we've been following for the last two years now just in our in our personal research now as a company. And it's moving so rapidly because the quality is now tied to compute. It's not it's not it's not like traditional photogrammetry at all, so it's only going to get better over the over the coming years.
Yeah, this is so I'm trying to make a connection between where we're going and where we currently are now. So we do not use LIDAR, but I'm saying that there's there are new techniques that are coming out that prevent any of this looking cartoonish. So the quality is increasing rapidly and have different techniques to do that Lidar is one of them. Another example maybe as a rebuttal to that is, this is the same team that put out the initial gumption splatting paper, they are doing huge, large scale reconstructions using nothing but RGB video Gaussian splatting. That's what you see here. And this is this is I would say the quality of what's coming on the horizon in the next couple of months and the quality we've seen just an explosion of it we've been following for the last two years now just in our in our personal research now as a company. And it's moving so rapidly because the quality is now tied to compute. It's not it's not it's not like traditional photogrammetry at all, so it's only going to get better over the over the coming years.
S Speaker 512:19So Matterport was traditional photogrammetry right, they're stitching images. Right? They did like the I guess starting with the iPhone 12 They actually had the Lidar and it's actually leverage that you can you can you don't you don't need to buy their you know, whatever. $3,000 Camera graduates the number obviously gets better you get more precision. So what you're saying here is that you you're the Gaussian splat blockchain is fundamentally a more a better way to generate these 3d renderings. You enter one, number two, on the front end you have I'm just trying to compare contrast and it's been a while that them but on the front end, you have the ability to prompt and do a lot of queries about what's what's in the image. I'm assuming they have some of that today. So but fundamentally the way we should think about this, this just builds
So Matterport was traditional photogrammetry right, they're stitching images. Right? They did like the I guess starting with the iPhone 12 They actually had the Lidar and it's actually leverage that you can you can you don't you don't need to buy their you know, whatever. $3,000 Camera graduates the number obviously gets better you get more precision. So what you're saying here is that you you're the Gaussian splat blockchain is fundamentally a more a better way to generate these 3d renderings. You enter one, number two, on the front end you have I'm just trying to compare contrast and it's been a while that them but on the front end, you have the ability to prompt and do a lot of queries about what's what's in the image. I'm assuming they have some of that today. So but fundamentally the way we should think about this, this just builds
So Matterport was traditional photogrammetry right, they're stitching images. Right? They did like the I guess starting with the iPhone 12 They actually had the Lidar and it's actually leverage that you can you can you don't you don't need to buy their you know, whatever. $3,000 Camera graduates the number obviously gets better you get more precision. So what you're saying here is that you you're the Gaussian splat blockchain is fundamentally a more a better way to generate these 3d renderings. You enter one, number two, on the front end you have I'm just trying to compare contrast and it's been a while that them but on the front end, you have the ability to prompt and do a lot of queries about what's what's in the image. I'm assuming they have some of that today. So but fundamentally the way we should think about this, this just builds
So Matterport was traditional photogrammetry right, they're stitching images. Right? They did like the I guess starting with the iPhone 12 They actually had the Lidar and it's actually leverage that you can you can you don't you don't need to buy their you know, whatever. $3,000 Camera graduates the number obviously gets better you get more precision. So what you're saying here is that you you're the Gaussian splat blockchain is fundamentally a more a better way to generate these 3d renderings. You enter one, number two, on the front end you have I'm just trying to compare contrast and it's been a while that them but on the front end, you have the ability to prompt and do a lot of queries about what's what's in the image. I'm assuming they have some of that today. So but fundamentally the way we should think about this, this just builds
S Speaker 513:26Than then what they have their technology.
S Speaker 113:31Yes, I think it builds better models than they have. And the technology is very fresh, it's very new, but it's it's it's gonna get even better. So it's going to be an exciting time to be in the space like even a year or two years from now. And then on top of that, once you can digitize like these are much, I would say more true digital twin than what Matterport has produced and it has been producing at a fraction of the cost. I mean truly like, just for the time that it takes to for the imprints for the compute when you're running it, and then on top of that, you have the ability to feed these models like another way of thinking about this that that is very exciting to us is that we are connecting a lot of the the AI models that we currently know and love, usually typically LM to that 3d data and I know it sounds a little buzz wordy but it it truly is, I think like a big mental shift, where it's like, once you when you have an outlet and you give it an image understand that image you can use that image in context for your application. You give an LM an entire 3d scene, semantically organized and able to be parsed through at inference time. And now you've given that model the ability to understand the natural world. I mean, this is this is true. Kind of spatial intelligence that we're trying to go after. I want to be able to just have this scene and have it be miles and miles and miles of scenery or entire apartments or buildings. And I want to be able to ask a question and it immediately takes me there and it shows me what I need to know and it lets me gain larger inferences from that 3d data. So what's the best path to go here? What is the distance from x to y, etc. And there's a lot of technical gaps before we get there. But that's that is truly what we're building and we're starting with virtual training.
Yes, I think it builds better models than they have. And the technology is very fresh, it's very new, but it's it's it's gonna get even better. So it's going to be an exciting time to be in the space like even a year or two years from now. And then on top of that, once you can digitize like these are much, I would say more true digital twin than what Matterport has produced and it has been producing at a fraction of the cost. I mean truly like, just for the time that it takes to for the imprints for the compute when you're running it, and then on top of that, you have the ability to feed these models like another way of thinking about this that that is very exciting to us is that we are connecting a lot of the the AI models that we currently know and love, usually typically LM to that 3d data and I know it sounds a little buzz wordy but it it truly is, I think like a big mental shift, where it's like, once you when you have an outlet and you give it an image understand that image you can use that image in context for your application. You give an LM an entire 3d scene, semantically organized and able to be parsed through at inference time. And now you've given that model the ability to understand the natural world. I mean, this is this is true. Kind of spatial intelligence that we're trying to go after. I want to be able to just have this scene and have it be miles and miles and miles of scenery or entire apartments or buildings. And I want to be able to ask a question and it immediately takes me there and it shows me what I need to know and it lets me gain larger inferences from that 3d data. So what's the best path to go here? What is the distance from x to y, etc. And there's a lot of technical gaps before we get there. But that's that is truly what we're building and we're starting with virtual training.
Yes, I think it builds better models than they have. And the technology is very fresh, it's very new, but it's it's it's gonna get even better. So it's going to be an exciting time to be in the space like even a year or two years from now. And then on top of that, once you can digitize like these are much, I would say more true digital twin than what Matterport has produced and it has been producing at a fraction of the cost. I mean truly like, just for the time that it takes to for the imprints for the compute when you're running it, and then on top of that, you have the ability to feed these models like another way of thinking about this that that is very exciting to us is that we are connecting a lot of the the AI models that we currently know and love, usually typically LM to that 3d data and I know it sounds a little buzz wordy but it it truly is, I think like a big mental shift, where it's like, once you when you have an outlet and you give it an image understand that image you can use that image in context for your application. You give an LM an entire 3d scene, semantically organized and able to be parsed through at inference time. And now you've given that model the ability to understand the natural world. I mean, this is this is true. Kind of spatial intelligence that we're trying to go after. I want to be able to just have this scene and have it be miles and miles and miles of scenery or entire apartments or buildings. And I want to be able to ask a question and it immediately takes me there and it shows me what I need to know and it lets me gain larger inferences from that 3d data. So what's the best path to go here? What is the distance from x to y, etc. And there's a lot of technical gaps before we get there. But that's that is truly what we're building and we're starting with virtual training.
Yes, I think it builds better models than they have. And the technology is very fresh, it's very new, but it's it's it's gonna get even better. So it's going to be an exciting time to be in the space like even a year or two years from now. And then on top of that, once you can digitize like these are much, I would say more true digital twin than what Matterport has produced and it has been producing at a fraction of the cost. I mean truly like, just for the time that it takes to for the imprints for the compute when you're running it, and then on top of that, you have the ability to feed these models like another way of thinking about this that that is very exciting to us is that we are connecting a lot of the the AI models that we currently know and love, usually typically LM to that 3d data and I know it sounds a little buzz wordy but it it truly is, I think like a big mental shift, where it's like, once you when you have an outlet and you give it an image understand that image you can use that image in context for your application. You give an LM an entire 3d scene, semantically organized and able to be parsed through at inference time. And now you've given that model the ability to understand the natural world. I mean, this is this is true. Kind of spatial intelligence that we're trying to go after. I want to be able to just have this scene and have it be miles and miles and miles of scenery or entire apartments or buildings. And I want to be able to ask a question and it immediately takes me there and it shows me what I need to know and it lets me gain larger inferences from that 3d data. So what's the best path to go here? What is the distance from x to y, etc. And there's a lot of technical gaps before we get there. But that's that is truly what we're building and we're starting with virtual training.
S Speaker 515:32Understood thanks and training from the go to market standpoint, that's basically primarily because of the industry you come from. Right? So you think you have connections to get into that? I understand this is applicable to multiple multiple industries to any setting right? But you've chosen to me again, because that's overall that's why obviously you've seen the pain point the first.
Understood thanks and training from the go to market standpoint, that's basically primarily because of the industry you come from. Right? So you think you have connections to get into that? I understand this is applicable to multiple multiple industries to any setting right? But you've chosen to me again, because that's overall that's why obviously you've seen the pain point the first.
Understood thanks and training from the go to market standpoint, that's basically primarily because of the industry you come from. Right? So you think you have connections to get into that? I understand this is applicable to multiple multiple industries to any setting right? But you've chosen to me again, because that's overall that's why obviously you've seen the pain point the first.
Understood thanks and training from the go to market standpoint, that's basically primarily because of the industry you come from. Right? So you think you have connections to get into that? I understand this is applicable to multiple multiple industries to any setting right? But you've chosen to me again, because that's overall that's why obviously you've seen the pain point the first.
S Speaker 115:55That's exactly right. It's a good branching off point. And then also after doing this for I mean, we made that pivot about six months ago now. The lab that we worked in, also has huge roots and training and is his driver that one of the largest VR companies in the world came out of our lab and focuses primarily on enterprise training. Also, Quinn works in an HR company that did corporate training, so it just seemed to have extremely good founder market fit and it's an area we know very well and we know the pain points specifically for dual use, like you said yes, thank you to char if you're about to say it, I know that we're almost at time and I can give the deck over to everybody. It has a few other things that you probably are interested about. Go Go ahead but that
That's exactly right. It's a good branching off point. And then also after doing this for I mean, we made that pivot about six months ago now. The lab that we worked in, also has huge roots and training and is his driver that one of the largest VR companies in the world came out of our lab and focuses primarily on enterprise training. Also, Quinn works in an HR company that did corporate training, so it just seemed to have extremely good founder market fit and it's an area we know very well and we know the pain points specifically for dual use, like you said yes, thank you to char if you're about to say it, I know that we're almost at time and I can give the deck over to everybody. It has a few other things that you probably are interested about. Go Go ahead but that
That's exactly right. It's a good branching off point. And then also after doing this for I mean, we made that pivot about six months ago now. The lab that we worked in, also has huge roots and training and is his driver that one of the largest VR companies in the world came out of our lab and focuses primarily on enterprise training. Also, Quinn works in an HR company that did corporate training, so it just seemed to have extremely good founder market fit and it's an area we know very well and we know the pain points specifically for dual use, like you said yes, thank you to char if you're about to say it, I know that we're almost at time and I can give the deck over to everybody. It has a few other things that you probably are interested about. Go Go ahead but that
That's exactly right. It's a good branching off point. And then also after doing this for I mean, we made that pivot about six months ago now. The lab that we worked in, also has huge roots and training and is his driver that one of the largest VR companies in the world came out of our lab and focuses primarily on enterprise training. Also, Quinn works in an HR company that did corporate training, so it just seemed to have extremely good founder market fit and it's an area we know very well and we know the pain points specifically for dual use, like you said yes, thank you to char if you're about to say it, I know that we're almost at time and I can give the deck over to everybody. It has a few other things that you probably are interested about. Go Go ahead but that
S Speaker 416:48would be great. I think giving an update since we're getting closer. Giving an update on where you are in terms of fundraising, how much have you raised that would be good too and then we can talk more offline. Absolutely.
would be great. I think giving an update since we're getting closer. Giving an update on where you are in terms of fundraising, how much have you raised that would be good too and then we can talk more offline. Absolutely.
would be great. I think giving an update since we're getting closer. Giving an update on where you are in terms of fundraising, how much have you raised that would be good too and then we can talk more offline. Absolutely.
would be great. I think giving an update since we're getting closer. Giving an update on where you are in terms of fundraising, how much have you raised that would be good too and then we can talk more offline. Absolutely.
S Speaker 117:03So in the in the week between last week with two car and now over the holiday, we actually were accepted into a16z speed run accelerator. So this is their games accelerator. They're very excited about the virtual training and simulation use case and this is typically in the military. So that closes our precede round and we were a little early to be talking with you all as well, I believe. But what the way that we're thinking about this that we will raise our seed in October after Demo Day at San Francisco tech week. And we are pretty excited about the opportunity to work with Qualcomm on this. And have you guys come in on the seed round? Only because I mean, even by virtue of the questions that you asked today, you're a very technical team that has deep roots in this specific industry through Matterport. And you understand that have done research we've looked at the research coming out of Qualcomm, specifically in nerfs and in 3d perception, so very open to having a conversation with you all and continuing to develop this relationship. And I even appreciate just spending the time with us on this Friday morning and asking the questions that you have so I will send this deck over to you afterwards. If you have any questions immediately following please let us know and it's been a pleasure. So thank you. Thanks. Thanks guys.
So in the in the week between last week with two car and now over the holiday, we actually were accepted into a16z speed run accelerator. So this is their games accelerator. They're very excited about the virtual training and simulation use case and this is typically in the military. So that closes our precede round and we were a little early to be talking with you all as well, I believe. But what the way that we're thinking about this that we will raise our seed in October after Demo Day at San Francisco tech week. And we are pretty excited about the opportunity to work with Qualcomm on this. And have you guys come in on the seed round? Only because I mean, even by virtue of the questions that you asked today, you're a very technical team that has deep roots in this specific industry through Matterport. And you understand that have done research we've looked at the research coming out of Qualcomm, specifically in nerfs and in 3d perception, so very open to having a conversation with you all and continuing to develop this relationship. And I even appreciate just spending the time with us on this Friday morning and asking the questions that you have so I will send this deck over to you afterwards. If you have any questions immediately following please let us know and it's been a pleasure. So thank you. Thanks. Thanks guys.
So in the in the week between last week with two car and now over the holiday, we actually were accepted into a16z speed run accelerator. So this is their games accelerator. They're very excited about the virtual training and simulation use case and this is typically in the military. So that closes our precede round and we were a little early to be talking with you all as well, I believe. But what the way that we're thinking about this that we will raise our seed in October after Demo Day at San Francisco tech week. And we are pretty excited about the opportunity to work with Qualcomm on this. And have you guys come in on the seed round? Only because I mean, even by virtue of the questions that you asked today, you're a very technical team that has deep roots in this specific industry through Matterport. And you understand that have done research we've looked at the research coming out of Qualcomm, specifically in nerfs and in 3d perception, so very open to having a conversation with you all and continuing to develop this relationship. And I even appreciate just spending the time with us on this Friday morning and asking the questions that you have so I will send this deck over to you afterwards. If you have any questions immediately following please let us know and it's been a pleasure. So thank you. Thanks. Thanks guys.
So in the in the week between last week with two car and now over the holiday, we actually were accepted into a16z speed run accelerator. So this is their games accelerator. They're very excited about the virtual training and simulation use case and this is typically in the military. So that closes our precede round and we were a little early to be talking with you all as well, I believe. But what the way that we're thinking about this that we will raise our seed in October after Demo Day at San Francisco tech week. And we are pretty excited about the opportunity to work with Qualcomm on this. And have you guys come in on the seed round? Only because I mean, even by virtue of the questions that you asked today, you're a very technical team that has deep roots in this specific industry through Matterport. And you understand that have done research we've looked at the research coming out of Qualcomm, specifically in nerfs and in 3d perception, so very open to having a conversation with you all and continuing to develop this relationship. And I even appreciate just spending the time with us on this Friday morning and asking the questions that you have so I will send this deck over to you afterwards. If you have any questions immediately following please let us know and it's been a pleasure. So thank you. Thanks. Thanks guys.
S Speaker 418:36You I think others can drop. Why don't I do this? I have to start driving. James, what's your phone number? I'll give you a quick call. You're available.
You I think others can drop. Why don't I do this? I have to start driving. James, what's your phone number? I'll give you a quick call. You're available.
You I think others can drop. Why don't I do this? I have to start driving. James, what's your phone number? I'll give you a quick call. You're available.
You I think others can drop. Why don't I do this? I have to start driving. James, what's your phone number? I'll give you a quick call. You're available.
S Speaker 118:46I would say if I can give you a call and maybe one hour, that'd be great. We're meeting with the USC research team actually right after this, but I'd love to talk with you later on today to sharp again. Oh,
I would say if I can give you a call and maybe one hour, that'd be great. We're meeting with the USC research team actually right after this, but I'd love to talk with you later on today to sharp again. Oh,
I would say if I can give you a call and maybe one hour, that'd be great. We're meeting with the USC research team actually right after this, but I'd love to talk with you later on today to sharp again. Oh,
I would say if I can give you a call and maybe one hour, that'd be great. We're meeting with the USC research team actually right after this, but I'd love to talk with you later on today to sharp again. Oh,
S Speaker 418:58let me send you my phone number and then let's do the coordination.
let me send you my phone number and then let's do the coordination.
let me send you my phone number and then let's do the coordination.
let me send you my phone number and then let's do the coordination.