Meeting: AnythingLLM
Fri, May 2
9:30 AM
20 min
Priyesh P
Introduction and Initial Context
0:56
Technical Capa
URL: https://otter.ai/u/h8o4REVBQKTjtJomBmbO1mCSusw
Downloaded: 2025-12-22T11:29:25.429000
Method: text_extraction
============================================================

0:56him. Hey, Hi, Tim, hey, how you doing? I'm doing great. How are you? Yes, Tim, remember you are NSF, right.
him. Hey, Hi, Tim, hey, how you doing? I'm doing great. How are you? Yes, Tim, remember you are NSF, right.
him. Hey, Hi, Tim, hey, how you doing? I'm doing great. How are you? Yes, Tim, remember you are NSF, right.
him. Hey, Hi, Tim, hey, how you doing? I'm doing great. How are you? Yes, Tim, remember you are NSF, right.
S Speaker 11:05No, I'm in Orange County. Actually, great to catch up again. Would love
No, I'm in Orange County. Actually, great to catch up again. Would love
No, I'm in Orange County. Actually, great to catch up again. Would love
No, I'm in Orange County. Actually, great to catch up again. Would love
S Speaker 21:10to start this conversation between anything else team here. I think we spoke a little bit about the product.
to start this conversation between anything else team here. I think we spoke a little bit about the product.
to start this conversation between anything else team here. I think we spoke a little bit about the product.
to start this conversation between anything else team here. I think we spoke a little bit about the product.
S Speaker 11:17I did a little bit of my own research to have an understanding of what you're doing, but we'd love to understand the product better from your perspective. And then what I'm thinking here is that maybe post this call, I can connect with some more folks of my team, people who have been instrumental in making sure that we have some partnerships between the startups. We talk to get them open up some JTM channels with them from an aipc or mobile view teams, and then these guys can get get some of these engagements started. Seems like a space you're building in could have huge relevance for both the ventures team and also some of the business units that we very closely work with. So looking for that particularly. And then I'm not sure if there is a fundraising round anytime soon, but I would like to talk about that as well, if something around that is coming up, absolutely sure. So I guess maybe you already familiar with a little bit of what we do, but just kind of dive into, like, what the bigger picture is of what we're doing, right? And so we're just trying to build an AI tool that is reasonably useful for the average, everyday, individual contributor at medium to large orgs. And to that, we've been succeeding. And the way that we do that is not by building a special LLM runtime, which you're probably familiar with, LLM studio, all of that, we have instead invested heavily in the tools that make an LLM useful. And that is what anything LLM is we are a LLM agnostic tool set that goes on top of any model, and you can just use it. So that's chatting with documents, running agents to perform a task. We are soon actually going to be even having anything LLM inside of the applications people already use, like Microsoft Office products. So all of this fully local, fully private, that is like our whole direction
I did a little bit of my own research to have an understanding of what you're doing, but we'd love to understand the product better from your perspective. And then what I'm thinking here is that maybe post this call, I can connect with some more folks of my team, people who have been instrumental in making sure that we have some partnerships between the startups. We talk to get them open up some JTM channels with them from an aipc or mobile view teams, and then these guys can get get some of these engagements started. Seems like a space you're building in could have huge relevance for both the ventures team and also some of the business units that we very closely work with. So looking for that particularly. And then I'm not sure if there is a fundraising round anytime soon, but I would like to talk about that as well, if something around that is coming up, absolutely sure. So I guess maybe you already familiar with a little bit of what we do, but just kind of dive into, like, what the bigger picture is of what we're doing, right? And so we're just trying to build an AI tool that is reasonably useful for the average, everyday, individual contributor at medium to large orgs. And to that, we've been succeeding. And the way that we do that is not by building a special LLM runtime, which you're probably familiar with, LLM studio, all of that, we have instead invested heavily in the tools that make an LLM useful. And that is what anything LLM is we are a LLM agnostic tool set that goes on top of any model, and you can just use it. So that's chatting with documents, running agents to perform a task. We are soon actually going to be even having anything LLM inside of the applications people already use, like Microsoft Office products. So all of this fully local, fully private, that is like our whole direction
I did a little bit of my own research to have an understanding of what you're doing, but we'd love to understand the product better from your perspective. And then what I'm thinking here is that maybe post this call, I can connect with some more folks of my team, people who have been instrumental in making sure that we have some partnerships between the startups. We talk to get them open up some JTM channels with them from an aipc or mobile view teams, and then these guys can get get some of these engagements started. Seems like a space you're building in could have huge relevance for both the ventures team and also some of the business units that we very closely work with. So looking for that particularly. And then I'm not sure if there is a fundraising round anytime soon, but I would like to talk about that as well, if something around that is coming up, absolutely sure. So I guess maybe you already familiar with a little bit of what we do, but just kind of dive into, like, what the bigger picture is of what we're doing, right? And so we're just trying to build an AI tool that is reasonably useful for the average, everyday, individual contributor at medium to large orgs. And to that, we've been succeeding. And the way that we do that is not by building a special LLM runtime, which you're probably familiar with, LLM studio, all of that, we have instead invested heavily in the tools that make an LLM useful. And that is what anything LLM is we are a LLM agnostic tool set that goes on top of any model, and you can just use it. So that's chatting with documents, running agents to perform a task. We are soon actually going to be even having anything LLM inside of the applications people already use, like Microsoft Office products. So all of this fully local, fully private, that is like our whole direction
I did a little bit of my own research to have an understanding of what you're doing, but we'd love to understand the product better from your perspective. And then what I'm thinking here is that maybe post this call, I can connect with some more folks of my team, people who have been instrumental in making sure that we have some partnerships between the startups. We talk to get them open up some JTM channels with them from an aipc or mobile view teams, and then these guys can get get some of these engagements started. Seems like a space you're building in could have huge relevance for both the ventures team and also some of the business units that we very closely work with. So looking for that particularly. And then I'm not sure if there is a fundraising round anytime soon, but I would like to talk about that as well, if something around that is coming up, absolutely sure. So I guess maybe you already familiar with a little bit of what we do, but just kind of dive into, like, what the bigger picture is of what we're doing, right? And so we're just trying to build an AI tool that is reasonably useful for the average, everyday, individual contributor at medium to large orgs. And to that, we've been succeeding. And the way that we do that is not by building a special LLM runtime, which you're probably familiar with, LLM studio, all of that, we have instead invested heavily in the tools that make an LLM useful. And that is what anything LLM is we are a LLM agnostic tool set that goes on top of any model, and you can just use it. So that's chatting with documents, running agents to perform a task. We are soon actually going to be even having anything LLM inside of the applications people already use, like Microsoft Office products. So all of this fully local, fully private, that is like our whole direction
S Speaker 22:38and so couple of very interesting things you said that. Let's unpack a little bit on how you are going to embed anything available on some of these websites, some of the internal apps that might already be there on the PC, one and two as well. Are there, or maybe first on the tech limitations? Are there any specific models that you may not be able to run locally today given the limitations of the hardware
and so couple of very interesting things you said that. Let's unpack a little bit on how you are going to embed anything available on some of these websites, some of the internal apps that might already be there on the PC, one and two as well. Are there, or maybe first on the tech limitations? Are there any specific models that you may not be able to run locally today given the limitations of the hardware
and so couple of very interesting things you said that. Let's unpack a little bit on how you are going to embed anything available on some of these websites, some of the internal apps that might already be there on the PC, one and two as well. Are there, or maybe first on the tech limitations? Are there any specific models that you may not be able to run locally today given the limitations of the hardware
and so couple of very interesting things you said that. Let's unpack a little bit on how you are going to embed anything available on some of these websites, some of the internal apps that might already be there on the PC, one and two as well. Are there, or maybe first on the tech limitations? Are there any specific models that you may not be able to run locally today given the limitations of the hardware
S Speaker 12:56there? So it depends on the user's device, right? So if they have a powerful machine like a desktop computer, right? Then whatever the limits are of their hardware they can run. The special use of anything out of them, though, is that we are one of the only apps, if not the only app of our form factor, that supports running models on APU. And we actually work pretty closely with the Vulcan engineering team to whenever a new model is afforded, we go and support that so that our users who install our app on Windows arm are able to leverage the
there? So it depends on the user's device, right? So if they have a powerful machine like a desktop computer, right? Then whatever the limits are of their hardware they can run. The special use of anything out of them, though, is that we are one of the only apps, if not the only app of our form factor, that supports running models on APU. And we actually work pretty closely with the Vulcan engineering team to whenever a new model is afforded, we go and support that so that our users who install our app on Windows arm are able to leverage the
there? So it depends on the user's device, right? So if they have a powerful machine like a desktop computer, right? Then whatever the limits are of their hardware they can run. The special use of anything out of them, though, is that we are one of the only apps, if not the only app of our form factor, that supports running models on APU. And we actually work pretty closely with the Vulcan engineering team to whenever a new model is afforded, we go and support that so that our users who install our app on Windows arm are able to leverage the
there? So it depends on the user's device, right? So if they have a powerful machine like a desktop computer, right? Then whatever the limits are of their hardware they can run. The special use of anything out of them, though, is that we are one of the only apps, if not the only app of our form factor, that supports running models on APU. And we actually work pretty closely with the Vulcan engineering team to whenever a new model is afforded, we go and support that so that our users who install our app on Windows arm are able to leverage the
S Speaker 23:19latest models that they can run. Right and do that matter, have you done any sort of way support them on npus again? Have you done any sort of fine tuning, reinforcement learning on some models to make them better for some specific applications that you see your users do are used more often? We have not invested
latest models that they can run. Right and do that matter, have you done any sort of way support them on npus again? Have you done any sort of fine tuning, reinforcement learning on some models to make them better for some specific applications that you see your users do are used more often? We have not invested
latest models that they can run. Right and do that matter, have you done any sort of way support them on npus again? Have you done any sort of fine tuning, reinforcement learning on some models to make them better for some specific applications that you see your users do are used more often? We have not invested
latest models that they can run. Right and do that matter, have you done any sort of way support them on npus again? Have you done any sort of fine tuning, reinforcement learning on some models to make them better for some specific applications that you see your users do are used more often? We have not invested
S Speaker 13:35heavily into a fine tuning pipeline. It does make sense as a natural evolution of our product, because as someone interacts with the model, uploads documents and has all of these chats, the very final stage is customizing that model right? And of course, that is possible to do one NPU, but right now, because our tooling is just so good, a lot of people the time investment it takes to tune a model is very minimal yield compared to us just building better tools, I'm sure, in the future, and we have that people ask for it, and we used to actually have a small business segment where we would fine tune models. We just didn't see great adoption with it, because people don't understand it. So we just decided to keep investing into what
heavily into a fine tuning pipeline. It does make sense as a natural evolution of our product, because as someone interacts with the model, uploads documents and has all of these chats, the very final stage is customizing that model right? And of course, that is possible to do one NPU, but right now, because our tooling is just so good, a lot of people the time investment it takes to tune a model is very minimal yield compared to us just building better tools, I'm sure, in the future, and we have that people ask for it, and we used to actually have a small business segment where we would fine tune models. We just didn't see great adoption with it, because people don't understand it. So we just decided to keep investing into what
heavily into a fine tuning pipeline. It does make sense as a natural evolution of our product, because as someone interacts with the model, uploads documents and has all of these chats, the very final stage is customizing that model right? And of course, that is possible to do one NPU, but right now, because our tooling is just so good, a lot of people the time investment it takes to tune a model is very minimal yield compared to us just building better tools, I'm sure, in the future, and we have that people ask for it, and we used to actually have a small business segment where we would fine tune models. We just didn't see great adoption with it, because people don't understand it. So we just decided to keep investing into what
heavily into a fine tuning pipeline. It does make sense as a natural evolution of our product, because as someone interacts with the model, uploads documents and has all of these chats, the very final stage is customizing that model right? And of course, that is possible to do one NPU, but right now, because our tooling is just so good, a lot of people the time investment it takes to tune a model is very minimal yield compared to us just building better tools, I'm sure, in the future, and we have that people ask for it, and we used to actually have a small business segment where we would fine tune models. We just didn't see great adoption with it, because people don't understand it. So we just decided to keep investing into what
S Speaker 24:06people do understand. Absolutely, I get that one of the things we have internally is that depends on who you are selling to. And many of the companies we have, we have seen in the past year or so everyone, if they're selling to the ML, engineering, ops sort of Persona, we don't see them go very far. So that makes sense on what you're saying. And like, coming back to the initial point that you mentioned on product, how do you plan to embed anything other than just from one desktop app that it does today to sort of go across the entire environment?
people do understand. Absolutely, I get that one of the things we have internally is that depends on who you are selling to. And many of the companies we have, we have seen in the past year or so everyone, if they're selling to the ML, engineering, ops sort of Persona, we don't see them go very far. So that makes sense on what you're saying. And like, coming back to the initial point that you mentioned on product, how do you plan to embed anything other than just from one desktop app that it does today to sort of go across the entire environment?
people do understand. Absolutely, I get that one of the things we have internally is that depends on who you are selling to. And many of the companies we have, we have seen in the past year or so everyone, if they're selling to the ML, engineering, ops sort of Persona, we don't see them go very far. So that makes sense on what you're saying. And like, coming back to the initial point that you mentioned on product, how do you plan to embed anything other than just from one desktop app that it does today to sort of go across the entire environment?
people do understand. Absolutely, I get that one of the things we have internally is that depends on who you are selling to. And many of the companies we have, we have seen in the past year or so everyone, if they're selling to the ML, engineering, ops sort of Persona, we don't see them go very far. So that makes sense on what you're saying. And like, coming back to the initial point that you mentioned on product, how do you plan to embed anything other than just from one desktop app that it does today to sort of go across the entire environment?
S Speaker 14:31So there's clearly power in what we built with anything other than like, you can use the app, and that's great. But the reality of AI on a specialty, like for factors like laptops, right, is going to be the everyday person using the app right? And right and right now you have to go to a dedicated interface, basically use any AI app. I think that is reasonable in the future of adoption. And so when we are able to, like, I said, like one of the future roadmap, I actually demoed this to the enterprise sales team at Qualcomm already, but we have a how I demoed a PowerPoint plugin to them that whenever it like, basically, you can generate slides using an on device model, using the NPU. And it was, it was using your documents on your computer as well, too, for content. Too, for context. So we did all of that fully local, no server, no anything, just the app on desktop. Imagine that experience, but not in PowerPoint, but also in Word, in Outlook, in the places where people are already contributing to their work, right? Like that. That is where AI needs to be, obviously. The next question is, okay, well, Microsoft copilot is obviously going to be planning on doing something like that. I recession for copilot has not been that great, according to all the customers that are interested in this product, that we build a lot more flexibility and what we can do. Because, as you're probably aware, NDU models in their current state, right? Are very different from your traditional like models that are published online. They have to prepare them. It takes a lot of time. The like, for example, on the quake I hub, like deep sea still isn't there just because of legal things. Like, there's a whole there's a billion hurdles that when we have this LLM agnostic back end doesn't apply. And so we're able to basically faster, farther and just easier than copilot could even deal with. It. Just going to make the user experience just better in general. And so that is kind of our unique advantage in that segment. And also, obviously we can then apply to Google as well too. So we
So there's clearly power in what we built with anything other than like, you can use the app, and that's great. But the reality of AI on a specialty, like for factors like laptops, right, is going to be the everyday person using the app right? And right and right now you have to go to a dedicated interface, basically use any AI app. I think that is reasonable in the future of adoption. And so when we are able to, like, I said, like one of the future roadmap, I actually demoed this to the enterprise sales team at Qualcomm already, but we have a how I demoed a PowerPoint plugin to them that whenever it like, basically, you can generate slides using an on device model, using the NPU. And it was, it was using your documents on your computer as well, too, for content. Too, for context. So we did all of that fully local, no server, no anything, just the app on desktop. Imagine that experience, but not in PowerPoint, but also in Word, in Outlook, in the places where people are already contributing to their work, right? Like that. That is where AI needs to be, obviously. The next question is, okay, well, Microsoft copilot is obviously going to be planning on doing something like that. I recession for copilot has not been that great, according to all the customers that are interested in this product, that we build a lot more flexibility and what we can do. Because, as you're probably aware, NDU models in their current state, right? Are very different from your traditional like models that are published online. They have to prepare them. It takes a lot of time. The like, for example, on the quake I hub, like deep sea still isn't there just because of legal things. Like, there's a whole there's a billion hurdles that when we have this LLM agnostic back end doesn't apply. And so we're able to basically faster, farther and just easier than copilot could even deal with. It. Just going to make the user experience just better in general. And so that is kind of our unique advantage in that segment. And also, obviously we can then apply to Google as well too. So we
So there's clearly power in what we built with anything other than like, you can use the app, and that's great. But the reality of AI on a specialty, like for factors like laptops, right, is going to be the everyday person using the app right? And right and right now you have to go to a dedicated interface, basically use any AI app. I think that is reasonable in the future of adoption. And so when we are able to, like, I said, like one of the future roadmap, I actually demoed this to the enterprise sales team at Qualcomm already, but we have a how I demoed a PowerPoint plugin to them that whenever it like, basically, you can generate slides using an on device model, using the NPU. And it was, it was using your documents on your computer as well, too, for content. Too, for context. So we did all of that fully local, no server, no anything, just the app on desktop. Imagine that experience, but not in PowerPoint, but also in Word, in Outlook, in the places where people are already contributing to their work, right? Like that. That is where AI needs to be, obviously. The next question is, okay, well, Microsoft copilot is obviously going to be planning on doing something like that. I recession for copilot has not been that great, according to all the customers that are interested in this product, that we build a lot more flexibility and what we can do. Because, as you're probably aware, NDU models in their current state, right? Are very different from your traditional like models that are published online. They have to prepare them. It takes a lot of time. The like, for example, on the quake I hub, like deep sea still isn't there just because of legal things. Like, there's a whole there's a billion hurdles that when we have this LLM agnostic back end doesn't apply. And so we're able to basically faster, farther and just easier than copilot could even deal with. It. Just going to make the user experience just better in general. And so that is kind of our unique advantage in that segment. And also, obviously we can then apply to Google as well too. So we
So there's clearly power in what we built with anything other than like, you can use the app, and that's great. But the reality of AI on a specialty, like for factors like laptops, right, is going to be the everyday person using the app right? And right and right now you have to go to a dedicated interface, basically use any AI app. I think that is reasonable in the future of adoption. And so when we are able to, like, I said, like one of the future roadmap, I actually demoed this to the enterprise sales team at Qualcomm already, but we have a how I demoed a PowerPoint plugin to them that whenever it like, basically, you can generate slides using an on device model, using the NPU. And it was, it was using your documents on your computer as well, too, for content. Too, for context. So we did all of that fully local, no server, no anything, just the app on desktop. Imagine that experience, but not in PowerPoint, but also in Word, in Outlook, in the places where people are already contributing to their work, right? Like that. That is where AI needs to be, obviously. The next question is, okay, well, Microsoft copilot is obviously going to be planning on doing something like that. I recession for copilot has not been that great, according to all the customers that are interested in this product, that we build a lot more flexibility and what we can do. Because, as you're probably aware, NDU models in their current state, right? Are very different from your traditional like models that are published online. They have to prepare them. It takes a lot of time. The like, for example, on the quake I hub, like deep sea still isn't there just because of legal things. Like, there's a whole there's a billion hurdles that when we have this LLM agnostic back end doesn't apply. And so we're able to basically faster, farther and just easier than copilot could even deal with. It. Just going to make the user experience just better in general. And so that is kind of our unique advantage in that segment. And also, obviously we can then apply to Google as well too. So we
S Speaker 16:12to give an idea, also the business model and how this works into it, right? Is, you can download the desktop app like as it is right now, without cost, and run that on your device. One of these plugins that I'm talking about will be a paid add on that the user then pays, pays us to be able to unlock, and that's mostly because, I mean, that's just a lot that's already that's a unique advantage for us, right? And it's still already local. But if we can obviously monetize on that specific integration that we are and yes, we are able to integrate basically with anything that has some kind of API, and once locally, our tool can definitely interface with it. That's without a question, possible. I can actually fly if you want. I can show you the demo that I showed to the sales. Team showed to the sales team, or the PowerPoint, okay, I should have it already available. You know, it's funny. I made this. I made this presentation in Canva because it has just, you know, just way easier for us to use them. That's what we use in our company. And so this is always funny, because it's a demo without PowerPoint. But let's see. But they really, really like this. So here I'll, I'll just jump through stuff. Okay, all right. Now it seems to work. So this was just me talking to them about just, like, the benefits of, like, anything, LLM and stuff like that, and capabilities. But this, this is all just okay. Here it is. So this video on the right here, that's blank. So what I'm showcasing here is, you know, just documents how we uploaded it. That's what the UI looks like in the in the pane. So this pain is inside of PowerPoint. And then you can select, you know, personality. This is a this is not published yet. So this is a very rough look of what it looks like. But you just type in a prompt as if you normally would. And then you can generate a title slide or a content slide, and that's what it did right there. And this is going to be generating a content slide that was a title slide. And then, let's see, generates a slide, and that's using now, obviously, formatting is obviously a little tricky, but obviously this is still beta, but the idea is that that leveraged documents that were already on my computer in a workspace. I had made the LM, and I just seamlessly brought that into an office product. And
to give an idea, also the business model and how this works into it, right? Is, you can download the desktop app like as it is right now, without cost, and run that on your device. One of these plugins that I'm talking about will be a paid add on that the user then pays, pays us to be able to unlock, and that's mostly because, I mean, that's just a lot that's already that's a unique advantage for us, right? And it's still already local. But if we can obviously monetize on that specific integration that we are and yes, we are able to integrate basically with anything that has some kind of API, and once locally, our tool can definitely interface with it. That's without a question, possible. I can actually fly if you want. I can show you the demo that I showed to the sales. Team showed to the sales team, or the PowerPoint, okay, I should have it already available. You know, it's funny. I made this. I made this presentation in Canva because it has just, you know, just way easier for us to use them. That's what we use in our company. And so this is always funny, because it's a demo without PowerPoint. But let's see. But they really, really like this. So here I'll, I'll just jump through stuff. Okay, all right. Now it seems to work. So this was just me talking to them about just, like, the benefits of, like, anything, LLM and stuff like that, and capabilities. But this, this is all just okay. Here it is. So this video on the right here, that's blank. So what I'm showcasing here is, you know, just documents how we uploaded it. That's what the UI looks like in the in the pane. So this pain is inside of PowerPoint. And then you can select, you know, personality. This is a this is not published yet. So this is a very rough look of what it looks like. But you just type in a prompt as if you normally would. And then you can generate a title slide or a content slide, and that's what it did right there. And this is going to be generating a content slide that was a title slide. And then, let's see, generates a slide, and that's using now, obviously, formatting is obviously a little tricky, but obviously this is still beta, but the idea is that that leveraged documents that were already on my computer in a workspace. I had made the LM, and I just seamlessly brought that into an office product. And
to give an idea, also the business model and how this works into it, right? Is, you can download the desktop app like as it is right now, without cost, and run that on your device. One of these plugins that I'm talking about will be a paid add on that the user then pays, pays us to be able to unlock, and that's mostly because, I mean, that's just a lot that's already that's a unique advantage for us, right? And it's still already local. But if we can obviously monetize on that specific integration that we are and yes, we are able to integrate basically with anything that has some kind of API, and once locally, our tool can definitely interface with it. That's without a question, possible. I can actually fly if you want. I can show you the demo that I showed to the sales. Team showed to the sales team, or the PowerPoint, okay, I should have it already available. You know, it's funny. I made this. I made this presentation in Canva because it has just, you know, just way easier for us to use them. That's what we use in our company. And so this is always funny, because it's a demo without PowerPoint. But let's see. But they really, really like this. So here I'll, I'll just jump through stuff. Okay, all right. Now it seems to work. So this was just me talking to them about just, like, the benefits of, like, anything, LLM and stuff like that, and capabilities. But this, this is all just okay. Here it is. So this video on the right here, that's blank. So what I'm showcasing here is, you know, just documents how we uploaded it. That's what the UI looks like in the in the pane. So this pain is inside of PowerPoint. And then you can select, you know, personality. This is a this is not published yet. So this is a very rough look of what it looks like. But you just type in a prompt as if you normally would. And then you can generate a title slide or a content slide, and that's what it did right there. And this is going to be generating a content slide that was a title slide. And then, let's see, generates a slide, and that's using now, obviously, formatting is obviously a little tricky, but obviously this is still beta, but the idea is that that leveraged documents that were already on my computer in a workspace. I had made the LM, and I just seamlessly brought that into an office product. And
to give an idea, also the business model and how this works into it, right? Is, you can download the desktop app like as it is right now, without cost, and run that on your device. One of these plugins that I'm talking about will be a paid add on that the user then pays, pays us to be able to unlock, and that's mostly because, I mean, that's just a lot that's already that's a unique advantage for us, right? And it's still already local. But if we can obviously monetize on that specific integration that we are and yes, we are able to integrate basically with anything that has some kind of API, and once locally, our tool can definitely interface with it. That's without a question, possible. I can actually fly if you want. I can show you the demo that I showed to the sales. Team showed to the sales team, or the PowerPoint, okay, I should have it already available. You know, it's funny. I made this. I made this presentation in Canva because it has just, you know, just way easier for us to use them. That's what we use in our company. And so this is always funny, because it's a demo without PowerPoint. But let's see. But they really, really like this. So here I'll, I'll just jump through stuff. Okay, all right. Now it seems to work. So this was just me talking to them about just, like, the benefits of, like, anything, LLM and stuff like that, and capabilities. But this, this is all just okay. Here it is. So this video on the right here, that's blank. So what I'm showcasing here is, you know, just documents how we uploaded it. That's what the UI looks like in the in the pane. So this pain is inside of PowerPoint. And then you can select, you know, personality. This is a this is not published yet. So this is a very rough look of what it looks like. But you just type in a prompt as if you normally would. And then you can generate a title slide or a content slide, and that's what it did right there. And this is going to be generating a content slide that was a title slide. And then, let's see, generates a slide, and that's using now, obviously, formatting is obviously a little tricky, but obviously this is still beta, but the idea is that that leveraged documents that were already on my computer in a workspace. I had made the LM, and I just seamlessly brought that into an office product. And
S Speaker 27:54this is very interesting to mix, makes a lot of sense. So and leveraging the context that you have on the app itself. Certainly, he says a lot of fiction. I get that. I get I get the value proposition that it will be way more easier for you to sort of expand across different apps than any other web based platform. For sure, anything that you've done in terms of long context understanding memory so that you are able to make the most use of this, I would say space that you have on someone's desktop already,
this is very interesting to mix, makes a lot of sense. So and leveraging the context that you have on the app itself. Certainly, he says a lot of fiction. I get that. I get I get the value proposition that it will be way more easier for you to sort of expand across different apps than any other web based platform. For sure, anything that you've done in terms of long context understanding memory so that you are able to make the most use of this, I would say space that you have on someone's desktop already,
this is very interesting to mix, makes a lot of sense. So and leveraging the context that you have on the app itself. Certainly, he says a lot of fiction. I get that. I get I get the value proposition that it will be way more easier for you to sort of expand across different apps than any other web based platform. For sure, anything that you've done in terms of long context understanding memory so that you are able to make the most use of this, I would say space that you have on someone's desktop already,
this is very interesting to mix, makes a lot of sense. So and leveraging the context that you have on the app itself. Certainly, he says a lot of fiction. I get that. I get I get the value proposition that it will be way more easier for you to sort of expand across different apps than any other web based platform. For sure, anything that you've done in terms of long context understanding memory so that you are able to make the most use of this, I would say space that you have on someone's desktop already,
S Speaker 18:18yeah, so along. So what we have right now is we have this long context management system already that's just been in the absence forever. And we're basically able to use an agentic flow to be able to contextually retrieve historical documents or, just like, you know, a piece of content that is in a really, really large document that normally would fit into the window, we have all that managed, all of that happened in that demo, just way behind the scenes, and where possible, we use the
yeah, so along. So what we have right now is we have this long context management system already that's just been in the absence forever. And we're basically able to use an agentic flow to be able to contextually retrieve historical documents or, just like, you know, a piece of content that is in a really, really large document that normally would fit into the window, we have all that managed, all of that happened in that demo, just way behind the scenes, and where possible, we use the
yeah, so along. So what we have right now is we have this long context management system already that's just been in the absence forever. And we're basically able to use an agentic flow to be able to contextually retrieve historical documents or, just like, you know, a piece of content that is in a really, really large document that normally would fit into the window, we have all that managed, all of that happened in that demo, just way behind the scenes, and where possible, we use the
yeah, so along. So what we have right now is we have this long context management system already that's just been in the absence forever. And we're basically able to use an agentic flow to be able to contextually retrieve historical documents or, just like, you know, a piece of content that is in a really, really large document that normally would fit into the window, we have all that managed, all of that happened in that demo, just way behind the scenes, and where possible, we use the
S Speaker 28:40NBU to actually do that. So product, product definitely makes sense. I haven't had a chance to use the app myself because Qualcomm laptop, I had to take a lot of permission, yeah, but I will try to platform myself on my personal computer. Will do that. But on terms in terms of traction, where are you? Today, I looked at the Open Source action that's
NBU to actually do that. So product, product definitely makes sense. I haven't had a chance to use the app myself because Qualcomm laptop, I had to take a lot of permission, yeah, but I will try to platform myself on my personal computer. Will do that. But on terms in terms of traction, where are you? Today, I looked at the Open Source action that's
NBU to actually do that. So product, product definitely makes sense. I haven't had a chance to use the app myself because Qualcomm laptop, I had to take a lot of permission, yeah, but I will try to platform myself on my personal computer. Will do that. But on terms in terms of traction, where are you? Today, I looked at the Open Source action that's
NBU to actually do that. So product, product definitely makes sense. I haven't had a chance to use the app myself because Qualcomm laptop, I had to take a lot of permission, yeah, but I will try to platform myself on my personal computer. Will do that. But on terms in terms of traction, where are you? Today, I looked at the Open Source action that's
S Speaker 210:37Interesting. And what, what was behind the crazy inflection between January, February to today, because I saw similar inflection in terms of open source action as on the star history chart. So anything that you launched around, I think
Interesting. And what, what was behind the crazy inflection between January, February to today, because I saw similar inflection in terms of open source action as on the star history chart. So anything that you launched around, I think
Interesting. And what, what was behind the crazy inflection between January, February to today, because I saw similar inflection in terms of open source action as on the star history chart. So anything that you launched around, I think
Interesting. And what, what was behind the crazy inflection between January, February to today, because I saw similar inflection in terms of open source action as on the star history chart. So anything that you launched around, I think
S Speaker 110:49we have been so one of the one of the main things for this is that we were one of the first like projects out there to really cater towards reasoning models. And because of that, a lot of people wound up adopting us earlier, because just the fit and the form factor for using a model like deep seek or something was just superior to every other product. So we had a lot of adoption. But also it's those pre built tools that make us super powerful, because most people want a zero cell experience, and that is one thing we can offer, whereas other people can give you a UI. But adding tools is like, Oh, you've got to go through all of these, like loops and debugging and like, oh, it might not work, or maybe it does. We just know it worked. And so we build whatever is important we know people are using, just bake right into the tool. Anything that people might want. We offer plugins that they go, that we can offer, and that kind of completes the
we have been so one of the one of the main things for this is that we were one of the first like projects out there to really cater towards reasoning models. And because of that, a lot of people wound up adopting us earlier, because just the fit and the form factor for using a model like deep seek or something was just superior to every other product. So we had a lot of adoption. But also it's those pre built tools that make us super powerful, because most people want a zero cell experience, and that is one thing we can offer, whereas other people can give you a UI. But adding tools is like, Oh, you've got to go through all of these, like loops and debugging and like, oh, it might not work, or maybe it does. We just know it worked. And so we build whatever is important we know people are using, just bake right into the tool. Anything that people might want. We offer plugins that they go, that we can offer, and that kind of completes the
we have been so one of the one of the main things for this is that we were one of the first like projects out there to really cater towards reasoning models. And because of that, a lot of people wound up adopting us earlier, because just the fit and the form factor for using a model like deep seek or something was just superior to every other product. So we had a lot of adoption. But also it's those pre built tools that make us super powerful, because most people want a zero cell experience, and that is one thing we can offer, whereas other people can give you a UI. But adding tools is like, Oh, you've got to go through all of these, like loops and debugging and like, oh, it might not work, or maybe it does. We just know it worked. And so we build whatever is important we know people are using, just bake right into the tool. Anything that people might want. We offer plugins that they go, that we can offer, and that kind of completes the
we have been so one of the one of the main things for this is that we were one of the first like projects out there to really cater towards reasoning models. And because of that, a lot of people wound up adopting us earlier, because just the fit and the form factor for using a model like deep seek or something was just superior to every other product. So we had a lot of adoption. But also it's those pre built tools that make us super powerful, because most people want a zero cell experience, and that is one thing we can offer, whereas other people can give you a UI. But adding tools is like, Oh, you've got to go through all of these, like loops and debugging and like, oh, it might not work, or maybe it does. We just know it worked. And so we build whatever is important we know people are using, just bake right into the tool. Anything that people might want. We offer plugins that they go, that we can offer, and that kind of completes the
S Speaker 211:28whole ecosystem there. I'm getting excited to just get COVID to use. Let me use some of this, because the rack just just plain, simple rag, which has been incredibly useful,
whole ecosystem there. I'm getting excited to just get COVID to use. Let me use some of this, because the rack just just plain, simple rag, which has been incredibly useful,
whole ecosystem there. I'm getting excited to just get COVID to use. Let me use some of this, because the rack just just plain, simple rag, which has been incredibly useful,
whole ecosystem there. I'm getting excited to just get COVID to use. Let me use some of this, because the rack just just plain, simple rag, which has been incredibly useful,
S Speaker 111:37yes, and actually, with the NPU, it runs pretty well, even on the xLP, like it's 40 tops right now. But I mean, even still, though you're getting tokens a second, which is, like, very readable, yeah, very usable. So, and then we also have the embedding and the RE ranking that runs on NPU very fast, and it's very accurate. So I don't know it's obviously we invested early in quants like NPU technology. We only support Qualcomm's npus, by the way, we don't support MDS or Intels or anybody else is just
yes, and actually, with the NPU, it runs pretty well, even on the xLP, like it's 40 tops right now. But I mean, even still, though you're getting tokens a second, which is, like, very readable, yeah, very usable. So, and then we also have the embedding and the RE ranking that runs on NPU very fast, and it's very accurate. So I don't know it's obviously we invested early in quants like NPU technology. We only support Qualcomm's npus, by the way, we don't support MDS or Intels or anybody else is just
yes, and actually, with the NPU, it runs pretty well, even on the xLP, like it's 40 tops right now. But I mean, even still, though you're getting tokens a second, which is, like, very readable, yeah, very usable. So, and then we also have the embedding and the RE ranking that runs on NPU very fast, and it's very accurate. So I don't know it's obviously we invested early in quants like NPU technology. We only support Qualcomm's npus, by the way, we don't support MDS or Intels or anybody else is just
yes, and actually, with the NPU, it runs pretty well, even on the xLP, like it's 40 tops right now. But I mean, even still, though you're getting tokens a second, which is, like, very readable, yeah, very usable. So, and then we also have the embedding and the RE ranking that runs on NPU very fast, and it's very accurate. So I don't know it's obviously we invested early in quants like NPU technology. We only support Qualcomm's npus, by the way, we don't support MDS or Intels or anybody else is just
S Speaker 212:01welcome. And is that advice that, why do you not want to be it's not
welcome. And is that advice that, why do you not want to be it's not
welcome. And is that advice that, why do you not want to be it's not
welcome. And is that advice that, why do you not want to be it's not
S Speaker 112:05that we don't want to be hardware agnostic. It is that the level of engineering effort to support any specific NPU is it's just, it's huge, right? There is no alum or LM studio supporting NPU right now. And it's just like we were approached early, early from Qualcomm about this whole NPU adoption. And I just kind of was like, Okay, well, I don't really think that Intel and AMD are gonna lead that market. And so it seems to partner with Qualcomm to be like, Okay, if we're gonna do this, let's at least do it from the first step correct. And so that's when we started investing in NDU. And I mean, we're ahead of any of our even, what you would say, competitors or people in our market, just because of that, like people, people do use, I mean, we do have people that run the NPU engine as their main insurance engine. Absolutely get
that we don't want to be hardware agnostic. It is that the level of engineering effort to support any specific NPU is it's just, it's huge, right? There is no alum or LM studio supporting NPU right now. And it's just like we were approached early, early from Qualcomm about this whole NPU adoption. And I just kind of was like, Okay, well, I don't really think that Intel and AMD are gonna lead that market. And so it seems to partner with Qualcomm to be like, Okay, if we're gonna do this, let's at least do it from the first step correct. And so that's when we started investing in NDU. And I mean, we're ahead of any of our even, what you would say, competitors or people in our market, just because of that, like people, people do use, I mean, we do have people that run the NPU engine as their main insurance engine. Absolutely get
that we don't want to be hardware agnostic. It is that the level of engineering effort to support any specific NPU is it's just, it's huge, right? There is no alum or LM studio supporting NPU right now. And it's just like we were approached early, early from Qualcomm about this whole NPU adoption. And I just kind of was like, Okay, well, I don't really think that Intel and AMD are gonna lead that market. And so it seems to partner with Qualcomm to be like, Okay, if we're gonna do this, let's at least do it from the first step correct. And so that's when we started investing in NDU. And I mean, we're ahead of any of our even, what you would say, competitors or people in our market, just because of that, like people, people do use, I mean, we do have people that run the NPU engine as their main insurance engine. Absolutely get
that we don't want to be hardware agnostic. It is that the level of engineering effort to support any specific NPU is it's just, it's huge, right? There is no alum or LM studio supporting NPU right now. And it's just like we were approached early, early from Qualcomm about this whole NPU adoption. And I just kind of was like, Okay, well, I don't really think that Intel and AMD are gonna lead that market. And so it seems to partner with Qualcomm to be like, Okay, if we're gonna do this, let's at least do it from the first step correct. And so that's when we started investing in NDU. And I mean, we're ahead of any of our even, what you would say, competitors or people in our market, just because of that, like people, people do use, I mean, we do have people that run the NPU engine as their main insurance engine. Absolutely get
S Speaker 212:39that. So one question on the competitor side, who competence could be very wide, as many indirect competitors as well. Who do you sort of are you tracking anyone closely in terms of feature parity today?
that. So one question on the competitor side, who competence could be very wide, as many indirect competitors as well. Who do you sort of are you tracking anyone closely in terms of feature parity today?
that. So one question on the competitor side, who competence could be very wide, as many indirect competitors as well. Who do you sort of are you tracking anyone closely in terms of feature parity today?
that. So one question on the competitor side, who competence could be very wide, as many indirect competitors as well. Who do you sort of are you tracking anyone closely in terms of feature parity today?
S Speaker 112:51The the closest, the closest, I would say, product to ours. And it's not close right now, but I just anticipate it would be, would be something like LM studio. They have similar amounts of traction. They know to ride in the space people know them just like they know. Them just like they know us. We actually have LM studio. So if you like their engine, you can use it in anything LLM. But one thing I anticipate is that LM studio is going to recognize that there isn't much money to be made in the engine running the market, and they're going to start adding more native tooling, like rag or agents or something like that. We're about a year ahead of them on that, because they just added rag and it's not even full accurate, like the full definition of rag. It's just loading a document into the context. When you added that a couple of months ago. Kind of gives me the idea they might move towards our direction. However, we've had just a lot more success with just that consumer segment, and we're kind of ahead of them on that and we have to keep that lead. But I imagine that they will start to realize that there's more money to be made in that tooling segment than there is in the actual engine inference section. And
The the closest, the closest, I would say, product to ours. And it's not close right now, but I just anticipate it would be, would be something like LM studio. They have similar amounts of traction. They know to ride in the space people know them just like they know. Them just like they know us. We actually have LM studio. So if you like their engine, you can use it in anything LLM. But one thing I anticipate is that LM studio is going to recognize that there isn't much money to be made in the engine running the market, and they're going to start adding more native tooling, like rag or agents or something like that. We're about a year ahead of them on that, because they just added rag and it's not even full accurate, like the full definition of rag. It's just loading a document into the context. When you added that a couple of months ago. Kind of gives me the idea they might move towards our direction. However, we've had just a lot more success with just that consumer segment, and we're kind of ahead of them on that and we have to keep that lead. But I imagine that they will start to realize that there's more money to be made in that tooling segment than there is in the actual engine inference section. And
The the closest, the closest, I would say, product to ours. And it's not close right now, but I just anticipate it would be, would be something like LM studio. They have similar amounts of traction. They know to ride in the space people know them just like they know. Them just like they know us. We actually have LM studio. So if you like their engine, you can use it in anything LLM. But one thing I anticipate is that LM studio is going to recognize that there isn't much money to be made in the engine running the market, and they're going to start adding more native tooling, like rag or agents or something like that. We're about a year ahead of them on that, because they just added rag and it's not even full accurate, like the full definition of rag. It's just loading a document into the context. When you added that a couple of months ago. Kind of gives me the idea they might move towards our direction. However, we've had just a lot more success with just that consumer segment, and we're kind of ahead of them on that and we have to keep that lead. But I imagine that they will start to realize that there's more money to be made in that tooling segment than there is in the actual engine inference section. And
The the closest, the closest, I would say, product to ours. And it's not close right now, but I just anticipate it would be, would be something like LM studio. They have similar amounts of traction. They know to ride in the space people know them just like they know. Them just like they know us. We actually have LM studio. So if you like their engine, you can use it in anything LLM. But one thing I anticipate is that LM studio is going to recognize that there isn't much money to be made in the engine running the market, and they're going to start adding more native tooling, like rag or agents or something like that. We're about a year ahead of them on that, because they just added rag and it's not even full accurate, like the full definition of rag. It's just loading a document into the context. When you added that a couple of months ago. Kind of gives me the idea they might move towards our direction. However, we've had just a lot more success with just that consumer segment, and we're kind of ahead of them on that and we have to keep that lead. But I imagine that they will start to realize that there's more money to be made in that tooling segment than there is in the actual engine inference section. And
S Speaker 213:39today, Tim, across all of the tools that you have, where do you see users spending most time on is it? Are the users just coming for simple drag application, simple chat bot interface? Are users actually building agents locally usage?
today, Tim, across all of the tools that you have, where do you see users spending most time on is it? Are the users just coming for simple drag application, simple chat bot interface? Are users actually building agents locally usage?
today, Tim, across all of the tools that you have, where do you see users spending most time on is it? Are the users just coming for simple drag application, simple chat bot interface? Are users actually building agents locally usage?
today, Tim, across all of the tools that you have, where do you see users spending most time on is it? Are the users just coming for simple drag application, simple chat bot interface? Are users actually building agents locally usage?
S Speaker 113:51So luckily, because of MCPS, which we support in the desktop app, that has made agent adoption like skyrocket, people use agents all the time now, whereas before, we offer like simple agents, like web scraping, document summarization, stuff like that. But once people were able to just easily plug CPS, we've now seen custom agents just skyrocket up traditionally, though, and even still, today, the majority of our use case is legitimately just rack, just people chatting with the documents, knowing it's fully local, using a local model or a provider that they trust, or their own provider running on a server in their organization's architecture. That's what they want. That's what we give them, and that's what they use. But obviously, as this whole system becomes even more we're going to make it more easily for people to adopt these more sophisticated AI implementations, like agents. So,
So luckily, because of MCPS, which we support in the desktop app, that has made agent adoption like skyrocket, people use agents all the time now, whereas before, we offer like simple agents, like web scraping, document summarization, stuff like that. But once people were able to just easily plug CPS, we've now seen custom agents just skyrocket up traditionally, though, and even still, today, the majority of our use case is legitimately just rack, just people chatting with the documents, knowing it's fully local, using a local model or a provider that they trust, or their own provider running on a server in their organization's architecture. That's what they want. That's what we give them, and that's what they use. But obviously, as this whole system becomes even more we're going to make it more easily for people to adopt these more sophisticated AI implementations, like agents. So,
So luckily, because of MCPS, which we support in the desktop app, that has made agent adoption like skyrocket, people use agents all the time now, whereas before, we offer like simple agents, like web scraping, document summarization, stuff like that. But once people were able to just easily plug CPS, we've now seen custom agents just skyrocket up traditionally, though, and even still, today, the majority of our use case is legitimately just rack, just people chatting with the documents, knowing it's fully local, using a local model or a provider that they trust, or their own provider running on a server in their organization's architecture. That's what they want. That's what we give them, and that's what they use. But obviously, as this whole system becomes even more we're going to make it more easily for people to adopt these more sophisticated AI implementations, like agents. So,
So luckily, because of MCPS, which we support in the desktop app, that has made agent adoption like skyrocket, people use agents all the time now, whereas before, we offer like simple agents, like web scraping, document summarization, stuff like that. But once people were able to just easily plug CPS, we've now seen custom agents just skyrocket up traditionally, though, and even still, today, the majority of our use case is legitimately just rack, just people chatting with the documents, knowing it's fully local, using a local model or a provider that they trust, or their own provider running on a server in their organization's architecture. That's what they want. That's what we give them, and that's what they use. But obviously, as this whole system becomes even more we're going to make it more easily for people to adopt these more sophisticated AI implementations, like agents. So,
14:30right, right. So and So.
right, right. So and So.
right, right. So and So.
right, right. So and So.
S Speaker 214:35Anyway, so MCP in what we did, MCB and log, a lot of guaranteed agent use. Is it just that function calling and tool calling is more easier and more native. Now also, we
Anyway, so MCP in what we did, MCB and log, a lot of guaranteed agent use. Is it just that function calling and tool calling is more easier and more native. Now also, we
Anyway, so MCP in what we did, MCB and log, a lot of guaranteed agent use. Is it just that function calling and tool calling is more easier and more native. Now also, we
Anyway, so MCP in what we did, MCB and log, a lot of guaranteed agent use. Is it just that function calling and tool calling is more easier and more native. Now also, we
S Speaker 215:21absolutely and any plans of going vertical. Do you see some vertical industries adopting you faster finance, healthcare on top of my mind, but defense, maybe. But any plans for you to go vertical or develop some specific features or for vertical applications?
absolutely and any plans of going vertical. Do you see some vertical industries adopting you faster finance, healthcare on top of my mind, but defense, maybe. But any plans for you to go vertical or develop some specific features or for vertical applications?
absolutely and any plans of going vertical. Do you see some vertical industries adopting you faster finance, healthcare on top of my mind, but defense, maybe. But any plans for you to go vertical or develop some specific features or for vertical applications?
absolutely and any plans of going vertical. Do you see some vertical industries adopting you faster finance, healthcare on top of my mind, but defense, maybe. But any plans for you to go vertical or develop some specific features or for vertical applications?
S Speaker 115:33We don't have any specific plans right now for verticalization, mostly because we don't have any customer right now who is predominant or customer segment who is predominantly in a single sector. That makes sense to do a bet on that. Make that being said, we do have customers in healthcare. We do have customers who are lawyers, and we do have government people who use us of both local county governments and larger like governmental organizations, where privacy is, of course, strictly parabola beyond that. We also have academic researchers and like consultants, and then also just your regular, everyday individual contributor at a mid sized company. But we don't have any specific growing demand in one specific vertical, simply because our tool is so usable boss those verticals that we just see adoption from kind of like all over, and that's that's really who we're catering to, is those people that people just want to use AI, but in a compliant way, without having to have their it. Folks you know, have an aneurysm,
We don't have any specific plans right now for verticalization, mostly because we don't have any customer right now who is predominant or customer segment who is predominantly in a single sector. That makes sense to do a bet on that. Make that being said, we do have customers in healthcare. We do have customers who are lawyers, and we do have government people who use us of both local county governments and larger like governmental organizations, where privacy is, of course, strictly parabola beyond that. We also have academic researchers and like consultants, and then also just your regular, everyday individual contributor at a mid sized company. But we don't have any specific growing demand in one specific vertical, simply because our tool is so usable boss those verticals that we just see adoption from kind of like all over, and that's that's really who we're catering to, is those people that people just want to use AI, but in a compliant way, without having to have their it. Folks you know, have an aneurysm,
We don't have any specific plans right now for verticalization, mostly because we don't have any customer right now who is predominant or customer segment who is predominantly in a single sector. That makes sense to do a bet on that. Make that being said, we do have customers in healthcare. We do have customers who are lawyers, and we do have government people who use us of both local county governments and larger like governmental organizations, where privacy is, of course, strictly parabola beyond that. We also have academic researchers and like consultants, and then also just your regular, everyday individual contributor at a mid sized company. But we don't have any specific growing demand in one specific vertical, simply because our tool is so usable boss those verticals that we just see adoption from kind of like all over, and that's that's really who we're catering to, is those people that people just want to use AI, but in a compliant way, without having to have their it. Folks you know, have an aneurysm,
We don't have any specific plans right now for verticalization, mostly because we don't have any customer right now who is predominant or customer segment who is predominantly in a single sector. That makes sense to do a bet on that. Make that being said, we do have customers in healthcare. We do have customers who are lawyers, and we do have government people who use us of both local county governments and larger like governmental organizations, where privacy is, of course, strictly parabola beyond that. We also have academic researchers and like consultants, and then also just your regular, everyday individual contributor at a mid sized company. But we don't have any specific growing demand in one specific vertical, simply because our tool is so usable boss those verticals that we just see adoption from kind of like all over, and that's that's really who we're catering to, is those people that people just want to use AI, but in a compliant way, without having to have their it. Folks you know, have an aneurysm,
S Speaker 216:15right? Makes sense? And Tim to that point. So now that you're focusing on monetization, then you definitely have a strong open source community, a strong usage based already. How do you think the monetization motion would look like you you mentioned, see perceived pricing as well. Would it be an outbound sales kind of motion? Do you think prosumer and then expanding in an organization? How do you see this happening? So
right? Makes sense? And Tim to that point. So now that you're focusing on monetization, then you definitely have a strong open source community, a strong usage based already. How do you think the monetization motion would look like you you mentioned, see perceived pricing as well. Would it be an outbound sales kind of motion? Do you think prosumer and then expanding in an organization? How do you see this happening? So
right? Makes sense? And Tim to that point. So now that you're focusing on monetization, then you definitely have a strong open source community, a strong usage based already. How do you think the monetization motion would look like you you mentioned, see perceived pricing as well. Would it be an outbound sales kind of motion? Do you think prosumer and then expanding in an organization? How do you see this happening? So
right? Makes sense? And Tim to that point. So now that you're focusing on monetization, then you definitely have a strong open source community, a strong usage based already. How do you think the monetization motion would look like you you mentioned, see perceived pricing as well. Would it be an outbound sales kind of motion? Do you think prosumer and then expanding in an organization? How do you see this happening? So
S Speaker 218:20totally and very, very interesting place. Would love to take this further in terms of fundraising. I saw that you were whatever. But apart from that, how much did you raise so far? And any plans to fundraise anything? So,
totally and very, very interesting place. Would love to take this further in terms of fundraising. I saw that you were whatever. But apart from that, how much did you raise so far? And any plans to fundraise anything? So,
totally and very, very interesting place. Would love to take this further in terms of fundraising. I saw that you were whatever. But apart from that, how much did you raise so far? And any plans to fundraise anything? So,
totally and very, very interesting place. Would love to take this further in terms of fundraising. I saw that you were whatever. But apart from that, how much did you raise so far? And any plans to fundraise anything? So,
S Speaker 219:32value possible, absolutely. So from now till usually, let's, let's try to work closely so that we are able to contribute to some of this overall modernization story that you're trying to build today, makes sense. Let me come back with another call with Bucha, manufacturer of Hong Kong, managers. He leads the US investment team. But this will be the week after the next week, because next week we are on an off site and that all of the team is out. So I'll come back to you with some availability, plus Tushar for the week after this. Okay, yeah, please let
value possible, absolutely. So from now till usually, let's, let's try to work closely so that we are able to contribute to some of this overall modernization story that you're trying to build today, makes sense. Let me come back with another call with Bucha, manufacturer of Hong Kong, managers. He leads the US investment team. But this will be the week after the next week, because next week we are on an off site and that all of the team is out. So I'll come back to you with some availability, plus Tushar for the week after this. Okay, yeah, please let
value possible, absolutely. So from now till usually, let's, let's try to work closely so that we are able to contribute to some of this overall modernization story that you're trying to build today, makes sense. Let me come back with another call with Bucha, manufacturer of Hong Kong, managers. He leads the US investment team. But this will be the week after the next week, because next week we are on an off site and that all of the team is out. So I'll come back to you with some availability, plus Tushar for the week after this. Okay, yeah, please let
value possible, absolutely. So from now till usually, let's, let's try to work closely so that we are able to contribute to some of this overall modernization story that you're trying to build today, makes sense. Let me come back with another call with Bucha, manufacturer of Hong Kong, managers. He leads the US investment team. But this will be the week after the next week, because next week we are on an off site and that all of the team is out. So I'll come back to you with some availability, plus Tushar for the week after this. Okay, yeah, please let
S Speaker 119:58me know in advance. Basically, the last week of this month, I won't be available. I'm not getting married. Oh, I definitely can. Married. I definitely can't do anything, so top of June or do it earlier. We'll figure it out. But yeah, just let you know. And I know Microsoft build is coming up. I was gonna go to it now. I'm not. I know there was a thing going on, but yeah, it's just like, there's a lot of stuff
me know in advance. Basically, the last week of this month, I won't be available. I'm not getting married. Oh, I definitely can. Married. I definitely can't do anything, so top of June or do it earlier. We'll figure it out. But yeah, just let you know. And I know Microsoft build is coming up. I was gonna go to it now. I'm not. I know there was a thing going on, but yeah, it's just like, there's a lot of stuff
me know in advance. Basically, the last week of this month, I won't be available. I'm not getting married. Oh, I definitely can. Married. I definitely can't do anything, so top of June or do it earlier. We'll figure it out. But yeah, just let you know. And I know Microsoft build is coming up. I was gonna go to it now. I'm not. I know there was a thing going on, but yeah, it's just like, there's a lot of stuff
me know in advance. Basically, the last week of this month, I won't be available. I'm not getting married. Oh, I definitely can. Married. I definitely can't do anything, so top of June or do it earlier. We'll figure it out. But yeah, just let you know. And I know Microsoft build is coming up. I was gonna go to it now. I'm not. I know there was a thing going on, but yeah, it's just like, there's a lot of stuff
S Speaker 220:14going on. Absolutely. Congrats on that. Congrats. Is the magic link to be here in California? Going to be here in California?
going on. Absolutely. Congrats on that. Congrats. Is the magic link to be here in California? Going to be here in California?
going on. Absolutely. Congrats on that. Congrats. Is the magic link to be here in California? Going to be here in California?
going on. Absolutely. Congrats on that. Congrats. Is the magic link to be here in California? Going to be here in California?
20:20No, it's gonna be back in New Orleans and also
No, it's gonna be back in New Orleans and also
No, it's gonna be back in New Orleans and also
No, it's gonna be back in New Orleans and also
S Speaker 320:23in the San Diego office. No, we are in the
in the San Diego office. No, we are in the
in the San Diego office. No, we are in the
in the San Diego office. No, we are in the
S Speaker 120:42I can I can download that one that I showed the executive sales team and I'll just send that to you. Absolutely.
I can I can download that one that I showed the executive sales team and I'll just send that to you. Absolutely.
I can I can download that one that I showed the executive sales team and I'll just send that to you. Absolutely.
I can I can download that one that I showed the executive sales team and I'll just send that to you. Absolutely.
S Speaker 220:49Watch out for your time here to then appreciate it and enjoy the conversation like straight forward. Thank you.
Watch out for your time here to then appreciate it and enjoy the conversation like straight forward. Thank you.
Watch out for your time here to then appreciate it and enjoy the conversation like straight forward. Thank you.
Watch out for your time here to then appreciate it and enjoy the conversation like straight forward. Thank you.