Meeting: Orkes Product demo
Mon, Oct 27
4:04 PM
53 min
Priyesh P
Workflow Orchestration Overview
0:00
Task Ex
URL: https://otter.ai/u/_lkjxPXj1adkSpGyuW9thFYtwSQ
Downloaded: 2025-12-21T19:49:46.269917
Method: text_extraction
============================================================

0:22and you enjoy while, while he's sharing
and you enjoy while, while he's sharing
and you enjoy while, while he's sharing
and you enjoy while, while he's sharing
S Speaker 10:25it, we would love to have an update on the round progression as well. Because I know you mentioned you have a term sheet. I don't know if you've formally signed it, and you know how much allocation and how much time. So maybe we should spend last five, minutes on that Sure? Yeah,
it, we would love to have an update on the round progression as well. Because I know you mentioned you have a term sheet. I don't know if you've formally signed it, and you know how much allocation and how much time. So maybe we should spend last five, minutes on that Sure? Yeah,
it, we would love to have an update on the round progression as well. Because I know you mentioned you have a term sheet. I don't know if you've formally signed it, and you know how much allocation and how much time. So maybe we should spend last five, minutes on that Sure? Yeah,
it, we would love to have an update on the round progression as well. Because I know you mentioned you have a term sheet. I don't know if you've formally signed it, and you know how much allocation and how much time. So maybe we should spend last five, minutes on that Sure? Yeah,
S Speaker 20:46you're able to see my screen right, like teams does not show me.
you're able to see my screen right, like teams does not show me.
you're able to see my screen right, like teams does not show me.
you're able to see my screen right, like teams does not show me.
0:51Yeah, we can see if you can. Yeah,
Yeah, we can see if you can. Yeah,
Yeah, we can see if you can. Yeah,
Yeah, we can see if you can. Yeah,
0:54sounds good. Yeah, right. So,
sounds good. Yeah, right. So,
sounds good. Yeah, right. So,
sounds good. Yeah, right. So,
S Speaker 10:59yeah, it's a little Can you zoom in a little bit if? If you can, but it's okay, if not.
yeah, it's a little Can you zoom in a little bit if? If you can, but it's okay, if not.
yeah, it's a little Can you zoom in a little bit if? If you can, but it's okay, if not.
yeah, it's a little Can you zoom in a little bit if? If you can, but it's okay, if not.
1:05How about now, is it good or
How about now, is it good or
How about now, is it good or
How about now, is it good or
1:08Oh yeah. This is Yeah. This is good for us. Yeah.
Oh yeah. This is Yeah. This is good for us. Yeah.
Oh yeah. This is Yeah. This is good for us. Yeah.
Oh yeah. This is Yeah. This is good for us. Yeah.
S Speaker 21:14All right. So as you kind of might have guessed, my research B is a workflow engine, and what you see here is the UI, which allows you to kind of do two things, right? So one, it allows you to kind of manage the execution of the workflow. So like you can look at all the workflows that are currently running, have completed, have failed. You can search so essentially, every workflow execution, every instance is captured, indexed, and, you know, kept in the system until the retention policy allows it to expire. And then you can, like, look at it and see what's going on there, right? You can also open up the workflow and see what's going on there, in terms of, you know, what really happened, right? So if the workflow has much of tasks, when was the task schedule, when it didn't start, when it didn't finish, which worker Did it run on, and things like that. So it gives you kind of visibility into it. When you have multiple tasks. You can also see, like, you know, the timeline view to see when did it start, what was the delay before it got picked up, and all of those things, right? So it kind of gives you a complete visibility into what is happening every task level. You can also see what were the inputs given, what was the output introduced, and so forth. So that's one aspect of it, and we'll dive deeper into it. The second aspect is that you as a developer can actually build workflows with the UI. So I'll just reset this for a bit, but essentially what we have is a editor that allows you to kind of create very complex workflows inside the browser itself, right when you think about a workflow, workflow, essentially is a graph of our tasks that are executed in a sequence, right, right? So you can very in its simplest form, you can have a worker task where the task is implemented using any of the supported languages. So like you know, we support pretty much every major language, ranging from Java to Golang, Python, dotnet, TypeScript. It allows you to, kind of, this is where you implement your business module. So let's say you are writing a workflow that does payment, transfer, yeah, you might have to want to debit, want to credit, yeah, those tasks event in in your language of choice. And that's, that's the simplest unit of
All right. So as you kind of might have guessed, my research B is a workflow engine, and what you see here is the UI, which allows you to kind of do two things, right? So one, it allows you to kind of manage the execution of the workflow. So like you can look at all the workflows that are currently running, have completed, have failed. You can search so essentially, every workflow execution, every instance is captured, indexed, and, you know, kept in the system until the retention policy allows it to expire. And then you can, like, look at it and see what's going on there, right? You can also open up the workflow and see what's going on there, in terms of, you know, what really happened, right? So if the workflow has much of tasks, when was the task schedule, when it didn't start, when it didn't finish, which worker Did it run on, and things like that. So it gives you kind of visibility into it. When you have multiple tasks. You can also see, like, you know, the timeline view to see when did it start, what was the delay before it got picked up, and all of those things, right? So it kind of gives you a complete visibility into what is happening every task level. You can also see what were the inputs given, what was the output introduced, and so forth. So that's one aspect of it, and we'll dive deeper into it. The second aspect is that you as a developer can actually build workflows with the UI. So I'll just reset this for a bit, but essentially what we have is a editor that allows you to kind of create very complex workflows inside the browser itself, right when you think about a workflow, workflow, essentially is a graph of our tasks that are executed in a sequence, right, right? So you can very in its simplest form, you can have a worker task where the task is implemented using any of the supported languages. So like you know, we support pretty much every major language, ranging from Java to Golang, Python, dotnet, TypeScript. It allows you to, kind of, this is where you implement your business module. So let's say you are writing a workflow that does payment, transfer, yeah, you might have to want to debit, want to credit, yeah, those tasks event in in your language of choice. And that's, that's the simplest unit of
All right. So as you kind of might have guessed, my research B is a workflow engine, and what you see here is the UI, which allows you to kind of do two things, right? So one, it allows you to kind of manage the execution of the workflow. So like you can look at all the workflows that are currently running, have completed, have failed. You can search so essentially, every workflow execution, every instance is captured, indexed, and, you know, kept in the system until the retention policy allows it to expire. And then you can, like, look at it and see what's going on there, right? You can also open up the workflow and see what's going on there, in terms of, you know, what really happened, right? So if the workflow has much of tasks, when was the task schedule, when it didn't start, when it didn't finish, which worker Did it run on, and things like that. So it gives you kind of visibility into it. When you have multiple tasks. You can also see, like, you know, the timeline view to see when did it start, what was the delay before it got picked up, and all of those things, right? So it kind of gives you a complete visibility into what is happening every task level. You can also see what were the inputs given, what was the output introduced, and so forth. So that's one aspect of it, and we'll dive deeper into it. The second aspect is that you as a developer can actually build workflows with the UI. So I'll just reset this for a bit, but essentially what we have is a editor that allows you to kind of create very complex workflows inside the browser itself, right when you think about a workflow, workflow, essentially is a graph of our tasks that are executed in a sequence, right, right? So you can very in its simplest form, you can have a worker task where the task is implemented using any of the supported languages. So like you know, we support pretty much every major language, ranging from Java to Golang, Python, dotnet, TypeScript. It allows you to, kind of, this is where you implement your business module. So let's say you are writing a workflow that does payment, transfer, yeah, you might have to want to debit, want to credit, yeah, those tasks event in in your language of choice. And that's, that's the simplest unit of
All right. So as you kind of might have guessed, my research B is a workflow engine, and what you see here is the UI, which allows you to kind of do two things, right? So one, it allows you to kind of manage the execution of the workflow. So like you can look at all the workflows that are currently running, have completed, have failed. You can search so essentially, every workflow execution, every instance is captured, indexed, and, you know, kept in the system until the retention policy allows it to expire. And then you can, like, look at it and see what's going on there, right? You can also open up the workflow and see what's going on there, in terms of, you know, what really happened, right? So if the workflow has much of tasks, when was the task schedule, when it didn't start, when it didn't finish, which worker Did it run on, and things like that. So it gives you kind of visibility into it. When you have multiple tasks. You can also see, like, you know, the timeline view to see when did it start, what was the delay before it got picked up, and all of those things, right? So it kind of gives you a complete visibility into what is happening every task level. You can also see what were the inputs given, what was the output introduced, and so forth. So that's one aspect of it, and we'll dive deeper into it. The second aspect is that you as a developer can actually build workflows with the UI. So I'll just reset this for a bit, but essentially what we have is a editor that allows you to kind of create very complex workflows inside the browser itself, right when you think about a workflow, workflow, essentially is a graph of our tasks that are executed in a sequence, right, right? So you can very in its simplest form, you can have a worker task where the task is implemented using any of the supported languages. So like you know, we support pretty much every major language, ranging from Java to Golang, Python, dotnet, TypeScript. It allows you to, kind of, this is where you implement your business module. So let's say you are writing a workflow that does payment, transfer, yeah, you might have to want to debit, want to credit, yeah, those tasks event in in your language of choice. And that's, that's the simplest unit of
3:43that's a worker, right with it?
that's a worker, right with it?
that's a worker, right with it?
that's a worker, right with it?
S Speaker 23:45That's a worker, yeah. Now, if you are into micro services world, you don't have to write a worker. You can have an HTTP task that can also call it micro service. Got it so you are not limited. And then, depending upon the use case, you can use whatever works the best for you, right? What we have done with conductor is that conductor also comes pre built with tasks like HTTP. We can also do unary gRPC calling, so, you know, all the well known things that otherwise you will end up writing the same piece of code multiple times. It comes out of the box. So you don't have to write the code. You can just you can just configure them and use them. Tasks can also be executed in parallel, so you're not limited to, just like, you know, sequential coding. And one of the areas where conductor really excels is that you can have massive parallel forks, right? Yeah. If you want to call 1000 tasks in parallel, you can do it as long as you can actually execute them once, connector schedules, right? You can also run them conditionally, so just like if then else in your code, you can do that, or you can run them in a group. And of course, you can typically combine a group with a parallel execution that allows you to do things like batch processing, right? Run a batch in a loop, and for every batch, run things in parallel. Yeah. Essentially the way we built conductor was that it essentially mimics your programming language constructs like function calling loops for joints decisions. So as a developer, the way you would like code is how you will define a workflow. You can also embed a workflow inside a workflow as a sub workflow. Yeah, so you know, you can take it in workflow and put it as a sub workflow. So now we don't also allows you to kind of compose workflows as components and reuse them, depending upon the use cases, right,
That's a worker, yeah. Now, if you are into micro services world, you don't have to write a worker. You can have an HTTP task that can also call it micro service. Got it so you are not limited. And then, depending upon the use case, you can use whatever works the best for you, right? What we have done with conductor is that conductor also comes pre built with tasks like HTTP. We can also do unary gRPC calling, so, you know, all the well known things that otherwise you will end up writing the same piece of code multiple times. It comes out of the box. So you don't have to write the code. You can just you can just configure them and use them. Tasks can also be executed in parallel, so you're not limited to, just like, you know, sequential coding. And one of the areas where conductor really excels is that you can have massive parallel forks, right? Yeah. If you want to call 1000 tasks in parallel, you can do it as long as you can actually execute them once, connector schedules, right? You can also run them conditionally, so just like if then else in your code, you can do that, or you can run them in a group. And of course, you can typically combine a group with a parallel execution that allows you to do things like batch processing, right? Run a batch in a loop, and for every batch, run things in parallel. Yeah. Essentially the way we built conductor was that it essentially mimics your programming language constructs like function calling loops for joints decisions. So as a developer, the way you would like code is how you will define a workflow. You can also embed a workflow inside a workflow as a sub workflow. Yeah, so you know, you can take it in workflow and put it as a sub workflow. So now we don't also allows you to kind of compose workflows as components and reuse them, depending upon the use cases, right,
That's a worker, yeah. Now, if you are into micro services world, you don't have to write a worker. You can have an HTTP task that can also call it micro service. Got it so you are not limited. And then, depending upon the use case, you can use whatever works the best for you, right? What we have done with conductor is that conductor also comes pre built with tasks like HTTP. We can also do unary gRPC calling, so, you know, all the well known things that otherwise you will end up writing the same piece of code multiple times. It comes out of the box. So you don't have to write the code. You can just you can just configure them and use them. Tasks can also be executed in parallel, so you're not limited to, just like, you know, sequential coding. And one of the areas where conductor really excels is that you can have massive parallel forks, right? Yeah. If you want to call 1000 tasks in parallel, you can do it as long as you can actually execute them once, connector schedules, right? You can also run them conditionally, so just like if then else in your code, you can do that, or you can run them in a group. And of course, you can typically combine a group with a parallel execution that allows you to do things like batch processing, right? Run a batch in a loop, and for every batch, run things in parallel. Yeah. Essentially the way we built conductor was that it essentially mimics your programming language constructs like function calling loops for joints decisions. So as a developer, the way you would like code is how you will define a workflow. You can also embed a workflow inside a workflow as a sub workflow. Yeah, so you know, you can take it in workflow and put it as a sub workflow. So now we don't also allows you to kind of compose workflows as components and reuse them, depending upon the use cases, right,
That's a worker, yeah. Now, if you are into micro services world, you don't have to write a worker. You can have an HTTP task that can also call it micro service. Got it so you are not limited. And then, depending upon the use case, you can use whatever works the best for you, right? What we have done with conductor is that conductor also comes pre built with tasks like HTTP. We can also do unary gRPC calling, so, you know, all the well known things that otherwise you will end up writing the same piece of code multiple times. It comes out of the box. So you don't have to write the code. You can just you can just configure them and use them. Tasks can also be executed in parallel, so you're not limited to, just like, you know, sequential coding. And one of the areas where conductor really excels is that you can have massive parallel forks, right? Yeah. If you want to call 1000 tasks in parallel, you can do it as long as you can actually execute them once, connector schedules, right? You can also run them conditionally, so just like if then else in your code, you can do that, or you can run them in a group. And of course, you can typically combine a group with a parallel execution that allows you to do things like batch processing, right? Run a batch in a loop, and for every batch, run things in parallel. Yeah. Essentially the way we built conductor was that it essentially mimics your programming language constructs like function calling loops for joints decisions. So as a developer, the way you would like code is how you will define a workflow. You can also embed a workflow inside a workflow as a sub workflow. Yeah, so you know, you can take it in workflow and put it as a sub workflow. So now we don't also allows you to kind of compose workflows as components and reuse them, depending upon the use cases, right,
S Speaker 15:37and within between the components of the workflow, right? Like maintaining a state or passing the parameters, or, like, you know, evaluating dependencies on workflows, right? So I get the drag and drop feature, and, you know, you can just create a worker node. But how are you like, can we maybe unpack one level two level detail, deeper into okay, we connected these two workers and these many, yeah, what is, what is going on in the background? Yeah.
and within between the components of the workflow, right? Like maintaining a state or passing the parameters, or, like, you know, evaluating dependencies on workflows, right? So I get the drag and drop feature, and, you know, you can just create a worker node. But how are you like, can we maybe unpack one level two level detail, deeper into okay, we connected these two workers and these many, yeah, what is, what is going on in the background? Yeah.
and within between the components of the workflow, right? Like maintaining a state or passing the parameters, or, like, you know, evaluating dependencies on workflows, right? So I get the drag and drop feature, and, you know, you can just create a worker node. But how are you like, can we maybe unpack one level two level detail, deeper into okay, we connected these two workers and these many, yeah, what is, what is going on in the background? Yeah.
and within between the components of the workflow, right? Like maintaining a state or passing the parameters, or, like, you know, evaluating dependencies on workflows, right? So I get the drag and drop feature, and, you know, you can just create a worker node. But how are you like, can we maybe unpack one level two level detail, deeper into okay, we connected these two workers and these many, yeah, what is, what is going on in the background? Yeah.
S Speaker 26:08So one, let's unpack, right? So, like one is, how do you pass the state across the task, right? So what you can do is that for every task that you have,
So one, let's unpack, right? So, like one is, how do you pass the state across the task, right? So what you can do is that for every task that you have,
So one, let's unpack, right? So, like one is, how do you pass the state across the task, right? So what you can do is that for every task that you have,
So one, let's unpack, right? So, like one is, how do you pass the state across the task, right? So what you can do is that for every task that you have,
6:19for every input parameters. You can wire them
for every input parameters. You can wire them
for every input parameters. You can wire them
for every input parameters. You can wire them
S Speaker 26:24up as part of the input to the workflow. Or let's say, let's say I'm doing payment processing, right, and I would need to know the account number. The account number could be coming as a workflow input. So I can say account number, yeah. Let's say authorized amount, right? Let's say, Yeah, this could be coming from the previous task. So, like, let's say,
up as part of the input to the workflow. Or let's say, let's say I'm doing payment processing, right, and I would need to know the account number. The account number could be coming as a workflow input. So I can say account number, yeah. Let's say authorized amount, right? Let's say, Yeah, this could be coming from the previous task. So, like, let's say,
up as part of the input to the workflow. Or let's say, let's say I'm doing payment processing, right, and I would need to know the account number. The account number could be coming as a workflow input. So I can say account number, yeah. Let's say authorized amount, right? Let's say, Yeah, this could be coming from the previous task. So, like, let's say,
up as part of the input to the workflow. Or let's say, let's say I'm doing payment processing, right, and I would need to know the account number. The account number could be coming as a workflow input. So I can say account number, yeah. Let's say authorized amount, right? Let's say, Yeah, this could be coming from the previous task. So, like, let's say,
6:50so this allows me to kind of wire the inputs as
so this allows me to kind of wire the inputs as
so this allows me to kind of wire the inputs as
so this allows me to kind of wire the inputs as
S Speaker 26:54the output of a previously executed task, right? Or input of output? No. There's also notion of a variable, which is basically a temporary variable that you can use it to. Can use it to pass, you know, temporary information across the task, and, you know, maintain some sort of context. So that's one way for every task. You can also define schema as a JSON schema, which means, like, you know, if you define input and output schema, and if the input doesn't conform to the schema, the task will fail with appropriate message. So you know you can enforce a set of validation rules right to make sure that, like you know, things are happening correctly. And the every task, every box that you see here right, is a task. Every task is backed by a queue. So one way what conductor does is that, essentially, conductor has its own we have built our own queuing system. These are time delay distributed priority queues, yeah. And every time a task is scheduled, a message is put into the queue, yeah, which is what is driving all the workers and everyone in a production movement, you would have like, like, for example, this might test eight queues. A production environment could have 1000s of queues. Yeah, maintenance. So when this queues also allows you to kind of schedule the task with priority or time delay. Things like, for example, you could put a wait task here. Let's say, if I were to run campaign on Christmas Eve, I can wait for December 24 year. Yeah, and this will wait. And the way to do that is, you know, we put a message in the queue with the timeout of December 24 midnight, the message will not come out and give that right? Got it so, you know the heartbeats of you know how we pass messages around,
the output of a previously executed task, right? Or input of output? No. There's also notion of a variable, which is basically a temporary variable that you can use it to. Can use it to pass, you know, temporary information across the task, and, you know, maintain some sort of context. So that's one way for every task. You can also define schema as a JSON schema, which means, like, you know, if you define input and output schema, and if the input doesn't conform to the schema, the task will fail with appropriate message. So you know you can enforce a set of validation rules right to make sure that, like you know, things are happening correctly. And the every task, every box that you see here right, is a task. Every task is backed by a queue. So one way what conductor does is that, essentially, conductor has its own we have built our own queuing system. These are time delay distributed priority queues, yeah. And every time a task is scheduled, a message is put into the queue, yeah, which is what is driving all the workers and everyone in a production movement, you would have like, like, for example, this might test eight queues. A production environment could have 1000s of queues. Yeah, maintenance. So when this queues also allows you to kind of schedule the task with priority or time delay. Things like, for example, you could put a wait task here. Let's say, if I were to run campaign on Christmas Eve, I can wait for December 24 year. Yeah, and this will wait. And the way to do that is, you know, we put a message in the queue with the timeout of December 24 midnight, the message will not come out and give that right? Got it so, you know the heartbeats of you know how we pass messages around,
the output of a previously executed task, right? Or input of output? No. There's also notion of a variable, which is basically a temporary variable that you can use it to. Can use it to pass, you know, temporary information across the task, and, you know, maintain some sort of context. So that's one way for every task. You can also define schema as a JSON schema, which means, like, you know, if you define input and output schema, and if the input doesn't conform to the schema, the task will fail with appropriate message. So you know you can enforce a set of validation rules right to make sure that, like you know, things are happening correctly. And the every task, every box that you see here right, is a task. Every task is backed by a queue. So one way what conductor does is that, essentially, conductor has its own we have built our own queuing system. These are time delay distributed priority queues, yeah. And every time a task is scheduled, a message is put into the queue, yeah, which is what is driving all the workers and everyone in a production movement, you would have like, like, for example, this might test eight queues. A production environment could have 1000s of queues. Yeah, maintenance. So when this queues also allows you to kind of schedule the task with priority or time delay. Things like, for example, you could put a wait task here. Let's say, if I were to run campaign on Christmas Eve, I can wait for December 24 year. Yeah, and this will wait. And the way to do that is, you know, we put a message in the queue with the timeout of December 24 midnight, the message will not come out and give that right? Got it so, you know the heartbeats of you know how we pass messages around,
the output of a previously executed task, right? Or input of output? No. There's also notion of a variable, which is basically a temporary variable that you can use it to. Can use it to pass, you know, temporary information across the task, and, you know, maintain some sort of context. So that's one way for every task. You can also define schema as a JSON schema, which means, like, you know, if you define input and output schema, and if the input doesn't conform to the schema, the task will fail with appropriate message. So you know you can enforce a set of validation rules right to make sure that, like you know, things are happening correctly. And the every task, every box that you see here right, is a task. Every task is backed by a queue. So one way what conductor does is that, essentially, conductor has its own we have built our own queuing system. These are time delay distributed priority queues, yeah. And every time a task is scheduled, a message is put into the queue, yeah, which is what is driving all the workers and everyone in a production movement, you would have like, like, for example, this might test eight queues. A production environment could have 1000s of queues. Yeah, maintenance. So when this queues also allows you to kind of schedule the task with priority or time delay. Things like, for example, you could put a wait task here. Let's say, if I were to run campaign on Christmas Eve, I can wait for December 24 year. Yeah, and this will wait. And the way to do that is, you know, we put a message in the queue with the timeout of December 24 midnight, the message will not come out and give that right? Got it so, you know the heartbeats of you know how we pass messages around,
S Speaker 18:42but when somebody has to trigger that, right? How is it getting triggered? Is it, is it constantly checking for that date to arrive? Or how does it get triggered?
but when somebody has to trigger that, right? How is it getting triggered? Is it, is it constantly checking for that date to arrive? Or how does it get triggered?
but when somebody has to trigger that, right? How is it getting triggered? Is it, is it constantly checking for that date to arrive? Or how does it get triggered?
but when somebody has to trigger that, right? How is it getting triggered? Is it, is it constantly checking for that date to arrive? Or how does it get triggered?
S Speaker 28:51Yeah. So that's the server, right. So conductor server is
Yeah. So that's the server, right. So conductor server is
Yeah. So that's the server, right. So conductor server is
Yeah. So that's the server, right. So conductor server is
S Speaker 18:55also that. So conductor server is checking your queue right and trying to execute what's in the queue, right?
also that. So conductor server is checking your queue right and trying to execute what's in the queue, right?
also that. So conductor server is checking your queue right and trying to execute what's in the queue, right?
also that. So conductor server is checking your queue right and trying to execute what's in the queue, right?
S Speaker 29:02That's, yeah, that's correct. And for worker task, essentially it is scheduling tasks in the worker when you are using the worker SDKs. SDKs are pulling, on a periodic basis, to the conductor server to say, if I have a task that's the worker based model, if it is HTTP, then, so you know, depending upon whether it's pull or push model, either the worker is pulling for it or the server is pushing it back for
That's, yeah, that's correct. And for worker task, essentially it is scheduling tasks in the worker when you are using the worker SDKs. SDKs are pulling, on a periodic basis, to the conductor server to say, if I have a task that's the worker based model, if it is HTTP, then, so you know, depending upon whether it's pull or push model, either the worker is pulling for it or the server is pushing it back for
That's, yeah, that's correct. And for worker task, essentially it is scheduling tasks in the worker when you are using the worker SDKs. SDKs are pulling, on a periodic basis, to the conductor server to say, if I have a task that's the worker based model, if it is HTTP, then, so you know, depending upon whether it's pull or push model, either the worker is pulling for it or the server is pushing it back for
That's, yeah, that's correct. And for worker task, essentially it is scheduling tasks in the worker when you are using the worker SDKs. SDKs are pulling, on a periodic basis, to the conductor server to say, if I have a task that's the worker based model, if it is HTTP, then, so you know, depending upon whether it's pull or push model, either the worker is pulling for it or the server is pushing it back for
S Speaker 19:24you, got it, got it, got it. So it's both ways, right? So the worker, and is that? How is that? How it enables fault fault tolerance? Or
you, got it, got it, got it. So it's both ways, right? So the worker, and is that? How is that? How it enables fault fault tolerance? Or
you, got it, got it, got it. So it's both ways, right? So the worker, and is that? How is that? How it enables fault fault tolerance? Or
you, got it, got it, got it. So it's both ways, right? So the worker, and is that? How is that? How it enables fault fault tolerance? Or
S Speaker 29:33Yes, yes. So other part there is that for every worker, you can also define, and we recommend people to define that is, is define the resiliency parameter for the task. For every task, you define a few things. One is, what happens when the task space, yeah, should you retry? You retry. You know how many times? What's your retry policy, whether it's fixed or linear or exponential, back off whatever might be the case. What about the timeout? So if you think about it right, your task could be very shortly. Could complete in few 100 millisecond. It can also be long running task, like the task would take, you know, days to complete, right? Long running task. Now last thing you want is something that is going to take three days to complete, and only to find out after three days that No, something went wrong and the worker died, right? So, right? You also, it also supports heartbeat mechanism. So you can also provide response times by saying that, like, you know, like, for example, here it says, you know, I want to get a heartbeat every five seconds, yeah, if I don't get done, I will retry the task, assuming that the worker was working on it has died, yeah. And I should reskill the task, right? So that allows me to make sure that, like you know, my tasks are not lost, right? And when this retry, you have enough metadata available, like a full counter will be incremented. So you know, it's a redelivery the queues implements at least once delivery semantics, so message is guaranteed to deliver at least once. In case of retries, there will be read delivery. And if there's a red delivery, you can put additional checks depending upon the idle potency of your task logic. If you want to check something extra and things like that. Other thing you can also enforce on at a task level is rate limits, yeah, and so you know, you can do either fixed concurrency or video based concurrency, right? How many requests that you are going to support, and this applies at the entire conductor server level, as long as this task is being used, even if across multiple workflows, the rate limits are going to be enforced. So those are all implemented at the last level, including the schema and everything.
Yes, yes. So other part there is that for every worker, you can also define, and we recommend people to define that is, is define the resiliency parameter for the task. For every task, you define a few things. One is, what happens when the task space, yeah, should you retry? You retry. You know how many times? What's your retry policy, whether it's fixed or linear or exponential, back off whatever might be the case. What about the timeout? So if you think about it right, your task could be very shortly. Could complete in few 100 millisecond. It can also be long running task, like the task would take, you know, days to complete, right? Long running task. Now last thing you want is something that is going to take three days to complete, and only to find out after three days that No, something went wrong and the worker died, right? So, right? You also, it also supports heartbeat mechanism. So you can also provide response times by saying that, like, you know, like, for example, here it says, you know, I want to get a heartbeat every five seconds, yeah, if I don't get done, I will retry the task, assuming that the worker was working on it has died, yeah. And I should reskill the task, right? So that allows me to make sure that, like you know, my tasks are not lost, right? And when this retry, you have enough metadata available, like a full counter will be incremented. So you know, it's a redelivery the queues implements at least once delivery semantics, so message is guaranteed to deliver at least once. In case of retries, there will be read delivery. And if there's a red delivery, you can put additional checks depending upon the idle potency of your task logic. If you want to check something extra and things like that. Other thing you can also enforce on at a task level is rate limits, yeah, and so you know, you can do either fixed concurrency or video based concurrency, right? How many requests that you are going to support, and this applies at the entire conductor server level, as long as this task is being used, even if across multiple workflows, the rate limits are going to be enforced. So those are all implemented at the last level, including the schema and everything.
Yes, yes. So other part there is that for every worker, you can also define, and we recommend people to define that is, is define the resiliency parameter for the task. For every task, you define a few things. One is, what happens when the task space, yeah, should you retry? You retry. You know how many times? What's your retry policy, whether it's fixed or linear or exponential, back off whatever might be the case. What about the timeout? So if you think about it right, your task could be very shortly. Could complete in few 100 millisecond. It can also be long running task, like the task would take, you know, days to complete, right? Long running task. Now last thing you want is something that is going to take three days to complete, and only to find out after three days that No, something went wrong and the worker died, right? So, right? You also, it also supports heartbeat mechanism. So you can also provide response times by saying that, like, you know, like, for example, here it says, you know, I want to get a heartbeat every five seconds, yeah, if I don't get done, I will retry the task, assuming that the worker was working on it has died, yeah. And I should reskill the task, right? So that allows me to make sure that, like you know, my tasks are not lost, right? And when this retry, you have enough metadata available, like a full counter will be incremented. So you know, it's a redelivery the queues implements at least once delivery semantics, so message is guaranteed to deliver at least once. In case of retries, there will be read delivery. And if there's a red delivery, you can put additional checks depending upon the idle potency of your task logic. If you want to check something extra and things like that. Other thing you can also enforce on at a task level is rate limits, yeah, and so you know, you can do either fixed concurrency or video based concurrency, right? How many requests that you are going to support, and this applies at the entire conductor server level, as long as this task is being used, even if across multiple workflows, the rate limits are going to be enforced. So those are all implemented at the last level, including the schema and everything.
Yes, yes. So other part there is that for every worker, you can also define, and we recommend people to define that is, is define the resiliency parameter for the task. For every task, you define a few things. One is, what happens when the task space, yeah, should you retry? You retry. You know how many times? What's your retry policy, whether it's fixed or linear or exponential, back off whatever might be the case. What about the timeout? So if you think about it right, your task could be very shortly. Could complete in few 100 millisecond. It can also be long running task, like the task would take, you know, days to complete, right? Long running task. Now last thing you want is something that is going to take three days to complete, and only to find out after three days that No, something went wrong and the worker died, right? So, right? You also, it also supports heartbeat mechanism. So you can also provide response times by saying that, like, you know, like, for example, here it says, you know, I want to get a heartbeat every five seconds, yeah, if I don't get done, I will retry the task, assuming that the worker was working on it has died, yeah. And I should reskill the task, right? So that allows me to make sure that, like you know, my tasks are not lost, right? And when this retry, you have enough metadata available, like a full counter will be incremented. So you know, it's a redelivery the queues implements at least once delivery semantics, so message is guaranteed to deliver at least once. In case of retries, there will be read delivery. And if there's a red delivery, you can put additional checks depending upon the idle potency of your task logic. If you want to check something extra and things like that. Other thing you can also enforce on at a task level is rate limits, yeah, and so you know, you can do either fixed concurrency or video based concurrency, right? How many requests that you are going to support, and this applies at the entire conductor server level, as long as this task is being used, even if across multiple workflows, the rate limits are going to be enforced. So those are all implemented at the last level, including the schema and everything.
S Speaker 311:43So, yeah, one more thing is that, you know, this is the way this is designed, is also very different from how other competitive products are designed, where the orchestration layer that you see here is fully decoupled from the services it's trying to orchestrate, right? So the task that you see in the workflow, they have no idea of like, which workflow it is, and so it encourages a lot more to use, right, like than other products that you see, right? It's also language dependent. So irrespective of which language those tasks are written in, whether it is written as a Python worker or, you know, Java worker, on a shift endpoint. It doesn't
So, yeah, one more thing is that, you know, this is the way this is designed, is also very different from how other competitive products are designed, where the orchestration layer that you see here is fully decoupled from the services it's trying to orchestrate, right? So the task that you see in the workflow, they have no idea of like, which workflow it is, and so it encourages a lot more to use, right, like than other products that you see, right? It's also language dependent. So irrespective of which language those tasks are written in, whether it is written as a Python worker or, you know, Java worker, on a shift endpoint. It doesn't
So, yeah, one more thing is that, you know, this is the way this is designed, is also very different from how other competitive products are designed, where the orchestration layer that you see here is fully decoupled from the services it's trying to orchestrate, right? So the task that you see in the workflow, they have no idea of like, which workflow it is, and so it encourages a lot more to use, right, like than other products that you see, right? It's also language dependent. So irrespective of which language those tasks are written in, whether it is written as a Python worker or, you know, Java worker, on a shift endpoint. It doesn't
So, yeah, one more thing is that, you know, this is the way this is designed, is also very different from how other competitive products are designed, where the orchestration layer that you see here is fully decoupled from the services it's trying to orchestrate, right? So the task that you see in the workflow, they have no idea of like, which workflow it is, and so it encourages a lot more to use, right, like than other products that you see, right? It's also language dependent. So irrespective of which language those tasks are written in, whether it is written as a Python worker or, you know, Java worker, on a shift endpoint. It doesn't
S Speaker 112:21really matter, right? That is super interesting. And maybe one, one question since on you talked about rate limiting, let's say you're calling in. I'm talking about an API endpoint that has some rate limits on it, right? Can you, can you pull it from that system that, what is the rate limit there? And if it, if you get closer to it, maybe add more scale, or wait for, you know, this much time to reset the rate limit. Like, how do you like, what? What level of detail do you get into?
really matter, right? That is super interesting. And maybe one, one question since on you talked about rate limiting, let's say you're calling in. I'm talking about an API endpoint that has some rate limits on it, right? Can you, can you pull it from that system that, what is the rate limit there? And if it, if you get closer to it, maybe add more scale, or wait for, you know, this much time to reset the rate limit. Like, how do you like, what? What level of detail do you get into?
really matter, right? That is super interesting. And maybe one, one question since on you talked about rate limiting, let's say you're calling in. I'm talking about an API endpoint that has some rate limits on it, right? Can you, can you pull it from that system that, what is the rate limit there? And if it, if you get closer to it, maybe add more scale, or wait for, you know, this much time to reset the rate limit. Like, how do you like, what? What level of detail do you get into?
really matter, right? That is super interesting. And maybe one, one question since on you talked about rate limiting, let's say you're calling in. I'm talking about an API endpoint that has some rate limits on it, right? Can you, can you pull it from that system that, what is the rate limit there? And if it, if you get closer to it, maybe add more scale, or wait for, you know, this much time to reset the rate limit. Like, how do you like, what? What level of detail do you get into?
S Speaker 212:53So what we don't pull from the remote service, because this was under protocol to get or get the information about rate limits, keyword, yeah. What you do is that when the rate limit is achieved, the task is not discarded. It's queued up, so it just waits for it. You will have, like, a slow, running workflow, yeah? Because, you know, things end up and you know, they are waiting for their turn, yeah. So you don't really is waiting. So the advantage there is an order you want to worry about it, you know, my workflow will execute. If there's a rate limit, it will run slowly. But run slowly, but it will run, got it, got it immediately. So follow does not have to worry about rate limits or handling those things,
So what we don't pull from the remote service, because this was under protocol to get or get the information about rate limits, keyword, yeah. What you do is that when the rate limit is achieved, the task is not discarded. It's queued up, so it just waits for it. You will have, like, a slow, running workflow, yeah? Because, you know, things end up and you know, they are waiting for their turn, yeah. So you don't really is waiting. So the advantage there is an order you want to worry about it, you know, my workflow will execute. If there's a rate limit, it will run slowly. But run slowly, but it will run, got it, got it immediately. So follow does not have to worry about rate limits or handling those things,
So what we don't pull from the remote service, because this was under protocol to get or get the information about rate limits, keyword, yeah. What you do is that when the rate limit is achieved, the task is not discarded. It's queued up, so it just waits for it. You will have, like, a slow, running workflow, yeah? Because, you know, things end up and you know, they are waiting for their turn, yeah. So you don't really is waiting. So the advantage there is an order you want to worry about it, you know, my workflow will execute. If there's a rate limit, it will run slowly. But run slowly, but it will run, got it, got it immediately. So follow does not have to worry about rate limits or handling those things,
So what we don't pull from the remote service, because this was under protocol to get or get the information about rate limits, keyword, yeah. What you do is that when the rate limit is achieved, the task is not discarded. It's queued up, so it just waits for it. You will have, like, a slow, running workflow, yeah? Because, you know, things end up and you know, they are waiting for their turn, yeah. So you don't really is waiting. So the advantage there is an order you want to worry about it, you know, my workflow will execute. If there's a rate limit, it will run slowly. But run slowly, but it will run, got it, got it immediately. So follow does not have to worry about rate limits or handling those things,
S Speaker 214:19example, that's got it, got it, got it. For HTTP tasks, we also support hedging. So, like you know, typically, what happens is, you know, if your downstream system sometimes, you know, has high latencies, it starts, you know, impacting everything else, right? So, you know, a common way to handle that is to hedging, right? So we also support hedging. Of course, hedging is going to issue multiple requests, so no, you will be careful in terms of what you are calling. And if it is either important, you can, you can apply hedging to, you know, also take the tail latencies, and, you know, improve the response time for your services,
example, that's got it, got it, got it. For HTTP tasks, we also support hedging. So, like you know, typically, what happens is, you know, if your downstream system sometimes, you know, has high latencies, it starts, you know, impacting everything else, right? So, you know, a common way to handle that is to hedging, right? So we also support hedging. Of course, hedging is going to issue multiple requests, so no, you will be careful in terms of what you are calling. And if it is either important, you can, you can apply hedging to, you know, also take the tail latencies, and, you know, improve the response time for your services,
example, that's got it, got it, got it. For HTTP tasks, we also support hedging. So, like you know, typically, what happens is, you know, if your downstream system sometimes, you know, has high latencies, it starts, you know, impacting everything else, right? So, you know, a common way to handle that is to hedging, right? So we also support hedging. Of course, hedging is going to issue multiple requests, so no, you will be careful in terms of what you are calling. And if it is either important, you can, you can apply hedging to, you know, also take the tail latencies, and, you know, improve the response time for your services,
example, that's got it, got it, got it. For HTTP tasks, we also support hedging. So, like you know, typically, what happens is, you know, if your downstream system sometimes, you know, has high latencies, it starts, you know, impacting everything else, right? So, you know, a common way to handle that is to hedging, right? So we also support hedging. Of course, hedging is going to issue multiple requests, so no, you will be careful in terms of what you are calling. And if it is either important, you can, you can apply hedging to, you know, also take the tail latencies, and, you know, improve the response time for your services,
14:53yeah, very interesting.
yeah, very interesting.
yeah, very interesting.
yeah, very interesting.
S Speaker 216:12No, we do Yeah, yeah. We do provide. We do provide like. We provide like. For example, we ask that like, you know, you you should write your workers such that they they follow signal responsibility principle, which means that they should be stateless. So, you know, don't build like mini state machines inside workers or maintain the state like workers should operate on a well defined input, do exactly one thing that is supposed to do, yeah, and produce a very output or an error, yeah, which means that now I can either build very complex logic. Similarly, if you have a bunch of things that should happen together, create a workflow, and use that workflow, the sub workflow, across different components. So like, if you change that workflow, everybody gets a benefit, right and right? People call that domain driven design, right? This one is perfect tool for doing domain right for a workflow that gets used by everybody else, and we have our back. So like, you know, people don't invariably or accidentally change your workflow without your permission, unless you give them. You can just give them execute permission, not the modification. Yeah? So that's other and then, more importantly, the for every worker, we put the default rate limits and all of those things. Yeah, rate limits, but other timeouts and all. But we recommend people that know, depending upon your business logic, you should adjust those values and always set something so that, like you know, you have the resiliency patterns like, you know, built into every task. Very
No, we do Yeah, yeah. We do provide. We do provide like. We provide like. For example, we ask that like, you know, you you should write your workers such that they they follow signal responsibility principle, which means that they should be stateless. So, you know, don't build like mini state machines inside workers or maintain the state like workers should operate on a well defined input, do exactly one thing that is supposed to do, yeah, and produce a very output or an error, yeah, which means that now I can either build very complex logic. Similarly, if you have a bunch of things that should happen together, create a workflow, and use that workflow, the sub workflow, across different components. So like, if you change that workflow, everybody gets a benefit, right and right? People call that domain driven design, right? This one is perfect tool for doing domain right for a workflow that gets used by everybody else, and we have our back. So like, you know, people don't invariably or accidentally change your workflow without your permission, unless you give them. You can just give them execute permission, not the modification. Yeah? So that's other and then, more importantly, the for every worker, we put the default rate limits and all of those things. Yeah, rate limits, but other timeouts and all. But we recommend people that know, depending upon your business logic, you should adjust those values and always set something so that, like you know, you have the resiliency patterns like, you know, built into every task. Very
No, we do Yeah, yeah. We do provide. We do provide like. We provide like. For example, we ask that like, you know, you you should write your workers such that they they follow signal responsibility principle, which means that they should be stateless. So, you know, don't build like mini state machines inside workers or maintain the state like workers should operate on a well defined input, do exactly one thing that is supposed to do, yeah, and produce a very output or an error, yeah, which means that now I can either build very complex logic. Similarly, if you have a bunch of things that should happen together, create a workflow, and use that workflow, the sub workflow, across different components. So like, if you change that workflow, everybody gets a benefit, right and right? People call that domain driven design, right? This one is perfect tool for doing domain right for a workflow that gets used by everybody else, and we have our back. So like, you know, people don't invariably or accidentally change your workflow without your permission, unless you give them. You can just give them execute permission, not the modification. Yeah? So that's other and then, more importantly, the for every worker, we put the default rate limits and all of those things. Yeah, rate limits, but other timeouts and all. But we recommend people that know, depending upon your business logic, you should adjust those values and always set something so that, like you know, you have the resiliency patterns like, you know, built into every task. Very
No, we do Yeah, yeah. We do provide. We do provide like. We provide like. For example, we ask that like, you know, you you should write your workers such that they they follow signal responsibility principle, which means that they should be stateless. So, you know, don't build like mini state machines inside workers or maintain the state like workers should operate on a well defined input, do exactly one thing that is supposed to do, yeah, and produce a very output or an error, yeah, which means that now I can either build very complex logic. Similarly, if you have a bunch of things that should happen together, create a workflow, and use that workflow, the sub workflow, across different components. So like, if you change that workflow, everybody gets a benefit, right and right? People call that domain driven design, right? This one is perfect tool for doing domain right for a workflow that gets used by everybody else, and we have our back. So like, you know, people don't invariably or accidentally change your workflow without your permission, unless you give them. You can just give them execute permission, not the modification. Yeah? So that's other and then, more importantly, the for every worker, we put the default rate limits and all of those things. Yeah, rate limits, but other timeouts and all. But we recommend people that know, depending upon your business logic, you should adjust those values and always set something so that, like you know, you have the resiliency patterns like, you know, built into every task. Very
S Speaker 117:35interesting, very I like the way you talked about, you know, workers and states and, you know, micro, micro worker nodes too, right? Which is, you can make it, make it reusable and replicable, right? So one, one question I had around that was, you know, as users are developing their application, right, there's a project manager who's in charge of, you know, how the whole use case will flow. And then there are developers building individual components of it, right? So there's a CICD pipeline through which they are pushing the code. Where does, where does a conductor or an orchesk Come in? Right? Is it an afterthought? Is it? I've built my components of code.
interesting, very I like the way you talked about, you know, workers and states and, you know, micro, micro worker nodes too, right? Which is, you can make it, make it reusable and replicable, right? So one, one question I had around that was, you know, as users are developing their application, right, there's a project manager who's in charge of, you know, how the whole use case will flow. And then there are developers building individual components of it, right? So there's a CICD pipeline through which they are pushing the code. Where does, where does a conductor or an orchesk Come in? Right? Is it an afterthought? Is it? I've built my components of code.
interesting, very I like the way you talked about, you know, workers and states and, you know, micro, micro worker nodes too, right? Which is, you can make it, make it reusable and replicable, right? So one, one question I had around that was, you know, as users are developing their application, right, there's a project manager who's in charge of, you know, how the whole use case will flow. And then there are developers building individual components of it, right? So there's a CICD pipeline through which they are pushing the code. Where does, where does a conductor or an orchesk Come in? Right? Is it an afterthought? Is it? I've built my components of code.
interesting, very I like the way you talked about, you know, workers and states and, you know, micro, micro worker nodes too, right? Which is, you can make it, make it reusable and replicable, right? So one, one question I had around that was, you know, as users are developing their application, right, there's a project manager who's in charge of, you know, how the whole use case will flow. And then there are developers building individual components of it, right? So there's a CICD pipeline through which they are pushing the code. Where does, where does a conductor or an orchesk Come in? Right? Is it an afterthought? Is it? I've built my components of code.
S Speaker 218:17What recommend is like? So what is that? As you build the workflows, we kind of give you the code right there and there right, which is basically a JSON so, you know, our recommendation is you should treat your workflows just like you treat your code, right? I see their business watching, and then you promote your workflows just like you promote your code from dev to staging or UAT to production. Oh, I see, I see, we also have a framework which allows you to kind of mock test your workflows and unit test your workflows. You also write unit test and regression test to make sure that, like you know, every path is covered, given the right inputs, it does the right output. Or if you give the wrong inputs, either it fails gracefully or does whatever it's supposed to do, yeah, but basically run the same set of rigor that you would do with the code to the workflow and promote them from one environment to another environment, and then production should have only, only the people with right level of access control should have access to production. Yeah? Is that you don't
What recommend is like? So what is that? As you build the workflows, we kind of give you the code right there and there right, which is basically a JSON so, you know, our recommendation is you should treat your workflows just like you treat your code, right? I see their business watching, and then you promote your workflows just like you promote your code from dev to staging or UAT to production. Oh, I see, I see, we also have a framework which allows you to kind of mock test your workflows and unit test your workflows. You also write unit test and regression test to make sure that, like you know, every path is covered, given the right inputs, it does the right output. Or if you give the wrong inputs, either it fails gracefully or does whatever it's supposed to do, yeah, but basically run the same set of rigor that you would do with the code to the workflow and promote them from one environment to another environment, and then production should have only, only the people with right level of access control should have access to production. Yeah? Is that you don't
What recommend is like? So what is that? As you build the workflows, we kind of give you the code right there and there right, which is basically a JSON so, you know, our recommendation is you should treat your workflows just like you treat your code, right? I see their business watching, and then you promote your workflows just like you promote your code from dev to staging or UAT to production. Oh, I see, I see, we also have a framework which allows you to kind of mock test your workflows and unit test your workflows. You also write unit test and regression test to make sure that, like you know, every path is covered, given the right inputs, it does the right output. Or if you give the wrong inputs, either it fails gracefully or does whatever it's supposed to do, yeah, but basically run the same set of rigor that you would do with the code to the workflow and promote them from one environment to another environment, and then production should have only, only the people with right level of access control should have access to production. Yeah? Is that you don't
What recommend is like? So what is that? As you build the workflows, we kind of give you the code right there and there right, which is basically a JSON so, you know, our recommendation is you should treat your workflows just like you treat your code, right? I see their business watching, and then you promote your workflows just like you promote your code from dev to staging or UAT to production. Oh, I see, I see, we also have a framework which allows you to kind of mock test your workflows and unit test your workflows. You also write unit test and regression test to make sure that, like you know, every path is covered, given the right inputs, it does the right output. Or if you give the wrong inputs, either it fails gracefully or does whatever it's supposed to do, yeah, but basically run the same set of rigor that you would do with the code to the workflow and promote them from one environment to another environment, and then production should have only, only the people with right level of access control should have access to production. Yeah? Is that you don't
S Speaker 119:16end up changing? Yeah, changing, of course, of course. So there is, like a CICD pipeline, just for workflows too, right? You have all the different very, very interesting and like, how do you interface with Are you pulling the worker code? Like, what I'm trying to say is, there are GitHub repos right there. They might be in multiple different languages. How are you connecting the two? Like, are you pulling the worker Microsoft? Okay,
end up changing? Yeah, changing, of course, of course. So there is, like a CICD pipeline, just for workflows too, right? You have all the different very, very interesting and like, how do you interface with Are you pulling the worker code? Like, what I'm trying to say is, there are GitHub repos right there. They might be in multiple different languages. How are you connecting the two? Like, are you pulling the worker Microsoft? Okay,
end up changing? Yeah, changing, of course, of course. So there is, like a CICD pipeline, just for workflows too, right? You have all the different very, very interesting and like, how do you interface with Are you pulling the worker code? Like, what I'm trying to say is, there are GitHub repos right there. They might be in multiple different languages. How are you connecting the two? Like, are you pulling the worker Microsoft? Okay,
end up changing? Yeah, changing, of course, of course. So there is, like a CICD pipeline, just for workflows too, right? You have all the different very, very interesting and like, how do you interface with Are you pulling the worker code? Like, what I'm trying to say is, there are GitHub repos right there. They might be in multiple different languages. How are you connecting the two? Like, are you pulling the worker Microsoft? Okay,
S Speaker 219:42I think, as you mentioned, right? Like, we have kept it very, like, very isolated in the sense that we follow the principle that your orchestration should not intermingle with your logic, right? Yeah, so your worker requirement is separate from conductor server. One thing that we have it in our roadmap is to actually support running the workers, because one feedback that we have gotten and request is like, yeah, you are running our workforce. Can you also run our work report? Yeah, that's what,
I think, as you mentioned, right? Like, we have kept it very, like, very isolated in the sense that we follow the principle that your orchestration should not intermingle with your logic, right? Yeah, so your worker requirement is separate from conductor server. One thing that we have it in our roadmap is to actually support running the workers, because one feedback that we have gotten and request is like, yeah, you are running our workforce. Can you also run our work report? Yeah, that's what,
I think, as you mentioned, right? Like, we have kept it very, like, very isolated in the sense that we follow the principle that your orchestration should not intermingle with your logic, right? Yeah, so your worker requirement is separate from conductor server. One thing that we have it in our roadmap is to actually support running the workers, because one feedback that we have gotten and request is like, yeah, you are running our workforce. Can you also run our work report? Yeah, that's what,
I think, as you mentioned, right? Like, we have kept it very, like, very isolated in the sense that we follow the principle that your orchestration should not intermingle with your logic, right? Yeah, so your worker requirement is separate from conductor server. One thing that we have it in our roadmap is to actually support running the workers, because one feedback that we have gotten and request is like, yeah, you are running our workforce. Can you also run our work report? Yeah, that's what,
S Speaker 120:08yeah, yeah. That's what I was asking. Because, you know, ultimately, everything should be in a single platform where I'm managing everything together. It might not be linked. You might still have that separation that Joe was talking about. But yeah, running it in the same platform is something that developers would want. Yeah, yes. So
yeah, yeah. That's what I was asking. Because, you know, ultimately, everything should be in a single platform where I'm managing everything together. It might not be linked. You might still have that separation that Joe was talking about. But yeah, running it in the same platform is something that developers would want. Yeah, yes. So
yeah, yeah. That's what I was asking. Because, you know, ultimately, everything should be in a single platform where I'm managing everything together. It might not be linked. You might still have that separation that Joe was talking about. But yeah, running it in the same platform is something that developers would want. Yeah, yes. So
yeah, yeah. That's what I was asking. Because, you know, ultimately, everything should be in a single platform where I'm managing everything together. It might not be linked. You might still have that separation that Joe was talking about. But yeah, running it in the same platform is something that developers would want. Yeah, yes. So
S Speaker 220:25that's something, you know, roadmap, and then we are actively, kind of looking into that at the moment. So that's something that we should be offering pretty soon. But, yeah, that's
that's something, you know, roadmap, and then we are actively, kind of looking into that at the moment. So that's something that we should be offering pretty soon. But, yeah, that's
that's something, you know, roadmap, and then we are actively, kind of looking into that at the moment. So that's something that we should be offering pretty soon. But, yeah, that's
that's something, you know, roadmap, and then we are actively, kind of looking into that at the moment. So that's something that we should be offering pretty soon. But, yeah, that's
S Speaker 320:33as part of our prom based stuff right now. They're just creating ones like, you know, even during the execution phase, right like defining your workflows, you know, writing piece of code that is also from base, that could define your workers, right? Existing stuff from other existing return reports, and I'm also hosting all of those things for you, right? But you know, if the user wants other things, that they have other choices as well. Got it, got it, got it. But one more thing that you mentioned about the CIC and the whole So, everything that you see here on the screen is backed by APIs, right? So, if someone wants to take this thing, and, you know, go through from internet to test or not, like, you know, there's API level support to take, workflow, definitions, full executions, do all of that stuff, right? Every single thing is step back by APIs,
as part of our prom based stuff right now. They're just creating ones like, you know, even during the execution phase, right like defining your workflows, you know, writing piece of code that is also from base, that could define your workers, right? Existing stuff from other existing return reports, and I'm also hosting all of those things for you, right? But you know, if the user wants other things, that they have other choices as well. Got it, got it, got it. But one more thing that you mentioned about the CIC and the whole So, everything that you see here on the screen is backed by APIs, right? So, if someone wants to take this thing, and, you know, go through from internet to test or not, like, you know, there's API level support to take, workflow, definitions, full executions, do all of that stuff, right? Every single thing is step back by APIs,
as part of our prom based stuff right now. They're just creating ones like, you know, even during the execution phase, right like defining your workflows, you know, writing piece of code that is also from base, that could define your workers, right? Existing stuff from other existing return reports, and I'm also hosting all of those things for you, right? But you know, if the user wants other things, that they have other choices as well. Got it, got it, got it. But one more thing that you mentioned about the CIC and the whole So, everything that you see here on the screen is backed by APIs, right? So, if someone wants to take this thing, and, you know, go through from internet to test or not, like, you know, there's API level support to take, workflow, definitions, full executions, do all of that stuff, right? Every single thing is step back by APIs,
as part of our prom based stuff right now. They're just creating ones like, you know, even during the execution phase, right like defining your workflows, you know, writing piece of code that is also from base, that could define your workers, right? Existing stuff from other existing return reports, and I'm also hosting all of those things for you, right? But you know, if the user wants other things, that they have other choices as well. Got it, got it, got it. But one more thing that you mentioned about the CIC and the whole So, everything that you see here on the screen is backed by APIs, right? So, if someone wants to take this thing, and, you know, go through from internet to test or not, like, you know, there's API level support to take, workflow, definitions, full executions, do all of that stuff, right? Every single thing is step back by APIs,
S Speaker 121:18backed by APIs. I call it. And maybe, you know, as I'm thinking this through, I know swiggy is one of the customers for you guys, right? So
backed by APIs. I call it. And maybe, you know, as I'm thinking this through, I know swiggy is one of the customers for you guys, right? So
backed by APIs. I call it. And maybe, you know, as I'm thinking this through, I know swiggy is one of the customers for you guys, right? So
backed by APIs. I call it. And maybe, you know, as I'm thinking this through, I know swiggy is one of the customers for you guys, right? So
S Speaker 321:27they're not, they were not a very early adopter of open source. Conductor, yeah, so they are open source. Their entire food delivery systems. Got it.
they're not, they were not a very early adopter of open source. Conductor, yeah, so they are open source. Their entire food delivery systems. Got it.
they're not, they were not a very early adopter of open source. Conductor, yeah, so they are open source. Their entire food delivery systems. Got it.
they're not, they were not a very early adopter of open source. Conductor, yeah, so they are open source. Their entire food delivery systems. Got it.
S Speaker 121:44Got it. Got it. Knows. The reason I was attributing that use case was there are multiple different ERP or financial systems that are involved, right? So let's say, in an order taking system, you have, you know, your order management system, you'll have your payment gateway, which is a different system. So who is, who is solving the integration pain points? So somebody's you're doing the workflow. Where is the whole integrations? Like, whether these systems, okay, I see,
Got it. Got it. Knows. The reason I was attributing that use case was there are multiple different ERP or financial systems that are involved, right? So let's say, in an order taking system, you have, you know, your order management system, you'll have your payment gateway, which is a different system. So who is, who is solving the integration pain points? So somebody's you're doing the workflow. Where is the whole integrations? Like, whether these systems, okay, I see,
Got it. Got it. Knows. The reason I was attributing that use case was there are multiple different ERP or financial systems that are involved, right? So let's say, in an order taking system, you have, you know, your order management system, you'll have your payment gateway, which is a different system. So who is, who is solving the integration pain points? So somebody's you're doing the workflow. Where is the whole integrations? Like, whether these systems, okay, I see,
Got it. Got it. Knows. The reason I was attributing that use case was there are multiple different ERP or financial systems that are involved, right? So let's say, in an order taking system, you have, you know, your order management system, you'll have your payment gateway, which is a different system. So who is, who is solving the integration pain points? So somebody's you're doing the workflow. Where is the whole integrations? Like, whether these systems, okay, I see,
S Speaker 222:11yeah. So I think one thing that we started investing into is integrations, because, you know, you know, systems do live in isolation,
yeah. So I think one thing that we started investing into is integrations, because, you know, you know, systems do live in isolation,
yeah. So I think one thing that we started investing into is integrations, because, you know, you know, systems do live in isolation,
yeah. So I think one thing that we started investing into is integrations, because, you know, you know, systems do live in isolation,
S Speaker 122:20exactly. That's part of it. Yeah, that's, that's what they are. Because even for workflows, you can't just define and hope they work right. You have to make sure, yeah,
exactly. That's part of it. Yeah, that's, that's what they are. Because even for workflows, you can't just define and hope they work right. You have to make sure, yeah,
exactly. That's part of it. Yeah, that's, that's what they are. Because even for workflows, you can't just define and hope they work right. You have to make sure, yeah,
exactly. That's part of it. Yeah, that's, that's what they are. Because even for workflows, you can't just define and hope they work right. You have to make sure, yeah,
S Speaker 222:28correct. That is correct. So what we started with was system level integrations. And very first thing that we got ourselves very popular with is, was being able to integrate with various eventing systems, open source conductor, already supported Kafka and Revit, MQ and SQS, and we integrate that to IBMs and queue and JCP and Azure also, right? But idea is that sometimes you have other systems that wants to communicate with conductor and vice versa. Yeah. So big support is bi directional. We can publish a message to any of the supported eventing system. We can also consume a message from there, and when the message comes, it can trigger a new workflow or move the workflow state forward both ways. Yeah. So now you can build very powerful event orchestration using conductor. So that's one thing that we implemented early on. And with AI, we kind of supported pretty much every major language model provider, both on a cloud provider like AWS and GCP, as well as their own services. So if you want to build a genetic experiences and aging workflows, you know, you don't really need to kind of go and drive those things. It just works out of the box, Yeah, same thing with data is, if you were to build, like, you know, a simple database lookups and queries, yeah, you can try those things. We are expanding this, right? So one idea where we are investing is the business level integration, like integrations with being able to send emails or, yeah, work with cloud providers for renovating and writing into file systems or office integrations or air table and things like that, allows you to kind of define more business level, yeah, so that, like, you know, you don't necessarily do it yourself, but yeah, that's, that's where
correct. That is correct. So what we started with was system level integrations. And very first thing that we got ourselves very popular with is, was being able to integrate with various eventing systems, open source conductor, already supported Kafka and Revit, MQ and SQS, and we integrate that to IBMs and queue and JCP and Azure also, right? But idea is that sometimes you have other systems that wants to communicate with conductor and vice versa. Yeah. So big support is bi directional. We can publish a message to any of the supported eventing system. We can also consume a message from there, and when the message comes, it can trigger a new workflow or move the workflow state forward both ways. Yeah. So now you can build very powerful event orchestration using conductor. So that's one thing that we implemented early on. And with AI, we kind of supported pretty much every major language model provider, both on a cloud provider like AWS and GCP, as well as their own services. So if you want to build a genetic experiences and aging workflows, you know, you don't really need to kind of go and drive those things. It just works out of the box, Yeah, same thing with data is, if you were to build, like, you know, a simple database lookups and queries, yeah, you can try those things. We are expanding this, right? So one idea where we are investing is the business level integration, like integrations with being able to send emails or, yeah, work with cloud providers for renovating and writing into file systems or office integrations or air table and things like that, allows you to kind of define more business level, yeah, so that, like, you know, you don't necessarily do it yourself, but yeah, that's, that's where
correct. That is correct. So what we started with was system level integrations. And very first thing that we got ourselves very popular with is, was being able to integrate with various eventing systems, open source conductor, already supported Kafka and Revit, MQ and SQS, and we integrate that to IBMs and queue and JCP and Azure also, right? But idea is that sometimes you have other systems that wants to communicate with conductor and vice versa. Yeah. So big support is bi directional. We can publish a message to any of the supported eventing system. We can also consume a message from there, and when the message comes, it can trigger a new workflow or move the workflow state forward both ways. Yeah. So now you can build very powerful event orchestration using conductor. So that's one thing that we implemented early on. And with AI, we kind of supported pretty much every major language model provider, both on a cloud provider like AWS and GCP, as well as their own services. So if you want to build a genetic experiences and aging workflows, you know, you don't really need to kind of go and drive those things. It just works out of the box, Yeah, same thing with data is, if you were to build, like, you know, a simple database lookups and queries, yeah, you can try those things. We are expanding this, right? So one idea where we are investing is the business level integration, like integrations with being able to send emails or, yeah, work with cloud providers for renovating and writing into file systems or office integrations or air table and things like that, allows you to kind of define more business level, yeah, so that, like, you know, you don't necessarily do it yourself, but yeah, that's, that's where
correct. That is correct. So what we started with was system level integrations. And very first thing that we got ourselves very popular with is, was being able to integrate with various eventing systems, open source conductor, already supported Kafka and Revit, MQ and SQS, and we integrate that to IBMs and queue and JCP and Azure also, right? But idea is that sometimes you have other systems that wants to communicate with conductor and vice versa. Yeah. So big support is bi directional. We can publish a message to any of the supported eventing system. We can also consume a message from there, and when the message comes, it can trigger a new workflow or move the workflow state forward both ways. Yeah. So now you can build very powerful event orchestration using conductor. So that's one thing that we implemented early on. And with AI, we kind of supported pretty much every major language model provider, both on a cloud provider like AWS and GCP, as well as their own services. So if you want to build a genetic experiences and aging workflows, you know, you don't really need to kind of go and drive those things. It just works out of the box, Yeah, same thing with data is, if you were to build, like, you know, a simple database lookups and queries, yeah, you can try those things. We are expanding this, right? So one idea where we are investing is the business level integration, like integrations with being able to send emails or, yeah, work with cloud providers for renovating and writing into file systems or office integrations or air table and things like that, allows you to kind of define more business level, yeah, so that, like, you know, you don't necessarily do it yourself, but yeah, that's, that's where
24:13also, also the template stuff as well, right?
also, also the template stuff as well, right?
also, also the template stuff as well, right?
also, also the template stuff as well, right?
24:18Yes. And the other piece is in terms of
Yes. And the other piece is in terms of
Yes. And the other piece is in terms of
Yes. And the other piece is in terms of
S Speaker 124:22so while you pull the templates out on the agentic workflows, right, being able to build agentic workflows, not using agents to build workflows, but can, is there a use case that you can showcase that, hey, this is how an agent can be built on your platform?
so while you pull the templates out on the agentic workflows, right, being able to build agentic workflows, not using agents to build workflows, but can, is there a use case that you can showcase that, hey, this is how an agent can be built on your platform?
so while you pull the templates out on the agentic workflows, right, being able to build agentic workflows, not using agents to build workflows, but can, is there a use case that you can showcase that, hey, this is how an agent can be built on your platform?
so while you pull the templates out on the agentic workflows, right, being able to build agentic workflows, not using agents to build workflows, but can, is there a use case that you can showcase that, hey, this is how an agent can be built on your platform?
24:44Yes, absolutely.
S Speaker 124:47But go ahead with the templates. We can, we can take it step by step, yeah, yeah.
But go ahead with the templates. We can, we can take it step by step, yeah, yeah.
But go ahead with the templates. We can, we can take it step by step, yeah, yeah.
But go ahead with the templates. We can, we can take it step by step, yeah, yeah.
S Speaker 224:52So yeah, one of the things that we have started building out is the templates, which are basically predefined templates, which contains a workflow and the code together, right? So, like, for example, maybe we can start with the objective research, right? So, like, if I went, like, you know, we have seen deep research agent, yeah, what they can do with conductor, your agents can really run much longer, right? If you want to keep running it for like, two days and keep doing research over and over, you can keep it running for two days.
So yeah, one of the things that we have started building out is the templates, which are basically predefined templates, which contains a workflow and the code together, right? So, like, for example, maybe we can start with the objective research, right? So, like, if I went, like, you know, we have seen deep research agent, yeah, what they can do with conductor, your agents can really run much longer, right? If you want to keep running it for like, two days and keep doing research over and over, you can keep it running for two days.
So yeah, one of the things that we have started building out is the templates, which are basically predefined templates, which contains a workflow and the code together, right? So, like, for example, maybe we can start with the objective research, right? So, like, if I went, like, you know, we have seen deep research agent, yeah, what they can do with conductor, your agents can really run much longer, right? If you want to keep running it for like, two days and keep doing research over and over, you can keep it running for two days.
So yeah, one of the things that we have started building out is the templates, which are basically predefined templates, which contains a workflow and the code together, right? So, like, for example, maybe we can start with the objective research, right? So, like, if I went, like, you know, we have seen deep research agent, yeah, what they can do with conductor, your agents can really run much longer, right? If you want to keep running it for like, two days and keep doing research over and over, you can keep it running for two days.
25:21I see, I see, that,
S Speaker 225:24like, you know, this templates, like, lets you visualize what it is there. You can import them at the API keys, and there you go, right and, like, if I look at it here, essentially, you know, you have llms running in parallel, running it over and over again and again, you know, fine tuning their prompts and responses. But that's, that's one of the simplest use case of energetic research that there is out there. Similarly, you know, you would have use cases where you can put humans in the room, right? So, yeah, traditional what when you use llms to do function calling elements are calling the functions themselves, which means if they make a there's no way to stop there, right? Versus our approach is that when you put an LLM, so like, I could, like, let's say chat, complete task, right? You specify which model provider, which model, what are the instructions? Yeah, and instead of directly calling the tools, its output itself is the set of tools that you want to call now. So inject a human task. So, like we also support human task. So you can also assign a human to review before it, you know, continues the execution further. So you can combine an LLM with a human, and, you know, put things like humans in the loop that allow the garbage. Or instead of a human, you can put a simple JavaScript to check for certain things. Or you can use an LLM as a judge and check if, you know, if some tool execution is going to be something that is good to, let's say, delete data.
like, you know, this templates, like, lets you visualize what it is there. You can import them at the API keys, and there you go, right and, like, if I look at it here, essentially, you know, you have llms running in parallel, running it over and over again and again, you know, fine tuning their prompts and responses. But that's, that's one of the simplest use case of energetic research that there is out there. Similarly, you know, you would have use cases where you can put humans in the room, right? So, yeah, traditional what when you use llms to do function calling elements are calling the functions themselves, which means if they make a there's no way to stop there, right? Versus our approach is that when you put an LLM, so like, I could, like, let's say chat, complete task, right? You specify which model provider, which model, what are the instructions? Yeah, and instead of directly calling the tools, its output itself is the set of tools that you want to call now. So inject a human task. So, like we also support human task. So you can also assign a human to review before it, you know, continues the execution further. So you can combine an LLM with a human, and, you know, put things like humans in the loop that allow the garbage. Or instead of a human, you can put a simple JavaScript to check for certain things. Or you can use an LLM as a judge and check if, you know, if some tool execution is going to be something that is good to, let's say, delete data.
like, you know, this templates, like, lets you visualize what it is there. You can import them at the API keys, and there you go, right and, like, if I look at it here, essentially, you know, you have llms running in parallel, running it over and over again and again, you know, fine tuning their prompts and responses. But that's, that's one of the simplest use case of energetic research that there is out there. Similarly, you know, you would have use cases where you can put humans in the room, right? So, yeah, traditional what when you use llms to do function calling elements are calling the functions themselves, which means if they make a there's no way to stop there, right? Versus our approach is that when you put an LLM, so like, I could, like, let's say chat, complete task, right? You specify which model provider, which model, what are the instructions? Yeah, and instead of directly calling the tools, its output itself is the set of tools that you want to call now. So inject a human task. So, like we also support human task. So you can also assign a human to review before it, you know, continues the execution further. So you can combine an LLM with a human, and, you know, put things like humans in the loop that allow the garbage. Or instead of a human, you can put a simple JavaScript to check for certain things. Or you can use an LLM as a judge and check if, you know, if some tool execution is going to be something that is good to, let's say, delete data.
like, you know, this templates, like, lets you visualize what it is there. You can import them at the API keys, and there you go, right and, like, if I look at it here, essentially, you know, you have llms running in parallel, running it over and over again and again, you know, fine tuning their prompts and responses. But that's, that's one of the simplest use case of energetic research that there is out there. Similarly, you know, you would have use cases where you can put humans in the room, right? So, yeah, traditional what when you use llms to do function calling elements are calling the functions themselves, which means if they make a there's no way to stop there, right? Versus our approach is that when you put an LLM, so like, I could, like, let's say chat, complete task, right? You specify which model provider, which model, what are the instructions? Yeah, and instead of directly calling the tools, its output itself is the set of tools that you want to call now. So inject a human task. So, like we also support human task. So you can also assign a human to review before it, you know, continues the execution further. So you can combine an LLM with a human, and, you know, put things like humans in the loop that allow the garbage. Or instead of a human, you can put a simple JavaScript to check for certain things. Or you can use an LLM as a judge and check if, you know, if some tool execution is going to be something that is good to, let's say, delete data.
S Speaker 127:01Yeah, yeah. So you can build, like, AI workflows too, right? With the with human in the loop and multiple different components of AI, could, could could work. There are you any of the customers using it for AI projects today?
Yeah, yeah. So you can build, like, AI workflows too, right? With the with human in the loop and multiple different components of AI, could, could could work. There are you any of the customers using it for AI projects today?
Yeah, yeah. So you can build, like, AI workflows too, right? With the with human in the loop and multiple different components of AI, could, could could work. There are you any of the customers using it for AI projects today?
Yeah, yeah. So you can build, like, AI workflows too, right? With the with human in the loop and multiple different components of AI, could, could could work. There are you any of the customers using it for AI projects today?
S Speaker 227:20I believe we have at least one customer who is using it for things around order management. I believe we published a use case around that one, obviously, okay,
I believe we have at least one customer who is using it for things around order management. I believe we published a use case around that one, obviously, okay,
I believe we have at least one customer who is using it for things around order management. I believe we published a use case around that one, obviously, okay,
I believe we have at least one customer who is using it for things around order management. I believe we published a use case around that one, obviously, okay,
S Speaker 327:30yeah, yeah. There are several that last year they were using for POCs, yeah, some of them have moved to production use cases. And this one customer they landed on, on started with an AI use case. There's a there's a case study that we also did with them.
yeah, yeah. There are several that last year they were using for POCs, yeah, some of them have moved to production use cases. And this one customer they landed on, on started with an AI use case. There's a there's a case study that we also did with them.
yeah, yeah. There are several that last year they were using for POCs, yeah, some of them have moved to production use cases. And this one customer they landed on, on started with an AI use case. There's a there's a case study that we also did with them.
yeah, yeah. There are several that last year they were using for POCs, yeah, some of them have moved to production use cases. And this one customer they landed on, on started with an AI use case. There's a there's a case study that we also did with them.
S Speaker 127:46No, we should, we should definitely unpack that a little more, because I think the question becomes like, how can we leverage this AI wave? Because the workflows will still exist, right? AI is only going to capture maybe a couple of, you know, checkpoints in this workflow, not the entire workflow. So how are you guys going to benefit from that trend? Is something that we should unpack. The second question I have is, What about workflows that involves hardware? The reason I ask this is because Qualcomm will have a lot of these smart cameras. There will be workflows triggered based on certain events that happens on a video feed, right? Or there's a sensor, and let's say a predictive maintenance process has to be, you know, kick started. So like samsara is a perfect example, right? Samsara is getting these data from vehicles telematics, and is running the whole Operations Dashboard on top of it. So do you have any of these edge devices workflows today, and any capabilities to build that in the platform?
No, we should, we should definitely unpack that a little more, because I think the question becomes like, how can we leverage this AI wave? Because the workflows will still exist, right? AI is only going to capture maybe a couple of, you know, checkpoints in this workflow, not the entire workflow. So how are you guys going to benefit from that trend? Is something that we should unpack. The second question I have is, What about workflows that involves hardware? The reason I ask this is because Qualcomm will have a lot of these smart cameras. There will be workflows triggered based on certain events that happens on a video feed, right? Or there's a sensor, and let's say a predictive maintenance process has to be, you know, kick started. So like samsara is a perfect example, right? Samsara is getting these data from vehicles telematics, and is running the whole Operations Dashboard on top of it. So do you have any of these edge devices workflows today, and any capabilities to build that in the platform?
No, we should, we should definitely unpack that a little more, because I think the question becomes like, how can we leverage this AI wave? Because the workflows will still exist, right? AI is only going to capture maybe a couple of, you know, checkpoints in this workflow, not the entire workflow. So how are you guys going to benefit from that trend? Is something that we should unpack. The second question I have is, What about workflows that involves hardware? The reason I ask this is because Qualcomm will have a lot of these smart cameras. There will be workflows triggered based on certain events that happens on a video feed, right? Or there's a sensor, and let's say a predictive maintenance process has to be, you know, kick started. So like samsara is a perfect example, right? Samsara is getting these data from vehicles telematics, and is running the whole Operations Dashboard on top of it. So do you have any of these edge devices workflows today, and any capabilities to build that in the platform?
No, we should, we should definitely unpack that a little more, because I think the question becomes like, how can we leverage this AI wave? Because the workflows will still exist, right? AI is only going to capture maybe a couple of, you know, checkpoints in this workflow, not the entire workflow. So how are you guys going to benefit from that trend? Is something that we should unpack. The second question I have is, What about workflows that involves hardware? The reason I ask this is because Qualcomm will have a lot of these smart cameras. There will be workflows triggered based on certain events that happens on a video feed, right? Or there's a sensor, and let's say a predictive maintenance process has to be, you know, kick started. So like samsara is a perfect example, right? Samsara is getting these data from vehicles telematics, and is running the whole Operations Dashboard on top of it. So do you have any of these edge devices workflows today, and any capabilities to build that in the platform?
S Speaker 228:51Here is how I would put it right, like if your cameras publishes events to underlying messaging system, right depending upon what you're using, let's say could be Kafka, yeah, repeated queue, or anything that, yeah, that can then trigger workflow, and that workflow can do, got it,
Here is how I would put it right, like if your cameras publishes events to underlying messaging system, right depending upon what you're using, let's say could be Kafka, yeah, repeated queue, or anything that, yeah, that can then trigger workflow, and that workflow can do, got it,
Here is how I would put it right, like if your cameras publishes events to underlying messaging system, right depending upon what you're using, let's say could be Kafka, yeah, repeated queue, or anything that, yeah, that can then trigger workflow, and that workflow can do, got it,
Here is how I would put it right, like if your cameras publishes events to underlying messaging system, right depending upon what you're using, let's say could be Kafka, yeah, repeated queue, or anything that, yeah, that can then trigger workflow, and that workflow can do, got it,
S Speaker 129:08got it so it is independent. It is independent of the where it's coming from, as long as you have some cloud native message queue, right, yeah,
got it so it is independent. It is independent of the where it's coming from, as long as you have some cloud native message queue, right, yeah,
got it so it is independent. It is independent of the where it's coming from, as long as you have some cloud native message queue, right, yeah,
got it so it is independent. It is independent of the where it's coming from, as long as you have some cloud native message queue, right, yeah,
S Speaker 229:18one edge or one very interesting use case we have seen with open source conductor was with the GE Healthcare. Yeah. What GE does is that, like, they take the open source conductor and they deploy on extra machines, okay, yeah. So, you know, when you take x ray, the whole portfolio runs inside the machine itself, yeah. So that's the interesting use case that we have seen.
one edge or one very interesting use case we have seen with open source conductor was with the GE Healthcare. Yeah. What GE does is that, like, they take the open source conductor and they deploy on extra machines, okay, yeah. So, you know, when you take x ray, the whole portfolio runs inside the machine itself, yeah. So that's the interesting use case that we have seen.
one edge or one very interesting use case we have seen with open source conductor was with the GE Healthcare. Yeah. What GE does is that, like, they take the open source conductor and they deploy on extra machines, okay, yeah. So, you know, when you take x ray, the whole portfolio runs inside the machine itself, yeah. So that's the interesting use case that we have seen.
one edge or one very interesting use case we have seen with open source conductor was with the GE Healthcare. Yeah. What GE does is that, like, they take the open source conductor and they deploy on extra machines, okay, yeah. So, you know, when you take x ray, the whole portfolio runs inside the machine itself, yeah. So that's the interesting use case that we have seen.
S Speaker 129:38No, I mean, what about like automotive? Right? Like Automotive is becoming there's so many compute elements in automotive today, right? There will be payments being made, orders being processed. Do you have any automotive customer today?
No, I mean, what about like automotive? Right? Like Automotive is becoming there's so many compute elements in automotive today, right? There will be payments being made, orders being processed. Do you have any automotive customer today?
No, I mean, what about like automotive? Right? Like Automotive is becoming there's so many compute elements in automotive today, right? There will be payments being made, orders being processed. Do you have any automotive customer today?
No, I mean, what about like automotive? Right? Like Automotive is becoming there's so many compute elements in automotive today, right? There will be payments being made, orders being processed. Do you have any automotive customer today?
S Speaker 329:55Tesla is a big open source. Yes, yes, they do that for, you know, when you go to the website and put an order for a car online, yeah, no, billing, yes, car flow is actually built on this, right? So manufacturing, taking a billing track, you know, tracking the process, delivering the vehicle home, sending notifications, the full system is built
Tesla is a big open source. Yes, yes, they do that for, you know, when you go to the website and put an order for a car online, yeah, no, billing, yes, car flow is actually built on this, right? So manufacturing, taking a billing track, you know, tracking the process, delivering the vehicle home, sending notifications, the full system is built
Tesla is a big open source. Yes, yes, they do that for, you know, when you go to the website and put an order for a car online, yeah, no, billing, yes, car flow is actually built on this, right? So manufacturing, taking a billing track, you know, tracking the process, delivering the vehicle home, sending notifications, the full system is built
Tesla is a big open source. Yes, yes, they do that for, you know, when you go to the website and put an order for a car online, yeah, no, billing, yes, car flow is actually built on this, right? So manufacturing, taking a billing track, you know, tracking the process, delivering the vehicle home, sending notifications, the full system is built
S Speaker 130:18on this, right? Got it? Got it, but you guys don't have, yeah,
on this, right? Got it? Got it, but you guys don't have, yeah,
on this, right? Got it? Got it, but you guys don't have, yeah,
on this, right? Got it? Got it, but you guys don't have, yeah,
S Speaker 330:23because I think a few other open source use cases, like there are companies that aim the networks, they know, firmware updates, provisioning of the actual physical devices using conductor. Another example, I think Vin, you work with these guys, like the Nokia IoT use cases. So there's several in that. And we have one customer, for example, celebrate, they actually do. What they do is a video analysis security devices, right? Like so they package conductor on the device, yeah, and they use it for the specific use case, right? So got it sitting on the device, capturing video. Then it does all of the stuff on the device itself, and then takes this and then pushes it off to
because I think a few other open source use cases, like there are companies that aim the networks, they know, firmware updates, provisioning of the actual physical devices using conductor. Another example, I think Vin, you work with these guys, like the Nokia IoT use cases. So there's several in that. And we have one customer, for example, celebrate, they actually do. What they do is a video analysis security devices, right? Like so they package conductor on the device, yeah, and they use it for the specific use case, right? So got it sitting on the device, capturing video. Then it does all of the stuff on the device itself, and then takes this and then pushes it off to
because I think a few other open source use cases, like there are companies that aim the networks, they know, firmware updates, provisioning of the actual physical devices using conductor. Another example, I think Vin, you work with these guys, like the Nokia IoT use cases. So there's several in that. And we have one customer, for example, celebrate, they actually do. What they do is a video analysis security devices, right? Like so they package conductor on the device, yeah, and they use it for the specific use case, right? So got it sitting on the device, capturing video. Then it does all of the stuff on the device itself, and then takes this and then pushes it off to
because I think a few other open source use cases, like there are companies that aim the networks, they know, firmware updates, provisioning of the actual physical devices using conductor. Another example, I think Vin, you work with these guys, like the Nokia IoT use cases. So there's several in that. And we have one customer, for example, celebrate, they actually do. What they do is a video analysis security devices, right? Like so they package conductor on the device, yeah, and they use it for the specific use case, right? So got it sitting on the device, capturing video. Then it does all of the stuff on the device itself, and then takes this and then pushes it off to
S Speaker 231:10analysis and stuff on the automatic one use case I've seen on the open source side was some companies in China, they have been using conductor to do the configuration of the
analysis and stuff on the automatic one use case I've seen on the open source side was some companies in China, they have been using conductor to do the configuration of the
analysis and stuff on the automatic one use case I've seen on the open source side was some companies in China, they have been using conductor to do the configuration of the
analysis and stuff on the automatic one use case I've seen on the open source side was some companies in China, they have been using conductor to do the configuration of the
S Speaker 131:28Yeah, the OTA, OTA updates right over the air, over the air updates, yeah. Actually, that's a big use case for us. So the reason I was asking is, if there's a customer that's actually using orchest to do that, it definitely strengthens. We can go back to our customers and figure out where it kind of intercepts, but that's that's good to know. At least the open source is facing some adoption, yeah,
Yeah, the OTA, OTA updates right over the air, over the air updates, yeah. Actually, that's a big use case for us. So the reason I was asking is, if there's a customer that's actually using orchest to do that, it definitely strengthens. We can go back to our customers and figure out where it kind of intercepts, but that's that's good to know. At least the open source is facing some adoption, yeah,
Yeah, the OTA, OTA updates right over the air, over the air updates, yeah. Actually, that's a big use case for us. So the reason I was asking is, if there's a customer that's actually using orchest to do that, it definitely strengthens. We can go back to our customers and figure out where it kind of intercepts, but that's that's good to know. At least the open source is facing some adoption, yeah,
Yeah, the OTA, OTA updates right over the air, over the air updates, yeah. Actually, that's a big use case for us. So the reason I was asking is, if there's a customer that's actually using orchest to do that, it definitely strengthens. We can go back to our customers and figure out where it kind of intercepts, but that's that's good to know. At least the open source is facing some adoption, yeah,
31:54but I believe Broadcom right. I think they have better
but I believe Broadcom right. I think they have better
but I believe Broadcom right. I think they have better
but I believe Broadcom right. I think they have better
S Speaker 332:00anymore device provisioning. Also, VMware is a big they, I think they have written like two blocks for publicly available blogs about this as well, like
anymore device provisioning. Also, VMware is a big they, I think they have written like two blocks for publicly available blogs about this as well, like
anymore device provisioning. Also, VMware is a big they, I think they have written like two blocks for publicly available blogs about this as well, like
anymore device provisioning. Also, VMware is a big they, I think they have written like two blocks for publicly available blogs about this as well, like
S Speaker 132:06amp.com right. Okay, yeah. So there you go, right? So device provisioning will be a great use case. Yeah, that's great to know.
amp.com right. Okay, yeah. So there you go, right? So device provisioning will be a great use case. Yeah, that's great to know.
amp.com right. Okay, yeah. So there you go, right? So device provisioning will be a great use case. Yeah, that's great to know.
amp.com right. Okay, yeah. So there you go, right? So device provisioning will be a great use case. Yeah, that's great to know.
S Speaker 232:14Yeah. I think the other use case that is currently promised is Walmart, so they are starting to use connector for in store checkouts. Yeah, currently the team is building it out like Walmart team, and once they go live like, every checkout in WalMart Stores is going to be driven, got
Yeah. I think the other use case that is currently promised is Walmart, so they are starting to use connector for in store checkouts. Yeah, currently the team is building it out like Walmart team, and once they go live like, every checkout in WalMart Stores is going to be driven, got
Yeah. I think the other use case that is currently promised is Walmart, so they are starting to use connector for in store checkouts. Yeah, currently the team is building it out like Walmart team, and once they go live like, every checkout in WalMart Stores is going to be driven, got
Yeah. I think the other use case that is currently promised is Walmart, so they are starting to use connector for in store checkouts. Yeah, currently the team is building it out like Walmart team, and once they go live like, every checkout in WalMart Stores is going to be driven, got
S Speaker 132:31it? Got it? No, that's super exciting. And so maybe shifting gears on the roadmap side, how, what are the couple of things that you guys are building? But by, before we go, there anything on the existing product? I think we're mostly clearing, if you can unpack one of the agentic use cases, and specifically I saw on the deck, you had more stuff around prompt management, things like that, and any specific features that you've built for agentic workflow, orchestration, yes.
it? Got it? No, that's super exciting. And so maybe shifting gears on the roadmap side, how, what are the couple of things that you guys are building? But by, before we go, there anything on the existing product? I think we're mostly clearing, if you can unpack one of the agentic use cases, and specifically I saw on the deck, you had more stuff around prompt management, things like that, and any specific features that you've built for agentic workflow, orchestration, yes.
it? Got it? No, that's super exciting. And so maybe shifting gears on the roadmap side, how, what are the couple of things that you guys are building? But by, before we go, there anything on the existing product? I think we're mostly clearing, if you can unpack one of the agentic use cases, and specifically I saw on the deck, you had more stuff around prompt management, things like that, and any specific features that you've built for agentic workflow, orchestration, yes.
it? Got it? No, that's super exciting. And so maybe shifting gears on the roadmap side, how, what are the couple of things that you guys are building? But by, before we go, there anything on the existing product? I think we're mostly clearing, if you can unpack one of the agentic use cases, and specifically I saw on the deck, you had more stuff around prompt management, things like that, and any specific features that you've built for agentic workflow, orchestration, yes.
S Speaker 232:58So I'll start with the prompt management. So, like, one thing that we have done is, like, you know, we have built out the entire prompt studio, which allows you to, kind of, you know, define a prompt, inject, like, for example, variables inside it. Yeah, you can associate the prompts with a model. And the whole idea is that, you know, typically what happens is, you know when you write prompts, you know, not every prompts are exactly same with every model. So you know, you want to kind of safeguard in some way, so that, like, you don't accidentally end up using a prompt that was not tested with a particular model, and, like, in order to production issues. And then you can test it out through prompts to your and once you are good to go, you know you can save it in your workflows. You don't really use the raw prompt text, but rather you put the template name, which means that, like, you know, if, let's say, a security researcher finds a vulnerability inside a prompt, like a prompt injection, you can just go update your template, and all the workflows will automatically get the update, as opposed to now changing the code, redeploying application. So that kind of simplifies the whole aspect of, you know, prompt management. Same thing with integrations. All the integrations are maintained in one place, which means that, like, you know, you don't have to, necessarily, you know, Embed API keys in applications. And you know, if one of them is compromised, and you're trying to figure out, everything is managed in one place, and manage to that way. And you can also provision which models are available, so you know if a new model is available. But, you know, let's say your security team or your compliance team has not yet certified them. People can't use it. So you can also put safeguards around those things. And as I mentioned, right like, one thing that we have done is that we have created, like, you know, pre built task. So you know, if you want to do things like chat completion or generate embeddings for vector databases or search for embeddings, those are pre built task, right. And when you and if you want to work with your own fine tune models, we also support olama. So you know, if you have hosted a model, you know, you can directly call the model. So which allows which allows you to, kind of, then use your own private models as well. So that's one area that we have spent which means that, like, you know, if you have a workflow and you want to put AI inside your workflow, you know, AI workflows becomes a very, very trigger to do with conductor overall. So that's one part of it that we have done. And of course, you can also associate a schema with the chat complete. So like, you know, if it generates the wrong output that does not come from to your schema, it just stays right there, or it doesn't retry, and, you know, tries one more time and things like that. So like, it makes it a little bit easier in terms of, you know, how do you build applications that uses elements?
So I'll start with the prompt management. So, like, one thing that we have done is, like, you know, we have built out the entire prompt studio, which allows you to, kind of, you know, define a prompt, inject, like, for example, variables inside it. Yeah, you can associate the prompts with a model. And the whole idea is that, you know, typically what happens is, you know when you write prompts, you know, not every prompts are exactly same with every model. So you know, you want to kind of safeguard in some way, so that, like, you don't accidentally end up using a prompt that was not tested with a particular model, and, like, in order to production issues. And then you can test it out through prompts to your and once you are good to go, you know you can save it in your workflows. You don't really use the raw prompt text, but rather you put the template name, which means that, like, you know, if, let's say, a security researcher finds a vulnerability inside a prompt, like a prompt injection, you can just go update your template, and all the workflows will automatically get the update, as opposed to now changing the code, redeploying application. So that kind of simplifies the whole aspect of, you know, prompt management. Same thing with integrations. All the integrations are maintained in one place, which means that, like, you know, you don't have to, necessarily, you know, Embed API keys in applications. And you know, if one of them is compromised, and you're trying to figure out, everything is managed in one place, and manage to that way. And you can also provision which models are available, so you know if a new model is available. But, you know, let's say your security team or your compliance team has not yet certified them. People can't use it. So you can also put safeguards around those things. And as I mentioned, right like, one thing that we have done is that we have created, like, you know, pre built task. So you know, if you want to do things like chat completion or generate embeddings for vector databases or search for embeddings, those are pre built task, right. And when you and if you want to work with your own fine tune models, we also support olama. So you know, if you have hosted a model, you know, you can directly call the model. So which allows which allows you to, kind of, then use your own private models as well. So that's one area that we have spent which means that, like, you know, if you have a workflow and you want to put AI inside your workflow, you know, AI workflows becomes a very, very trigger to do with conductor overall. So that's one part of it that we have done. And of course, you can also associate a schema with the chat complete. So like, you know, if it generates the wrong output that does not come from to your schema, it just stays right there, or it doesn't retry, and, you know, tries one more time and things like that. So like, it makes it a little bit easier in terms of, you know, how do you build applications that uses elements?
So I'll start with the prompt management. So, like, one thing that we have done is, like, you know, we have built out the entire prompt studio, which allows you to, kind of, you know, define a prompt, inject, like, for example, variables inside it. Yeah, you can associate the prompts with a model. And the whole idea is that, you know, typically what happens is, you know when you write prompts, you know, not every prompts are exactly same with every model. So you know, you want to kind of safeguard in some way, so that, like, you don't accidentally end up using a prompt that was not tested with a particular model, and, like, in order to production issues. And then you can test it out through prompts to your and once you are good to go, you know you can save it in your workflows. You don't really use the raw prompt text, but rather you put the template name, which means that, like, you know, if, let's say, a security researcher finds a vulnerability inside a prompt, like a prompt injection, you can just go update your template, and all the workflows will automatically get the update, as opposed to now changing the code, redeploying application. So that kind of simplifies the whole aspect of, you know, prompt management. Same thing with integrations. All the integrations are maintained in one place, which means that, like, you know, you don't have to, necessarily, you know, Embed API keys in applications. And you know, if one of them is compromised, and you're trying to figure out, everything is managed in one place, and manage to that way. And you can also provision which models are available, so you know if a new model is available. But, you know, let's say your security team or your compliance team has not yet certified them. People can't use it. So you can also put safeguards around those things. And as I mentioned, right like, one thing that we have done is that we have created, like, you know, pre built task. So you know, if you want to do things like chat completion or generate embeddings for vector databases or search for embeddings, those are pre built task, right. And when you and if you want to work with your own fine tune models, we also support olama. So you know, if you have hosted a model, you know, you can directly call the model. So which allows which allows you to, kind of, then use your own private models as well. So that's one area that we have spent which means that, like, you know, if you have a workflow and you want to put AI inside your workflow, you know, AI workflows becomes a very, very trigger to do with conductor overall. So that's one part of it that we have done. And of course, you can also associate a schema with the chat complete. So like, you know, if it generates the wrong output that does not come from to your schema, it just stays right there, or it doesn't retry, and, you know, tries one more time and things like that. So like, it makes it a little bit easier in terms of, you know, how do you build applications that uses elements?
So I'll start with the prompt management. So, like, one thing that we have done is, like, you know, we have built out the entire prompt studio, which allows you to, kind of, you know, define a prompt, inject, like, for example, variables inside it. Yeah, you can associate the prompts with a model. And the whole idea is that, you know, typically what happens is, you know when you write prompts, you know, not every prompts are exactly same with every model. So you know, you want to kind of safeguard in some way, so that, like, you don't accidentally end up using a prompt that was not tested with a particular model, and, like, in order to production issues. And then you can test it out through prompts to your and once you are good to go, you know you can save it in your workflows. You don't really use the raw prompt text, but rather you put the template name, which means that, like, you know, if, let's say, a security researcher finds a vulnerability inside a prompt, like a prompt injection, you can just go update your template, and all the workflows will automatically get the update, as opposed to now changing the code, redeploying application. So that kind of simplifies the whole aspect of, you know, prompt management. Same thing with integrations. All the integrations are maintained in one place, which means that, like, you know, you don't have to, necessarily, you know, Embed API keys in applications. And you know, if one of them is compromised, and you're trying to figure out, everything is managed in one place, and manage to that way. And you can also provision which models are available, so you know if a new model is available. But, you know, let's say your security team or your compliance team has not yet certified them. People can't use it. So you can also put safeguards around those things. And as I mentioned, right like, one thing that we have done is that we have created, like, you know, pre built task. So you know, if you want to do things like chat completion or generate embeddings for vector databases or search for embeddings, those are pre built task, right. And when you and if you want to work with your own fine tune models, we also support olama. So you know, if you have hosted a model, you know, you can directly call the model. So which allows which allows you to, kind of, then use your own private models as well. So that's one area that we have spent which means that, like, you know, if you have a workflow and you want to put AI inside your workflow, you know, AI workflows becomes a very, very trigger to do with conductor overall. So that's one part of it that we have done. And of course, you can also associate a schema with the chat complete. So like, you know, if it generates the wrong output that does not come from to your schema, it just stays right there, or it doesn't retry, and, you know, tries one more time and things like that. So like, it makes it a little bit easier in terms of, you know, how do you build applications that uses elements?
35:40So that's one part of it,
So that's one part of it,
So that's one part of it,
So that's one part of it,
S Speaker 235:45I think, is like an example of a tool calling that I was talking about, right? So, like, this is an agent that does HubSpot, a tool calling when you give an output. Let's say I find the one particular iteration, right, instead of actually calling the tools. So I know this is good enough or not, but here is a good example. So in this case, you know, the request was to search for contacts from HubSpot. Of course, you don't want this to be done by everybody, because, you know, it's, it's CRM data. Not everybody should be able to have access. But this is a test case, by the way, and this is regular production data also. So it decided to do a tool called saying, I'm going to search contacts in HubSpot. We have support for MCP integration, so it's going to use HubSpot MCP, but we have guardrail, and this guardrail gets triggered to say, you know, if you are trying to do anything around getting the contract values or try to do or update customer information, you should not do this, which means now, if I want to use it like read, operations are going to be fine, sensitive operations are going to be talked and that also means that, like you know, if I want to now deploy this in a production environment, I can safely deploy this, but that's, that's one example of using llms with guardrails in a production environment.
I think, is like an example of a tool calling that I was talking about, right? So, like, this is an agent that does HubSpot, a tool calling when you give an output. Let's say I find the one particular iteration, right, instead of actually calling the tools. So I know this is good enough or not, but here is a good example. So in this case, you know, the request was to search for contacts from HubSpot. Of course, you don't want this to be done by everybody, because, you know, it's, it's CRM data. Not everybody should be able to have access. But this is a test case, by the way, and this is regular production data also. So it decided to do a tool called saying, I'm going to search contacts in HubSpot. We have support for MCP integration, so it's going to use HubSpot MCP, but we have guardrail, and this guardrail gets triggered to say, you know, if you are trying to do anything around getting the contract values or try to do or update customer information, you should not do this, which means now, if I want to use it like read, operations are going to be fine, sensitive operations are going to be talked and that also means that, like you know, if I want to now deploy this in a production environment, I can safely deploy this, but that's, that's one example of using llms with guardrails in a production environment.
I think, is like an example of a tool calling that I was talking about, right? So, like, this is an agent that does HubSpot, a tool calling when you give an output. Let's say I find the one particular iteration, right, instead of actually calling the tools. So I know this is good enough or not, but here is a good example. So in this case, you know, the request was to search for contacts from HubSpot. Of course, you don't want this to be done by everybody, because, you know, it's, it's CRM data. Not everybody should be able to have access. But this is a test case, by the way, and this is regular production data also. So it decided to do a tool called saying, I'm going to search contacts in HubSpot. We have support for MCP integration, so it's going to use HubSpot MCP, but we have guardrail, and this guardrail gets triggered to say, you know, if you are trying to do anything around getting the contract values or try to do or update customer information, you should not do this, which means now, if I want to use it like read, operations are going to be fine, sensitive operations are going to be talked and that also means that, like you know, if I want to now deploy this in a production environment, I can safely deploy this, but that's, that's one example of using llms with guardrails in a production environment.
I think, is like an example of a tool calling that I was talking about, right? So, like, this is an agent that does HubSpot, a tool calling when you give an output. Let's say I find the one particular iteration, right, instead of actually calling the tools. So I know this is good enough or not, but here is a good example. So in this case, you know, the request was to search for contacts from HubSpot. Of course, you don't want this to be done by everybody, because, you know, it's, it's CRM data. Not everybody should be able to have access. But this is a test case, by the way, and this is regular production data also. So it decided to do a tool called saying, I'm going to search contacts in HubSpot. We have support for MCP integration, so it's going to use HubSpot MCP, but we have guardrail, and this guardrail gets triggered to say, you know, if you are trying to do anything around getting the contract values or try to do or update customer information, you should not do this, which means now, if I want to use it like read, operations are going to be fine, sensitive operations are going to be talked and that also means that, like you know, if I want to now deploy this in a production environment, I can safely deploy this, but that's, that's one example of using llms with guardrails in a production environment.
S Speaker 137:10And how are the how are these guardrails set in the workflow itself we're in? And are these natural language guardrails
And how are the how are these guardrails set in the workflow itself we're in? And are these natural language guardrails
And how are the how are these guardrails set in the workflow itself we're in? And are these natural language guardrails
And how are the how are these guardrails set in the workflow itself we're in? And are these natural language guardrails
S Speaker 237:17so we support NLP? So like you know, you can put in natural language with an agent. You can write a script. There are also open source projects, like guardrails.ai so, you know, you can also make an HTTP or so, you know, it supports all three of them. And finally, is human guardrail so, like, if you want somebody to come and review manually, it assigns a human, we have human task form. So, you know, it gets to sign a form. You can look at the form and see what should be happening like, and then you can approve, reject and continue forward.
so we support NLP? So like you know, you can put in natural language with an agent. You can write a script. There are also open source projects, like guardrails.ai so, you know, you can also make an HTTP or so, you know, it supports all three of them. And finally, is human guardrail so, like, if you want somebody to come and review manually, it assigns a human, we have human task form. So, you know, it gets to sign a form. You can look at the form and see what should be happening like, and then you can approve, reject and continue forward.
so we support NLP? So like you know, you can put in natural language with an agent. You can write a script. There are also open source projects, like guardrails.ai so, you know, you can also make an HTTP or so, you know, it supports all three of them. And finally, is human guardrail so, like, if you want somebody to come and review manually, it assigns a human, we have human task form. So, you know, it gets to sign a form. You can look at the form and see what should be happening like, and then you can approve, reject and continue forward.
so we support NLP? So like you know, you can put in natural language with an agent. You can write a script. There are also open source projects, like guardrails.ai so, you know, you can also make an HTTP or so, you know, it supports all three of them. And finally, is human guardrail so, like, if you want somebody to come and review manually, it assigns a human, we have human task form. So, you know, it gets to sign a form. You can look at the form and see what should be happening like, and then you can approve, reject and continue forward.
S Speaker 337:50One more thing, right, like, so we talked about the roadmap items like, one is the whole process building right? Some more
One more thing, right, like, so we talked about the roadmap items like, one is the whole process building right? Some more
One more thing, right, like, so we talked about the roadmap items like, one is the whole process building right? Some more
One more thing, right, like, so we talked about the roadmap items like, one is the whole process building right? Some more
38:00of a first class product. Also, like, that's a purview for roadmap
of a first class product. Also, like, that's a purview for roadmap
of a first class product. Also, like, that's a purview for roadmap
of a first class product. Also, like, that's a purview for roadmap
S Speaker 238:03also. So one of the things that we want to build is we want to make it easy to for people to build agents. And if you think about an agent, right, like an agent inherently has what like you you have, let's say, you know, test agent this, more on this floor. You take an agent, you associate a particular model. So it is like, so what we have done is like, you know, connector is a very powerful back end for agent runtime. Because you know, if you think about an agent, agent is you have a language model that you know, given the right context and knowledge can like, essentially give you what to execute, which is essentially a workflow, if you think about it right, and then you need a runtime to execute the workflow. So the first part can be done by entropy or open AI. Last part is something that computer already does. It very well. Computer can take a dynamic workflow graph and execute it, yeah. The second part is what we are building, and this is kind of a very high level UI on it, right? So the AI is built a very open ended UI that allows you to define agents. The agents are basically, you know, you give, you choose a model, you provide the
also. So one of the things that we want to build is we want to make it easy to for people to build agents. And if you think about an agent, right, like an agent inherently has what like you you have, let's say, you know, test agent this, more on this floor. You take an agent, you associate a particular model. So it is like, so what we have done is like, you know, connector is a very powerful back end for agent runtime. Because you know, if you think about an agent, agent is you have a language model that you know, given the right context and knowledge can like, essentially give you what to execute, which is essentially a workflow, if you think about it right, and then you need a runtime to execute the workflow. So the first part can be done by entropy or open AI. Last part is something that computer already does. It very well. Computer can take a dynamic workflow graph and execute it, yeah. The second part is what we are building, and this is kind of a very high level UI on it, right? So the AI is built a very open ended UI that allows you to define agents. The agents are basically, you know, you give, you choose a model, you provide the
also. So one of the things that we want to build is we want to make it easy to for people to build agents. And if you think about an agent, right, like an agent inherently has what like you you have, let's say, you know, test agent this, more on this floor. You take an agent, you associate a particular model. So it is like, so what we have done is like, you know, connector is a very powerful back end for agent runtime. Because you know, if you think about an agent, agent is you have a language model that you know, given the right context and knowledge can like, essentially give you what to execute, which is essentially a workflow, if you think about it right, and then you need a runtime to execute the workflow. So the first part can be done by entropy or open AI. Last part is something that computer already does. It very well. Computer can take a dynamic workflow graph and execute it, yeah. The second part is what we are building, and this is kind of a very high level UI on it, right? So the AI is built a very open ended UI that allows you to define agents. The agents are basically, you know, you give, you choose a model, you provide the
also. So one of the things that we want to build is we want to make it easy to for people to build agents. And if you think about an agent, right, like an agent inherently has what like you you have, let's say, you know, test agent this, more on this floor. You take an agent, you associate a particular model. So it is like, so what we have done is like, you know, connector is a very powerful back end for agent runtime. Because you know, if you think about an agent, agent is you have a language model that you know, given the right context and knowledge can like, essentially give you what to execute, which is essentially a workflow, if you think about it right, and then you need a runtime to execute the workflow. So the first part can be done by entropy or open AI. Last part is something that computer already does. It very well. Computer can take a dynamic workflow graph and execute it, yeah. The second part is what we are building, and this is kind of a very high level UI on it, right? So the AI is built a very open ended UI that allows you to define agents. The agents are basically, you know, you give, you choose a model, you provide the
39:28context or system instructions
context or system instructions
context or system instructions
context or system instructions
39:32only do tool calling, for example,
only do tool calling, for example,
only do tool calling, for example,
only do tool calling, for example,
39:35we'll see what you associate tools with. It like,
we'll see what you associate tools with. It like,
we'll see what you associate tools with. It like,
we'll see what you associate tools with. It like,
39:41for example, I can say, you know, I can use senders.
for example, I can say, you know, I can use senders.
for example, I can say, you know, I can use senders.
for example, I can say, you know, I can use senders.
39:45Use this to find the customer tickets.
Use this to find the customer tickets.
Use this to find the customer tickets.
Use this to find the customer tickets.
S Speaker 239:50I can also see the guardrail here. So, like, you know, I have HTTP guardrails, human guardrails, LLM, as a judge, and things like that. I can say, use HubSpot. Use it to get the the customer and pipeline information. And I can also provide knowledge base right? Like, for example, I have my invoices and customer contacts in s3 bucket somewhere. Yeah, I can say, use that to get more information from there. And then you go on, that's my agent, and I can start conversing with the agent. It has all the tools and everything. Like, for example, here is an execution. I have your right of that agent. Like here in this case, like, for example, I asked for the data information is rejected because, you know, Gary was triggered. But you can ask more information. You can chat with it,
I can also see the guardrail here. So, like, you know, I have HTTP guardrails, human guardrails, LLM, as a judge, and things like that. I can say, use HubSpot. Use it to get the the customer and pipeline information. And I can also provide knowledge base right? Like, for example, I have my invoices and customer contacts in s3 bucket somewhere. Yeah, I can say, use that to get more information from there. And then you go on, that's my agent, and I can start conversing with the agent. It has all the tools and everything. Like, for example, here is an execution. I have your right of that agent. Like here in this case, like, for example, I asked for the data information is rejected because, you know, Gary was triggered. But you can ask more information. You can chat with it,
I can also see the guardrail here. So, like, you know, I have HTTP guardrails, human guardrails, LLM, as a judge, and things like that. I can say, use HubSpot. Use it to get the the customer and pipeline information. And I can also provide knowledge base right? Like, for example, I have my invoices and customer contacts in s3 bucket somewhere. Yeah, I can say, use that to get more information from there. And then you go on, that's my agent, and I can start conversing with the agent. It has all the tools and everything. Like, for example, here is an execution. I have your right of that agent. Like here in this case, like, for example, I asked for the data information is rejected because, you know, Gary was triggered. But you can ask more information. You can chat with it,
I can also see the guardrail here. So, like, you know, I have HTTP guardrails, human guardrails, LLM, as a judge, and things like that. I can say, use HubSpot. Use it to get the the customer and pipeline information. And I can also provide knowledge base right? Like, for example, I have my invoices and customer contacts in s3 bucket somewhere. Yeah, I can say, use that to get more information from there. And then you go on, that's my agent, and I can start conversing with the agent. It has all the tools and everything. Like, for example, here is an execution. I have your right of that agent. Like here in this case, like, for example, I asked for the data information is rejected because, you know, Gary was triggered. But you can ask more information. You can chat with it,
S Speaker 340:52yeah, this opens up. So this is a, you know, conversation agent example, right? Like, but again, this is, you can embed this into whichever application that you want, yeah, and also APIs, right? But we also have, you know, you can use this to do background agents as well, right? So there's no conversation happening. So for example, there's a Kafka pipeline with a bank is doing a payment. Transaction flows are being captured in Kafka pipeline. The agent can go and inspect every single message and look a product, right? And when it comes up, it can say, I think that this product can detect them and show it up right the user wants. They can configure it to make even actions based on contents, right, like then I reject or put it up to a human right, so the user will have an option to do all of that in their enterprise based upon, you know, what the guidelines are for that?
yeah, this opens up. So this is a, you know, conversation agent example, right? Like, but again, this is, you can embed this into whichever application that you want, yeah, and also APIs, right? But we also have, you know, you can use this to do background agents as well, right? So there's no conversation happening. So for example, there's a Kafka pipeline with a bank is doing a payment. Transaction flows are being captured in Kafka pipeline. The agent can go and inspect every single message and look a product, right? And when it comes up, it can say, I think that this product can detect them and show it up right the user wants. They can configure it to make even actions based on contents, right, like then I reject or put it up to a human right, so the user will have an option to do all of that in their enterprise based upon, you know, what the guidelines are for that?
yeah, this opens up. So this is a, you know, conversation agent example, right? Like, but again, this is, you can embed this into whichever application that you want, yeah, and also APIs, right? But we also have, you know, you can use this to do background agents as well, right? So there's no conversation happening. So for example, there's a Kafka pipeline with a bank is doing a payment. Transaction flows are being captured in Kafka pipeline. The agent can go and inspect every single message and look a product, right? And when it comes up, it can say, I think that this product can detect them and show it up right the user wants. They can configure it to make even actions based on contents, right, like then I reject or put it up to a human right, so the user will have an option to do all of that in their enterprise based upon, you know, what the guidelines are for that?
yeah, this opens up. So this is a, you know, conversation agent example, right? Like, but again, this is, you can embed this into whichever application that you want, yeah, and also APIs, right? But we also have, you know, you can use this to do background agents as well, right? So there's no conversation happening. So for example, there's a Kafka pipeline with a bank is doing a payment. Transaction flows are being captured in Kafka pipeline. The agent can go and inspect every single message and look a product, right? And when it comes up, it can say, I think that this product can detect them and show it up right the user wants. They can configure it to make even actions based on contents, right, like then I reject or put it up to a human right, so the user will have an option to do all of that in their enterprise based upon, you know, what the guidelines are for that?
S Speaker 141:45Anything else we should cover on the product side, since we have the rain API,
Anything else we should cover on the product side, since we have the rain API,
Anything else we should cover on the product side, since we have the rain API,
Anything else we should cover on the product side, since we have the rain API,
S Speaker 341:52the synchronous execution stuff, right? Like, that's a new other thing that we did roll out last quarter, customers are probably using it. But yeah, it's also packed, yeah. And then CP gateway talks as one. We can talk
the synchronous execution stuff, right? Like, that's a new other thing that we did roll out last quarter, customers are probably using it. But yeah, it's also packed, yeah. And then CP gateway talks as one. We can talk
the synchronous execution stuff, right? Like, that's a new other thing that we did roll out last quarter, customers are probably using it. But yeah, it's also packed, yeah. And then CP gateway talks as one. We can talk
the synchronous execution stuff, right? Like, that's a new other thing that we did roll out last quarter, customers are probably using it. But yeah, it's also packed, yeah. And then CP gateway talks as one. We can talk
S Speaker 242:08about one other thing that we built out was so you have workflows. You want to use this workflow in your application, right? So one is, you know, we allow you to kind of expose your workflow as APIs. Yeah, and this APIs are then exposed as open API specification and all, which means that, like you know, it becomes much easier for you to consume as a developer. Like every API point here is about flow in the bucket, and depending upon you know, whether this workflow is on HTTP tasks or has long running task it can complete immediately or while complete and whatever not. The other thing you know with agentic systems, right, is that being able to kind of leverage workflows as MCB tools? Yeah, llms needs MCB tools. There is a very clear demand that we have seen from customers where they want to be able to leverage rage and systems like Claude. But being able to connect with internal systems is not easy, because, you know, one, you need to have an MCP gateway to do that right. That means that you want to kind of expose them as an MCP service. And, you know, put the right level of access control everything. So this is one area where, you know, we we added the MCP gateway as part of our office. So you can take a workflow, you can create APIs of this workflows and this API services are also available, both as an HTTP endpoint as well as an MCP tool, okay? Or, you know, I can take this MCP based on the the authentication scheme that is required by this particular service. You can connect it with your plot or any llms and start using it. So now you know it also take a workflow and use it as a tool inside any llms.
about one other thing that we built out was so you have workflows. You want to use this workflow in your application, right? So one is, you know, we allow you to kind of expose your workflow as APIs. Yeah, and this APIs are then exposed as open API specification and all, which means that, like you know, it becomes much easier for you to consume as a developer. Like every API point here is about flow in the bucket, and depending upon you know, whether this workflow is on HTTP tasks or has long running task it can complete immediately or while complete and whatever not. The other thing you know with agentic systems, right, is that being able to kind of leverage workflows as MCB tools? Yeah, llms needs MCB tools. There is a very clear demand that we have seen from customers where they want to be able to leverage rage and systems like Claude. But being able to connect with internal systems is not easy, because, you know, one, you need to have an MCP gateway to do that right. That means that you want to kind of expose them as an MCP service. And, you know, put the right level of access control everything. So this is one area where, you know, we we added the MCP gateway as part of our office. So you can take a workflow, you can create APIs of this workflows and this API services are also available, both as an HTTP endpoint as well as an MCP tool, okay? Or, you know, I can take this MCP based on the the authentication scheme that is required by this particular service. You can connect it with your plot or any llms and start using it. So now you know it also take a workflow and use it as a tool inside any llms.
about one other thing that we built out was so you have workflows. You want to use this workflow in your application, right? So one is, you know, we allow you to kind of expose your workflow as APIs. Yeah, and this APIs are then exposed as open API specification and all, which means that, like you know, it becomes much easier for you to consume as a developer. Like every API point here is about flow in the bucket, and depending upon you know, whether this workflow is on HTTP tasks or has long running task it can complete immediately or while complete and whatever not. The other thing you know with agentic systems, right, is that being able to kind of leverage workflows as MCB tools? Yeah, llms needs MCB tools. There is a very clear demand that we have seen from customers where they want to be able to leverage rage and systems like Claude. But being able to connect with internal systems is not easy, because, you know, one, you need to have an MCP gateway to do that right. That means that you want to kind of expose them as an MCP service. And, you know, put the right level of access control everything. So this is one area where, you know, we we added the MCP gateway as part of our office. So you can take a workflow, you can create APIs of this workflows and this API services are also available, both as an HTTP endpoint as well as an MCP tool, okay? Or, you know, I can take this MCP based on the the authentication scheme that is required by this particular service. You can connect it with your plot or any llms and start using it. So now you know it also take a workflow and use it as a tool inside any llms.
about one other thing that we built out was so you have workflows. You want to use this workflow in your application, right? So one is, you know, we allow you to kind of expose your workflow as APIs. Yeah, and this APIs are then exposed as open API specification and all, which means that, like you know, it becomes much easier for you to consume as a developer. Like every API point here is about flow in the bucket, and depending upon you know, whether this workflow is on HTTP tasks or has long running task it can complete immediately or while complete and whatever not. The other thing you know with agentic systems, right, is that being able to kind of leverage workflows as MCB tools? Yeah, llms needs MCB tools. There is a very clear demand that we have seen from customers where they want to be able to leverage rage and systems like Claude. But being able to connect with internal systems is not easy, because, you know, one, you need to have an MCP gateway to do that right. That means that you want to kind of expose them as an MCP service. And, you know, put the right level of access control everything. So this is one area where, you know, we we added the MCP gateway as part of our office. So you can take a workflow, you can create APIs of this workflows and this API services are also available, both as an HTTP endpoint as well as an MCP tool, okay? Or, you know, I can take this MCP based on the the authentication scheme that is required by this particular service. You can connect it with your plot or any llms and start using it. So now you know it also take a workflow and use it as a tool inside any llms.
S Speaker 143:51Very interesting. The workflow as API piece is super interesting because then you make it very flexible, right? You can, you can, yeah,
Very interesting. The workflow as API piece is super interesting because then you make it very flexible, right? You can, you can, yeah,
Very interesting. The workflow as API piece is super interesting because then you make it very flexible, right? You can, you can, yeah,
Very interesting. The workflow as API piece is super interesting because then you make it very flexible, right? You can, you can, yeah,
S Speaker 244:02if you look at the traditional workflow, just right? Traditional workflow engines are asynchronous, long running, yeah, one area where we saw clear demand and was synchronous, workflows where the workflows themselves don't take really long time to complete. So we are talking about 2030, millisecond to complete the entire workflow, end to end, right? So when you trigger a workflow in conductor, you can also trigger them in a synchronous mode, right? Trading of the durability at a workflow level, as opposed to task level, right? So you can do some trade offs there. But then workflow can complete end to end in less than you know, I mean, depending upon the sum of tasks, in terms of how fast your API calls will
if you look at the traditional workflow, just right? Traditional workflow engines are asynchronous, long running, yeah, one area where we saw clear demand and was synchronous, workflows where the workflows themselves don't take really long time to complete. So we are talking about 2030, millisecond to complete the entire workflow, end to end, right? So when you trigger a workflow in conductor, you can also trigger them in a synchronous mode, right? Trading of the durability at a workflow level, as opposed to task level, right? So you can do some trade offs there. But then workflow can complete end to end in less than you know, I mean, depending upon the sum of tasks, in terms of how fast your API calls will
if you look at the traditional workflow, just right? Traditional workflow engines are asynchronous, long running, yeah, one area where we saw clear demand and was synchronous, workflows where the workflows themselves don't take really long time to complete. So we are talking about 2030, millisecond to complete the entire workflow, end to end, right? So when you trigger a workflow in conductor, you can also trigger them in a synchronous mode, right? Trading of the durability at a workflow level, as opposed to task level, right? So you can do some trade offs there. But then workflow can complete end to end in less than you know, I mean, depending upon the sum of tasks, in terms of how fast your API calls will
if you look at the traditional workflow, just right? Traditional workflow engines are asynchronous, long running, yeah, one area where we saw clear demand and was synchronous, workflows where the workflows themselves don't take really long time to complete. So we are talking about 2030, millisecond to complete the entire workflow, end to end, right? So when you trigger a workflow in conductor, you can also trigger them in a synchronous mode, right? Trading of the durability at a workflow level, as opposed to task level, right? So you can do some trade offs there. But then workflow can complete end to end in less than you know, I mean, depending upon the sum of tasks, in terms of how fast your API calls will
S Speaker 144:41complete. Very, very interesting. So I think, I think the magic happens behind this dashboard right, like creating that scalable infrastructure that you build, right that's able to run these workflows? Is there a way, maybe you we can unpack the architecture a little bit and how hard it is to build that, because I think the barrier to entry, because I'll tell you where our team is thinking about right? So they might, they might be worried about these agentic workflow systems where you can come in, it's a low code, no code, you know, you are prompting it. You're basically building these, but they won't run at the scale where conductor would run today, right? So you guys definitely have an advantage, but I want to be able to understand that. So I see, I see the slides, slides up for that.
complete. Very, very interesting. So I think, I think the magic happens behind this dashboard right, like creating that scalable infrastructure that you build, right that's able to run these workflows? Is there a way, maybe you we can unpack the architecture a little bit and how hard it is to build that, because I think the barrier to entry, because I'll tell you where our team is thinking about right? So they might, they might be worried about these agentic workflow systems where you can come in, it's a low code, no code, you know, you are prompting it. You're basically building these, but they won't run at the scale where conductor would run today, right? So you guys definitely have an advantage, but I want to be able to understand that. So I see, I see the slides, slides up for that.
complete. Very, very interesting. So I think, I think the magic happens behind this dashboard right, like creating that scalable infrastructure that you build, right that's able to run these workflows? Is there a way, maybe you we can unpack the architecture a little bit and how hard it is to build that, because I think the barrier to entry, because I'll tell you where our team is thinking about right? So they might, they might be worried about these agentic workflow systems where you can come in, it's a low code, no code, you know, you are prompting it. You're basically building these, but they won't run at the scale where conductor would run today, right? So you guys definitely have an advantage, but I want to be able to understand that. So I see, I see the slides, slides up for that.
complete. Very, very interesting. So I think, I think the magic happens behind this dashboard right, like creating that scalable infrastructure that you build, right that's able to run these workflows? Is there a way, maybe you we can unpack the architecture a little bit and how hard it is to build that, because I think the barrier to entry, because I'll tell you where our team is thinking about right? So they might, they might be worried about these agentic workflow systems where you can come in, it's a low code, no code, you know, you are prompting it. You're basically building these, but they won't run at the scale where conductor would run today, right? So you guys definitely have an advantage, but I want to be able to understand that. So I see, I see the slides, slides up for that.
S Speaker 245:33Like, you know, probably architecture of conductor, right? So, yeah, at the core of conductor, right? Like, there are basically three things. So one is you have integrations with external systems, which is what allows conductor to operate seamlessly across other enterprise systems. At the conductor, you essentially have, of course, you have APIs, but you know, you have a very powerful distributed scheduler so you can schedule workflows to run at a periodic interval so itself. It's a completely distributed cron scheduler. You can replace your entire with conductor. It has a distributed state machine evaluator, which means that can operate in a very resilient manner, like you know, you have multiple conductor servers running at a given point in time, operating on workflows even above that how it continues to operate. You can, yes, every task that you see is backed by a queue. The queues are bit by conductor. So, like, this is our own proprietary queuing system, okay? This is not distributed time delay, priority queue. And they are fully durable queues, and they are what we call it, like a zero configuration, right, right? Traditionally, what happens is you, you have an admin who is responsible for managing queues on Kafka, yeah, or NAT remedy, right? Connector. There is no such thing. In a typical production environment, you could have 1000s of queues, yeah, getting spun up and destroyed depending upon the workload. And it's all happens behind the scenes. There is no mechanistic point, and that really gives the most powerful, powerful conductor, right? Because these are the at the heart of conductor, right? This is how we scale. This is how we yeah, we operate to scale.
Like, you know, probably architecture of conductor, right? So, yeah, at the core of conductor, right? Like, there are basically three things. So one is you have integrations with external systems, which is what allows conductor to operate seamlessly across other enterprise systems. At the conductor, you essentially have, of course, you have APIs, but you know, you have a very powerful distributed scheduler so you can schedule workflows to run at a periodic interval so itself. It's a completely distributed cron scheduler. You can replace your entire with conductor. It has a distributed state machine evaluator, which means that can operate in a very resilient manner, like you know, you have multiple conductor servers running at a given point in time, operating on workflows even above that how it continues to operate. You can, yes, every task that you see is backed by a queue. The queues are bit by conductor. So, like, this is our own proprietary queuing system, okay? This is not distributed time delay, priority queue. And they are fully durable queues, and they are what we call it, like a zero configuration, right, right? Traditionally, what happens is you, you have an admin who is responsible for managing queues on Kafka, yeah, or NAT remedy, right? Connector. There is no such thing. In a typical production environment, you could have 1000s of queues, yeah, getting spun up and destroyed depending upon the workload. And it's all happens behind the scenes. There is no mechanistic point, and that really gives the most powerful, powerful conductor, right? Because these are the at the heart of conductor, right? This is how we scale. This is how we yeah, we operate to scale.
Like, you know, probably architecture of conductor, right? So, yeah, at the core of conductor, right? Like, there are basically three things. So one is you have integrations with external systems, which is what allows conductor to operate seamlessly across other enterprise systems. At the conductor, you essentially have, of course, you have APIs, but you know, you have a very powerful distributed scheduler so you can schedule workflows to run at a periodic interval so itself. It's a completely distributed cron scheduler. You can replace your entire with conductor. It has a distributed state machine evaluator, which means that can operate in a very resilient manner, like you know, you have multiple conductor servers running at a given point in time, operating on workflows even above that how it continues to operate. You can, yes, every task that you see is backed by a queue. The queues are bit by conductor. So, like, this is our own proprietary queuing system, okay? This is not distributed time delay, priority queue. And they are fully durable queues, and they are what we call it, like a zero configuration, right, right? Traditionally, what happens is you, you have an admin who is responsible for managing queues on Kafka, yeah, or NAT remedy, right? Connector. There is no such thing. In a typical production environment, you could have 1000s of queues, yeah, getting spun up and destroyed depending upon the workload. And it's all happens behind the scenes. There is no mechanistic point, and that really gives the most powerful, powerful conductor, right? Because these are the at the heart of conductor, right? This is how we scale. This is how we yeah, we operate to scale.
Like, you know, probably architecture of conductor, right? So, yeah, at the core of conductor, right? Like, there are basically three things. So one is you have integrations with external systems, which is what allows conductor to operate seamlessly across other enterprise systems. At the conductor, you essentially have, of course, you have APIs, but you know, you have a very powerful distributed scheduler so you can schedule workflows to run at a periodic interval so itself. It's a completely distributed cron scheduler. You can replace your entire with conductor. It has a distributed state machine evaluator, which means that can operate in a very resilient manner, like you know, you have multiple conductor servers running at a given point in time, operating on workflows even above that how it continues to operate. You can, yes, every task that you see is backed by a queue. The queues are bit by conductor. So, like, this is our own proprietary queuing system, okay? This is not distributed time delay, priority queue. And they are fully durable queues, and they are what we call it, like a zero configuration, right, right? Traditionally, what happens is you, you have an admin who is responsible for managing queues on Kafka, yeah, or NAT remedy, right? Connector. There is no such thing. In a typical production environment, you could have 1000s of queues, yeah, getting spun up and destroyed depending upon the workload. And it's all happens behind the scenes. There is no mechanistic point, and that really gives the most powerful, powerful conductor, right? Because these are the at the heart of conductor, right? This is how we scale. This is how we yeah, we operate to scale.
S Speaker 147:11So would it be fair to say, the high availability and all these result resiliency and redundancy piece, it's kind of, you know, how Kubernetes and containers are built. So is it like, are you leveraging some bit of that or not?
So would it be fair to say, the high availability and all these result resiliency and redundancy piece, it's kind of, you know, how Kubernetes and containers are built. So is it like, are you leveraging some bit of that or not?
So would it be fair to say, the high availability and all these result resiliency and redundancy piece, it's kind of, you know, how Kubernetes and containers are built. So is it like, are you leveraging some bit of that or not?
So would it be fair to say, the high availability and all these result resiliency and redundancy piece, it's kind of, you know, how Kubernetes and containers are built. So is it like, are you leveraging some bit of that or not?
S Speaker 247:26Really. We deploy connector typically on Kubernetes environment. Okay. I mean, today, that's the one, right? Yeah, Kubernetes. Other thing we do is like, you know, every workflow is captured an index, which means that you can now search for it, and once the buffalo completes, we we kind of compress them, put it in a cloud store. So, you know, you can keep them around for days, months, right? Years, if you for compliance results, or later on, for some sort of analytics, we ourselves do that kind of stuff. So what about Developer Edition, for example? Right? And, we run everything using a combination of a tiered storage, right? So we use Redis for queues running workflows, Postgres for metadata as well as indexing of the workflows, yeah. And the cloud store for like, completed workflows, right? So it moves seamlessly. When you give an ID for a workflow and fetch the workflow, and fetch the workflow. It just gets it from wherever it is wherever. As a user, you don't worry about whether it's completed running. It doesn't really matter. It gets what you need to and the biggest advantage of having something like conductor is that it knows the state of the world. So we capture metrics in terms of what is happening. How many workflows are running, how many are completing, what's the average duration, failure rate? And all of those are published into the metric system, which is open, telemetric applied, right? Yeah. So we might publish to Prometheus. Our customers also use things like Datadog, yeah, and,
Really. We deploy connector typically on Kubernetes environment. Okay. I mean, today, that's the one, right? Yeah, Kubernetes. Other thing we do is like, you know, every workflow is captured an index, which means that you can now search for it, and once the buffalo completes, we we kind of compress them, put it in a cloud store. So, you know, you can keep them around for days, months, right? Years, if you for compliance results, or later on, for some sort of analytics, we ourselves do that kind of stuff. So what about Developer Edition, for example? Right? And, we run everything using a combination of a tiered storage, right? So we use Redis for queues running workflows, Postgres for metadata as well as indexing of the workflows, yeah. And the cloud store for like, completed workflows, right? So it moves seamlessly. When you give an ID for a workflow and fetch the workflow, and fetch the workflow. It just gets it from wherever it is wherever. As a user, you don't worry about whether it's completed running. It doesn't really matter. It gets what you need to and the biggest advantage of having something like conductor is that it knows the state of the world. So we capture metrics in terms of what is happening. How many workflows are running, how many are completing, what's the average duration, failure rate? And all of those are published into the metric system, which is open, telemetric applied, right? Yeah. So we might publish to Prometheus. Our customers also use things like Datadog, yeah, and,
Really. We deploy connector typically on Kubernetes environment. Okay. I mean, today, that's the one, right? Yeah, Kubernetes. Other thing we do is like, you know, every workflow is captured an index, which means that you can now search for it, and once the buffalo completes, we we kind of compress them, put it in a cloud store. So, you know, you can keep them around for days, months, right? Years, if you for compliance results, or later on, for some sort of analytics, we ourselves do that kind of stuff. So what about Developer Edition, for example? Right? And, we run everything using a combination of a tiered storage, right? So we use Redis for queues running workflows, Postgres for metadata as well as indexing of the workflows, yeah. And the cloud store for like, completed workflows, right? So it moves seamlessly. When you give an ID for a workflow and fetch the workflow, and fetch the workflow. It just gets it from wherever it is wherever. As a user, you don't worry about whether it's completed running. It doesn't really matter. It gets what you need to and the biggest advantage of having something like conductor is that it knows the state of the world. So we capture metrics in terms of what is happening. How many workflows are running, how many are completing, what's the average duration, failure rate? And all of those are published into the metric system, which is open, telemetric applied, right? Yeah. So we might publish to Prometheus. Our customers also use things like Datadog, yeah, and,
Really. We deploy connector typically on Kubernetes environment. Okay. I mean, today, that's the one, right? Yeah, Kubernetes. Other thing we do is like, you know, every workflow is captured an index, which means that you can now search for it, and once the buffalo completes, we we kind of compress them, put it in a cloud store. So, you know, you can keep them around for days, months, right? Years, if you for compliance results, or later on, for some sort of analytics, we ourselves do that kind of stuff. So what about Developer Edition, for example? Right? And, we run everything using a combination of a tiered storage, right? So we use Redis for queues running workflows, Postgres for metadata as well as indexing of the workflows, yeah. And the cloud store for like, completed workflows, right? So it moves seamlessly. When you give an ID for a workflow and fetch the workflow, and fetch the workflow. It just gets it from wherever it is wherever. As a user, you don't worry about whether it's completed running. It doesn't really matter. It gets what you need to and the biggest advantage of having something like conductor is that it knows the state of the world. So we capture metrics in terms of what is happening. How many workflows are running, how many are completing, what's the average duration, failure rate? And all of those are published into the metric system, which is open, telemetric applied, right? Yeah. So we might publish to Prometheus. Our customers also use things like Datadog, yeah, and,
48:56and then you have complete visibility to what's
and then you have complete visibility to what's
and then you have complete visibility to what's
and then you have complete visibility to what's
S Speaker 148:59going on. What's going on, right? Right, right. No, this is super, super interesting.
going on. What's going on, right? Right, right. No, this is super, super interesting.
going on. What's going on, right? Right, right. No, this is super, super interesting.
going on. What's going on, right? Right, right. No, this is super, super interesting.
49:06Yes, you can build this, but getting it right.
Yes, you can build this, but getting it right.
Yes, you can build this, but getting it right.
Yes, you can build this, but getting it right.
S Speaker 149:09No is tough. Yeah, I know you put it on one slide, it's super complicated to actually build and scale this right, yeah. And
No is tough. Yeah, I know you put it on one slide, it's super complicated to actually build and scale this right, yeah. And
No is tough. Yeah, I know you put it on one slide, it's super complicated to actually build and scale this right, yeah. And
No is tough. Yeah, I know you put it on one slide, it's super complicated to actually build and scale this right, yeah. And
S Speaker 249:18most important thing is that, you know, when you build a Netflix, you know it has to follow, and it follows the chaos engineering principle, right? Because no matter whatever happens, underlying infra can fail, but workflows are guaranteed to complete, guaranteed to complete if the system is down for a long period of time, yeah, but they are guaranteed to complete.
most important thing is that, you know, when you build a Netflix, you know it has to follow, and it follows the chaos engineering principle, right? Because no matter whatever happens, underlying infra can fail, but workflows are guaranteed to complete, guaranteed to complete if the system is down for a long period of time, yeah, but they are guaranteed to complete.
most important thing is that, you know, when you build a Netflix, you know it has to follow, and it follows the chaos engineering principle, right? Because no matter whatever happens, underlying infra can fail, but workflows are guaranteed to complete, guaranteed to complete if the system is down for a long period of time, yeah, but they are guaranteed to complete.
most important thing is that, you know, when you build a Netflix, you know it has to follow, and it follows the chaos engineering principle, right? Because no matter whatever happens, underlying infra can fail, but workflows are guaranteed to complete, guaranteed to complete if the system is down for a long period of time, yeah, but they are guaranteed to complete.
49:38Got it, got it. And is it a
Got it, got it. And is it a
Got it, got it. And is it a
Got it, got it. And is it a
S Speaker 249:42V run close to 100 plus clusters in US east, yeah, that's what, yeah, we didn't we have issues because, yes, we had availability zone failovers happening, but right things, things normally happen, for most part, what was would have slowed down a bit, but availability was not
V run close to 100 plus clusters in US east, yeah, that's what, yeah, we didn't we have issues because, yes, we had availability zone failovers happening, but right things, things normally happen, for most part, what was would have slowed down a bit, but availability was not
V run close to 100 plus clusters in US east, yeah, that's what, yeah, we didn't we have issues because, yes, we had availability zone failovers happening, but right things, things normally happen, for most part, what was would have slowed down a bit, but availability was not
V run close to 100 plus clusters in US east, yeah, that's what, yeah, we didn't we have issues because, yes, we had availability zone failovers happening, but right things, things normally happen, for most part, what was would have slowed down a bit, but availability was not
S Speaker 149:59compromised. Got it? Got it? Got it? No, this is super interesting. And in terms of the scale, what is the largest customer deployment? Like, how many workflows, or how many instances? Like, what is the metric that you I just want to understand, like, what is the highest scale, largest scale that you've deployed on conductor,
compromised. Got it? Got it? Got it? No, this is super interesting. And in terms of the scale, what is the largest customer deployment? Like, how many workflows, or how many instances? Like, what is the metric that you I just want to understand, like, what is the highest scale, largest scale that you've deployed on conductor,
compromised. Got it? Got it? Got it? No, this is super interesting. And in terms of the scale, what is the largest customer deployment? Like, how many workflows, or how many instances? Like, what is the metric that you I just want to understand, like, what is the highest scale, largest scale that you've deployed on conductor,
compromised. Got it? Got it? Got it? No, this is super interesting. And in terms of the scale, what is the largest customer deployment? Like, how many workflows, or how many instances? Like, what is the metric that you I just want to understand, like, what is the highest scale, largest scale that you've deployed on conductor,
S Speaker 250:18we have tested all the way till almost 60,000 tasks per second, 60,000 so, yeah, 1660, and basically, because the terminal server itself is pretty much stateless, you can basically keep on adding more and more clusters and sharp them, and, you know, keep on scaling there. So if you were to support like, a million transactions per second, yeah, in the end, what really matters is how many tasks you can evaluate per CPU core. And that number is close to a few 1000 tasks per CPU core, okay. And then your bottleneck is going to be network, which is where you add one more network, yeah. And traditional Yeah, and then
we have tested all the way till almost 60,000 tasks per second, 60,000 so, yeah, 1660, and basically, because the terminal server itself is pretty much stateless, you can basically keep on adding more and more clusters and sharp them, and, you know, keep on scaling there. So if you were to support like, a million transactions per second, yeah, in the end, what really matters is how many tasks you can evaluate per CPU core. And that number is close to a few 1000 tasks per CPU core, okay. And then your bottleneck is going to be network, which is where you add one more network, yeah. And traditional Yeah, and then
we have tested all the way till almost 60,000 tasks per second, 60,000 so, yeah, 1660, and basically, because the terminal server itself is pretty much stateless, you can basically keep on adding more and more clusters and sharp them, and, you know, keep on scaling there. So if you were to support like, a million transactions per second, yeah, in the end, what really matters is how many tasks you can evaluate per CPU core. And that number is close to a few 1000 tasks per CPU core, okay. And then your bottleneck is going to be network, which is where you add one more network, yeah. And traditional Yeah, and then
we have tested all the way till almost 60,000 tasks per second, 60,000 so, yeah, 1660, and basically, because the terminal server itself is pretty much stateless, you can basically keep on adding more and more clusters and sharp them, and, you know, keep on scaling there. So if you were to support like, a million transactions per second, yeah, in the end, what really matters is how many tasks you can evaluate per CPU core. And that number is close to a few 1000 tasks per CPU core, okay. And then your bottleneck is going to be network, which is where you add one more network, yeah. And traditional Yeah, and then
S Speaker 351:04the problem. Since you talked about the scale, there is one that is in the works right now, which is a few billion,
the problem. Since you talked about the scale, there is one that is in the works right now, which is a few billion,
the problem. Since you talked about the scale, there is one that is in the works right now, which is a few billion,
the problem. Since you talked about the scale, there is one that is in the works right now, which is a few billion,
S Speaker 251:11okay, 1 trillion tasks per day.
okay, 1 trillion tasks per day.
okay, 1 trillion tasks per day.
okay, 1 trillion tasks per day.
51:171 trillion tasks per day.
1 trillion tasks per day.
1 trillion tasks per day.
1 trillion tasks per day.
51:20Wow. That's, that's the ask is
Wow. That's, that's the ask is
Wow. That's, that's the ask is
Wow. That's, that's the ask is
S Speaker 251:27that the payment processing? Which one? Yeah, I think the use case is the payment processing.
that the payment processing? Which one? Yeah, I think the use case is the payment processing.
that the payment processing? Which one? Yeah, I think the use case is the payment processing.
that the payment processing? Which one? Yeah, I think the use case is the payment processing.
S Speaker 151:33Okay, okay, super, super interesting. No, this is, this is great. Thanks for walking us through Priyesh. Any questions on the product? So, Joe and on the fundraising side, I know, I know you have term sheet on the table. Can you give us some more color as to what's what's the status?
Okay, okay, super, super interesting. No, this is, this is great. Thanks for walking us through Priyesh. Any questions on the product? So, Joe and on the fundraising side, I know, I know you have term sheet on the table. Can you give us some more color as to what's what's the status?
Okay, okay, super, super interesting. No, this is, this is great. Thanks for walking us through Priyesh. Any questions on the product? So, Joe and on the fundraising side, I know, I know you have term sheet on the table. Can you give us some more color as to what's what's the status?
Okay, okay, super, super interesting. No, this is, this is great. Thanks for walking us through Priyesh. Any questions on the product? So, Joe and on the fundraising side, I know, I know you have term sheet on the table. Can you give us some more color as to what's what's the status?
S Speaker 351:54Yeah, so we signed the term sheet. We are hoping to close the do the first closing somewhere, but now, but
Yeah, so we signed the term sheet. We are hoping to close the do the first closing somewhere, but now, but
Yeah, so we signed the term sheet. We are hoping to close the do the first closing somewhere, but now, but
Yeah, so we signed the term sheet. We are hoping to close the do the first closing somewhere, but now, but
52:02mid November, okay,
S Speaker 352:03mid number and we have a few days, like, maybe 60 days or so after that, right? Yeah, to then all the strategic center
mid number and we have a few days, like, maybe 60 days or so after that, right? Yeah, to then all the strategic center
mid number and we have a few days, like, maybe 60 days or so after that, right? Yeah, to then all the strategic center
mid number and we have a few days, like, maybe 60 days or so after that, right? Yeah, to then all the strategic center
S Speaker 352:17yeah. So that's the conversation that we are in right now, right? Like, and I think over the next few days, we'll have a much more clear understanding of
yeah. So that's the conversation that we are in right now, right? Like, and I think over the next few days, we'll have a much more clear understanding of
yeah. So that's the conversation that we are in right now, right? Like, and I think over the next few days, we'll have a much more clear understanding of
yeah. So that's the conversation that we are in right now, right? Like, and I think over the next few days, we'll have a much more clear understanding of