Meeting: Luma AI
URL: https://otter.ai/u/TlyscNAvsHsvCa4BlhEAN9Wtve4
Downloaded: 2025-12-22T12:43:48.002918
Method: parallel_text_extraction
============================================================

More options
Priyesh P
MarÂ 12 at 2:39 pm
31 min
copy summary
Summary
Transcript
Template:
General
Low audio quality detected. Learn more about how to improve transcription quality.
Keywords
Luma AI, natural language processing, video cameras, real-time video, AI chat, research and production, COVID testing, video direction, deep learning, enterprise partnerships, Netflix collaboration, Amazon Studios, self-serve product, simulation accuracy, premium production.
Speakers
Speaker 1 (95%), Speaker 2 (5%)
Speaker 1
0:00
General models that are able to produce a modality are also very good at understanding that value. So the another word here would be, if you're familiar with natural language processing, before trying to understand what a user is saying, was a big problem in argument that the world had to actually solve. Once we got a labs, that problem went away because, you know, lens are good at generating elements, also good and very good at understanding. But actually so our models show similar performance, attractive understanding. So immediate use case that we are going after is very large scale video cameras that are 1000s and hundreds of 1000s of cameras deployed for the same use case. How do you understand what's happening in that cell does actually really well. And finally, real time video, AI chat. So what we mean by that is being able to freestyle you type in and you get millions of words back and so forth. The future already is moving towards voice. The are the three areas,
1:18
well understanding, so I'll show you
Speaker 1
1:21
sort of how we see that. I think method of research and how understanding production actually
1:28
is very talented.
Speaker 1
1:32
So basically, the company is currently looking at the barrier, and a lot of us are our researchers, and most of our experience has been in building pieces of business. So a little bit of the work that our teams have done before the three pieces that takes the level of information and produces understandable instruments, the general model and then the COVID testing. These two data box, people on our team have built both of those parts, especially for video direction. So we had the bottleneck that actually made generate models possible. That made generate models possible. This is the work from our Josh Dylan, on our team, wasn't in line for 15 years. All the images you see today are possible because of this paper DM because that's work from RC 90 song he was before he joined us in 2023 is actually in the middle of 23 and this was his work from on this video, audio plus language, joint generation problem, then Google and Google's video.
2:54
So we have four people
2:56
responsible,
Speaker 1
3:01
especially very science research. So that's sort of the position of the team research. We believe that to unlock the market, we need to build the use case you can optimize both at the same time. And that's actually a big part of it here is the same time, and that's actually a big part of, right, of course,
3:31
the research. So
Speaker 1
3:36
this discussion my background. I work on Apple vision Pro for topics, specifically on the internet of humans already. So vision Pro has the most expressive and most natural work on the Quantum
Speaker 1
3:59
Mechanics before that. Josh,
4:10
if you
4:15
want
Speaker 1
4:17
to
4:22
train, want to
4:23
train, if you did
4:26
this
Speaker 1
4:28
nightly, it will cost more than 90, but cheaper, because before that, it was also Disney imagined video. We acquired company three researchers from Stanford. They were working on maternal research, especially understanding how the world and real time generation understanding so Matt is the offer of new readings. So which is the first paper actually revolutionized the three generation field. Before his work, it was not possible to use deep learning for three this was the thing that showed that you can actually do something like that. Our CEO, COVID CEO. And before that, she was investment banking. Now she's leading the business and operations here to him, who's head of design. Him and I worked together apple, and he was also the design lead for vision. OS, the operating system, provision, pro before that. Jesse, she just joined us. She's leading GTM. She was before that. Spotify, before Spiel before that, Pandora John, our CFO. He was the head of finance for Assad. He helped them go public. And
Speaker 1
5:55
our last round was led by recent, last price round, at least, was led by in recent Horowitz and video. Then we raised another round after that that was led by AWS. So series A was
6:13
led by amplify and matrix and
Speaker 1
6:16
other angels. I Some people, but I didn't write.
6:23
And recently, from their side, it's more in recent
Speaker 1
6:27
Anjali manhau, if you're familiar with him, they are partners. Interviews. Is very interesting. I'll tell you in a second about that. And I think for this audience, we can't write that down. But I was mentioning to Albert, if you're familiar with inside the AI, if effort, the ten billion enterprise that they're creating in the next couple of months, they are leading on it.
6:52
And, you
Speaker 1
6:54
know, it's a distribution as well as infrastructure partnership to do a lot of things, sorry, discussion on valuation so
7:11
twice,
Speaker 1
7:14
but this is a bit interesting. In this space, every company you can imagine that you can imagine, they have a need for a loan. Pfizer. Pfizer spends 120 million to 400 million, depending on the year, on video advertising, right? Some of these companies either find out, especially so much so what matters is distribution of the productivity partnership is the only other equity investment that Amazon has done other than electronic So Amazon is not usually too relevant. And as you see, the fruits of that Amazon was looking around for same thing for same thing for video, because they have several customers who are either work the next thing that drives a lot of infrastructure
8:12
space and maybe show them way too,
Speaker 1
8:18
are modeling one that's available for Video Duration, basically today and rate two is going to be the first and probably the only, for real time approved model in Amazon Prime studios as the approval model that can be used At least
8:37
but it is because train with
Speaker 1
8:41
licenses. So it was the largest stage that we had. So that's a little bit of fundamental company. So far, we have raised business coming from the our billion out of which two actually
9:11
not only
9:17
it
9:19
understands
Speaker 1
9:22
It understands audio, video, language, and it is that is able to produce whatever range audio video element.
9:34
every night.
Speaker 1
10:07
So these are results generated by our users. That's the most important thing you should know about. These are not sharing the results that are called in our field in AI, right? If you generate 50 and then you pick the one best one, right? These are pretty much compiled from our users who generated it after the launch rate. That's generally how we do pretty much all marketing and benchmarking, not on what we generate, but our onboard. These are great, but engineers Great. Two is the highest quality model that's available on market today, and it's the only model that crosses the threshold of being useful for production became real. But pretty much all the models,
10:49
including our first
10:49
model, but including our first
Speaker 1
10:54
one, was just a demo that you can generate, but it Could mean some text, right? We can generate text here.
11:13
So
11:24
I
Speaker 1
11:27
because of what this giant region is making. So we compare ourselves against the rate to surpasses Sora. Basically,
11:55
it's
12:00
the
Speaker 1
12:07
4k is
12:13
like
12:16
the bigger
12:20
one for
12:27
rate, just for elevation. So this is one
Speaker 1
12:31
that actually creates can use for now.
12:39
Questions, how does it like?
Speaker 1
12:44
Every month we have a new generation model that comes out and then, and then we've got a benchmark competition similar to llms over there as well. So is there fundamentally, a significant leap in terms of so the models that you are generating now creating with Ray two or Ray three compared to others, because also now you have Google also, which is VO two also supposedly has good benchmark results. Gen mom supposedly has good modules as well. Clean, of course, you're showing them here, but we're seeing them like every month somebody releases a new reiteration model, and they're beating other benchmarks, right? So actually, there are no benchmarks in our field. There's only hype. That's the actual thing you have, actually benchmarks that you can look at, right? Like, you know, LGBTQA, or human success, all these other things. But basically it's a very much human eval driven field, and really interesting. Okay, how does it do across the various things that people are talking about? Right? So let's talk about all the ones people are raising and how different companies stack. So you can think about the current reiteration space in three states. You have on the bottom companies that are targeting like those are Totally very small in terms of quality, very small startups and
14:11
investing
14:13
in
Speaker 1
14:16
the goal of those models is to a goal of that area. They operate video platforms. These are two places, and they want more of their users to post. The base model right now is the ratio it will post is one is to 1000 to 10,000 so these models are mostly geared towards the to make something right, and that's if You're a startup in that space, you're a startup in that space, you're not competing with these models that will be available, not only for free, but integrated into all these distributions. So we don't target that quality affects audience, because then you compete against free, and then competing against our goal is, how can we actually, then come to the middle companies that are targeting premium production? So here companies, all the ones you mentioned, any of our customers, they will never touch the whole enterprise. They need. These companies, what happened in language models, just how many companies like Dropbox and Databricks and every other random data company, because methods are known and you can actually produce a new empathy. Would anyone make? Well, what is the best so complete, basically entering that phase the alpha is mostly in. How can you make the highest quality months? And how do you do the research to solve the actual problems people have, right? So now, when you think about this, like runway and others, they are making video calls. That's a problem, because if you want to solve the problem of actual creative audience, the $1.1 trillion creation, they don't just want video module. What they actually want is the simulation of the world understanding. Let me go talk to directors at Netflix or Amazon Studios or whatever have you. They want to model the stats that it's an accurate tracking. And you're gonna have a five say, Okay, I want to replace this with this, right? Or I'm gonna take a video that is unique models that are actually very intelligent. So video model completes. Actually, I don't have any other example. This is a great mistake, because regular task,
Speaker 1
17:09
that's why you must focus on multi model research is the actual solution, right? So that's how we establish ourselves from these companies,
17:28
and if
Speaker 1
17:30
you go in our space, at the moment, you're not going to find a company that can span across 3d across Language, across audio, but that's practically everybody's producing. This is this is by our research team, and our research is the big assets that you need to have. Who has similar research in terms of the part that I understood is essentially, you're focusing on world, better word, understanding better physics, understanding of the world, to be able to train these models, which are very different from some of the others. Video generation only model. Others, video generation only models. Then, how does like the research from these company or labs and your physical intelligence and skill? So is that similar research? Are they doing similar research in European or is that different categories, your biggest company and physical intelligence and others? Right? So bond Labs has taken a path which we consider to be technically so in the first year of our life, we actually did right? Because in 2022 it was not possible to train physically, the cards weren't there. The research was using the came from our team. So it should be. The issue with their approach of training with 3d is that data is not Where are you going to get 4d data that if you want to actually be able to train like if you've seen the demos, the demos are going from an image to the 3d right? Nothing in the world groups. It can't, because four dimensional data doesn't exist. There's no sensors. There's nothing like that. So if you're familiar with the singular truth of AI, that is the bitter lesson right again, from Rick Sutton, who just won, actually.
Speaker 1
19:26
So the only truth in machine learning is that the only thing that works are general methods, transformers that can consume all of the data. If you don't have data, what are you going to do? Right? So modern labs,
19:41
we know the team very well.
Speaker 1
19:44
I have six of our students in my my company, we know what they're doing. Another person who was on the nerve paper is one of the co founders of that. We know that the problem is definitely they will try to do this for a while, and then they will come back to the same conclusion that, hey, to actually work in video, but it will take two years or so. Typically comes from computer vision type around that computer vision people are a little bit stuck. They need to learn that lesson. It's, it's that kind of thing. We also shared an investor, because, at least, you know also as an investor, but we're firewall and these kind of things, so we know a lot about them, if you talk about skilled and physical incumbents. So they are both actually our partners. Okay, so physical intelligence. Their founders are investors in dual I am an angel investor in Indian company. I don't know investing, but just because we wanted to force these ties, they are currently working with teleop data. They're spending about two $30 million this year to get gathered on teleop data, first step for them, because they need something to operate the robots with exact kind of action data that you but the final frontier here are world models, like the ones we enter, because humans don't operate with teleop data. Right Action Model Data. If I tell you, like, hey, think about entering your home and then walking into your bed. You can do that, right? They tell you, Okay, so the final solution of deployable robotics is actually world models where they can robots actually walk things and answer questions like, What happens if I walk 50 meters over all these kind of things, how do I solve it? People try to do this with language models, and this is where our approach also considered by they couldn't and, like you do not see language models being deployed into what people are trying to do with BLMs, which is built in language models, but they are very weak, right? So scaled is currently showing, like, some demos of like, you know, being able to do, like, a little more general stuff, right? Yeah.
21:54
But the final solution is our models,
Speaker 1
21:59
so they would rely but then, okay, I think we can pick that up Len as well later. But let's continue. Have you talked about that? Yeah, so maybe we can switch gear a little bit enterprise use case, but absolutely how you build up the revenue. So let's talk about the
22:24
GTM view.
Speaker 1
22:25
The markets we are focused on is selling in real time in real time,
22:33
video chat,
Speaker 1
22:36
so in the premium production space, why we are working in this space, and like you know,
22:42
how to actually address so
Speaker 1
22:46
video content, the is ridiculous. The easier you make it, the more spend actually goes into it, because the consumption side is so crazy. But if you give people a choice between consumer video versus, you know, anything else, they will go out today. So this is actually a time calculation that's done by Morgan Stanley on our behalf. So this is not like, you know, I pulled it out of thin air. Morgan Stanley wanted to lead our Series C, not Lee hasn't invest, but they wanted to, like, you know, as backers, but we didn't need them maybe next time when we raised the next so, but the problem with these targets is that, one, it's produced by professionals, and two unique models to be really, really good, right? Like, you know, how do you actually solve that? So we are targeting this, this particular market, and the way we think about how we go after it is as the world simulation accuracy increases. So your question on sequencing right, rate process, the boundary of producing media that can be used in production, and with rate three, that's when the world understanding and like in real time chat and these little things come up. So we actually track this, this metric on research side, well, simulation accuracy and the better it gets, the more you can do, the less you can do. Humanity, right? But today, grade two is being adopted very heavily by professionals who produce a so our approach actually, I'll skip that one, and then we'll come back to the next one. So I was also production is about $100,000 if you just look at our cost per minute of video. So we are targeting this market in two ways. One is through enterprise partnerships with large studios. So you should think about us as not a tool builder for large enterprises, but rather like Palantir. And then there's a specific reason why what we are changing is not like, Hey, this is how you do VFX. This is how you're
24:50
changing production. These companies,
Speaker 1
24:54
production is what they do every day. Is the 90% of the spend at the moment. So we have no discussion but live studios, those are the ones that have the most setting up, absolutely
25:09
genetic models
Speaker 1
25:11
doing 15 to 32nd heads kind of squarely fits by then, that's right. So that's why the second part comes into play, right? So we're not skipping one or the other. When you have the foundation models, you're able to build these products like the right thing, actually. But once you have really strong models, but you have my studio, so last year, in January, when we would approach these two news, they're like, oh, you know, we're taking our fans, we're trying it out, these kind of things. But today, when you push them, it's top down first of all, because they see that it can be reinvented the entire studio. So just within this group, I'll tell you a little bit about one of the publishers. It's extremely confidential, but it's Netflix. So Netflix spent last year 18.7 5 billion on production. 11 point I think three or 11.4 came outside of United States where there is no sign, right? They're also the most tech forward Studio you can ever imagine, and everybody copies what they do. So Luma and Netflix partnership is not again, a tool partnership. It's a joint research and commercial partnership, and Luma is the only company in the world with access to all of Netflix's data. This is every pixel that has been received and every pixel that has been captured by the camera and through edit processes and things like that, that allows us to invent the workflows that are necessary. Because, you know, you don't, if you're just scraping the internet, oh, you only have the final pixels. You never know how you got there, the process data, right? This is what everyone wants in languages. So that's a very unique partnership. And the way it will work is we are working with them jointly to build the workflows in those cases, as well as the foundation models. This allows us to not only understand premium production, but also target their support. So initially they'll start out at the 10 to 50 million range of others being deployed. They also identified the first two productions in which have been called regiment. They project in years 26 next year and the year after, they spent on alternative AI video was brought up to 500
27:24
million in the next year. So
Speaker 1
27:29
what we are doing, basically is with them, is not again as a tool provider for better. Okay?
Speaker 1
27:46
So this is so why do we care about students, despite them having psych after and these infections? Because they will, actually, they spend immense amount of money. Hollywood is not actually dying, but they control IP. They control, like, you know, the distribution channels and things of that nature. They are actually investing impendence amount of money in general. They just don't know, right? So, for instance, like, you know John Chu if you know about him, the director of picket and crazy invitations, things like that. He is an investor in demand. Bono is investor in Dubai, the CEO of Warner Music and born animations and professor, but it just, you're never going to hear about it. It's going to happen. And the director of Black Mirror, David sled, is one of the most prolific users of the month. But of course, he will never tell you that it is happening. And it's happening at a very fast rate that you'll never find out. Secondly, Luma is the only approved model in Amazon Studios. Amazon, Amazon owns MGM. MGM is James Bond. If you want to jump one branch, I think of that Luma is the only approved model in Amazon Studios, and we are doing basically a similar kind of research and commercial partnership with them. Of course, it is a function of our relationship. It's another very tech forward study. So think about it this way. You learn premium production from these people, right? And then you're able to deploy it to every advertising studio. You're able to deploy them actually, which is produces Coca Cola ads, for instance, submitting a $15 billion budget. Is our self serve product. So it's the large studios that produce, like, you know, a lot of things we watch. And then there are small studios, small groups, small teams, that make majority of stuff that, like, you know, goes on YouTube, that goes on on smaller budget movies. And these people, you're you have to have a self serve. But their needs and their needs are actually not very different, because the way it is produced is not terribly different between these two items, but they're more sophisticated. So what we learned from here and at the workflows we built here, you're able to deploy self serve, which grows rapidly. So currently, rate two, we launched in January, 13, January 14, 2025, so about six, six weeks ago. Now, our self production machine is now at $17.8 million or run rate. This doesn't include any of our enterprise deals or APS.
30:29
So that is why we are,
Speaker 1
30:33
like, you know, you can, of course, be scared of the studios and go off make like, you know, these kind of things, but this is where the spend is. It's a concentrated industry, right? You can be the right approaches. You can actually, you can get entrenched in them. So that's why we're taking this research and commercial partnership approach together.
30:56
So this was the fixed thing. So
Speaker 1
31:01
yeah, that's, I think, probably where I should stop.
31:07
So I guess I think there's a lot more to dig into, but second session,
Speaker 1
31:14
yeah, so we have next we have EU meetings, awesome. We're gonna switch over to that. What everybody online. Thank you very much. Thanks. Thank you. Amit. Yeah,

How accurate was this transcription?

00:00
31:37
1x