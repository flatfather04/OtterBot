Meeting: Sapient AI
Tue, Oct 21
9:10 AM
42 min
Priyesh P
Technical Difficulties and Initial Introductions
0:4
URL: https://otter.ai/u/VLC4X3I3PA_jNrNWOnHZukUZm1E
Downloaded: 2025-12-21T20:00:32.861851
Method: text_extraction
============================================================

0:40You I Hey, hi, William, can you hear
You I Hey, hi, William, can you hear
You I Hey, hi, William, can you hear
You I Hey, hi, William, can you hear
1:29me? Hey, Priyesh, yeah, I can hear you. Give me
me? Hey, Priyesh, yeah, I can hear you. Give me
me? Hey, Priyesh, yeah, I can hear you. Give me
me? Hey, Priyesh, yeah, I can hear you. Give me
1:33one second. Let me turn on my camera.
one second. Let me turn on my camera.
one second. Let me turn on my camera.
one second. Let me turn on my camera.
1:35Absolutely. Yeah. You
Absolutely. Yeah. You
Absolutely. Yeah. You
Absolutely. Yeah. You
1:45Yeah, it's popping up. Fail to start video camera. I don't know what's
Yeah, it's popping up. Fail to start video camera. I don't know what's
Yeah, it's popping up. Fail to start video camera. I don't know what's
Yeah, it's popping up. Fail to start video camera. I don't know what's
S Speaker 11:51going on. Maybe you have, like, multiple cameras connected to your device. Maybe sometimes that happens.
going on. Maybe you have, like, multiple cameras connected to your device. Maybe sometimes that happens.
going on. Maybe you have, like, multiple cameras connected to your device. Maybe sometimes that happens.
going on. Maybe you have, like, multiple cameras connected to your device. Maybe sometimes that happens.
S Speaker 21:57No, it was, it was working when I was testing camera with the with the virtual background. I don't know why it's looking right. Give me one second. I'm gonna leave the room real quick. I'll rejoin
No, it was, it was working when I was testing camera with the with the virtual background. I don't know why it's looking right. Give me one second. I'm gonna leave the room real quick. I'll rejoin
No, it was, it was working when I was testing camera with the with the virtual background. I don't know why it's looking right. Give me one second. I'm gonna leave the room real quick. I'll rejoin
No, it was, it was working when I was testing camera with the with the virtual background. I don't know why it's looking right. Give me one second. I'm gonna leave the room real quick. I'll rejoin
2:07Absolutely. No worries. Yeah, my bad. You. I i The i
Absolutely. No worries. Yeah, my bad. You. I i The i
Absolutely. No worries. Yeah, my bad. You. I i The i
Absolutely. No worries. Yeah, my bad. You. I i The i
S Speaker 14:20i I You Hi, hey, hi, William. I can see you
i I You Hi, hey, hi, William. I can see you
i I You Hi, hey, hi, William. I can see you
i I You Hi, hey, hi, William. I can see you
S Speaker 26:09now. Hey, hey, frysh, there we go. There we go. My apologies. I just switched to, I just switched to another laptop. I don't know why. I think we got caught into the middle of the Windows Update thingy, okay?
now. Hey, hey, frysh, there we go. There we go. My apologies. I just switched to, I just switched to another laptop. I don't know why. I think we got caught into the middle of the Windows Update thingy, okay?
now. Hey, hey, frysh, there we go. There we go. My apologies. I just switched to, I just switched to another laptop. I don't know why. I think we got caught into the middle of the Windows Update thingy, okay?
now. Hey, hey, frysh, there we go. There we go. My apologies. I just switched to, I just switched to another laptop. I don't know why. I think we got caught into the middle of the Windows Update thingy, okay?
6:23And kind of maybe invest up the drivers.
And kind of maybe invest up the drivers.
And kind of maybe invest up the drivers.
And kind of maybe invest up the drivers.
S Speaker 16:25Yeah, no worries, William, that sort of thing always happens. And then, you know, we use teams in the in the corporate. So every time I use teams, something like this pops up. So I am totally familiar.
Yeah, no worries, William, that sort of thing always happens. And then, you know, we use teams in the in the corporate. So every time I use teams, something like this pops up. So I am totally familiar.
Yeah, no worries, William, that sort of thing always happens. And then, you know, we use teams in the in the corporate. So every time I use teams, something like this pops up. So I am totally familiar.
Yeah, no worries, William, that sort of thing always happens. And then, you know, we use teams in the in the corporate. So every time I use teams, something like this pops up. So I am totally familiar.
S Speaker 26:40I know. I'm so sorry about that. Yeah, just a second ago, I was testing the camera and on the on the virtual background, it worked. And then, I don't know when, as soon as you joined, it just stopped working. I don't know what
I know. I'm so sorry about that. Yeah, just a second ago, I was testing the camera and on the on the virtual background, it worked. And then, I don't know when, as soon as you joined, it just stopped working. I don't know what
I know. I'm so sorry about that. Yeah, just a second ago, I was testing the camera and on the on the virtual background, it worked. And then, I don't know when, as soon as you joined, it just stopped working. I don't know what
I know. I'm so sorry about that. Yeah, just a second ago, I was testing the camera and on the on the virtual background, it worked. And then, I don't know when, as soon as you joined, it just stopped working. I don't know what
6:52that's okay. Well, I'm very calling in from now today.
that's okay. Well, I'm very calling in from now today.
that's okay. Well, I'm very calling in from now today.
that's okay. Well, I'm very calling in from now today.
S Speaker 26:55I'm actually in Beijing right now. Okay, we have a, we have a Beijing office. You know, Gwen and I both graduated from Tsinghua University. It's a it's like the Chinese equivalent of MIT. Guan is from, he graduated from computer science. I graduated from action robotics.
I'm actually in Beijing right now. Okay, we have a, we have a Beijing office. You know, Gwen and I both graduated from Tsinghua University. It's a it's like the Chinese equivalent of MIT. Guan is from, he graduated from computer science. I graduated from action robotics.
I'm actually in Beijing right now. Okay, we have a, we have a Beijing office. You know, Gwen and I both graduated from Tsinghua University. It's a it's like the Chinese equivalent of MIT. Guan is from, he graduated from computer science. I graduated from action robotics.
I'm actually in Beijing right now. Okay, we have a, we have a Beijing office. You know, Gwen and I both graduated from Tsinghua University. It's a it's like the Chinese equivalent of MIT. Guan is from, he graduated from computer science. I graduated from action robotics.
7:14And we co founded this company in 2024
And we co founded this company in 2024
And we co founded this company in 2024
And we co founded this company in 2024
7:19and we're building new architectures for, hopefully the next
and we're building new architectures for, hopefully the next
and we're building new architectures for, hopefully the next
and we're building new architectures for, hopefully the next
7:23generation of AI, yeah,
generation of AI, yeah,
generation of AI, yeah,
generation of AI, yeah,
S Speaker 17:26that's super interesting. I came across your recent paper, and then a lot of a lot of people picked it up as well. So there was widespread news all over. I wanted to reach out have a chat about it, particularly because at Qualcomm, we are trying William, a lot, to sort of host mobiles on the edge and run AI workloads on the edge. And we have, in the last, say, couple of years trying, tried to diversify our portfolio quite a lot, from initially being a mobile and communications player to now our silicon, which goes into PCs. It's a there's a cloud inferencing platform. It goes into a lot of xr devices, so it makes us a very strong strategic partner to have a lot of AI workloads work on edge. And that's a proposition I just wanted to have a chat with you try to understand where the team is, how what are you building, and if there is any potential for our partnership.
that's super interesting. I came across your recent paper, and then a lot of a lot of people picked it up as well. So there was widespread news all over. I wanted to reach out have a chat about it, particularly because at Qualcomm, we are trying William, a lot, to sort of host mobiles on the edge and run AI workloads on the edge. And we have, in the last, say, couple of years trying, tried to diversify our portfolio quite a lot, from initially being a mobile and communications player to now our silicon, which goes into PCs. It's a there's a cloud inferencing platform. It goes into a lot of xr devices, so it makes us a very strong strategic partner to have a lot of AI workloads work on edge. And that's a proposition I just wanted to have a chat with you try to understand where the team is, how what are you building, and if there is any potential for our partnership.
that's super interesting. I came across your recent paper, and then a lot of a lot of people picked it up as well. So there was widespread news all over. I wanted to reach out have a chat about it, particularly because at Qualcomm, we are trying William, a lot, to sort of host mobiles on the edge and run AI workloads on the edge. And we have, in the last, say, couple of years trying, tried to diversify our portfolio quite a lot, from initially being a mobile and communications player to now our silicon, which goes into PCs. It's a there's a cloud inferencing platform. It goes into a lot of xr devices, so it makes us a very strong strategic partner to have a lot of AI workloads work on edge. And that's a proposition I just wanted to have a chat with you try to understand where the team is, how what are you building, and if there is any potential for our partnership.
that's super interesting. I came across your recent paper, and then a lot of a lot of people picked it up as well. So there was widespread news all over. I wanted to reach out have a chat about it, particularly because at Qualcomm, we are trying William, a lot, to sort of host mobiles on the edge and run AI workloads on the edge. And we have, in the last, say, couple of years trying, tried to diversify our portfolio quite a lot, from initially being a mobile and communications player to now our silicon, which goes into PCs. It's a there's a cloud inferencing platform. It goes into a lot of xr devices, so it makes us a very strong strategic partner to have a lot of AI workloads work on edge. And that's a proposition I just wanted to have a chat with you try to understand where the team is, how what are you building, and if there is any potential for our partnership.
8:37near future. I'm going to
near future. I'm going to
near future. I'm going to
near future. I'm going to
S Speaker 28:39give you a little teaser today, but it's not going to be a full introduction, yeah, yeah, but it's gonna be very, very exciting. So my, for me, I'm a big, you know, Qualcomm fan. I'm a Snapdragon user. I have a, I have a Snapdragon phone Gen four, I think I also have a Snapdragon laptop. Yeah, next,
give you a little teaser today, but it's not going to be a full introduction, yeah, yeah, but it's gonna be very, very exciting. So my, for me, I'm a big, you know, Qualcomm fan. I'm a Snapdragon user. I have a, I have a Snapdragon phone Gen four, I think I also have a Snapdragon laptop. Yeah, next,
give you a little teaser today, but it's not going to be a full introduction, yeah, yeah, but it's gonna be very, very exciting. So my, for me, I'm a big, you know, Qualcomm fan. I'm a Snapdragon user. I have a, I have a Snapdragon phone Gen four, I think I also have a Snapdragon laptop. Yeah, next,
give you a little teaser today, but it's not going to be a full introduction, yeah, yeah, but it's gonna be very, very exciting. So my, for me, I'm a big, you know, Qualcomm fan. I'm a Snapdragon user. I have a, I have a Snapdragon phone Gen four, I think I also have a Snapdragon laptop. Yeah, next,
9:06yeah, it's, I'm a big fan of Snapdragon.
yeah, it's, I'm a big fan of Snapdragon.
yeah, it's, I'm a big fan of Snapdragon.
yeah, it's, I'm a big fan of Snapdragon.
9:10That's great to hear. William. Thanks a lot for that. Yeah,
That's great to hear. William. Thanks a lot for that. Yeah,
That's great to hear. William. Thanks a lot for that. Yeah,
That's great to hear. William. Thanks a lot for that. Yeah,
S Speaker 29:13yeah. And, you know, I'm in. I'm a user myself. And also, what we're building right now, I have a slides prepared for today, which I'm going to go over real quick. It's a rather intro slide. We're working on a full size business plans for series eight, which was we're we're planning to release by around early to mid November, which that'll be like an actual business plan for. For now, I'll have a little size for our model or and architecture, yeah.
yeah. And, you know, I'm in. I'm a user myself. And also, what we're building right now, I have a slides prepared for today, which I'm going to go over real quick. It's a rather intro slide. We're working on a full size business plans for series eight, which was we're we're planning to release by around early to mid November, which that'll be like an actual business plan for. For now, I'll have a little size for our model or and architecture, yeah.
yeah. And, you know, I'm in. I'm a user myself. And also, what we're building right now, I have a slides prepared for today, which I'm going to go over real quick. It's a rather intro slide. We're working on a full size business plans for series eight, which was we're we're planning to release by around early to mid November, which that'll be like an actual business plan for. For now, I'll have a little size for our model or and architecture, yeah.
yeah. And, you know, I'm in. I'm a user myself. And also, what we're building right now, I have a slides prepared for today, which I'm going to go over real quick. It's a rather intro slide. We're working on a full size business plans for series eight, which was we're we're planning to release by around early to mid November, which that'll be like an actual business plan for. For now, I'll have a little size for our model or and architecture, yeah.
S Speaker 19:43And before, before we jump into that, I'm excited to know more, but very quickly, wanted to understand, how are you thinking about the company? Where, where you think the company would be based in the future, where the founders would sit? How you're thinking about hiring?
And before, before we jump into that, I'm excited to know more, but very quickly, wanted to understand, how are you thinking about the company? Where, where you think the company would be based in the future, where the founders would sit? How you're thinking about hiring?
And before, before we jump into that, I'm excited to know more, but very quickly, wanted to understand, how are you thinking about the company? Where, where you think the company would be based in the future, where the founders would sit? How you're thinking about hiring?
And before, before we jump into that, I'm excited to know more, but very quickly, wanted to understand, how are you thinking about the company? Where, where you think the company would be based in the future, where the founders would sit? How you're thinking about hiring?
S Speaker 29:59Yeah, that's a good question. Actually, one, and I will be going to, we're actually planning on building our US base right now. We're, you know, we're headquartered in Singapore as right now, with Beijing office. The next one we're going to build is, hopefully Palo Alto, or around that area, because we, right now, we have about 30 people, about 70% of them are research scientists. And then only out of the 30 people, only 10 people are in the Beijing office. Another 10 is scattered around the North American actually, we have people joining us from DeepMind, Google. We have people joining us from, you know, open, AI and stuff, very experienced senior researchers. So we're building a base hopefully. We're planning on Palo Alto right now with that's going to take place, hopefully by the end of this year or the beginning of next year, that we're going to have an office there, and our founder one, he will, he'll be going there and stay there for, hopefully for the next few years, at least. And then we are also actively communicating with a lot of VCs from the Silicon Valley areas, and also in front of states. That's our plan for base, yeah,
Yeah, that's a good question. Actually, one, and I will be going to, we're actually planning on building our US base right now. We're, you know, we're headquartered in Singapore as right now, with Beijing office. The next one we're going to build is, hopefully Palo Alto, or around that area, because we, right now, we have about 30 people, about 70% of them are research scientists. And then only out of the 30 people, only 10 people are in the Beijing office. Another 10 is scattered around the North American actually, we have people joining us from DeepMind, Google. We have people joining us from, you know, open, AI and stuff, very experienced senior researchers. So we're building a base hopefully. We're planning on Palo Alto right now with that's going to take place, hopefully by the end of this year or the beginning of next year, that we're going to have an office there, and our founder one, he will, he'll be going there and stay there for, hopefully for the next few years, at least. And then we are also actively communicating with a lot of VCs from the Silicon Valley areas, and also in front of states. That's our plan for base, yeah,
Yeah, that's a good question. Actually, one, and I will be going to, we're actually planning on building our US base right now. We're, you know, we're headquartered in Singapore as right now, with Beijing office. The next one we're going to build is, hopefully Palo Alto, or around that area, because we, right now, we have about 30 people, about 70% of them are research scientists. And then only out of the 30 people, only 10 people are in the Beijing office. Another 10 is scattered around the North American actually, we have people joining us from DeepMind, Google. We have people joining us from, you know, open, AI and stuff, very experienced senior researchers. So we're building a base hopefully. We're planning on Palo Alto right now with that's going to take place, hopefully by the end of this year or the beginning of next year, that we're going to have an office there, and our founder one, he will, he'll be going there and stay there for, hopefully for the next few years, at least. And then we are also actively communicating with a lot of VCs from the Silicon Valley areas, and also in front of states. That's our plan for base, yeah,
Yeah, that's a good question. Actually, one, and I will be going to, we're actually planning on building our US base right now. We're, you know, we're headquartered in Singapore as right now, with Beijing office. The next one we're going to build is, hopefully Palo Alto, or around that area, because we, right now, we have about 30 people, about 70% of them are research scientists. And then only out of the 30 people, only 10 people are in the Beijing office. Another 10 is scattered around the North American actually, we have people joining us from DeepMind, Google. We have people joining us from, you know, open, AI and stuff, very experienced senior researchers. So we're building a base hopefully. We're planning on Palo Alto right now with that's going to take place, hopefully by the end of this year or the beginning of next year, that we're going to have an office there, and our founder one, he will, he'll be going there and stay there for, hopefully for the next few years, at least. And then we are also actively communicating with a lot of VCs from the Silicon Valley areas, and also in front of states. That's our plan for base, yeah,
S Speaker 111:14when you move here, let me know. Would love to sort of meet in person as well. We the entire Qualcomm and just team we are here and in San Diego, so most of the team members are here in Palo Alto, closer to that area, so we can meet in person the next
when you move here, let me know. Would love to sort of meet in person as well. We the entire Qualcomm and just team we are here and in San Diego, so most of the team members are here in Palo Alto, closer to that area, so we can meet in person the next
when you move here, let me know. Would love to sort of meet in person as well. We the entire Qualcomm and just team we are here and in San Diego, so most of the team members are here in Palo Alto, closer to that area, so we can meet in person the next
when you move here, let me know. Would love to sort of meet in person as well. We the entire Qualcomm and just team we are here and in San Diego, so most of the team members are here in Palo Alto, closer to that area, so we can meet in person the next
S Speaker 211:27time. Yeah, that's right. Actually, I stayed in San Diego for a call for years, actually, okay, I think I've been through in front of the headquarters, Qualcomm headquarters. Yeah, right? Yeah, very beautiful. I'm very familiar with that. We'll definitely meet with it in person when we when we get there, I'll let you know,
time. Yeah, that's right. Actually, I stayed in San Diego for a call for years, actually, okay, I think I've been through in front of the headquarters, Qualcomm headquarters. Yeah, right? Yeah, very beautiful. I'm very familiar with that. We'll definitely meet with it in person when we when we get there, I'll let you know,
time. Yeah, that's right. Actually, I stayed in San Diego for a call for years, actually, okay, I think I've been through in front of the headquarters, Qualcomm headquarters. Yeah, right? Yeah, very beautiful. I'm very familiar with that. We'll definitely meet with it in person when we when we get there, I'll let you know,
time. Yeah, that's right. Actually, I stayed in San Diego for a call for years, actually, okay, I think I've been through in front of the headquarters, Qualcomm headquarters. Yeah, right? Yeah, very beautiful. I'm very familiar with that. We'll definitely meet with it in person when we when we get there, I'll let you know,
11:50yeah, yes, already.
12:11Are you seeing the slides?
Are you seeing the slides?
Are you seeing the slides?
Are you seeing the slides?
12:12Yes, it's loading right now.
Yes, it's loading right now.
Yes, it's loading right now.
Yes, it's loading right now.
S Speaker 212:17Okay, perfect. It's from about, actually, from August. I think a little some of the information on here is a little bit outdated, but I'll, I'll keep you in track, and I'll walk you through the actual business plan, when, when it comes out in, in November. So this one basically briefly introduces the HRM architecture and the reasoning models. I'm sure you probably know the name. So a little bit about ourselves. We do call ourselves global AGI research company. We're about 70% on research and then 30 to 40% on engineering and product, you know, making making our research into viable products and deployments and such. So we our core idea is building novel AI architectures by using reinforcement learning evolution algorithms and emphasize findings. So basically, right now, we do think we are still a little bit far away from AGI in short terms, we're still looking to solve a few scientific problems that is continual learning, lifelong learning. The model doesn't stop right there when you stop training it, but it keeps learning. And also we call autonomous iterations, so the model will, by itself, know how to adapt and develop on its own, and also adaptability, which is what we call zero shop capability. It can tackle things that's never seen before or it's not can tackle things that's not in the training set. So we're using RL evolution algorithms and neuroscience findings to hopefully build new architecture that solves those scientific problems. There's a little, this is a little bit outdated, but, you know, large language models right now, they are all probability models, so everything's based on basically guesstimations. But right now, we provided them with, you know, just a lot of a lot of data, so they make really accurate assumptions or guesses. So that's large language models. So the reason why we're building a new architecture is we do want AIS to think like humans. So we're building thinking mechanisms, taking inspiration from the brain, and then put it into large models to make it to give it like super human capability. That's that's why we're building a new architecture. You probably heard of the scaling law. We're feeding a lot of a lot of data into the large language models. But you know, the Moore's Law of Large language models is set to be broken by 2028 ish, that's when the existing public data for AI training is depleted. Ilya is a vix supporter of a new architecture. That's part of the reason why he left opening AI for SSI. They're building a new architecture. It's just more it's just more safety oriented, but for us, it's more reasoning, and, you know, evolutionary oriented. So that's why we launched hierarchical reasoning model. It's a reasoning model as well as a architecture itself. It can be trained into various expert models. So we we take inspiration from the brain's hierarchical recurrence and fast and slow, fast and slow thinking process. So we basically have a low level acts as a faster worker module handling the detailed computations. And on top of that, we have a high level that's acting as a slower controller module that monitors the overall progress and controls the convergence of the model. So if you look at the conversion graph, the high level is continue. It's keep resetting the low level and then making it to generate new thoughts and then monitors entire convergence of the model.
Okay, perfect. It's from about, actually, from August. I think a little some of the information on here is a little bit outdated, but I'll, I'll keep you in track, and I'll walk you through the actual business plan, when, when it comes out in, in November. So this one basically briefly introduces the HRM architecture and the reasoning models. I'm sure you probably know the name. So a little bit about ourselves. We do call ourselves global AGI research company. We're about 70% on research and then 30 to 40% on engineering and product, you know, making making our research into viable products and deployments and such. So we our core idea is building novel AI architectures by using reinforcement learning evolution algorithms and emphasize findings. So basically, right now, we do think we are still a little bit far away from AGI in short terms, we're still looking to solve a few scientific problems that is continual learning, lifelong learning. The model doesn't stop right there when you stop training it, but it keeps learning. And also we call autonomous iterations, so the model will, by itself, know how to adapt and develop on its own, and also adaptability, which is what we call zero shop capability. It can tackle things that's never seen before or it's not can tackle things that's not in the training set. So we're using RL evolution algorithms and neuroscience findings to hopefully build new architecture that solves those scientific problems. There's a little, this is a little bit outdated, but, you know, large language models right now, they are all probability models, so everything's based on basically guesstimations. But right now, we provided them with, you know, just a lot of a lot of data, so they make really accurate assumptions or guesses. So that's large language models. So the reason why we're building a new architecture is we do want AIS to think like humans. So we're building thinking mechanisms, taking inspiration from the brain, and then put it into large models to make it to give it like super human capability. That's that's why we're building a new architecture. You probably heard of the scaling law. We're feeding a lot of a lot of data into the large language models. But you know, the Moore's Law of Large language models is set to be broken by 2028 ish, that's when the existing public data for AI training is depleted. Ilya is a vix supporter of a new architecture. That's part of the reason why he left opening AI for SSI. They're building a new architecture. It's just more it's just more safety oriented, but for us, it's more reasoning, and, you know, evolutionary oriented. So that's why we launched hierarchical reasoning model. It's a reasoning model as well as a architecture itself. It can be trained into various expert models. So we we take inspiration from the brain's hierarchical recurrence and fast and slow, fast and slow thinking process. So we basically have a low level acts as a faster worker module handling the detailed computations. And on top of that, we have a high level that's acting as a slower controller module that monitors the overall progress and controls the convergence of the model. So if you look at the conversion graph, the high level is continue. It's keep resetting the low level and then making it to generate new thoughts and then monitors entire convergence of the model.
Okay, perfect. It's from about, actually, from August. I think a little some of the information on here is a little bit outdated, but I'll, I'll keep you in track, and I'll walk you through the actual business plan, when, when it comes out in, in November. So this one basically briefly introduces the HRM architecture and the reasoning models. I'm sure you probably know the name. So a little bit about ourselves. We do call ourselves global AGI research company. We're about 70% on research and then 30 to 40% on engineering and product, you know, making making our research into viable products and deployments and such. So we our core idea is building novel AI architectures by using reinforcement learning evolution algorithms and emphasize findings. So basically, right now, we do think we are still a little bit far away from AGI in short terms, we're still looking to solve a few scientific problems that is continual learning, lifelong learning. The model doesn't stop right there when you stop training it, but it keeps learning. And also we call autonomous iterations, so the model will, by itself, know how to adapt and develop on its own, and also adaptability, which is what we call zero shop capability. It can tackle things that's never seen before or it's not can tackle things that's not in the training set. So we're using RL evolution algorithms and neuroscience findings to hopefully build new architecture that solves those scientific problems. There's a little, this is a little bit outdated, but, you know, large language models right now, they are all probability models, so everything's based on basically guesstimations. But right now, we provided them with, you know, just a lot of a lot of data, so they make really accurate assumptions or guesses. So that's large language models. So the reason why we're building a new architecture is we do want AIS to think like humans. So we're building thinking mechanisms, taking inspiration from the brain, and then put it into large models to make it to give it like super human capability. That's that's why we're building a new architecture. You probably heard of the scaling law. We're feeding a lot of a lot of data into the large language models. But you know, the Moore's Law of Large language models is set to be broken by 2028 ish, that's when the existing public data for AI training is depleted. Ilya is a vix supporter of a new architecture. That's part of the reason why he left opening AI for SSI. They're building a new architecture. It's just more it's just more safety oriented, but for us, it's more reasoning, and, you know, evolutionary oriented. So that's why we launched hierarchical reasoning model. It's a reasoning model as well as a architecture itself. It can be trained into various expert models. So we we take inspiration from the brain's hierarchical recurrence and fast and slow, fast and slow thinking process. So we basically have a low level acts as a faster worker module handling the detailed computations. And on top of that, we have a high level that's acting as a slower controller module that monitors the overall progress and controls the convergence of the model. So if you look at the conversion graph, the high level is continue. It's keep resetting the low level and then making it to generate new thoughts and then monitors entire convergence of the model.
Okay, perfect. It's from about, actually, from August. I think a little some of the information on here is a little bit outdated, but I'll, I'll keep you in track, and I'll walk you through the actual business plan, when, when it comes out in, in November. So this one basically briefly introduces the HRM architecture and the reasoning models. I'm sure you probably know the name. So a little bit about ourselves. We do call ourselves global AGI research company. We're about 70% on research and then 30 to 40% on engineering and product, you know, making making our research into viable products and deployments and such. So we our core idea is building novel AI architectures by using reinforcement learning evolution algorithms and emphasize findings. So basically, right now, we do think we are still a little bit far away from AGI in short terms, we're still looking to solve a few scientific problems that is continual learning, lifelong learning. The model doesn't stop right there when you stop training it, but it keeps learning. And also we call autonomous iterations, so the model will, by itself, know how to adapt and develop on its own, and also adaptability, which is what we call zero shop capability. It can tackle things that's never seen before or it's not can tackle things that's not in the training set. So we're using RL evolution algorithms and neuroscience findings to hopefully build new architecture that solves those scientific problems. There's a little, this is a little bit outdated, but, you know, large language models right now, they are all probability models, so everything's based on basically guesstimations. But right now, we provided them with, you know, just a lot of a lot of data, so they make really accurate assumptions or guesses. So that's large language models. So the reason why we're building a new architecture is we do want AIS to think like humans. So we're building thinking mechanisms, taking inspiration from the brain, and then put it into large models to make it to give it like super human capability. That's that's why we're building a new architecture. You probably heard of the scaling law. We're feeding a lot of a lot of data into the large language models. But you know, the Moore's Law of Large language models is set to be broken by 2028 ish, that's when the existing public data for AI training is depleted. Ilya is a vix supporter of a new architecture. That's part of the reason why he left opening AI for SSI. They're building a new architecture. It's just more it's just more safety oriented, but for us, it's more reasoning, and, you know, evolutionary oriented. So that's why we launched hierarchical reasoning model. It's a reasoning model as well as a architecture itself. It can be trained into various expert models. So we we take inspiration from the brain's hierarchical recurrence and fast and slow, fast and slow thinking process. So we basically have a low level acts as a faster worker module handling the detailed computations. And on top of that, we have a high level that's acting as a slower controller module that monitors the overall progress and controls the convergence of the model. So if you look at the conversion graph, the high level is continue. It's keep resetting the low level and then making it to generate new thoughts and then monitors entire convergence of the model.
S Speaker 116:10And I'd love to sort of understand that a little bit better. What do you mean by convergence one and both the L level and edge level models, they are both transformer based, or they have different architectures themselves as well.
And I'd love to sort of understand that a little bit better. What do you mean by convergence one and both the L level and edge level models, they are both transformer based, or they have different architectures themselves as well.
And I'd love to sort of understand that a little bit better. What do you mean by convergence one and both the L level and edge level models, they are both transformer based, or they have different architectures themselves as well.
And I'd love to sort of understand that a little bit better. What do you mean by convergence one and both the L level and edge level models, they are both transformer based, or they have different architectures themselves as well.
S Speaker 216:27Yes, they are actually high level is not a transformer based level. Low Level is a transformer based architecture acts as a worker. Level that's, you know, it's actually generating the detailed computations, high level will be what we call a more like a will be more like a self critical level. It will take the feedback from the low level and then, basically, by convergence, we mean milling. It directs where where the model goes in terms of thinking process. And eventually it will converge into a single point, and that will be the output. Got it. Got it, yes. And with this architecture, we are putting all the input tokens into the encoder simultaneously, as compared to, you know, in in transformer only models, they are put into the encoders sequentially or in series. So each of the tokens are generated one by one in transformer only models, and the next token is generated on top of the results from the last token. And that's basically where hallucinations form. So if you get one token diverged from the answer, the next token will be diverted further from the answer, and then eventually we'll get a very large hallucinations. But by putting them in parallel, we'll get a we can drastically reduce hallucination, and it will give us the second point here will give us a self correction capability as well.
Yes, they are actually high level is not a transformer based level. Low Level is a transformer based architecture acts as a worker. Level that's, you know, it's actually generating the detailed computations, high level will be what we call a more like a will be more like a self critical level. It will take the feedback from the low level and then, basically, by convergence, we mean milling. It directs where where the model goes in terms of thinking process. And eventually it will converge into a single point, and that will be the output. Got it. Got it, yes. And with this architecture, we are putting all the input tokens into the encoder simultaneously, as compared to, you know, in in transformer only models, they are put into the encoders sequentially or in series. So each of the tokens are generated one by one in transformer only models, and the next token is generated on top of the results from the last token. And that's basically where hallucinations form. So if you get one token diverged from the answer, the next token will be diverted further from the answer, and then eventually we'll get a very large hallucinations. But by putting them in parallel, we'll get a we can drastically reduce hallucination, and it will give us the second point here will give us a self correction capability as well.
Yes, they are actually high level is not a transformer based level. Low Level is a transformer based architecture acts as a worker. Level that's, you know, it's actually generating the detailed computations, high level will be what we call a more like a will be more like a self critical level. It will take the feedback from the low level and then, basically, by convergence, we mean milling. It directs where where the model goes in terms of thinking process. And eventually it will converge into a single point, and that will be the output. Got it. Got it, yes. And with this architecture, we are putting all the input tokens into the encoder simultaneously, as compared to, you know, in in transformer only models, they are put into the encoders sequentially or in series. So each of the tokens are generated one by one in transformer only models, and the next token is generated on top of the results from the last token. And that's basically where hallucinations form. So if you get one token diverged from the answer, the next token will be diverted further from the answer, and then eventually we'll get a very large hallucinations. But by putting them in parallel, we'll get a we can drastically reduce hallucination, and it will give us the second point here will give us a self correction capability as well.
Yes, they are actually high level is not a transformer based level. Low Level is a transformer based architecture acts as a worker. Level that's, you know, it's actually generating the detailed computations, high level will be what we call a more like a will be more like a self critical level. It will take the feedback from the low level and then, basically, by convergence, we mean milling. It directs where where the model goes in terms of thinking process. And eventually it will converge into a single point, and that will be the output. Got it. Got it, yes. And with this architecture, we are putting all the input tokens into the encoder simultaneously, as compared to, you know, in in transformer only models, they are put into the encoders sequentially or in series. So each of the tokens are generated one by one in transformer only models, and the next token is generated on top of the results from the last token. And that's basically where hallucinations form. So if you get one token diverged from the answer, the next token will be diverted further from the answer, and then eventually we'll get a very large hallucinations. But by putting them in parallel, we'll get a we can drastically reduce hallucination, and it will give us the second point here will give us a self correction capability as well.
S Speaker 117:56That's fair. Just a quick question there as well. When you say convergence, and the high level model is sort of directing the output towards one particular convergence. Do you need a lot of, say, input, output, eval, data, to do that, or can you zero short it? And if so, how are you zero shorting it towards one output?
That's fair. Just a quick question there as well. When you say convergence, and the high level model is sort of directing the output towards one particular convergence. Do you need a lot of, say, input, output, eval, data, to do that, or can you zero short it? And if so, how are you zero shorting it towards one output?
That's fair. Just a quick question there as well. When you say convergence, and the high level model is sort of directing the output towards one particular convergence. Do you need a lot of, say, input, output, eval, data, to do that, or can you zero short it? And if so, how are you zero shorting it towards one output?
That's fair. Just a quick question there as well. When you say convergence, and the high level model is sort of directing the output towards one particular convergence. Do you need a lot of, say, input, output, eval, data, to do that, or can you zero short it? And if so, how are you zero shorting it towards one output?
S Speaker 218:18That's That's a very good question. So the convergence in the previous slides is in in the inference level and not in the training level, but in the in the training level, we'll provide it with the with actually with a very small sample of learning. With this mechanism, it will train with a very great results. Well, there's the slides showing the actual advantages we're going to get. I'm going to jump, jump to the slides as right now. So for normally, for large language models, without the thinking mechanism, without our architecture, you will need at least hundreds of 1000s of samples, or at least or even millions of samples to train for you to get a good enough result, or for have the model converge to a good enough results, right? But for us, with this mechanism and architecture, we only need 1000s, if not even hundreds, of samples, we can get to very great results, and that results in a very much smaller model and much greater reasoning. So if you were to look at the some of the benchmarks rkgi You might, you might have, it's a it's a benchmark for, it's a very famous benchmark for finding rules between different change of geographic, not geographic, sorry, graphical changes. So it will provide you with, it's basically like an IQ testing set. We'll give you a few, you know, boxes with squares, triangles and whatever, and then you're going to find the next one, choose the next one. So we train on each of the models, we only give them 960 samples, or around 1000 samples. Hrm will learn much better and faster than all the other large language models. And then that's also based on without pre training. So it's a very small model, 0.027 be as composed, as opposed to 100 something B or 500 something B. And it can also do something that's even impossible to be achieved using language models, something like Sudoku, extreme, very hard Sudoku maze. These are something that's basically impossible to be done using language models. These are very hard reasoning tasks. Yeah, let me get back to the site, and one of the reasons we can achieve that is we have a much greater inference depth. That's one of the big reasons we can eliminate a lot of the hallucinations and also do a lot of advanced reasoning, and that's also contributed to by the new architecture. So for transformer only models, they only have around 64 layers. That applies to GPT, four, GP five, deep, seek, you name it. But for us, we can stretch it to hundreds and even 1000s of layers. Right now, we'll keep it around 500 it's it's enough for most of the tasks. If you were to ask both of the models like something like a rather complicated question that requires 200 layers for transformer only models, it will break it down using what we call chain of thoughts, break it down into little steps, and then might give you something that's not quite there, and you're going to have to ask them, oh, think again. It's going to take that output feed it back into the input again, and maybe eventually we'll get to the right answer, or maybe eventually we'll never get to the right answer. Yeah. So that's the advantages of a greater reasoning and death. And also quick question, sorry to break you again. Just yeah, no, no, go ahead. This is, this is good. Yeah, one
That's That's a very good question. So the convergence in the previous slides is in in the inference level and not in the training level, but in the in the training level, we'll provide it with the with actually with a very small sample of learning. With this mechanism, it will train with a very great results. Well, there's the slides showing the actual advantages we're going to get. I'm going to jump, jump to the slides as right now. So for normally, for large language models, without the thinking mechanism, without our architecture, you will need at least hundreds of 1000s of samples, or at least or even millions of samples to train for you to get a good enough result, or for have the model converge to a good enough results, right? But for us, with this mechanism and architecture, we only need 1000s, if not even hundreds, of samples, we can get to very great results, and that results in a very much smaller model and much greater reasoning. So if you were to look at the some of the benchmarks rkgi You might, you might have, it's a it's a benchmark for, it's a very famous benchmark for finding rules between different change of geographic, not geographic, sorry, graphical changes. So it will provide you with, it's basically like an IQ testing set. We'll give you a few, you know, boxes with squares, triangles and whatever, and then you're going to find the next one, choose the next one. So we train on each of the models, we only give them 960 samples, or around 1000 samples. Hrm will learn much better and faster than all the other large language models. And then that's also based on without pre training. So it's a very small model, 0.027 be as composed, as opposed to 100 something B or 500 something B. And it can also do something that's even impossible to be achieved using language models, something like Sudoku, extreme, very hard Sudoku maze. These are something that's basically impossible to be done using language models. These are very hard reasoning tasks. Yeah, let me get back to the site, and one of the reasons we can achieve that is we have a much greater inference depth. That's one of the big reasons we can eliminate a lot of the hallucinations and also do a lot of advanced reasoning, and that's also contributed to by the new architecture. So for transformer only models, they only have around 64 layers. That applies to GPT, four, GP five, deep, seek, you name it. But for us, we can stretch it to hundreds and even 1000s of layers. Right now, we'll keep it around 500 it's it's enough for most of the tasks. If you were to ask both of the models like something like a rather complicated question that requires 200 layers for transformer only models, it will break it down using what we call chain of thoughts, break it down into little steps, and then might give you something that's not quite there, and you're going to have to ask them, oh, think again. It's going to take that output feed it back into the input again, and maybe eventually we'll get to the right answer, or maybe eventually we'll never get to the right answer. Yeah. So that's the advantages of a greater reasoning and death. And also quick question, sorry to break you again. Just yeah, no, no, go ahead. This is, this is good. Yeah, one
That's That's a very good question. So the convergence in the previous slides is in in the inference level and not in the training level, but in the in the training level, we'll provide it with the with actually with a very small sample of learning. With this mechanism, it will train with a very great results. Well, there's the slides showing the actual advantages we're going to get. I'm going to jump, jump to the slides as right now. So for normally, for large language models, without the thinking mechanism, without our architecture, you will need at least hundreds of 1000s of samples, or at least or even millions of samples to train for you to get a good enough result, or for have the model converge to a good enough results, right? But for us, with this mechanism and architecture, we only need 1000s, if not even hundreds, of samples, we can get to very great results, and that results in a very much smaller model and much greater reasoning. So if you were to look at the some of the benchmarks rkgi You might, you might have, it's a it's a benchmark for, it's a very famous benchmark for finding rules between different change of geographic, not geographic, sorry, graphical changes. So it will provide you with, it's basically like an IQ testing set. We'll give you a few, you know, boxes with squares, triangles and whatever, and then you're going to find the next one, choose the next one. So we train on each of the models, we only give them 960 samples, or around 1000 samples. Hrm will learn much better and faster than all the other large language models. And then that's also based on without pre training. So it's a very small model, 0.027 be as composed, as opposed to 100 something B or 500 something B. And it can also do something that's even impossible to be achieved using language models, something like Sudoku, extreme, very hard Sudoku maze. These are something that's basically impossible to be done using language models. These are very hard reasoning tasks. Yeah, let me get back to the site, and one of the reasons we can achieve that is we have a much greater inference depth. That's one of the big reasons we can eliminate a lot of the hallucinations and also do a lot of advanced reasoning, and that's also contributed to by the new architecture. So for transformer only models, they only have around 64 layers. That applies to GPT, four, GP five, deep, seek, you name it. But for us, we can stretch it to hundreds and even 1000s of layers. Right now, we'll keep it around 500 it's it's enough for most of the tasks. If you were to ask both of the models like something like a rather complicated question that requires 200 layers for transformer only models, it will break it down using what we call chain of thoughts, break it down into little steps, and then might give you something that's not quite there, and you're going to have to ask them, oh, think again. It's going to take that output feed it back into the input again, and maybe eventually we'll get to the right answer, or maybe eventually we'll never get to the right answer. Yeah. So that's the advantages of a greater reasoning and death. And also quick question, sorry to break you again. Just yeah, no, no, go ahead. This is, this is good. Yeah, one
That's That's a very good question. So the convergence in the previous slides is in in the inference level and not in the training level, but in the in the training level, we'll provide it with the with actually with a very small sample of learning. With this mechanism, it will train with a very great results. Well, there's the slides showing the actual advantages we're going to get. I'm going to jump, jump to the slides as right now. So for normally, for large language models, without the thinking mechanism, without our architecture, you will need at least hundreds of 1000s of samples, or at least or even millions of samples to train for you to get a good enough result, or for have the model converge to a good enough results, right? But for us, with this mechanism and architecture, we only need 1000s, if not even hundreds, of samples, we can get to very great results, and that results in a very much smaller model and much greater reasoning. So if you were to look at the some of the benchmarks rkgi You might, you might have, it's a it's a benchmark for, it's a very famous benchmark for finding rules between different change of geographic, not geographic, sorry, graphical changes. So it will provide you with, it's basically like an IQ testing set. We'll give you a few, you know, boxes with squares, triangles and whatever, and then you're going to find the next one, choose the next one. So we train on each of the models, we only give them 960 samples, or around 1000 samples. Hrm will learn much better and faster than all the other large language models. And then that's also based on without pre training. So it's a very small model, 0.027 be as composed, as opposed to 100 something B or 500 something B. And it can also do something that's even impossible to be achieved using language models, something like Sudoku, extreme, very hard Sudoku maze. These are something that's basically impossible to be done using language models. These are very hard reasoning tasks. Yeah, let me get back to the site, and one of the reasons we can achieve that is we have a much greater inference depth. That's one of the big reasons we can eliminate a lot of the hallucinations and also do a lot of advanced reasoning, and that's also contributed to by the new architecture. So for transformer only models, they only have around 64 layers. That applies to GPT, four, GP five, deep, seek, you name it. But for us, we can stretch it to hundreds and even 1000s of layers. Right now, we'll keep it around 500 it's it's enough for most of the tasks. If you were to ask both of the models like something like a rather complicated question that requires 200 layers for transformer only models, it will break it down using what we call chain of thoughts, break it down into little steps, and then might give you something that's not quite there, and you're going to have to ask them, oh, think again. It's going to take that output feed it back into the input again, and maybe eventually we'll get to the right answer, or maybe eventually we'll never get to the right answer. Yeah. So that's the advantages of a greater reasoning and death. And also quick question, sorry to break you again. Just yeah, no, no, go ahead. This is, this is good. Yeah, one
S Speaker 124:31between us, absolutely you can trust me, it will be between you and I, and probably a couple of members of my Qualcomm interest team. But very quickly, we have a strong thesis in embodied AI as well. We are working with a few leading, I would say, humanoid robot companies, companies building brains for robots. So great. Great to hear that we can absolutely look at some collaborations over there as well.
between us, absolutely you can trust me, it will be between you and I, and probably a couple of members of my Qualcomm interest team. But very quickly, we have a strong thesis in embodied AI as well. We are working with a few leading, I would say, humanoid robot companies, companies building brains for robots. So great. Great to hear that we can absolutely look at some collaborations over there as well.
between us, absolutely you can trust me, it will be between you and I, and probably a couple of members of my Qualcomm interest team. But very quickly, we have a strong thesis in embodied AI as well. We are working with a few leading, I would say, humanoid robot companies, companies building brains for robots. So great. Great to hear that we can absolutely look at some collaborations over there as well.
between us, absolutely you can trust me, it will be between you and I, and probably a couple of members of my Qualcomm interest team. But very quickly, we have a strong thesis in embodied AI as well. We are working with a few leading, I would say, humanoid robot companies, companies building brains for robots. So great. Great to hear that we can absolutely look at some collaborations over there as well.
S Speaker 224:54Yeah, that will be very exciting. We have a we just aboarded a few robotics experts as well. Gwen and I, we are also robotics experts. I used to work at DJI, the drone company, so this will be something that we are really looking to, to actually deploy that. And if not, it will be a very lightweight image generation model as well. You can, you can probably make that into, I don't know, like, like, a locally deployed Photoshop, AI and stuff, or locally deployed nano banana 500
Yeah, that will be very exciting. We have a we just aboarded a few robotics experts as well. Gwen and I, we are also robotics experts. I used to work at DJI, the drone company, so this will be something that we are really looking to, to actually deploy that. And if not, it will be a very lightweight image generation model as well. You can, you can probably make that into, I don't know, like, like, a locally deployed Photoshop, AI and stuff, or locally deployed nano banana 500
Yeah, that will be very exciting. We have a we just aboarded a few robotics experts as well. Gwen and I, we are also robotics experts. I used to work at DJI, the drone company, so this will be something that we are really looking to, to actually deploy that. And if not, it will be a very lightweight image generation model as well. You can, you can probably make that into, I don't know, like, like, a locally deployed Photoshop, AI and stuff, or locally deployed nano banana 500
Yeah, that will be very exciting. We have a we just aboarded a few robotics experts as well. Gwen and I, we are also robotics experts. I used to work at DJI, the drone company, so this will be something that we are really looking to, to actually deploy that. And if not, it will be a very lightweight image generation model as well. You can, you can probably make that into, I don't know, like, like, a locally deployed Photoshop, AI and stuff, or locally deployed nano banana 500
S Speaker 125:26quality of that model compare with the lives of nano banana seed dancers model.
quality of that model compare with the lives of nano banana seed dancers model.
quality of that model compare with the lives of nano banana seed dancers model.
quality of that model compare with the lives of nano banana seed dancers model.
S Speaker 225:32Actually it's it's very comparable with the same quality we are only about 15 to 20% of the size of their of their model, yeah, with the same output quality we already tested that. We're waiting for a right time to launch that, and that's hopefully around our Series A so that's, that's the that's the exciting news. Let me just go back here. Yeah, we already went through the slides. We were kind of just, you know, in the back and forth, arcade gi already introduced that. That's, that's basically our KGI. So for the first few examples, on the right, you can probably see, the first one you fill the corners with the yellow box. The second one, you move all the light blue blocks into the bottom. These are very these are simplified examples. The actual questions are way harder. So we are. We achieved 41% on Archi one and 4% on Archi two. We're already beating Oh, three, Mini, o4, Mini, and with a 0.02 7b, small model. Theirs is around at least 200 people. They didn't disclose it with it's probably around 200 B or even bigger. And the cost per task is just very slow, very small. We're the cheapest. So that that in a nutshell, we have a few very great advantages. One is the very small form factor. We can perform deep, advanced reasoning with no pre training or CLT at all, and with a size of 0.02, 7b we can now perform 200 plus b models in reasoning tasks. It will be optimal for edge or lightweight deployment. Right now, we are already deploying this model in a lot of clinical research facilities, because they haven't, you know, they have to run all the data in locally, on their servers, on their laptops and stuff. And secondly, we are very sensitive to numbers and patterns. As you might see, it will be very optimal for time series data training, forecast prediction, numeric data processing. I'm going to go back and forth a little bit. So one of the classic application we are running on that is climate prediction. We're actually already at Sultan level that that is a time series data. You look at your take some of the data from past and do forecast for the for the future. We're also doing quant trading. We have a team on quant trading that's collaborating with some of the firms already. We have also a team on medical AI, you know, health data is also time series, so that will be something we're really sensitive and good at, much better than large language models. And thirdly, we're really good at complex, what we call multi skill or multi input problem reasoning capability. So So again, for climate prediction, it will be a multi skill problem. You have the synoptic scale that's ranging across a few days. You have the planetary scale that's ranging across few weeks, and then you have the intracellular skill that's ranging that's ranging across few months. You have something that's changing very fast, like, you know, wind, air temperature. You have something that's changing rather slow, like ocean and and stuff. So you have to take and take account of different time skills, and then together to make your forecast, and also for multi channel that will be, you have, we can put input, you know, you can put in 20 or 30 different channels for different variables, like wind, temperature and stuff. For medical AI, it's the same deal. You have your blood pressure, you have your something protein, or whatever, and it's going to do a it's very good at making predictions and other forecasts and also data analysis using multi channel or multi skills.
Actually it's it's very comparable with the same quality we are only about 15 to 20% of the size of their of their model, yeah, with the same output quality we already tested that. We're waiting for a right time to launch that, and that's hopefully around our Series A so that's, that's the that's the exciting news. Let me just go back here. Yeah, we already went through the slides. We were kind of just, you know, in the back and forth, arcade gi already introduced that. That's, that's basically our KGI. So for the first few examples, on the right, you can probably see, the first one you fill the corners with the yellow box. The second one, you move all the light blue blocks into the bottom. These are very these are simplified examples. The actual questions are way harder. So we are. We achieved 41% on Archi one and 4% on Archi two. We're already beating Oh, three, Mini, o4, Mini, and with a 0.02 7b, small model. Theirs is around at least 200 people. They didn't disclose it with it's probably around 200 B or even bigger. And the cost per task is just very slow, very small. We're the cheapest. So that that in a nutshell, we have a few very great advantages. One is the very small form factor. We can perform deep, advanced reasoning with no pre training or CLT at all, and with a size of 0.02, 7b we can now perform 200 plus b models in reasoning tasks. It will be optimal for edge or lightweight deployment. Right now, we are already deploying this model in a lot of clinical research facilities, because they haven't, you know, they have to run all the data in locally, on their servers, on their laptops and stuff. And secondly, we are very sensitive to numbers and patterns. As you might see, it will be very optimal for time series data training, forecast prediction, numeric data processing. I'm going to go back and forth a little bit. So one of the classic application we are running on that is climate prediction. We're actually already at Sultan level that that is a time series data. You look at your take some of the data from past and do forecast for the for the future. We're also doing quant trading. We have a team on quant trading that's collaborating with some of the firms already. We have also a team on medical AI, you know, health data is also time series, so that will be something we're really sensitive and good at, much better than large language models. And thirdly, we're really good at complex, what we call multi skill or multi input problem reasoning capability. So So again, for climate prediction, it will be a multi skill problem. You have the synoptic scale that's ranging across a few days. You have the planetary scale that's ranging across few weeks, and then you have the intracellular skill that's ranging that's ranging across few months. You have something that's changing very fast, like, you know, wind, air temperature. You have something that's changing rather slow, like ocean and and stuff. So you have to take and take account of different time skills, and then together to make your forecast, and also for multi channel that will be, you have, we can put input, you know, you can put in 20 or 30 different channels for different variables, like wind, temperature and stuff. For medical AI, it's the same deal. You have your blood pressure, you have your something protein, or whatever, and it's going to do a it's very good at making predictions and other forecasts and also data analysis using multi channel or multi skills.
Actually it's it's very comparable with the same quality we are only about 15 to 20% of the size of their of their model, yeah, with the same output quality we already tested that. We're waiting for a right time to launch that, and that's hopefully around our Series A so that's, that's the that's the exciting news. Let me just go back here. Yeah, we already went through the slides. We were kind of just, you know, in the back and forth, arcade gi already introduced that. That's, that's basically our KGI. So for the first few examples, on the right, you can probably see, the first one you fill the corners with the yellow box. The second one, you move all the light blue blocks into the bottom. These are very these are simplified examples. The actual questions are way harder. So we are. We achieved 41% on Archi one and 4% on Archi two. We're already beating Oh, three, Mini, o4, Mini, and with a 0.02 7b, small model. Theirs is around at least 200 people. They didn't disclose it with it's probably around 200 B or even bigger. And the cost per task is just very slow, very small. We're the cheapest. So that that in a nutshell, we have a few very great advantages. One is the very small form factor. We can perform deep, advanced reasoning with no pre training or CLT at all, and with a size of 0.02, 7b we can now perform 200 plus b models in reasoning tasks. It will be optimal for edge or lightweight deployment. Right now, we are already deploying this model in a lot of clinical research facilities, because they haven't, you know, they have to run all the data in locally, on their servers, on their laptops and stuff. And secondly, we are very sensitive to numbers and patterns. As you might see, it will be very optimal for time series data training, forecast prediction, numeric data processing. I'm going to go back and forth a little bit. So one of the classic application we are running on that is climate prediction. We're actually already at Sultan level that that is a time series data. You look at your take some of the data from past and do forecast for the for the future. We're also doing quant trading. We have a team on quant trading that's collaborating with some of the firms already. We have also a team on medical AI, you know, health data is also time series, so that will be something we're really sensitive and good at, much better than large language models. And thirdly, we're really good at complex, what we call multi skill or multi input problem reasoning capability. So So again, for climate prediction, it will be a multi skill problem. You have the synoptic scale that's ranging across a few days. You have the planetary scale that's ranging across few weeks, and then you have the intracellular skill that's ranging that's ranging across few months. You have something that's changing very fast, like, you know, wind, air temperature. You have something that's changing rather slow, like ocean and and stuff. So you have to take and take account of different time skills, and then together to make your forecast, and also for multi channel that will be, you have, we can put input, you know, you can put in 20 or 30 different channels for different variables, like wind, temperature and stuff. For medical AI, it's the same deal. You have your blood pressure, you have your something protein, or whatever, and it's going to do a it's very good at making predictions and other forecasts and also data analysis using multi channel or multi skills.
Actually it's it's very comparable with the same quality we are only about 15 to 20% of the size of their of their model, yeah, with the same output quality we already tested that. We're waiting for a right time to launch that, and that's hopefully around our Series A so that's, that's the that's the exciting news. Let me just go back here. Yeah, we already went through the slides. We were kind of just, you know, in the back and forth, arcade gi already introduced that. That's, that's basically our KGI. So for the first few examples, on the right, you can probably see, the first one you fill the corners with the yellow box. The second one, you move all the light blue blocks into the bottom. These are very these are simplified examples. The actual questions are way harder. So we are. We achieved 41% on Archi one and 4% on Archi two. We're already beating Oh, three, Mini, o4, Mini, and with a 0.02 7b, small model. Theirs is around at least 200 people. They didn't disclose it with it's probably around 200 B or even bigger. And the cost per task is just very slow, very small. We're the cheapest. So that that in a nutshell, we have a few very great advantages. One is the very small form factor. We can perform deep, advanced reasoning with no pre training or CLT at all, and with a size of 0.02, 7b we can now perform 200 plus b models in reasoning tasks. It will be optimal for edge or lightweight deployment. Right now, we are already deploying this model in a lot of clinical research facilities, because they haven't, you know, they have to run all the data in locally, on their servers, on their laptops and stuff. And secondly, we are very sensitive to numbers and patterns. As you might see, it will be very optimal for time series data training, forecast prediction, numeric data processing. I'm going to go back and forth a little bit. So one of the classic application we are running on that is climate prediction. We're actually already at Sultan level that that is a time series data. You look at your take some of the data from past and do forecast for the for the future. We're also doing quant trading. We have a team on quant trading that's collaborating with some of the firms already. We have also a team on medical AI, you know, health data is also time series, so that will be something we're really sensitive and good at, much better than large language models. And thirdly, we're really good at complex, what we call multi skill or multi input problem reasoning capability. So So again, for climate prediction, it will be a multi skill problem. You have the synoptic scale that's ranging across a few days. You have the planetary scale that's ranging across few weeks, and then you have the intracellular skill that's ranging that's ranging across few months. You have something that's changing very fast, like, you know, wind, air temperature. You have something that's changing rather slow, like ocean and and stuff. So you have to take and take account of different time skills, and then together to make your forecast, and also for multi channel that will be, you have, we can put input, you know, you can put in 20 or 30 different channels for different variables, like wind, temperature and stuff. For medical AI, it's the same deal. You have your blood pressure, you have your something protein, or whatever, and it's going to do a it's very good at making predictions and other forecasts and also data analysis using multi channel or multi skills.
S Speaker 229:42me as of right now. There isn't any benchmarks that's targeting these kind of areas, but you know for we will kind of just run the actual benchmarks in terms of applications like for weather forecast, we are about 25% more accurate than some of the SOTA level models. The ECM, WF is the European equivalent of NASA. It's basically the SOTA level model, where the fusi is a very famous Chinese model. There, they claim to be more accurate than the CW, MWF, but using actually, we're using just the open source data. We didn't even use any proprietary data to run and we got a 25% improvement in the uncertainty for weather forecast right now. We're actually collaborating with some of the firms, though they're they're working on, they're actually working on we call energy, energy transportation. They need a lot of weather forecasts. We're actually collaborating with them to on that matter. For why the weather forecast for Matt AI, we're actually collaborating with a few major hospitals in Singapore, and very soon, the US, I will be meeting with some of the head of hospitals in November as well. So we're kind of just testing this and benchmarking this in real application. There isn't a benchmark for multi channel, not, not that I know
me as of right now. There isn't any benchmarks that's targeting these kind of areas, but you know for we will kind of just run the actual benchmarks in terms of applications like for weather forecast, we are about 25% more accurate than some of the SOTA level models. The ECM, WF is the European equivalent of NASA. It's basically the SOTA level model, where the fusi is a very famous Chinese model. There, they claim to be more accurate than the CW, MWF, but using actually, we're using just the open source data. We didn't even use any proprietary data to run and we got a 25% improvement in the uncertainty for weather forecast right now. We're actually collaborating with some of the firms, though they're they're working on, they're actually working on we call energy, energy transportation. They need a lot of weather forecasts. We're actually collaborating with them to on that matter. For why the weather forecast for Matt AI, we're actually collaborating with a few major hospitals in Singapore, and very soon, the US, I will be meeting with some of the head of hospitals in November as well. So we're kind of just testing this and benchmarking this in real application. There isn't a benchmark for multi channel, not, not that I know
me as of right now. There isn't any benchmarks that's targeting these kind of areas, but you know for we will kind of just run the actual benchmarks in terms of applications like for weather forecast, we are about 25% more accurate than some of the SOTA level models. The ECM, WF is the European equivalent of NASA. It's basically the SOTA level model, where the fusi is a very famous Chinese model. There, they claim to be more accurate than the CW, MWF, but using actually, we're using just the open source data. We didn't even use any proprietary data to run and we got a 25% improvement in the uncertainty for weather forecast right now. We're actually collaborating with some of the firms, though they're they're working on, they're actually working on we call energy, energy transportation. They need a lot of weather forecasts. We're actually collaborating with them to on that matter. For why the weather forecast for Matt AI, we're actually collaborating with a few major hospitals in Singapore, and very soon, the US, I will be meeting with some of the head of hospitals in November as well. So we're kind of just testing this and benchmarking this in real application. There isn't a benchmark for multi channel, not, not that I know
me as of right now. There isn't any benchmarks that's targeting these kind of areas, but you know for we will kind of just run the actual benchmarks in terms of applications like for weather forecast, we are about 25% more accurate than some of the SOTA level models. The ECM, WF is the European equivalent of NASA. It's basically the SOTA level model, where the fusi is a very famous Chinese model. There, they claim to be more accurate than the CW, MWF, but using actually, we're using just the open source data. We didn't even use any proprietary data to run and we got a 25% improvement in the uncertainty for weather forecast right now. We're actually collaborating with some of the firms, though they're they're working on, they're actually working on we call energy, energy transportation. They need a lot of weather forecasts. We're actually collaborating with them to on that matter. For why the weather forecast for Matt AI, we're actually collaborating with a few major hospitals in Singapore, and very soon, the US, I will be meeting with some of the head of hospitals in November as well. So we're kind of just testing this and benchmarking this in real application. There isn't a benchmark for multi channel, not, not that I know
S Speaker 231:14but, but, you know, it's something that we're really good at. And lastly, we have a very great small sample activity. We already went through this one with, you know, 300 to 1000 samples, or even around that hundreds or 1000s of samples, we can learn much faster and better than any large language models out there. We're actually doing one of the projects is doing a this one, yeah, a protein ligand affinity prediction. So that's only based on a, I think, 500 samples, and it's and it's working much better than traditional machine learning models. So, so areas like AI for science, that's something they don't really have a plethora of data. They only have about like 500 samples for each of the targets that they're aiming for, proteins and that AI as well. Even some of the largest hospital they don't have a lot of samples for some rare diseases. That's something a small sample learning capability you will be really good at. So these are some of the core advantages provided by the architecture itself. And we're, you know, we're building a lot of expert models based on top of that, and and all of that can be run on a on laptop and even on a phone with with 100% of utility of our model. So that's something very exciting we can look into. Basically, you can build anything based on the this architecture, and very soon the generative side of the architecture will be released. We're planning on building image to photo, image, the picture generation and as well as video, very soon. So again, I mentioned this. It will be 500 times faster than the traditional VIP methods, and also very, very lightweight.
but, but, you know, it's something that we're really good at. And lastly, we have a very great small sample activity. We already went through this one with, you know, 300 to 1000 samples, or even around that hundreds or 1000s of samples, we can learn much faster and better than any large language models out there. We're actually doing one of the projects is doing a this one, yeah, a protein ligand affinity prediction. So that's only based on a, I think, 500 samples, and it's and it's working much better than traditional machine learning models. So, so areas like AI for science, that's something they don't really have a plethora of data. They only have about like 500 samples for each of the targets that they're aiming for, proteins and that AI as well. Even some of the largest hospital they don't have a lot of samples for some rare diseases. That's something a small sample learning capability you will be really good at. So these are some of the core advantages provided by the architecture itself. And we're, you know, we're building a lot of expert models based on top of that, and and all of that can be run on a on laptop and even on a phone with with 100% of utility of our model. So that's something very exciting we can look into. Basically, you can build anything based on the this architecture, and very soon the generative side of the architecture will be released. We're planning on building image to photo, image, the picture generation and as well as video, very soon. So again, I mentioned this. It will be 500 times faster than the traditional VIP methods, and also very, very lightweight.
but, but, you know, it's something that we're really good at. And lastly, we have a very great small sample activity. We already went through this one with, you know, 300 to 1000 samples, or even around that hundreds or 1000s of samples, we can learn much faster and better than any large language models out there. We're actually doing one of the projects is doing a this one, yeah, a protein ligand affinity prediction. So that's only based on a, I think, 500 samples, and it's and it's working much better than traditional machine learning models. So, so areas like AI for science, that's something they don't really have a plethora of data. They only have about like 500 samples for each of the targets that they're aiming for, proteins and that AI as well. Even some of the largest hospital they don't have a lot of samples for some rare diseases. That's something a small sample learning capability you will be really good at. So these are some of the core advantages provided by the architecture itself. And we're, you know, we're building a lot of expert models based on top of that, and and all of that can be run on a on laptop and even on a phone with with 100% of utility of our model. So that's something very exciting we can look into. Basically, you can build anything based on the this architecture, and very soon the generative side of the architecture will be released. We're planning on building image to photo, image, the picture generation and as well as video, very soon. So again, I mentioned this. It will be 500 times faster than the traditional VIP methods, and also very, very lightweight.
but, but, you know, it's something that we're really good at. And lastly, we have a very great small sample activity. We already went through this one with, you know, 300 to 1000 samples, or even around that hundreds or 1000s of samples, we can learn much faster and better than any large language models out there. We're actually doing one of the projects is doing a this one, yeah, a protein ligand affinity prediction. So that's only based on a, I think, 500 samples, and it's and it's working much better than traditional machine learning models. So, so areas like AI for science, that's something they don't really have a plethora of data. They only have about like 500 samples for each of the targets that they're aiming for, proteins and that AI as well. Even some of the largest hospital they don't have a lot of samples for some rare diseases. That's something a small sample learning capability you will be really good at. So these are some of the core advantages provided by the architecture itself. And we're, you know, we're building a lot of expert models based on top of that, and and all of that can be run on a on laptop and even on a phone with with 100% of utility of our model. So that's something very exciting we can look into. Basically, you can build anything based on the this architecture, and very soon the generative side of the architecture will be released. We're planning on building image to photo, image, the picture generation and as well as video, very soon. So again, I mentioned this. It will be 500 times faster than the traditional VIP methods, and also very, very lightweight.
33:17Can run on basically any quantum chips, absolutely.
Can run on basically any quantum chips, absolutely.
Can run on basically any quantum chips, absolutely.
Can run on basically any quantum chips, absolutely.
S Speaker 133:19William, this is, this is a super interesting proposition, for sure. Internally, I know we have a lot of efforts going on to sort of work with companies application as well as foundation, model and infra companies to enable the future where you will have more edge applications. We just see that that's something that will eventually happen the future, and Qualcomm has a big, big bet on it. I love the proposition at sapient that sounds super interesting. Couple of quick questions, William, I know we are over time, so I'll just keep it short. So you mentioned the roadmap looks like from here. There will be an image generation model, which you will be launching. And you also plan to launch a video generation, image to video models in the future. What's the is there plan to sort of launch the next gen reasoning model anytime soon. And where do you see that hit in terms of benchmark?
William, this is, this is a super interesting proposition, for sure. Internally, I know we have a lot of efforts going on to sort of work with companies application as well as foundation, model and infra companies to enable the future where you will have more edge applications. We just see that that's something that will eventually happen the future, and Qualcomm has a big, big bet on it. I love the proposition at sapient that sounds super interesting. Couple of quick questions, William, I know we are over time, so I'll just keep it short. So you mentioned the roadmap looks like from here. There will be an image generation model, which you will be launching. And you also plan to launch a video generation, image to video models in the future. What's the is there plan to sort of launch the next gen reasoning model anytime soon. And where do you see that hit in terms of benchmark?
William, this is, this is a super interesting proposition, for sure. Internally, I know we have a lot of efforts going on to sort of work with companies application as well as foundation, model and infra companies to enable the future where you will have more edge applications. We just see that that's something that will eventually happen the future, and Qualcomm has a big, big bet on it. I love the proposition at sapient that sounds super interesting. Couple of quick questions, William, I know we are over time, so I'll just keep it short. So you mentioned the roadmap looks like from here. There will be an image generation model, which you will be launching. And you also plan to launch a video generation, image to video models in the future. What's the is there plan to sort of launch the next gen reasoning model anytime soon. And where do you see that hit in terms of benchmark?
William, this is, this is a super interesting proposition, for sure. Internally, I know we have a lot of efforts going on to sort of work with companies application as well as foundation, model and infra companies to enable the future where you will have more edge applications. We just see that that's something that will eventually happen the future, and Qualcomm has a big, big bet on it. I love the proposition at sapient that sounds super interesting. Couple of quick questions, William, I know we are over time, so I'll just keep it short. So you mentioned the roadmap looks like from here. There will be an image generation model, which you will be launching. And you also plan to launch a video generation, image to video models in the future. What's the is there plan to sort of launch the next gen reasoning model anytime soon. And where do you see that hit in terms of benchmark?
S Speaker 234:17So in terms of benchmark, we can definitely compare to some of the SOTA level, image generation, even video generation models and we and even with the same level of performance, our model will, we're predicting it to be about at least 20% of the size, 20% not 20% less. It will be 20% of the 20% of the existing model at the very least. And we are, we're still optimizing that each, each of the days I'll go to our research team, then they're going to give me a new number. So that might, that might change drastically by the time we launch it. We can we for in terms of the model itself. We are actually done solving all the scientific problems. We're just right now,
So in terms of benchmark, we can definitely compare to some of the SOTA level, image generation, even video generation models and we and even with the same level of performance, our model will, we're predicting it to be about at least 20% of the size, 20% not 20% less. It will be 20% of the 20% of the existing model at the very least. And we are, we're still optimizing that each, each of the days I'll go to our research team, then they're going to give me a new number. So that might, that might change drastically by the time we launch it. We can we for in terms of the model itself. We are actually done solving all the scientific problems. We're just right now,
So in terms of benchmark, we can definitely compare to some of the SOTA level, image generation, even video generation models and we and even with the same level of performance, our model will, we're predicting it to be about at least 20% of the size, 20% not 20% less. It will be 20% of the 20% of the existing model at the very least. And we are, we're still optimizing that each, each of the days I'll go to our research team, then they're going to give me a new number. So that might, that might change drastically by the time we launch it. We can we for in terms of the model itself. We are actually done solving all the scientific problems. We're just right now,
So in terms of benchmark, we can definitely compare to some of the SOTA level, image generation, even video generation models and we and even with the same level of performance, our model will, we're predicting it to be about at least 20% of the size, 20% not 20% less. It will be 20% of the 20% of the existing model at the very least. And we are, we're still optimizing that each, each of the days I'll go to our research team, then they're going to give me a new number. So that might, that might change drastically by the time we launch it. We can we for in terms of the model itself. We are actually done solving all the scientific problems. We're just right now,
35:07to be very honest with you, we're collecting
to be very honest with you, we're collecting
to be very honest with you, we're collecting
to be very honest with you, we're collecting
S Speaker 235:11higher quality image data to further train the model to just to make, to make the performance better. Right now, we are only using open source data. It's, it's, it's like images from 10 years ago. It's very shitty, but it's already showing very, very promising results on this new architecture, the architecture itself, it will be the, it will be the actual gold that will be shining when we launch it, not the model itself. And, you know, it can always be improved using better data or better quality. So we, we, as of right now, we, we are planning to use that on heavily on embodied AI, because two of the main advantages, fast and small will both apply to robots as well as anything you know that requires very real time. Edge, about edge, deployment. Things could be, could be robot. Robots could be satellite, that might be one of the applications, or anything. There are a lot of things that you Qualcomm's might have in mind, and you might have in mind that'll be very interesting to look at. Yeah. So we're done with, we're done with the all the scientific problem behind the architecture, more like there's a few engineering challenges, including getting, you know, higher quality data images for training. Those are not really, you know, not really difficult. And it to be honest, if we were to release the architecture, it will take at most a few weeks or a month. But right now, we are also trying to match it with our series, a funding we want to, you know, gather all the other investor I'm talking to a lot of investors right now, once they're in, we're probably going to launch it right right after we finish series eight, just to give everyone a very good head start, it will be a very good thing to do. Yeah, that'd
higher quality image data to further train the model to just to make, to make the performance better. Right now, we are only using open source data. It's, it's, it's like images from 10 years ago. It's very shitty, but it's already showing very, very promising results on this new architecture, the architecture itself, it will be the, it will be the actual gold that will be shining when we launch it, not the model itself. And, you know, it can always be improved using better data or better quality. So we, we, as of right now, we, we are planning to use that on heavily on embodied AI, because two of the main advantages, fast and small will both apply to robots as well as anything you know that requires very real time. Edge, about edge, deployment. Things could be, could be robot. Robots could be satellite, that might be one of the applications, or anything. There are a lot of things that you Qualcomm's might have in mind, and you might have in mind that'll be very interesting to look at. Yeah. So we're done with, we're done with the all the scientific problem behind the architecture, more like there's a few engineering challenges, including getting, you know, higher quality data images for training. Those are not really, you know, not really difficult. And it to be honest, if we were to release the architecture, it will take at most a few weeks or a month. But right now, we are also trying to match it with our series, a funding we want to, you know, gather all the other investor I'm talking to a lot of investors right now, once they're in, we're probably going to launch it right right after we finish series eight, just to give everyone a very good head start, it will be a very good thing to do. Yeah, that'd
higher quality image data to further train the model to just to make, to make the performance better. Right now, we are only using open source data. It's, it's, it's like images from 10 years ago. It's very shitty, but it's already showing very, very promising results on this new architecture, the architecture itself, it will be the, it will be the actual gold that will be shining when we launch it, not the model itself. And, you know, it can always be improved using better data or better quality. So we, we, as of right now, we, we are planning to use that on heavily on embodied AI, because two of the main advantages, fast and small will both apply to robots as well as anything you know that requires very real time. Edge, about edge, deployment. Things could be, could be robot. Robots could be satellite, that might be one of the applications, or anything. There are a lot of things that you Qualcomm's might have in mind, and you might have in mind that'll be very interesting to look at. Yeah. So we're done with, we're done with the all the scientific problem behind the architecture, more like there's a few engineering challenges, including getting, you know, higher quality data images for training. Those are not really, you know, not really difficult. And it to be honest, if we were to release the architecture, it will take at most a few weeks or a month. But right now, we are also trying to match it with our series, a funding we want to, you know, gather all the other investor I'm talking to a lot of investors right now, once they're in, we're probably going to launch it right right after we finish series eight, just to give everyone a very good head start, it will be a very good thing to do. Yeah, that'd
higher quality image data to further train the model to just to make, to make the performance better. Right now, we are only using open source data. It's, it's, it's like images from 10 years ago. It's very shitty, but it's already showing very, very promising results on this new architecture, the architecture itself, it will be the, it will be the actual gold that will be shining when we launch it, not the model itself. And, you know, it can always be improved using better data or better quality. So we, we, as of right now, we, we are planning to use that on heavily on embodied AI, because two of the main advantages, fast and small will both apply to robots as well as anything you know that requires very real time. Edge, about edge, deployment. Things could be, could be robot. Robots could be satellite, that might be one of the applications, or anything. There are a lot of things that you Qualcomm's might have in mind, and you might have in mind that'll be very interesting to look at. Yeah. So we're done with, we're done with the all the scientific problem behind the architecture, more like there's a few engineering challenges, including getting, you know, higher quality data images for training. Those are not really, you know, not really difficult. And it to be honest, if we were to release the architecture, it will take at most a few weeks or a month. But right now, we are also trying to match it with our series, a funding we want to, you know, gather all the other investor I'm talking to a lot of investors right now, once they're in, we're probably going to launch it right right after we finish series eight, just to give everyone a very good head start, it will be a very good thing to do. Yeah, that'd
S Speaker 137:09be a good, good launch, for sure. Totally agree. And William, have you raised capital so far? Yeah.
be a good, good launch, for sure. Totally agree. And William, have you raised capital so far? Yeah.
be a good, good launch, for sure. Totally agree. And William, have you raised capital so far? Yeah.
be a good, good launch, for sure. Totally agree. And William, have you raised capital so far? Yeah.
S Speaker 237:17So we are done with our seed round. Our seed round was $22 million seed round valued at 200 million. So for our series, a we're looking into raising about 50 million to up to 100 million, with estimation of 500 million to around 1000
So we are done with our seed round. Our seed round was $22 million seed round valued at 200 million. So for our series, a we're looking into raising about 50 million to up to 100 million, with estimation of 500 million to around 1000
So we are done with our seed round. Our seed round was $22 million seed round valued at 200 million. So for our series, a we're looking into raising about 50 million to up to 100 million, with estimation of 500 million to around 1000
So we are done with our seed round. Our seed round was $22 million seed round valued at 200 million. So for our series, a we're looking into raising about 50 million to up to 100 million, with estimation of 500 million to around 1000
S Speaker 137:41Yeah, yeah, yeah. Got it. Got it. William and last question, how does the traction look like right now? I know you mentioned a lot of POCs, has that converted into revenue? Yeah?
Yeah, yeah, yeah. Got it. Got it. William and last question, how does the traction look like right now? I know you mentioned a lot of POCs, has that converted into revenue? Yeah?
Yeah, yeah, yeah. Got it. Got it. William and last question, how does the traction look like right now? I know you mentioned a lot of POCs, has that converted into revenue? Yeah?
Yeah, yeah, yeah. Got it. Got it. William and last question, how does the traction look like right now? I know you mentioned a lot of POCs, has that converted into revenue? Yeah?
S Speaker 237:54So right now we have not, because we're not very we're not we're more like a research oriented company. We do have a product team right now we're generating about about a few 100k in revenue or in mostly from a little bit of from quant trading, a little bit from climate prediction and a little bit from reasoning agents. We also have an agent team that's, you know, because some of the companies, they do want a a full size solution, as opposed to just a model. So we do have an agent team that's building some of the agent tools for for our models. So, but we are the prediction will, projection will go very high. So right now, after we're actually not very active hiring of engineers, and especially model product engineers. They can convert our reasoning model into extra models. For full disclosure, we have a lot of company that's waiting in line to get into collaborations, but we just don't have a lot of engineers that can handle all those revenues, potential revenues. So right now we are we're in a role in hiring. We're hiring. We're at 30 right now. We're probably going to get to 50 by the end of November.
So right now we have not, because we're not very we're not we're more like a research oriented company. We do have a product team right now we're generating about about a few 100k in revenue or in mostly from a little bit of from quant trading, a little bit from climate prediction and a little bit from reasoning agents. We also have an agent team that's, you know, because some of the companies, they do want a a full size solution, as opposed to just a model. So we do have an agent team that's building some of the agent tools for for our models. So, but we are the prediction will, projection will go very high. So right now, after we're actually not very active hiring of engineers, and especially model product engineers. They can convert our reasoning model into extra models. For full disclosure, we have a lot of company that's waiting in line to get into collaborations, but we just don't have a lot of engineers that can handle all those revenues, potential revenues. So right now we are we're in a role in hiring. We're hiring. We're at 30 right now. We're probably going to get to 50 by the end of November.
So right now we have not, because we're not very we're not we're more like a research oriented company. We do have a product team right now we're generating about about a few 100k in revenue or in mostly from a little bit of from quant trading, a little bit from climate prediction and a little bit from reasoning agents. We also have an agent team that's, you know, because some of the companies, they do want a a full size solution, as opposed to just a model. So we do have an agent team that's building some of the agent tools for for our models. So, but we are the prediction will, projection will go very high. So right now, after we're actually not very active hiring of engineers, and especially model product engineers. They can convert our reasoning model into extra models. For full disclosure, we have a lot of company that's waiting in line to get into collaborations, but we just don't have a lot of engineers that can handle all those revenues, potential revenues. So right now we are we're in a role in hiring. We're hiring. We're at 30 right now. We're probably going to get to 50 by the end of November.
So right now we have not, because we're not very we're not we're more like a research oriented company. We do have a product team right now we're generating about about a few 100k in revenue or in mostly from a little bit of from quant trading, a little bit from climate prediction and a little bit from reasoning agents. We also have an agent team that's, you know, because some of the companies, they do want a a full size solution, as opposed to just a model. So we do have an agent team that's building some of the agent tools for for our models. So, but we are the prediction will, projection will go very high. So right now, after we're actually not very active hiring of engineers, and especially model product engineers. They can convert our reasoning model into extra models. For full disclosure, we have a lot of company that's waiting in line to get into collaborations, but we just don't have a lot of engineers that can handle all those revenues, potential revenues. So right now we are we're in a role in hiring. We're hiring. We're at 30 right now. We're probably going to get to 50 by the end of November.
S Speaker 139:19Great problem to have, though, like, if that's you, I'll take that
Great problem to have, though, like, if that's you, I'll take that
Great problem to have, though, like, if that's you, I'll take that
Great problem to have, though, like, if that's you, I'll take that
39:24any day. Yeah, yeah.
S Speaker 239:26And to be honest, when we launched the generative model, I'm sure there will be more companies going and coming after us, because right now, they're all going after the reasoning for now, but generation, generative model will just open up another world for for customers and businesses.
And to be honest, when we launched the generative model, I'm sure there will be more companies going and coming after us, because right now, they're all going after the reasoning for now, but generation, generative model will just open up another world for for customers and businesses.
And to be honest, when we launched the generative model, I'm sure there will be more companies going and coming after us, because right now, they're all going after the reasoning for now, but generation, generative model will just open up another world for for customers and businesses.
And to be honest, when we launched the generative model, I'm sure there will be more companies going and coming after us, because right now, they're all going after the reasoning for now, but generation, generative model will just open up another world for for customers and businesses.
S Speaker 139:43100% William, I totally agree. And it sounds amazing. I am sure there'll be a lot of potential partnership opportunities internally. So some you mentioned, William, that the round will kick off sometime early November. That's when you will have the entire data room and everything put up, right?
100% William, I totally agree. And it sounds amazing. I am sure there'll be a lot of potential partnership opportunities internally. So some you mentioned, William, that the round will kick off sometime early November. That's when you will have the entire data room and everything put up, right?
100% William, I totally agree. And it sounds amazing. I am sure there'll be a lot of potential partnership opportunities internally. So some you mentioned, William, that the round will kick off sometime early November. That's when you will have the entire data room and everything put up, right?
100% William, I totally agree. And it sounds amazing. I am sure there'll be a lot of potential partnership opportunities internally. So some you mentioned, William, that the round will kick off sometime early November. That's when you will have the entire data room and everything put up, right?
S Speaker 240:01Yes, yes. That'll be that'll be the correct we'll have the actual business plan. The deck will be ready probably around early to mid November,
Yes, yes. That'll be that'll be the correct we'll have the actual business plan. The deck will be ready probably around early to mid November,
Yes, yes. That'll be that'll be the correct we'll have the actual business plan. The deck will be ready probably around early to mid November,
Yes, yes. That'll be that'll be the correct we'll have the actual business plan. The deck will be ready probably around early to mid November,
40:13and we can probably catch up again by that time, I'll send you
and we can probably catch up again by that time, I'll send you
and we can probably catch up again by that time, I'll send you
and we can probably catch up again by that time, I'll send you
S Speaker 240:37Yeah, yeah. Of course, I'll send you the PDF version of this. Yeah, but keep in mind, keep in mind some of the data on here are a little bit outdated. Things change really fast, different from months to months.
Yeah, yeah. Of course, I'll send you the PDF version of this. Yeah, but keep in mind, keep in mind some of the data on here are a little bit outdated. Things change really fast, different from months to months.
Yeah, yeah. Of course, I'll send you the PDF version of this. Yeah, but keep in mind, keep in mind some of the data on here are a little bit outdated. Things change really fast, different from months to months.
Yeah, yeah. Of course, I'll send you the PDF version of this. Yeah, but keep in mind, keep in mind some of the data on here are a little bit outdated. Things change really fast, different from months to months.
S Speaker 140:51And last question, do you have, say, an endpoint exposed somewhere that I can go try out the model myself, just prompted with a few
And last question, do you have, say, an endpoint exposed somewhere that I can go try out the model myself, just prompted with a few
And last question, do you have, say, an endpoint exposed somewhere that I can go try out the model myself, just prompted with a few
And last question, do you have, say, an endpoint exposed somewhere that I can go try out the model myself, just prompted with a few
S Speaker 241:00things as of right now, we don't have a live demonstration. We do have the model on GitHub. You can probably, you can, you can download it and, you know, try some of the stuff. We are launching a small demo on on reasoning, especially on Sudoku, the number of puzzle game, yeah, that will be live in a few days. I'll actually send you an email that will be on our website, sapien dot A All right, find it. There will be, you'll be able to, I'll give you a preview. So, so on the website, there will be a Sudoku game UI. You can click on it, you can click on it, you can play with it, and you can click, there's a but there's going to be a button called solve through HRM, yeah. And once you click it, it's actually going to download the model to your laptop, and it's going to run the model on your laptop locally, but it's going to, it's going to show on the website, but it's going to, everything's going to run on your laptop locally. So that'll be a very interesting experience.
things as of right now, we don't have a live demonstration. We do have the model on GitHub. You can probably, you can, you can download it and, you know, try some of the stuff. We are launching a small demo on on reasoning, especially on Sudoku, the number of puzzle game, yeah, that will be live in a few days. I'll actually send you an email that will be on our website, sapien dot A All right, find it. There will be, you'll be able to, I'll give you a preview. So, so on the website, there will be a Sudoku game UI. You can click on it, you can click on it, you can play with it, and you can click, there's a but there's going to be a button called solve through HRM, yeah. And once you click it, it's actually going to download the model to your laptop, and it's going to run the model on your laptop locally, but it's going to, it's going to show on the website, but it's going to, everything's going to run on your laptop locally. So that'll be a very interesting experience.
things as of right now, we don't have a live demonstration. We do have the model on GitHub. You can probably, you can, you can download it and, you know, try some of the stuff. We are launching a small demo on on reasoning, especially on Sudoku, the number of puzzle game, yeah, that will be live in a few days. I'll actually send you an email that will be on our website, sapien dot A All right, find it. There will be, you'll be able to, I'll give you a preview. So, so on the website, there will be a Sudoku game UI. You can click on it, you can click on it, you can play with it, and you can click, there's a but there's going to be a button called solve through HRM, yeah. And once you click it, it's actually going to download the model to your laptop, and it's going to run the model on your laptop locally, but it's going to, it's going to show on the website, but it's going to, everything's going to run on your laptop locally. So that'll be a very interesting experience.
things as of right now, we don't have a live demonstration. We do have the model on GitHub. You can probably, you can, you can download it and, you know, try some of the stuff. We are launching a small demo on on reasoning, especially on Sudoku, the number of puzzle game, yeah, that will be live in a few days. I'll actually send you an email that will be on our website, sapien dot A All right, find it. There will be, you'll be able to, I'll give you a preview. So, so on the website, there will be a Sudoku game UI. You can click on it, you can click on it, you can play with it, and you can click, there's a but there's going to be a button called solve through HRM, yeah. And once you click it, it's actually going to download the model to your laptop, and it's going to run the model on your laptop locally, but it's going to, it's going to show on the website, but it's going to, everything's going to run on your laptop locally. So that'll be a very interesting experience.
42:17Yeah, yeah. That'll be,
Yeah, yeah. That'll be,
Yeah, yeah. That'll be,
Yeah, yeah. That'll be,
S Speaker 242:19actually, that was very interesting proposal. I'll talk to our PR team with that. Yeah, we're gonna launch that in probably in a few days.
actually, that was very interesting proposal. I'll talk to our PR team with that. Yeah, we're gonna launch that in probably in a few days.
actually, that was very interesting proposal. I'll talk to our PR team with that. Yeah, we're gonna launch that in probably in a few days.
actually, that was very interesting proposal. I'll talk to our PR team with that. Yeah, we're gonna launch that in probably in a few days.
S Speaker 142:28Actually, that sounds amazing. Thanks a lot for this discussion. Really enjoyed it. We would love to sort of catch up again sometime early November, when the round is live. I look forward to this one PDF, and then I'll keep you posted on my end as well.
Actually, that sounds amazing. Thanks a lot for this discussion. Really enjoyed it. We would love to sort of catch up again sometime early November, when the round is live. I look forward to this one PDF, and then I'll keep you posted on my end as well.
Actually, that sounds amazing. Thanks a lot for this discussion. Really enjoyed it. We would love to sort of catch up again sometime early November, when the round is live. I look forward to this one PDF, and then I'll keep you posted on my end as well.
Actually, that sounds amazing. Thanks a lot for this discussion. Really enjoyed it. We would love to sort of catch up again sometime early November, when the round is live. I look forward to this one PDF, and then I'll keep you posted on my end as well.
S Speaker 242:42Already sounds good, it's it's a very it's a big pleasure talking to you as well. Thanks.
Already sounds good, it's it's a very it's a big pleasure talking to you as well. Thanks.
Already sounds good, it's it's a very it's a big pleasure talking to you as well. Thanks.
Already sounds good, it's it's a very it's a big pleasure talking to you as well. Thanks.
S Speaker 142:48Enjoyed the conversation. See you again.
Enjoyed the conversation. See you again.
Enjoyed the conversation. See you again.
Enjoyed the conversation. See you again.
42:51All right. I see you. Thank you.
All right. I see you. Thank you.
All right. I see you. Thank you.
All right. I see you. Thank you.